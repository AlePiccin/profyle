{"traceEvents": [{"ph": "M", "pid": 19908, "tid": 19908, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 19908, "tid": 4092835, "name": "thread_name", "args": {"name": "AnyIO worker thread"}}, {"ph": "M", "pid": 19908, "tid": 4092831, "name": "thread_name", "args": {"name": "ThreadPoolExecutor-1_0"}}, {"ph": "M", "pid": 19908, "tid": 4091293, "name": "thread_name", "args": {"name": "Thread-6"}}, {"ph": "M", "pid": 19908, "tid": 4091284, "name": "thread_name", "args": {"name": "AnyIO worker thread"}}, {"ph": "M", "pid": 19908, "tid": 4091280, "name": "thread_name", "args": {"name": "ThreadPoolExecutor-0_0"}}, {"pid": 19908, "tid": 4092831, "ts": 1038626975738.0, "dur": 4.0, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:387)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975748.0, "dur": 0.02, "name": "__aenter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:628)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975840.0, "dur": 2.0, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975832.0, "dur": 22.0, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975859.0, "dur": 1.0, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975858.0, "dur": 3.0, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975868.0, "dur": 1.0, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975863.0, "dur": 6.02, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975873.0, "dur": 1.0, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975871.0, "dur": 3.02, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975890.02, "dur": 0.98, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975890.0, "dur": 2.0, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975886.0, "dur": 6.02, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:471)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975896.02, "dur": 0.98, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975897.02, "dur": 0.98, "name": "re.Match.groupdict", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975898.02, "dur": 0.98, "name": "dict.items", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975900.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975901.0, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975896.0, "dur": 9.0, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975894.0, "dur": 12.0, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:471)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975906.02, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975959.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975954.0, "dur": 5.04, "name": "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976006.0, "dur": 3.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:69)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975998.0, "dur": 13.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:193)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976100.0, "dur": 0.02, "name": "render (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976105.02, "dur": 0.98, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976107.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976110.0, "dur": 1.0, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976111.02, "dur": 0.98, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976105.0, "dur": 9.0, "name": "init_headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:65)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976087.0, "dur": 27.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976117.02, "dur": 0.98, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976150.0, "dur": 1.0, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976141.0, "dur": 10.02, "name": "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976168.0, "dur": 3.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:514)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976117.0, "dur": 54.02, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:98)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976181.0, "dur": 0.02, "name": "str.lower", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976181.04, "dur": 0.96, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976184.0, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976179.0, "dur": 9.0, "name": "__delitem__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:619)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976196.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976195.0, "dur": 1.04, "name": "path_params (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:122)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976213.0, "dur": 0.02, "name": "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py:595)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976223.0, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976235.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976233.0, "dur": 2.04, "name": "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976249.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976250.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976250.04, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976254.0, "dur": 0.02, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976265.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976265.0, "dur": 1.0, "name": "_coerce_args (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py:112)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976267.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976268.0, "dur": 0.02, "name": "str.split", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976271.0, "dur": 0.02, "name": "<listcomp> (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py:739)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976262.0, "dur": 10.0, "name": "parse_qsl (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py:697)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976286.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976288.0, "dur": 2.0, "name": "<dictcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:291)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976286.0, "dur": 6.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:257)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976294.0, "dur": 0.02, "name": "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:420)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976295.0, "dur": 0.02, "name": "dict.items", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976297.0, "dur": 0.02, "name": "<dictcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:421)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976249.0, "dur": 49.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:397)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976222.0, "dur": 77.0, "name": "query_params (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:116)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976301.0, "dur": 0.02, "name": "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py:595)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976306.02, "dur": 0.98, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976312.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976311.0, "dur": 1.04, "name": "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976314.0, "dur": 4.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:514)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976306.0, "dur": 13.0, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:110)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976321.0, "dur": 0.02, "name": "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py:595)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976324.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976326.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976326.0, "dur": 0.06, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:110)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976334.02, "dur": 0.02, "name": "str.lower", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976335.0, "dur": 1.0, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976334.0, "dur": 4.0, "name": "__getitem__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:563)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976328.0, "dur": 12.0, "name": "get (/usr/local/opt/python@3.7/bin/../Frameworks/Python.framework/Versions/3.7/lib/python3.7/_collections_abc.py:657)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976324.0, "dur": 16.02, "name": "cookies (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:126)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976342.0, "dur": 1.0, "name": "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py:595)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976343.02, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976344.0, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976353.0, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976353.04, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976072.0, "dur": 285.0, "name": "solve_dependencies (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py:467)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976393.02, "dur": 0.98, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976399.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976399.04, "dur": 0.96, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976398.0, "dur": 2.02, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976393.0, "dur": 7.04, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976390.0, "dur": 11.0, "name": "get_asynclib (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976411.0, "dur": 0.02, "name": "__sleep0 (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:570)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976409.0, "dur": 2.04, "name": "sleep (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:582)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976407.0, "dur": 4.06, "name": "checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:514)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976405.0, "dur": 6.08, "name": "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:894)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976384.0, "dur": 27.1, "name": "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/to_thread.py:10)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976372.0, "dur": 40.0, "name": "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/concurrency.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976361.0, "dur": 51.02, "name": "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:155)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976036.0, "dur": 376.04, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:190)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975935.0, "dur": 477.06, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975927.0, "dur": 485.08, "name": "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975807.0, "dur": 605.1, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:685)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975733.0, "dur": 679.12, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626975713.0, "dur": 699.14, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976423.0, "dur": 0.02, "name": "_check_closed (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:477)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976430.0, "dur": 0.02, "name": "get_debug (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1804)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976427.0, "dur": 3.04, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:39)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976431.0, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976425.0, "dur": 6.04, "name": "_call_soon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:710)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976421.0, "dur": 11.0, "name": "call_soon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:681)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976437.02, "dur": 0.98, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976441.02, "dur": 0.98, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976443.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976443.04, "dur": 17.96, "name": "select.kqueue.control", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976441.0, "dur": 21.0, "name": "select (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py:553)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976464.0, "dur": 1.0, "name": "_process_events (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py:557)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976467.0, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976466.0, "dur": 1.04, "name": "time (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:634)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976468.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976470.0, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976515.0, "dur": 0.02, "name": "__sleep0 (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:570)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976514.0, "dur": 2.0, "name": "sleep (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:582)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976512.0, "dur": 4.02, "name": "checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:514)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976549.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976554.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976554.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976553.0, "dur": 1.08, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976548.0, "dur": 7.0, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976546.0, "dur": 9.02, "name": "get_asynclib (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976556.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976545.0, "dur": 11.04, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976558.0, "dur": 3.0, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976563.0, "dur": 1.0, "name": "__setitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:408)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976541.0, "dur": 24.0, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976536.0, "dur": 36.0, "name": "get (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:131)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976581.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976583.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976583.06, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976583.0, "dur": 1.0, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976580.0, "dur": 4.02, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976579.0, "dur": 5.04, "name": "get_asynclib (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976585.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976578.0, "dur": 7.04, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976586.0, "dur": 1.0, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976576.0, "dur": 11.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976595.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976599.0, "dur": 1.0, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976598.0, "dur": 2.02, "name": "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976601.0, "dur": 5.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:82)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976575.0, "dur": 31.02, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976614.02, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976616.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976617.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976616.0, "dur": 1.04, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976614.0, "dur": 3.06, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976612.0, "dur": 6.0, "name": "get_asynclib (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976618.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976611.0, "dur": 7.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976619.0, "dur": 4.0, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976610.0, "dur": 13.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976623.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976626.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976625.0, "dur": 1.04, "name": "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976627.0, "dur": 1.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:82)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976608.0, "dur": 20.02, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976641.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976643.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976643.06, "dur": 0.94, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976643.0, "dur": 1.02, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976640.0, "dur": 4.04, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976639.0, "dur": 5.06, "name": "get_asynclib (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976645.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976637.0, "dur": 8.04, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976646.0, "dur": 1.0, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976636.0, "dur": 11.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976635.0, "dur": 14.0, "name": "get (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:131)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976652.02, "dur": 1.98, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976652.0, "dur": 2.02, "name": "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1852)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976659.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976660.0, "dur": 1.0, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976659.0, "dur": 3.0, "name": "total_tokens (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1875)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976655.0, "dur": 7.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1855)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976669.02, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976671.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976672.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976671.0, "dur": 1.04, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976669.0, "dur": 3.06, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976667.0, "dur": 6.0, "name": "get_asynclib (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976673.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976666.0, "dur": 7.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976674.0, "dur": 1.0, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976665.0, "dur": 10.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976675.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976678.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976677.0, "dur": 1.04, "name": "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976679.0, "dur": 1.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:82)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976664.0, "dur": 16.02, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976633.0, "dur": 47.04, "name": "current_default_thread_limiter (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1973)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976686.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976686.06, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976686.0, "dur": 0.1, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976693.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976693.06, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976693.0, "dur": 0.1, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976695.0, "dur": 0.02, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976698.0, "dur": 0.02, "name": "cancel_called (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:498)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976699.0, "dur": 1.0, "name": "shield (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:502)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976701.0, "dur": 1.0, "name": "cancel_called (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:498)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976703.0, "dur": 0.02, "name": "shield (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:502)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976692.0, "dur": 11.04, "name": "checkpoint_if_cancelled (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:518)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976710.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976711.0, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976713.0, "dur": 2.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_compat.py:126)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976709.0, "dur": 6.02, "name": "acquire_on_behalf_of_nowait (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1908)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976719.02, "dur": 0.98, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976719.0, "dur": 1.02, "name": "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:310)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976721.0, "dur": 3.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:315)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976728.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976728.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976727.0, "dur": 1.08, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976730.0, "dur": 0.02, "name": "cast (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:898)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976730.04, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976732.0, "dur": 0.02, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976734.0, "dur": 1.0, "name": "_timeout (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:401)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976726.0, "dur": 9.02, "name": "__enter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:327)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976739.0, "dur": 0.02, "name": "__sleep0 (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:570)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976737.0, "dur": 2.04, "name": "sleep (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:582)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976717.0, "dur": 22.06, "name": "cancel_shielded_checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:537)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976689.0, "dur": 50.08, "name": "acquire_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1924)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976684.0, "dur": 55.1, "name": "acquire (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1921)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976682.0, "dur": 57.12, "name": "__aenter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1860)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976511.0, "dur": 228.14, "name": "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:894)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976509.0, "dur": 230.16, "name": "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/to_thread.py:10)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976508.0, "dur": 231.18, "name": "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/concurrency.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976507.0, "dur": 232.2, "name": "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:155)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976505.0, "dur": 235.0, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:190)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976503.0, "dur": 237.02, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976502.0, "dur": 238.04, "name": "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976500.0, "dur": 240.06, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:685)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976498.0, "dur": 242.08, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976497.0, "dur": 243.1, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976491.0, "dur": 252.0, "name": "__call__ (/Users/shopbox/projects/profyle/profyle/middleware/fastapi.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976483.0, "dur": 260.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/errors.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976481.0, "dur": 262.04, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/applications.py:114)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976479.0, "dur": 264.06, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/applications.py:268)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976477.0, "dur": 266.08, "name": "_call_func (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/from_thread.py:200)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976475.0, "dur": 268.1, "name": "_run_wrapped_task (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:694)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976748.0, "dur": 0.02, "name": "_check_closed (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:477)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976753.0, "dur": 1.0, "name": "get_debug (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1804)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976751.0, "dur": 3.02, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:39)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976754.04, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976749.0, "dur": 6.0, "name": "_call_soon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:710)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976746.0, "dur": 9.02, "name": "call_soon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:681)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976473.0, "dur": 282.04, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976472.0, "dur": 284.0, "name": "_run (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:86)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976437.0, "dur": 320.0, "name": "_run_once (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1690)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976761.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976764.0, "dur": 0.02, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976764.04, "dur": 0.96, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976765.02, "dur": 13.98, "name": "select.kqueue.control", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976763.0, "dur": 17.0, "name": "select (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py:553)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976782.0, "dur": 0.02, "name": "_process_events (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py:557)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976784.02, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976784.0, "dur": 0.06, "name": "time (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:634)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976785.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976786.0, "dur": 1.0, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976826.0, "dur": 0.02, "name": "__sleep0 (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:570)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976825.0, "dur": 2.0, "name": "sleep (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:582)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976832.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976832.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976831.0, "dur": 1.08, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976836.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976835.0, "dur": 1.04, "name": "get (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:432)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976837.0, "dur": 1.0, "name": "set.remove", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976840.0, "dur": 2.0, "name": "_deliver_cancellation_to_parent (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:445)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976830.0, "dur": 12.02, "name": "__exit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:349)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976824.0, "dur": 19.0, "name": "cancel_shielded_checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:537)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976822.0, "dur": 21.02, "name": "acquire_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1924)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976821.0, "dur": 23.0, "name": "acquire (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1921)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976820.0, "dur": 24.02, "name": "__aenter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1860)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976847.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976846.0, "dur": 1.04, "name": "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:310)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976848.0, "dur": 2.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:315)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976854.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976854.06, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976854.0, "dur": 0.1, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976856.0, "dur": 0.02, "name": "cast (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:898)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976856.04, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976858.0, "dur": 0.02, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976860.0, "dur": 1.0, "name": "_timeout (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:401)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976852.0, "dur": 9.02, "name": "__enter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:327)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976863.0, "dur": 0.02, "name": "get_debug (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1804)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976874.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976876.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976876.06, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976876.0, "dur": 1.0, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976873.0, "dur": 4.02, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976872.0, "dur": 5.04, "name": "get_asynclib (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976877.06, "dur": 0.94, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976870.0, "dur": 8.02, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976879.0, "dur": 1.0, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976869.0, "dur": 11.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976868.0, "dur": 14.0, "name": "get (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:131)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976885.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976889.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976889.04, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976888.0, "dur": 2.0, "name": "__len__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:67)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976893.0, "dur": 0.02, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:16)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976895.02, "dur": 3.98, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976895.0, "dur": 4.02, "name": "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:20)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976891.0, "dur": 9.0, "name": "__iter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976902.0, "dur": 0.02, "name": "__iter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976903.0, "dur": 1.0, "name": "__iter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976907.0, "dur": 0.02, "name": "set.remove", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976909.0, "dur": 0.02, "name": "_commit_removals (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:52)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976906.0, "dur": 4.0, "name": "__exit__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:26)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976905.0, "dur": 5.02, "name": "__iter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976915.0, "dur": 0.02, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976914.0, "dur": 1.04, "name": "_get_loop (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py:275)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976916.0, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976918.02, "dur": 0.02, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976918.0, "dur": 0.06, "name": "_get_loop (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py:275)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976919.02, "dur": 0.02, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976919.0, "dur": 0.06, "name": "_get_loop (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py:275)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976920.0, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976912.0, "dur": 8.04, "name": "<setcomp> (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976885.0, "dur": 35.06, "name": "all_tasks (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:34)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976922.0, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976925.0, "dur": 1.0, "name": "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:89)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976924.0, "dur": 2.02, "name": "_get_task_callbacks (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:88)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976933.02, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976935.02, "dur": 0.98, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976936.02, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976935.0, "dur": 1.06, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976933.0, "dur": 3.08, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976931.0, "dur": 6.0, "name": "get_asynclib (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976937.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976930.0, "dur": 7.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976938.0, "dur": 1.0, "name": "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976929.0, "dur": 10.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976939.04, "dur": 0.96, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976942.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976941.0, "dur": 1.04, "name": "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976944.0, "dur": 0.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:82)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976928.0, "dur": 16.04, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976866.0, "dur": 79.0, "name": "find_root_task (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:187)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976958.0, "dur": 0.02, "name": "_thread.get_ident", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976957.0, "dur": 1.04, "name": "current_thread (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:1225)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976960.0, "dur": 0.02, "name": "daemon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:1116)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976967.0, "dur": 1.0, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976971.0, "dur": 5.0, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:216)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976963.0, "dur": 14.0, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:499)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976980.02, "dur": 0.98, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976980.0, "dur": 1.02, "name": "add (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:81)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976953.0, "dur": 28.04, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:763)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976989.0, "dur": 3.0, "name": "_init (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:205)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976992.02, "dur": 0.98, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976995.0, "dur": 3.0, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:216)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977002.0, "dur": 4.0, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:216)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977007.0, "dur": 4.0, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:216)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976985.0, "dur": 26.02, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:33)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977013.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977015.02, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977015.0, "dur": 0.06, "name": "time (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:634)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977013.0, "dur": 2.08, "name": "current_time (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:559)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976950.0, "dur": 68.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:823)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977022.0, "dur": 1.0, "name": "is_set (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:507)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977029.0, "dur": 0.02, "name": "__init__ (/Users/shopbox/.vscode/extensions/ms-python.python-2023.4.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:1084)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977029.04, "dur": 54.96, "name": "_thread.start_new_thread", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977027.0, "dur": 57.02, "name": "pydev_start_new_thread (/Users/shopbox/.vscode/extensions/ms-python.python-2023.4.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:1174)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977090.0, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977089.0, "dur": 1.04, "name": "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:240)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977097.0, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977096.0, "dur": 1.04, "name": "_is_owned (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:255)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977097.06, "dur": 0.94, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977098.02, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977098.06, "dur": 0.94, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977100.02, "dur": 0.98, "name": "_thread.lock.release", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977100.0, "dur": 1.02, "name": "_release_save (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:249)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977101.04, "dur": 706.96, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977817.0, "dur": 1.0, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977816.0, "dur": 2.02, "name": "_acquire_restore (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:252)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977095.0, "dur": 723.04, "name": "wait (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:264)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977822.02, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977822.0, "dur": 0.06, "name": "__exit__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:243)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977088.0, "dur": 735.0, "name": "wait (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:534)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977021.0, "dur": 802.02, "name": "start (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:834)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977824.0, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977825.0, "dur": 2.0, "name": "_asyncio.Task.add_done_callback", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977827.02, "dur": 0.02, "name": "_contextvars.copy_context", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977829.0, "dur": 1.0, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977838.02, "dur": 0.98, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977838.0, "dur": 1.02, "name": "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:240)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977841.02, "dur": 0.98, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977841.0, "dur": 1.02, "name": "_qsize (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:208)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977844.02, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977844.0, "dur": 0.06, "name": "_put (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:212)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977848.02, "dur": 0.98, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977848.0, "dur": 1.02, "name": "_is_owned (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:255)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977847.0, "dur": 4.0, "name": "notify (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:335)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977853.0, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977852.0, "dur": 1.04, "name": "__exit__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:243)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977836.0, "dur": 17.06, "name": "put (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977834.0, "dur": 19.08, "name": "put_nowait (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:184)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976818.0, "dur": 1040.0, "name": "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:894)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976817.0, "dur": 1042.0, "name": "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/to_thread.py:10)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976815.0, "dur": 1044.02, "name": "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/concurrency.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976814.0, "dur": 1045.04, "name": "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:155)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976813.0, "dur": 1047.0, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:190)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976811.0, "dur": 1049.02, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976809.0, "dur": 1051.04, "name": "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976808.0, "dur": 1053.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:685)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976806.0, "dur": 1055.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976805.0, "dur": 1056.04, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976803.0, "dur": 1066.0, "name": "__call__ (/Users/shopbox/projects/profyle/profyle/middleware/fastapi.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976797.0, "dur": 1072.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/errors.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976795.0, "dur": 1075.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/applications.py:114)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976794.0, "dur": 1076.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/applications.py:268)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976793.0, "dur": 1077.04, "name": "_call_func (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/from_thread.py:200)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976791.0, "dur": 1080.0, "name": "_run_wrapped_task (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:694)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976789.02, "dur": 1082.98, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976789.0, "dur": 1084.0, "name": "_run (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:86)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626976760.0, "dur": 1114.0, "name": "_run_once (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1690)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977878.02, "dur": 0.98, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977883.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978039.0, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978036.0, "dur": 4.0, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:81)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978033.0, "dur": 7.02, "name": "helper (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:237)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978045.0, "dur": 3.0, "name": "claim_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:137)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978043.0, "dur": 5.02, "name": "builtins.next", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978042.0, "dur": 6.04, "name": "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978054.0, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978053.0, "dur": 1.04, "name": "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:240)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978056.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978056.0, "dur": 0.06, "name": "_qsize (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:208)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978058.02, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978058.0, "dur": 0.06, "name": "_get (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:216)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978062.0, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978061.0, "dur": 1.04, "name": "_is_owned (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:255)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978060.0, "dur": 3.0, "name": "notify (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:335)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978065.02, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978065.0, "dur": 0.06, "name": "__exit__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:243)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978052.0, "dur": 14.0, "name": "get (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:153)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978067.0, "dur": 0.02, "name": "_asyncio.Future.cancelled", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978164.0, "dur": 505049.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038627483226.0, "dur": 3.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038627483234.0, "dur": 505824.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038627989087.0, "dur": 4.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038627989105.0, "dur": 505233.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038628494353.0, "dur": 2.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038628494361.0, "dur": 503581.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038628997958.0, "dur": 3.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038628997969.0, "dur": 505064.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038629503046.0, "dur": 8.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038629503062.0, "dur": 505019.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038630008083.0, "dur": 1.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038630008085.0, "dur": 506255.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038630514352.0, "dur": 8.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038630514365.0, "dur": 503315.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038631017688.0, "dur": 1.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038631017694.0, "dur": 503291.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038631520991.0, "dur": 4.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038631520998.0, "dur": 500240.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038632021256.0, "dur": 3.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978162.0, "dur": 5043106.0, "name": "tarda (/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py:11)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038632021373.0, "dur": 503987.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038632525376.0, "dur": 5.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038632525390.0, "dur": 505047.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038633030447.0, "dur": 2.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038633030454.0, "dur": 504109.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038633534577.0, "dur": 3.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038633534590.0, "dur": 505087.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038634039704.0, "dur": 5.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038634039722.0, "dur": 501828.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038634541579.0, "dur": 16.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038634541612.0, "dur": 502051.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038635043679.0, "dur": 3.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038635043691.0, "dur": 527899.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038635571604.0, "dur": 1.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038635571610.0, "dur": 505121.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038636076762.0, "dur": 9.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038636076782.0, "dur": 501295.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038636578093.0, "dur": 10.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038636578149.0, "dur": 505030.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038637083182.0, "dur": 1.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038632021361.0, "dur": 5061823.0, "name": "tarda (/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py:11)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038637083209.0, "dur": 503208.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038637586447.0, "dur": 8.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038637586473.0, "dur": 503514.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038638090014.0, "dur": 3.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038638090028.0, "dur": 502282.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038638592324.0, "dur": 1.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038638592333.0, "dur": 515749.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038639108109.0, "dur": 6.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038639108130.0, "dur": 505902.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038639614056.0, "dur": 13.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038639614081.0, "dur": 505028.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038640119131.0, "dur": 4.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038640119145.0, "dur": 505060.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038640624214.0, "dur": 3.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038640624223.0, "dur": 505072.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038641129316.0, "dur": 6.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038641129333.0, "dur": 505354.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038641634697.0, "dur": 7.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038641634711.0, "dur": 501102.0, "name": "time.sleep", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135820.0, "dur": 1.0, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038637083205.0, "dur": 5052619.0, "name": "tarda (/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py:11)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978122.0, "dur": 15157709.0, "name": "run_middleware_1 (/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py:22)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038626978068.0, "dur": 15157764.0, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135859.0, "dur": 0.02, "name": "is_closed (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:619)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135943.0, "dur": 1.0, "name": "_check_closed (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:477)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135958.0, "dur": 1.0, "name": "_contextvars.copy_context", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135964.0, "dur": 1.0, "name": "get_debug (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1804)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135956.0, "dur": 9.02, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:39)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135967.0, "dur": 1.0, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135949.0, "dur": 19.02, "name": "_call_soon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:710)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135974.0, "dur": 19.0, "name": "socket.send", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135971.0, "dur": 22.02, "name": "_write_to_self (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py:128)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135866.0, "dur": 128.0, "name": "call_soon_threadsafe (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:734)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136005.0, "dur": 3.0, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136004.0, "dur": 4.02, "name": "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:240)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136013.0, "dur": 1.0, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136018.02, "dur": 0.98, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136018.0, "dur": 2.0, "name": "_is_owned (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:255)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136016.0, "dur": 9.0, "name": "notify (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:335)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136012.0, "dur": 13.02, "name": "notify_all (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:358)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136029.0, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136028.0, "dur": 1.04, "name": "__exit__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:243)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642135999.0, "dur": 31.0, "name": "task_done (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:56)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136036.02, "dur": 0.98, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136036.0, "dur": 1.02, "name": "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:240)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136040.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136039.0, "dur": 1.04, "name": "_qsize (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:208)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136048.02, "dur": 0.98, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136048.0, "dur": 1.02, "name": "_is_owned (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:255)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136049.04, "dur": 2.96, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136052.02, "dur": 0.98, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136053.02, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136055.02, "dur": 0.02, "name": "_thread.lock.release", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092835, "ts": 1038642136055.0, "dur": 1.0, "name": "_release_save (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:249)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977883.04, "dur": 15158184.96, "name": "select.kqueue.control", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136082.0, "dur": 1.0, "name": "_key_from_fd (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py:275)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136086.0, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977882.0, "dur": 15158205.0, "name": "select (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py:553)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136096.0, "dur": 1.0, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136097.02, "dur": 0.98, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136099.0, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136095.0, "dur": 4.04, "name": "_add_callback (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1672)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136091.0, "dur": 9.0, "name": "_process_events (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py:557)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136103.0, "dur": 1.0, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136102.0, "dur": 2.02, "name": "time (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:634)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136110.0, "dur": 1.0, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136114.0, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136125.0, "dur": 1.0, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136128.02, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136128.0, "dur": 0.06, "name": "time (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:634)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136124.0, "dur": 4.08, "name": "current_time (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:559)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136129.0, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136130.0, "dur": 1.0, "name": "_asyncio.Future.cancelled", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136208.0, "dur": 0.02, "name": "_check_closed (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:477)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136220.0, "dur": 1.0, "name": "get_debug (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1804)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136217.0, "dur": 4.02, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:39)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136222.0, "dur": 1.0, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136211.0, "dur": 12.02, "name": "_call_soon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:710)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136203.0, "dur": 20.04, "name": "call_soon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:681)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136131.02, "dur": 92.98, "name": "_asyncio.Future.set_result", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136122.0, "dur": 103.0, "name": "_report_result (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:840)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136119.0, "dur": 106.02, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136117.0, "dur": 109.0, "name": "_run (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:86)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136227.0, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136238.0, "dur": 20.0, "name": "socket.recv", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136262.0, "dur": 2.0, "name": "_process_self_data (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/unix_events.py:68)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136264.02, "dur": 11.98, "name": "socket.recv", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136237.0, "dur": 44.0, "name": "_read_from_self (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py:116)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136234.0, "dur": 47.02, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136232.0, "dur": 49.04, "name": "_run (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:86)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038626977878.0, "dur": 15158404.0, "name": "_run_once (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1690)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136289.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136295.02, "dur": 1.98, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136298.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136299.0, "dur": 18.0, "name": "select.kqueue.control", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136295.0, "dur": 23.0, "name": "select (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py:553)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136321.0, "dur": 1.0, "name": "_process_events (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py:557)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136325.0, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136324.0, "dur": 1.04, "name": "time (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:634)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136326.0, "dur": 1.0, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136330.0, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136420.02, "dur": 0.98, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136422.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136420.0, "dur": 2.04, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136429.0, "dur": 1.0, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136427.0, "dur": 4.0, "name": "get (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:432)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136433.0, "dur": 2.0, "name": "set.remove", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136438.0, "dur": 2.0, "name": "_deliver_cancellation_to_parent (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:445)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136415.0, "dur": 25.02, "name": "__exit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:349)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136449.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136450.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136449.0, "dur": 1.04, "name": "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136452.0, "dur": 0.02, "name": "set.remove", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136451.0, "dur": 2.0, "name": "release_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1948)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136447.0, "dur": 6.02, "name": "release (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1945)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136445.0, "dur": 8.04, "name": "__aexit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1863)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136408.0, "dur": 46.0, "name": "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:894)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136405.0, "dur": 50.0, "name": "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/to_thread.py:10)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136401.0, "dur": 54.02, "name": "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/concurrency.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136394.0, "dur": 62.0, "name": "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:155)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136457.0, "dur": 1.0, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136525.0, "dur": 62.0, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136628.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136629.0, "dur": 2.0, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136627.0, "dur": 4.02, "name": "is_dataclass (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/dataclasses.py:1036)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136632.0, "dur": 2.0, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136640.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136642.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136642.04, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136643.0, "dur": 1.0, "name": "dict.keys", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136674.0, "dur": 1.0, "name": "dict.items", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136677.0, "dur": 1.0, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136678.02, "dur": 2.98, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136696.0, "dur": 7.0, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136707.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136708.0, "dur": 2.0, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136707.0, "dur": 3.02, "name": "is_dataclass (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/dataclasses.py:1036)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136711.0, "dur": 1.0, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136712.02, "dur": 0.98, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136713.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136695.0, "dur": 19.0, "name": "jsonable_encoder (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/encoders.py:29)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136718.0, "dur": 1.0, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136720.02, "dur": 0.98, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136721.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136720.0, "dur": 2.0, "name": "is_dataclass (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/dataclasses.py:1036)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136722.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136722.06, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136723.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136717.0, "dur": 6.04, "name": "jsonable_encoder (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/encoders.py:29)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136515.0, "dur": 210.0, "name": "jsonable_encoder (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/encoders.py:29)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136487.0, "dur": 239.0, "name": "serialize_response (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:110)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136786.0, "dur": 6.0, "name": "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py:104)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136795.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136798.0, "dur": 13.0, "name": "iterencode (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py:204)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136812.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136812.04, "dur": 0.96, "name": "str.join", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136795.0, "dur": 18.02, "name": "encode (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py:182)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136768.0, "dur": 46.0, "name": "dumps (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py:183)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136815.0, "dur": 2.0, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136764.0, "dur": 53.02, "name": "render (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:198)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136822.0, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136823.0, "dur": 1.0, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136826.0, "dur": 0.02, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136827.0, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136828.0, "dur": 0.02, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136832.0, "dur": 0.02, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136832.04, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136821.0, "dur": 12.0, "name": "init_headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:65)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136760.0, "dur": 73.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136755.0, "dur": 78.04, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:188)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136840.0, "dur": 11.0, "name": "is_body_allowed_for_status_code (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/utils.py:21)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136854.0, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136863.0, "dur": 1.0, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136861.0, "dur": 4.0, "name": "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136868.0, "dur": 1.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:514)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136853.0, "dur": 17.0, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:98)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136874.0, "dur": 1.0, "name": "raw (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:646)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136877.0, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136876.0, "dur": 1.04, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:98)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136878.0, "dur": 1.0, "name": "raw (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:646)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136879.02, "dur": 0.02, "name": "list.extend", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136392.0, "dur": 487.06, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:190)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136894.0, "dur": 1.0, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136897.02, "dur": 0.98, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136898.02, "dur": 0.02, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136899.0, "dur": 0.02, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136899.04, "dur": 0.02, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136897.0, "dur": 3.0, "name": "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/testclient.py:309)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136893.0, "dur": 7.02, "name": "send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/testclient.py:300)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136890.0, "dur": 11.0, "name": "_send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/errors.py:154)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136887.0, "dur": 14.02, "name": "sender (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136915.0, "dur": 1.0, "name": "is_set (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/locks.py:258)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136910.0, "dur": 6.02, "name": "is_set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1838)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136916.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136916.08, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136922.0, "dur": 1.0, "name": "_io.BytesIO.write", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136923.02, "dur": 0.98, "name": "_io.BytesIO.seek", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136928.0, "dur": 1.0, "name": "set (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/locks.py:262)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136932.0, "dur": 3.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_compat.py:126)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136926.0, "dur": 9.02, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1834)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136907.0, "dur": 29.0, "name": "send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/testclient.py:300)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136905.0, "dur": 31.02, "name": "_send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/errors.py:154)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136904.0, "dur": 33.0, "name": "sender (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136885.0, "dur": 52.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:163)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136388.0, "dur": 550.0, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136385.0, "dur": 556.0, "name": "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136383.0, "dur": 559.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:685)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136950.0, "dur": 1.0, "name": "sys.exc_info", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136948.0, "dur": 6.0, "name": "__aexit__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:631)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136380.0, "dur": 575.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 19908, "tid": 4092831, "ts": 1038642136377.0, "dur": 579.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py:53)", "ph": "X", "cat": "FEE"}], "viztracer_metadata": {"version": "0.15.6", "overflow": false}, "file_info": {"files": {"/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py": ["\"\"\"Utilities for with-statement contexts.  See PEP 343.\"\"\"\nimport abc\nimport sys\nimport _collections_abc\nfrom collections import deque\nfrom functools import wraps\n\n__all__ = [\"asynccontextmanager\", \"contextmanager\", \"closing\", \"nullcontext\",\n           \"AbstractContextManager\", \"AbstractAsyncContextManager\",\n           \"AsyncExitStack\", \"ContextDecorator\", \"ExitStack\",\n           \"redirect_stdout\", \"redirect_stderr\", \"suppress\"]\n\n\nclass AbstractContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for context managers.\"\"\"\n\n    def __enter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractContextManager:\n            return _collections_abc._check_methods(C, \"__enter__\", \"__exit__\")\n        return NotImplemented\n\n\nclass AbstractAsyncContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for asynchronous context managers.\"\"\"\n\n    async def __aenter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractAsyncContextManager:\n            return _collections_abc._check_methods(C, \"__aenter__\",\n                                                   \"__aexit__\")\n        return NotImplemented\n\n\nclass ContextDecorator(object):\n    \"A base class or mixin that enables context managers to work as decorators.\"\n\n    def _recreate_cm(self):\n        \"\"\"Return a recreated instance of self.\n\n        Allows an otherwise one-shot context manager like\n        _GeneratorContextManager to support use as\n        a decorator via implicit recreation.\n\n        This is a private interface just for _GeneratorContextManager.\n        See issue #11647 for details.\n        \"\"\"\n        return self\n\n    def __call__(self, func):\n        @wraps(func)\n        def inner(*args, **kwds):\n            with self._recreate_cm():\n                return func(*args, **kwds)\n        return inner\n\n\nclass _GeneratorContextManagerBase:\n    \"\"\"Shared functionality for @contextmanager and @asynccontextmanager.\"\"\"\n\n    def __init__(self, func, args, kwds):\n        self.gen = func(*args, **kwds)\n        self.func, self.args, self.kwds = func, args, kwds\n        # Issue 19330: ensure context manager instances have good docstrings\n        doc = getattr(func, \"__doc__\", None)\n        if doc is None:\n            doc = type(self).__doc__\n        self.__doc__ = doc\n        # Unfortunately, this still doesn't provide good help output when\n        # inspecting the created context manager instances, since pydoc\n        # currently bypasses the instance docstring and shows the docstring\n        # for the class instead.\n        # See http://bugs.python.org/issue19404 for more details.\n\n\nclass _GeneratorContextManager(_GeneratorContextManagerBase,\n                               AbstractContextManager,\n                               ContextDecorator):\n    \"\"\"Helper for @contextmanager decorator.\"\"\"\n\n    def _recreate_cm(self):\n        # _GCM instances are one-shot context managers, so the\n        # CM must be recreated each time a decorated function is\n        # called\n        return self.__class__(self.func, self.args, self.kwds)\n\n    def __enter__(self):\n        # do not keep args and kwds alive unnecessarily\n        # they are only needed for recreation, which is not possible anymore\n        del self.args, self.kwds, self.func\n        try:\n            return next(self.gen)\n        except StopIteration:\n            raise RuntimeError(\"generator didn't yield\") from None\n\n    def __exit__(self, type, value, traceback):\n        if type is None:\n            try:\n                next(self.gen)\n            except StopIteration:\n                return False\n            else:\n                raise RuntimeError(\"generator didn't stop\")\n        else:\n            if value is None:\n                # Need to force instantiation so we can reliably\n                # tell if we get the same exception back\n                value = type()\n            try:\n                self.gen.throw(type, value, traceback)\n            except StopIteration as exc:\n                # Suppress StopIteration *unless* it's the same exception that\n                # was passed to throw().  This prevents a StopIteration\n                # raised inside the \"with\" statement from being suppressed.\n                return exc is not value\n            except RuntimeError as exc:\n                # Don't re-raise the passed in exception. (issue27122)\n                if exc is value:\n                    return False\n                # Likewise, avoid suppressing if a StopIteration exception\n                # was passed to throw() and later wrapped into a RuntimeError\n                # (see PEP 479).\n                if type is StopIteration and exc.__cause__ is value:\n                    return False\n                raise\n            except:\n                # only re-raise if it's *not* the exception that was\n                # passed to throw(), because __exit__() must not raise\n                # an exception unless __exit__() itself failed.  But throw()\n                # has to raise the exception to signal propagation, so this\n                # fixes the impedance mismatch between the throw() protocol\n                # and the __exit__() protocol.\n                #\n                # This cannot use 'except BaseException as exc' (as in the\n                # async implementation) to maintain compatibility with\n                # Python 2, where old-style class exceptions are not caught\n                # by 'except BaseException'.\n                if sys.exc_info()[1] is value:\n                    return False\n                raise\n            raise RuntimeError(\"generator didn't stop after throw()\")\n\n\nclass _AsyncGeneratorContextManager(_GeneratorContextManagerBase,\n                                    AbstractAsyncContextManager):\n    \"\"\"Helper for @asynccontextmanager.\"\"\"\n\n    async def __aenter__(self):\n        try:\n            return await self.gen.__anext__()\n        except StopAsyncIteration:\n            raise RuntimeError(\"generator didn't yield\") from None\n\n    async def __aexit__(self, typ, value, traceback):\n        if typ is None:\n            try:\n                await self.gen.__anext__()\n            except StopAsyncIteration:\n                return\n            else:\n                raise RuntimeError(\"generator didn't stop\")\n        else:\n            if value is None:\n                value = typ()\n            # See _GeneratorContextManager.__exit__ for comments on subtleties\n            # in this implementation\n            try:\n                await self.gen.athrow(typ, value, traceback)\n                raise RuntimeError(\"generator didn't stop after athrow()\")\n            except StopAsyncIteration as exc:\n                return exc is not value\n            except RuntimeError as exc:\n                if exc is value:\n                    return False\n                # Avoid suppressing if a StopIteration exception\n                # was passed to throw() and later wrapped into a RuntimeError\n                # (see PEP 479 for sync generators; async generators also\n                # have this behavior). But do this only if the exception wrapped\n                # by the RuntimeError is actully Stop(Async)Iteration (see\n                # issue29692).\n                if isinstance(value, (StopIteration, StopAsyncIteration)):\n                    if exc.__cause__ is value:\n                        return False\n                raise\n            except BaseException as exc:\n                if exc is not value:\n                    raise\n\n\ndef contextmanager(func):\n    \"\"\"@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    \"\"\"\n    @wraps(func)\n    def helper(*args, **kwds):\n        return _GeneratorContextManager(func, args, kwds)\n    return helper\n\n\ndef asynccontextmanager(func):\n    \"\"\"@asynccontextmanager decorator.\n\n    Typical usage:\n\n        @asynccontextmanager\n        async def some_async_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        async with some_async_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    \"\"\"\n    @wraps(func)\n    def helper(*args, **kwds):\n        return _AsyncGeneratorContextManager(func, args, kwds)\n    return helper\n\n\nclass closing(AbstractContextManager):\n    \"\"\"Context to automatically close something at the end of a block.\n\n    Code like this:\n\n        with closing(<module>.open(<arguments>)) as f:\n            <block>\n\n    is equivalent to this:\n\n        f = <module>.open(<arguments>)\n        try:\n            <block>\n        finally:\n            f.close()\n\n    \"\"\"\n    def __init__(self, thing):\n        self.thing = thing\n    def __enter__(self):\n        return self.thing\n    def __exit__(self, *exc_info):\n        self.thing.close()\n\n\nclass _RedirectStream(AbstractContextManager):\n\n    _stream = None\n\n    def __init__(self, new_target):\n        self._new_target = new_target\n        # We use a list of old targets to make this CM re-entrant\n        self._old_targets = []\n\n    def __enter__(self):\n        self._old_targets.append(getattr(sys, self._stream))\n        setattr(sys, self._stream, self._new_target)\n        return self._new_target\n\n    def __exit__(self, exctype, excinst, exctb):\n        setattr(sys, self._stream, self._old_targets.pop())\n\n\nclass redirect_stdout(_RedirectStream):\n    \"\"\"Context manager for temporarily redirecting stdout to another file.\n\n        # How to send help() to stderr\n        with redirect_stdout(sys.stderr):\n            help(dir)\n\n        # How to write help() to a file\n        with open('help.txt', 'w') as f:\n            with redirect_stdout(f):\n                help(pow)\n    \"\"\"\n\n    _stream = \"stdout\"\n\n\nclass redirect_stderr(_RedirectStream):\n    \"\"\"Context manager for temporarily redirecting stderr to another file.\"\"\"\n\n    _stream = \"stderr\"\n\n\nclass suppress(AbstractContextManager):\n    \"\"\"Context manager to suppress specified exceptions\n\n    After the exception is suppressed, execution proceeds with the next\n    statement following the with statement.\n\n         with suppress(FileNotFoundError):\n             os.remove(somefile)\n         # Execution still resumes here if the file was already removed\n    \"\"\"\n\n    def __init__(self, *exceptions):\n        self._exceptions = exceptions\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exctype, excinst, exctb):\n        # Unlike isinstance and issubclass, CPython exception handling\n        # currently only looks at the concrete type hierarchy (ignoring\n        # the instance and subclass checking hooks). While Guido considers\n        # that a bug rather than a feature, it's a fairly hard one to fix\n        # due to various internal implementation details. suppress provides\n        # the simpler issubclass based semantics, rather than trying to\n        # exactly reproduce the limitations of the CPython interpreter.\n        #\n        # See http://bugs.python.org/issue12029 for more details\n        return exctype is not None and issubclass(exctype, self._exceptions)\n\n\nclass _BaseExitStack:\n    \"\"\"A base class for ExitStack and AsyncExitStack.\"\"\"\n\n    @staticmethod\n    def _create_exit_wrapper(cm, cm_exit):\n        def _exit_wrapper(exc_type, exc, tb):\n            return cm_exit(cm, exc_type, exc, tb)\n        return _exit_wrapper\n\n    @staticmethod\n    def _create_cb_wrapper(*args, **kwds):\n        callback, *args = args\n        def _exit_wrapper(exc_type, exc, tb):\n            callback(*args, **kwds)\n        return _exit_wrapper\n\n    def __init__(self):\n        self._exit_callbacks = deque()\n\n    def pop_all(self):\n        \"\"\"Preserve the context stack by transferring it to a new instance.\"\"\"\n        new_stack = type(self)()\n        new_stack._exit_callbacks = self._exit_callbacks\n        self._exit_callbacks = deque()\n        return new_stack\n\n    def push(self, exit):\n        \"\"\"Registers a callback with the standard __exit__ method signature.\n\n        Can suppress exceptions the same way __exit__ method can.\n        Also accepts any object with an __exit__ method (registering a call\n        to the method instead of the object itself).\n        \"\"\"\n        # We use an unbound method rather than a bound method to follow\n        # the standard lookup behaviour for special methods.\n        _cb_type = type(exit)\n\n        try:\n            exit_method = _cb_type.__exit__\n        except AttributeError:\n            # Not a context manager, so assume it's a callable.\n            self._push_exit_callback(exit)\n        else:\n            self._push_cm_exit(exit, exit_method)\n        return exit  # Allow use as a decorator.\n\n    def enter_context(self, cm):\n        \"\"\"Enters the supplied context manager.\n\n        If successful, also pushes its __exit__ method as a callback and\n        returns the result of the __enter__ method.\n        \"\"\"\n        # We look up the special methods on the type to match the with\n        # statement.\n        _cm_type = type(cm)\n        _exit = _cm_type.__exit__\n        result = _cm_type.__enter__(cm)\n        self._push_cm_exit(cm, _exit)\n        return result\n\n    def callback(*args, **kwds):\n        \"\"\"Registers an arbitrary callback and arguments.\n\n        Cannot suppress exceptions.\n        \"\"\"\n        if len(args) >= 2:\n            self, callback, *args = args\n        elif not args:\n            raise TypeError(\"descriptor 'callback' of '_BaseExitStack' object \"\n                            \"needs an argument\")\n        elif 'callback' in kwds:\n            callback = kwds.pop('callback')\n            self, *args = args\n        else:\n            raise TypeError('callback expected at least 1 positional argument, '\n                            'got %d' % (len(args)-1))\n\n        _exit_wrapper = self._create_cb_wrapper(callback, *args, **kwds)\n\n        # We changed the signature, so using @wraps is not appropriate, but\n        # setting __wrapped__ may still help with introspection.\n        _exit_wrapper.__wrapped__ = callback\n        self._push_exit_callback(_exit_wrapper)\n        return callback  # Allow use as a decorator\n\n    def _push_cm_exit(self, cm, cm_exit):\n        \"\"\"Helper to correctly register callbacks to __exit__ methods.\"\"\"\n        _exit_wrapper = self._create_exit_wrapper(cm, cm_exit)\n        _exit_wrapper.__self__ = cm\n        self._push_exit_callback(_exit_wrapper, True)\n\n    def _push_exit_callback(self, callback, is_sync=True):\n        self._exit_callbacks.append((is_sync, callback))\n\n\n# Inspired by discussions on http://bugs.python.org/issue13585\nclass ExitStack(_BaseExitStack, AbstractContextManager):\n    \"\"\"Context manager for dynamic management of a stack of exit callbacks.\n\n    For example:\n        with ExitStack() as stack:\n            files = [stack.enter_context(open(fname)) for fname in filenames]\n            # All opened files will automatically be closed at the end of\n            # the with statement, even if attempts to open files later\n            # in the list raise an exception.\n    \"\"\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc_details):\n        received_exc = exc_details[0] is not None\n\n        # We manipulate the exception state so it behaves as though\n        # we were actually nesting multiple with statements\n        frame_exc = sys.exc_info()[1]\n        def _fix_exception_context(new_exc, old_exc):\n            # Context may not be correct, so find the end of the chain\n            while 1:\n                exc_context = new_exc.__context__\n                if exc_context is old_exc:\n                    # Context is already set correctly (see issue 20317)\n                    return\n                if exc_context is None or exc_context is frame_exc:\n                    break\n                new_exc = exc_context\n            # Change the end of the chain to point to the exception\n            # we expect it to reference\n            new_exc.__context__ = old_exc\n\n        # Callbacks are invoked in LIFO order to match the behaviour of\n        # nested context managers\n        suppressed_exc = False\n        pending_raise = False\n        while self._exit_callbacks:\n            is_sync, cb = self._exit_callbacks.pop()\n            assert is_sync\n            try:\n                if cb(*exc_details):\n                    suppressed_exc = True\n                    pending_raise = False\n                    exc_details = (None, None, None)\n            except:\n                new_exc_details = sys.exc_info()\n                # simulate the stack of exceptions by setting the context\n                _fix_exception_context(new_exc_details[1], exc_details[1])\n                pending_raise = True\n                exc_details = new_exc_details\n        if pending_raise:\n            try:\n                # bare \"raise exc_details[1]\" replaces our carefully\n                # set-up context\n                fixed_ctx = exc_details[1].__context__\n                raise exc_details[1]\n            except BaseException:\n                exc_details[1].__context__ = fixed_ctx\n                raise\n        return received_exc and suppressed_exc\n\n    def close(self):\n        \"\"\"Immediately unwind the context stack.\"\"\"\n        self.__exit__(None, None, None)\n\n\n# Inspired by discussions on https://bugs.python.org/issue29302\nclass AsyncExitStack(_BaseExitStack, AbstractAsyncContextManager):\n    \"\"\"Async context manager for dynamic management of a stack of exit\n    callbacks.\n\n    For example:\n        async with AsyncExitStack() as stack:\n            connections = [await stack.enter_async_context(get_connection())\n                for i in range(5)]\n            # All opened connections will automatically be released at the\n            # end of the async with statement, even if attempts to open a\n            # connection later in the list raise an exception.\n    \"\"\"\n\n    @staticmethod\n    def _create_async_exit_wrapper(cm, cm_exit):\n        async def _exit_wrapper(exc_type, exc, tb):\n            return await cm_exit(cm, exc_type, exc, tb)\n        return _exit_wrapper\n\n    @staticmethod\n    def _create_async_cb_wrapper(*args, **kwds):\n        callback, *args = args\n        async def _exit_wrapper(exc_type, exc, tb):\n            await callback(*args, **kwds)\n        return _exit_wrapper\n\n    async def enter_async_context(self, cm):\n        \"\"\"Enters the supplied async context manager.\n\n        If successful, also pushes its __aexit__ method as a callback and\n        returns the result of the __aenter__ method.\n        \"\"\"\n        _cm_type = type(cm)\n        _exit = _cm_type.__aexit__\n        result = await _cm_type.__aenter__(cm)\n        self._push_async_cm_exit(cm, _exit)\n        return result\n\n    def push_async_exit(self, exit):\n        \"\"\"Registers a coroutine function with the standard __aexit__ method\n        signature.\n\n        Can suppress exceptions the same way __aexit__ method can.\n        Also accepts any object with an __aexit__ method (registering a call\n        to the method instead of the object itself).\n        \"\"\"\n        _cb_type = type(exit)\n        try:\n            exit_method = _cb_type.__aexit__\n        except AttributeError:\n            # Not an async context manager, so assume it's a coroutine function\n            self._push_exit_callback(exit, False)\n        else:\n            self._push_async_cm_exit(exit, exit_method)\n        return exit  # Allow use as a decorator\n\n    def push_async_callback(*args, **kwds):\n        \"\"\"Registers an arbitrary coroutine function and arguments.\n\n        Cannot suppress exceptions.\n        \"\"\"\n        if len(args) >= 2:\n            self, callback, *args = args\n        elif not args:\n            raise TypeError(\"descriptor 'push_async_callback' of \"\n                            \"'AsyncExitStack' object needs an argument\")\n        elif 'callback' in kwds:\n            callback = kwds.pop('callback')\n            self, *args = args\n        else:\n            raise TypeError('push_async_callback expected at least 1 '\n                            'positional argument, got %d' % (len(args)-1))\n\n        _exit_wrapper = self._create_async_cb_wrapper(callback, *args, **kwds)\n\n        # We changed the signature, so using @wraps is not appropriate, but\n        # setting __wrapped__ may still help with introspection.\n        _exit_wrapper.__wrapped__ = callback\n        self._push_exit_callback(_exit_wrapper, False)\n        return callback  # Allow use as a decorator\n\n    async def aclose(self):\n        \"\"\"Immediately unwind the context stack.\"\"\"\n        await self.__aexit__(None, None, None)\n\n    def _push_async_cm_exit(self, cm, cm_exit):\n        \"\"\"Helper to correctly register coroutine function to __aexit__\n        method.\"\"\"\n        _exit_wrapper = self._create_async_exit_wrapper(cm, cm_exit)\n        _exit_wrapper.__self__ = cm\n        self._push_exit_callback(_exit_wrapper, False)\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc_details):\n        received_exc = exc_details[0] is not None\n\n        # We manipulate the exception state so it behaves as though\n        # we were actually nesting multiple with statements\n        frame_exc = sys.exc_info()[1]\n        def _fix_exception_context(new_exc, old_exc):\n            # Context may not be correct, so find the end of the chain\n            while 1:\n                exc_context = new_exc.__context__\n                if exc_context is old_exc:\n                    # Context is already set correctly (see issue 20317)\n                    return\n                if exc_context is None or exc_context is frame_exc:\n                    break\n                new_exc = exc_context\n            # Change the end of the chain to point to the exception\n            # we expect it to reference\n            new_exc.__context__ = old_exc\n\n        # Callbacks are invoked in LIFO order to match the behaviour of\n        # nested context managers\n        suppressed_exc = False\n        pending_raise = False\n        while self._exit_callbacks:\n            is_sync, cb = self._exit_callbacks.pop()\n            try:\n                if is_sync:\n                    cb_suppress = cb(*exc_details)\n                else:\n                    cb_suppress = await cb(*exc_details)\n\n                if cb_suppress:\n                    suppressed_exc = True\n                    pending_raise = False\n                    exc_details = (None, None, None)\n            except:\n                new_exc_details = sys.exc_info()\n                # simulate the stack of exceptions by setting the context\n                _fix_exception_context(new_exc_details[1], exc_details[1])\n                pending_raise = True\n                exc_details = new_exc_details\n        if pending_raise:\n            try:\n                # bare \"raise exc_details[1]\" replaces our carefully\n                # set-up context\n                fixed_ctx = exc_details[1].__context__\n                raise exc_details[1]\n            except BaseException:\n                exc_details[1].__context__ = fixed_ctx\n                raise\n        return received_exc and suppressed_exc\n\n\nclass nullcontext(AbstractContextManager):\n    \"\"\"Context manager that does no additional processing.\n\n    Used as a stand-in for a normal context manager, when a particular\n    block of code is only sometimes used with a normal context manager:\n\n    cm = optional_cm if condition else nullcontext()\n    with cm:\n        # Perform operation, using optional_cm if condition is True\n    \"\"\"\n\n    def __init__(self, enter_result=None):\n        self.enter_result = enter_result\n\n    def __enter__(self):\n        return self.enter_result\n\n    def __exit__(self, *excinfo):\n        pass\n", 703], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py": ["import contextlib\nimport functools\nimport inspect\nimport re\nimport traceback\nimport types\nimport typing\nimport warnings\nfrom contextlib import asynccontextmanager\nfrom enum import Enum\n\nfrom starlette._utils import is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.convertors import CONVERTOR_TYPES, Convertor\nfrom starlette.datastructures import URL, Headers, URLPath\nfrom starlette.exceptions import HTTPException\nfrom starlette.middleware import Middleware\nfrom starlette.requests import Request\nfrom starlette.responses import PlainTextResponse, RedirectResponse\nfrom starlette.types import ASGIApp, Receive, Scope, Send\nfrom starlette.websockets import WebSocket, WebSocketClose\n\n\nclass NoMatchFound(Exception):\n    \"\"\"\n    Raised by `.url_for(name, **path_params)` and `.url_path_for(name, **path_params)`\n    if no matching route exists.\n    \"\"\"\n\n    def __init__(self, name: str, path_params: typing.Dict[str, typing.Any]) -> None:\n        params = \", \".join(list(path_params.keys()))\n        super().__init__(f'No route exists for name \"{name}\" and params \"{params}\".')\n\n\nclass Match(Enum):\n    NONE = 0\n    PARTIAL = 1\n    FULL = 2\n\n\ndef iscoroutinefunction_or_partial(obj: typing.Any) -> bool:  # pragma: no cover\n    \"\"\"\n    Correctly determines if an object is a coroutine function,\n    including those wrapped in functools.partial objects.\n    \"\"\"\n    warnings.warn(\n        \"iscoroutinefunction_or_partial is deprecated, \"\n        \"and will be removed in a future release.\",\n        DeprecationWarning,\n    )\n    while isinstance(obj, functools.partial):\n        obj = obj.func\n    return inspect.iscoroutinefunction(obj)\n\n\ndef request_response(func: typing.Callable) -> ASGIApp:\n    \"\"\"\n    Takes a function or coroutine `func(request) -> response`,\n    and returns an ASGI application.\n    \"\"\"\n    is_coroutine = is_async_callable(func)\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        request = Request(scope, receive=receive, send=send)\n        if is_coroutine:\n            response = await func(request)\n        else:\n            response = await run_in_threadpool(func, request)\n        await response(scope, receive, send)\n\n    return app\n\n\ndef websocket_session(func: typing.Callable) -> ASGIApp:\n    \"\"\"\n    Takes a coroutine `func(session)`, and returns an ASGI application.\n    \"\"\"\n    # assert asyncio.iscoroutinefunction(func), \"WebSocket endpoints must be async\"\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        session = WebSocket(scope, receive=receive, send=send)\n        await func(session)\n\n    return app\n\n\ndef get_name(endpoint: typing.Callable) -> str:\n    if inspect.isroutine(endpoint) or inspect.isclass(endpoint):\n        return endpoint.__name__\n    return endpoint.__class__.__name__\n\n\ndef replace_params(\n    path: str,\n    param_convertors: typing.Dict[str, Convertor],\n    path_params: typing.Dict[str, str],\n) -> typing.Tuple[str, dict]:\n    for key, value in list(path_params.items()):\n        if \"{\" + key + \"}\" in path:\n            convertor = param_convertors[key]\n            value = convertor.to_string(value)\n            path = path.replace(\"{\" + key + \"}\", value)\n            path_params.pop(key)\n    return path, path_params\n\n\n# Match parameters in URL paths, eg. '{param}', and '{param:int}'\nPARAM_REGEX = re.compile(\"{([a-zA-Z_][a-zA-Z0-9_]*)(:[a-zA-Z_][a-zA-Z0-9_]*)?}\")\n\n\ndef compile_path(\n    path: str,\n) -> typing.Tuple[typing.Pattern, str, typing.Dict[str, Convertor]]:\n    \"\"\"\n    Given a path string, like: \"/{username:str}\",\n    or a host string, like: \"{subdomain}.mydomain.org\", return a three-tuple\n    of (regex, format, {param_name:convertor}).\n\n    regex:      \"/(?P<username>[^/]+)\"\n    format:     \"/{username}\"\n    convertors: {\"username\": StringConvertor()}\n    \"\"\"\n    is_host = not path.startswith(\"/\")\n\n    path_regex = \"^\"\n    path_format = \"\"\n    duplicated_params = set()\n\n    idx = 0\n    param_convertors = {}\n    for match in PARAM_REGEX.finditer(path):\n        param_name, convertor_type = match.groups(\"str\")\n        convertor_type = convertor_type.lstrip(\":\")\n        assert (\n            convertor_type in CONVERTOR_TYPES\n        ), f\"Unknown path convertor '{convertor_type}'\"\n        convertor = CONVERTOR_TYPES[convertor_type]\n\n        path_regex += re.escape(path[idx : match.start()])\n        path_regex += f\"(?P<{param_name}>{convertor.regex})\"\n\n        path_format += path[idx : match.start()]\n        path_format += \"{%s}\" % param_name\n\n        if param_name in param_convertors:\n            duplicated_params.add(param_name)\n\n        param_convertors[param_name] = convertor\n\n        idx = match.end()\n\n    if duplicated_params:\n        names = \", \".join(sorted(duplicated_params))\n        ending = \"s\" if len(duplicated_params) > 1 else \"\"\n        raise ValueError(f\"Duplicated param name{ending} {names} at path {path}\")\n\n    if is_host:\n        # Align with `Host.matches()` behavior, which ignores port.\n        hostname = path[idx:].split(\":\")[0]\n        path_regex += re.escape(hostname) + \"$\"\n    else:\n        path_regex += re.escape(path[idx:]) + \"$\"\n\n    path_format += path[idx:]\n\n    return re.compile(path_regex), path_format, param_convertors\n\n\nclass BaseRoute:\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        raise NotImplementedError()  # pragma: no cover\n\n    def url_path_for(self, name: str, **path_params: typing.Any) -> URLPath:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        \"\"\"\n        A route may be used in isolation as a stand-alone ASGI app.\n        This is a somewhat contrived case, as they'll almost always be used\n        within a Router, but could be useful for some tooling and minimal apps.\n        \"\"\"\n        match, child_scope = self.matches(scope)\n        if match == Match.NONE:\n            if scope[\"type\"] == \"http\":\n                response = PlainTextResponse(\"Not Found\", status_code=404)\n                await response(scope, receive, send)\n            elif scope[\"type\"] == \"websocket\":\n                websocket_close = WebSocketClose()\n                await websocket_close(scope, receive, send)\n            return\n\n        scope.update(child_scope)\n        await self.handle(scope, receive, send)\n\n\nclass Route(BaseRoute):\n    def __init__(\n        self,\n        path: str,\n        endpoint: typing.Callable,\n        *,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> None:\n        assert path.startswith(\"/\"), \"Routed paths must start with '/'\"\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n        self.include_in_schema = include_in_schema\n\n        endpoint_handler = endpoint\n        while isinstance(endpoint_handler, functools.partial):\n            endpoint_handler = endpoint_handler.func\n        if inspect.isfunction(endpoint_handler) or inspect.ismethod(endpoint_handler):\n            # Endpoint is function or method. Treat it as `func(request) -> response`.\n            self.app = request_response(endpoint)\n            if methods is None:\n                methods = [\"GET\"]\n        else:\n            # Endpoint is a class. Treat it as ASGI.\n            self.app = endpoint\n\n        if methods is None:\n            self.methods = None\n        else:\n            self.methods = {method.upper() for method in methods}\n            if \"GET\" in self.methods:\n                self.methods.add(\"HEAD\")\n\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        if scope[\"type\"] == \"http\":\n            match = self.path_regex.match(scope[\"path\"])\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\"endpoint\": self.endpoint, \"path_params\": path_params}\n                if self.methods and scope[\"method\"] not in self.methods:\n                    return Match.PARTIAL, child_scope\n                else:\n                    return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, name: str, **path_params: typing.Any) -> URLPath:\n        seen_params = set(path_params.keys())\n        expected_params = set(self.param_convertors.keys())\n\n        if name != self.name or seen_params != expected_params:\n            raise NoMatchFound(name, path_params)\n\n        path, remaining_params = replace_params(\n            self.path_format, self.param_convertors, path_params\n        )\n        assert not remaining_params\n        return URLPath(path=path, protocol=\"http\")\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if self.methods and scope[\"method\"] not in self.methods:\n            headers = {\"Allow\": \", \".join(self.methods)}\n            if \"app\" in scope:\n                raise HTTPException(status_code=405, headers=headers)\n            else:\n                response = PlainTextResponse(\n                    \"Method Not Allowed\", status_code=405, headers=headers\n                )\n            await response(scope, receive, send)\n        else:\n            await self.app(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return (\n            isinstance(other, Route)\n            and self.path == other.path\n            and self.endpoint == other.endpoint\n            and self.methods == other.methods\n        )\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        methods = sorted(self.methods or [])\n        path, name = self.path, self.name\n        return f\"{class_name}(path={path!r}, name={name!r}, methods={methods!r})\"\n\n\nclass WebSocketRoute(BaseRoute):\n    def __init__(\n        self, path: str, endpoint: typing.Callable, *, name: typing.Optional[str] = None\n    ) -> None:\n        assert path.startswith(\"/\"), \"Routed paths must start with '/'\"\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n\n        endpoint_handler = endpoint\n        while isinstance(endpoint_handler, functools.partial):\n            endpoint_handler = endpoint_handler.func\n        if inspect.isfunction(endpoint_handler) or inspect.ismethod(endpoint_handler):\n            # Endpoint is function or method. Treat it as `func(websocket)`.\n            self.app = websocket_session(endpoint)\n        else:\n            # Endpoint is a class. Treat it as ASGI.\n            self.app = endpoint\n\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        if scope[\"type\"] == \"websocket\":\n            match = self.path_regex.match(scope[\"path\"])\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\"endpoint\": self.endpoint, \"path_params\": path_params}\n                return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, name: str, **path_params: typing.Any) -> URLPath:\n        seen_params = set(path_params.keys())\n        expected_params = set(self.param_convertors.keys())\n\n        if name != self.name or seen_params != expected_params:\n            raise NoMatchFound(name, path_params)\n\n        path, remaining_params = replace_params(\n            self.path_format, self.param_convertors, path_params\n        )\n        assert not remaining_params\n        return URLPath(path=path, protocol=\"websocket\")\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await self.app(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return (\n            isinstance(other, WebSocketRoute)\n            and self.path == other.path\n            and self.endpoint == other.endpoint\n        )\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(path={self.path!r}, name={self.name!r})\"\n\n\nclass Mount(BaseRoute):\n    def __init__(\n        self,\n        path: str,\n        app: typing.Optional[ASGIApp] = None,\n        routes: typing.Optional[typing.Sequence[BaseRoute]] = None,\n        name: typing.Optional[str] = None,\n        *,\n        middleware: typing.Optional[typing.Sequence[Middleware]] = None,\n    ) -> None:\n        assert path == \"\" or path.startswith(\"/\"), \"Routed paths must start with '/'\"\n        assert (\n            app is not None or routes is not None\n        ), \"Either 'app=...', or 'routes=' must be specified\"\n        self.path = path.rstrip(\"/\")\n        if app is not None:\n            self._base_app: ASGIApp = app\n        else:\n            self._base_app = Router(routes=routes)\n        self.app = self._base_app\n        if middleware is not None:\n            for cls, options in reversed(middleware):\n                self.app = cls(app=self.app, **options)\n        self.name = name\n        self.path_regex, self.path_format, self.param_convertors = compile_path(\n            self.path + \"/{path:path}\"\n        )\n\n    @property\n    def routes(self) -> typing.List[BaseRoute]:\n        return getattr(self._base_app, \"routes\", [])\n\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        if scope[\"type\"] in (\"http\", \"websocket\"):\n            path = scope[\"path\"]\n            match = self.path_regex.match(path)\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                remaining_path = \"/\" + matched_params.pop(\"path\")\n                matched_path = path[: -len(remaining_path)]\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                root_path = scope.get(\"root_path\", \"\")\n                child_scope = {\n                    \"path_params\": path_params,\n                    \"app_root_path\": scope.get(\"app_root_path\", root_path),\n                    \"root_path\": root_path + matched_path,\n                    \"path\": remaining_path,\n                    \"endpoint\": self.app,\n                }\n                return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, name: str, **path_params: typing.Any) -> URLPath:\n        if self.name is not None and name == self.name and \"path\" in path_params:\n            # 'name' matches \"<mount_name>\".\n            path_params[\"path\"] = path_params[\"path\"].lstrip(\"/\")\n            path, remaining_params = replace_params(\n                self.path_format, self.param_convertors, path_params\n            )\n            if not remaining_params:\n                return URLPath(path=path)\n        elif self.name is None or name.startswith(self.name + \":\"):\n            if self.name is None:\n                # No mount name.\n                remaining_name = name\n            else:\n                # 'name' matches \"<mount_name>:<child_name>\".\n                remaining_name = name[len(self.name) + 1 :]\n            path_kwarg = path_params.get(\"path\")\n            path_params[\"path\"] = \"\"\n            path_prefix, remaining_params = replace_params(\n                self.path_format, self.param_convertors, path_params\n            )\n            if path_kwarg is not None:\n                remaining_params[\"path\"] = path_kwarg\n            for route in self.routes or []:\n                try:\n                    url = route.url_path_for(remaining_name, **remaining_params)\n                    return URLPath(\n                        path=path_prefix.rstrip(\"/\") + str(url), protocol=url.protocol\n                    )\n                except NoMatchFound:\n                    pass\n        raise NoMatchFound(name, path_params)\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await self.app(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return (\n            isinstance(other, Mount)\n            and self.path == other.path\n            and self.app == other.app\n        )\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        name = self.name or \"\"\n        return f\"{class_name}(path={self.path!r}, name={name!r}, app={self.app!r})\"\n\n\nclass Host(BaseRoute):\n    def __init__(\n        self, host: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:\n        assert not host.startswith(\"/\"), \"Host must not start with '/'\"\n        self.host = host\n        self.app = app\n        self.name = name\n        self.host_regex, self.host_format, self.param_convertors = compile_path(host)\n\n    @property\n    def routes(self) -> typing.List[BaseRoute]:\n        return getattr(self.app, \"routes\", [])\n\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        if scope[\"type\"] in (\"http\", \"websocket\"):\n            headers = Headers(scope=scope)\n            host = headers.get(\"host\", \"\").split(\":\")[0]\n            match = self.host_regex.match(host)\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\"path_params\": path_params, \"endpoint\": self.app}\n                return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, name: str, **path_params: typing.Any) -> URLPath:\n        if self.name is not None and name == self.name and \"path\" in path_params:\n            # 'name' matches \"<mount_name>\".\n            path = path_params.pop(\"path\")\n            host, remaining_params = replace_params(\n                self.host_format, self.param_convertors, path_params\n            )\n            if not remaining_params:\n                return URLPath(path=path, host=host)\n        elif self.name is None or name.startswith(self.name + \":\"):\n            if self.name is None:\n                # No mount name.\n                remaining_name = name\n            else:\n                # 'name' matches \"<mount_name>:<child_name>\".\n                remaining_name = name[len(self.name) + 1 :]\n            host, remaining_params = replace_params(\n                self.host_format, self.param_convertors, path_params\n            )\n            for route in self.routes or []:\n                try:\n                    url = route.url_path_for(remaining_name, **remaining_params)\n                    return URLPath(path=str(url), protocol=url.protocol, host=host)\n                except NoMatchFound:\n                    pass\n        raise NoMatchFound(name, path_params)\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await self.app(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return (\n            isinstance(other, Host)\n            and self.host == other.host\n            and self.app == other.app\n        )\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        name = self.name or \"\"\n        return f\"{class_name}(host={self.host!r}, name={name!r}, app={self.app!r})\"\n\n\n_T = typing.TypeVar(\"_T\")\n\n\nclass _AsyncLiftContextManager(typing.AsyncContextManager[_T]):\n    def __init__(self, cm: typing.ContextManager[_T]):\n        self._cm = cm\n\n    async def __aenter__(self) -> _T:\n        return self._cm.__enter__()\n\n    async def __aexit__(\n        self,\n        exc_type: typing.Optional[typing.Type[BaseException]],\n        exc_value: typing.Optional[BaseException],\n        traceback: typing.Optional[types.TracebackType],\n    ) -> typing.Optional[bool]:\n        return self._cm.__exit__(exc_type, exc_value, traceback)\n\n\ndef _wrap_gen_lifespan_context(\n    lifespan_context: typing.Callable[[typing.Any], typing.Generator]\n) -> typing.Callable[[typing.Any], typing.AsyncContextManager]:\n    cmgr = contextlib.contextmanager(lifespan_context)\n\n    @functools.wraps(cmgr)\n    def wrapper(app: typing.Any) -> _AsyncLiftContextManager:\n        return _AsyncLiftContextManager(cmgr(app))\n\n    return wrapper\n\n\nclass _DefaultLifespan:\n    def __init__(self, router: \"Router\"):\n        self._router = router\n\n    async def __aenter__(self) -> None:\n        await self._router.startup()\n\n    async def __aexit__(self, *exc_info: object) -> None:\n        await self._router.shutdown()\n\n    def __call__(self: _T, app: object) -> _T:\n        return self\n\n\nclass Router:\n    def __init__(\n        self,\n        routes: typing.Optional[typing.Sequence[BaseRoute]] = None,\n        redirect_slashes: bool = True,\n        default: typing.Optional[ASGIApp] = None,\n        on_startup: typing.Optional[typing.Sequence[typing.Callable]] = None,\n        on_shutdown: typing.Optional[typing.Sequence[typing.Callable]] = None,\n        lifespan: typing.Optional[\n            typing.Callable[[typing.Any], typing.AsyncContextManager]\n        ] = None,\n    ) -> None:\n        self.routes = [] if routes is None else list(routes)\n        self.redirect_slashes = redirect_slashes\n        self.default = self.not_found if default is None else default\n        self.on_startup = [] if on_startup is None else list(on_startup)\n        self.on_shutdown = [] if on_shutdown is None else list(on_shutdown)\n\n        if lifespan is None:\n            self.lifespan_context: typing.Callable[\n                [typing.Any], typing.AsyncContextManager\n            ] = _DefaultLifespan(self)\n\n        elif inspect.isasyncgenfunction(lifespan):\n            warnings.warn(\n                \"async generator function lifespans are deprecated, \"\n                \"use an @contextlib.asynccontextmanager function instead\",\n                DeprecationWarning,\n            )\n            self.lifespan_context = asynccontextmanager(\n                lifespan,  # type: ignore[arg-type]\n            )\n        elif inspect.isgeneratorfunction(lifespan):\n            warnings.warn(\n                \"generator function lifespans are deprecated, \"\n                \"use an @contextlib.asynccontextmanager function instead\",\n                DeprecationWarning,\n            )\n            self.lifespan_context = _wrap_gen_lifespan_context(\n                lifespan,  # type: ignore[arg-type]\n            )\n        else:\n            self.lifespan_context = lifespan\n\n    async def not_found(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] == \"websocket\":\n            websocket_close = WebSocketClose()\n            await websocket_close(scope, receive, send)\n            return\n\n        # If we're running inside a starlette application then raise an\n        # exception, so that the configurable exception handler can deal with\n        # returning the response. For plain ASGI apps, just return the response.\n        if \"app\" in scope:\n            raise HTTPException(status_code=404)\n        else:\n            response = PlainTextResponse(\"Not Found\", status_code=404)\n        await response(scope, receive, send)\n\n    def url_path_for(self, name: str, **path_params: typing.Any) -> URLPath:\n        for route in self.routes:\n            try:\n                return route.url_path_for(name, **path_params)\n            except NoMatchFound:\n                pass\n        raise NoMatchFound(name, path_params)\n\n    async def startup(self) -> None:\n        \"\"\"\n        Run any `.on_startup` event handlers.\n        \"\"\"\n        for handler in self.on_startup:\n            if is_async_callable(handler):\n                await handler()\n            else:\n                handler()\n\n    async def shutdown(self) -> None:\n        \"\"\"\n        Run any `.on_shutdown` event handlers.\n        \"\"\"\n        for handler in self.on_shutdown:\n            if is_async_callable(handler):\n                await handler()\n            else:\n                handler()\n\n    async def lifespan(self, scope: Scope, receive: Receive, send: Send) -> None:\n        \"\"\"\n        Handle ASGI lifespan messages, which allows us to manage application\n        startup and shutdown events.\n        \"\"\"\n        started = False\n        app = scope.get(\"app\")\n        await receive()\n        try:\n            async with self.lifespan_context(app):\n                await send({\"type\": \"lifespan.startup.complete\"})\n                started = True\n                await receive()\n        except BaseException:\n            exc_text = traceback.format_exc()\n            if started:\n                await send({\"type\": \"lifespan.shutdown.failed\", \"message\": exc_text})\n            else:\n                await send({\"type\": \"lifespan.startup.failed\", \"message\": exc_text})\n            raise\n        else:\n            await send({\"type\": \"lifespan.shutdown.complete\"})\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        \"\"\"\n        The main entry point to the Router class.\n        \"\"\"\n        assert scope[\"type\"] in (\"http\", \"websocket\", \"lifespan\")\n\n        if \"router\" not in scope:\n            scope[\"router\"] = self\n\n        if scope[\"type\"] == \"lifespan\":\n            await self.lifespan(scope, receive, send)\n            return\n\n        partial = None\n\n        for route in self.routes:\n            # Determine if any route matches the incoming scope,\n            # and hand over to the matching route if found.\n            match, child_scope = route.matches(scope)\n            if match == Match.FULL:\n                scope.update(child_scope)\n                await route.handle(scope, receive, send)\n                return\n            elif match == Match.PARTIAL and partial is None:\n                partial = route\n                partial_scope = child_scope\n\n        if partial is not None:\n            # \u00a0Handle partial matches. These are cases where an endpoint is\n            # able to handle the request, but is not a preferred option.\n            # We use this in particular to deal with \"405 Method Not Allowed\".\n            scope.update(partial_scope)\n            await partial.handle(scope, receive, send)\n            return\n\n        if scope[\"type\"] == \"http\" and self.redirect_slashes and scope[\"path\"] != \"/\":\n            redirect_scope = dict(scope)\n            if scope[\"path\"].endswith(\"/\"):\n                redirect_scope[\"path\"] = redirect_scope[\"path\"].rstrip(\"/\")\n            else:\n                redirect_scope[\"path\"] = redirect_scope[\"path\"] + \"/\"\n\n            for route in self.routes:\n                match, child_scope = route.matches(redirect_scope)\n                if match != Match.NONE:\n                    redirect_url = URL(scope=redirect_scope)\n                    response = RedirectResponse(url=str(redirect_url))\n                    await response(scope, receive, send)\n                    return\n\n        await self.default(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return isinstance(other, Router) and self.routes == other.routes\n\n    def mount(\n        self, path: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:  # pragma: nocover\n        route = Mount(path, app=app, name=name)\n        self.routes.append(route)\n\n    def host(\n        self, host: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:  # pragma: no cover\n        route = Host(host, app=app, name=name)\n        self.routes.append(route)\n\n    def add_route(\n        self,\n        path: str,\n        endpoint: typing.Callable,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> None:  # pragma: nocover\n        route = Route(\n            path,\n            endpoint=endpoint,\n            methods=methods,\n            name=name,\n            include_in_schema=include_in_schema,\n        )\n        self.routes.append(route)\n\n    def add_websocket_route(\n        self, path: str, endpoint: typing.Callable, name: typing.Optional[str] = None\n    ) -> None:  # pragma: no cover\n        route = WebSocketRoute(path, endpoint=endpoint, name=name)\n        self.routes.append(route)\n\n    def route(\n        self,\n        path: str,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [Route(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `route` decorator is deprecated, and will be removed in version 1.0.0.\"\n            \"Refer to https://www.starlette.io/routing/#http-routing for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_route(\n                path,\n                func,\n                methods=methods,\n                name=name,\n                include_in_schema=include_in_schema,\n            )\n            return func\n\n        return decorator\n\n    def websocket_route(\n        self, path: str, name: typing.Optional[str] = None\n    ) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [WebSocketRoute(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `websocket_route` decorator is deprecated, and will be removed in version 1.0.0. Refer to \"  # noqa: E501\n            \"https://www.starlette.io/routing/#websocket-routing for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def add_event_handler(\n        self, event_type: str, func: typing.Callable\n    ) -> None:  # pragma: no cover\n        assert event_type in (\"startup\", \"shutdown\")\n\n        if event_type == \"startup\":\n            self.on_startup.append(func)\n        else:\n            self.on_shutdown.append(func)\n\n    def on_event(self, event_type: str) -> typing.Callable:\n        warnings.warn(\n            \"The `on_event` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/events/#registering-events for recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_event_handler(event_type, func)\n            return func\n\n        return decorator\n", 850], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py": ["import asyncio\nimport dataclasses\nimport email.message\nimport inspect\nimport json\nfrom contextlib import AsyncExitStack\nfrom enum import Enum, IntEnum\nfrom typing import (\n    Any,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom fastapi import params\nfrom fastapi.datastructures import Default, DefaultPlaceholder\nfrom fastapi.dependencies.models import Dependant\nfrom fastapi.dependencies.utils import (\n    get_body_field,\n    get_dependant,\n    get_parameterless_sub_dependant,\n    get_typed_return_annotation,\n    solve_dependencies,\n)\nfrom fastapi.encoders import DictIntStrAny, SetIntStr, jsonable_encoder\nfrom fastapi.exceptions import RequestValidationError, WebSocketRequestValidationError\nfrom fastapi.types import DecoratedCallable\nfrom fastapi.utils import (\n    create_cloned_field,\n    create_response_field,\n    generate_unique_id,\n    get_value_or_default,\n    is_body_allowed_for_status_code,\n)\nfrom pydantic import BaseModel\nfrom pydantic.error_wrappers import ErrorWrapper, ValidationError\nfrom pydantic.fields import ModelField, Undefined\nfrom pydantic.utils import lenient_issubclass\nfrom starlette import routing\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.exceptions import HTTPException\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse, Response\nfrom starlette.routing import BaseRoute, Match\nfrom starlette.routing import Mount as Mount  # noqa\nfrom starlette.routing import (\n    compile_path,\n    get_name,\n    request_response,\n    websocket_session,\n)\nfrom starlette.status import WS_1008_POLICY_VIOLATION\nfrom starlette.types import ASGIApp, Scope\nfrom starlette.websockets import WebSocket\n\n\ndef _prepare_response_content(\n    res: Any,\n    *,\n    exclude_unset: bool,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n) -> Any:\n    if isinstance(res, BaseModel):\n        read_with_orm_mode = getattr(res.__config__, \"read_with_orm_mode\", None)\n        if read_with_orm_mode:\n            # Let from_orm extract the data from this model instead of converting\n            # it now to a dict.\n            # Otherwise there's no way to extract lazy data that requires attribute\n            # access instead of dict iteration, e.g. lazy relationships.\n            return res\n        return res.dict(\n            by_alias=True,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n    elif isinstance(res, list):\n        return [\n            _prepare_response_content(\n                item,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n            for item in res\n        ]\n    elif isinstance(res, dict):\n        return {\n            k: _prepare_response_content(\n                v,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n            for k, v in res.items()\n        }\n    elif dataclasses.is_dataclass(res):\n        return dataclasses.asdict(res)\n    return res\n\n\nasync def serialize_response(\n    *,\n    field: Optional[ModelField] = None,\n    response_content: Any,\n    include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n    exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n    by_alias: bool = True,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    is_coroutine: bool = True,\n) -> Any:\n    if field:\n        errors = []\n        response_content = _prepare_response_content(\n            response_content,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n        if is_coroutine:\n            value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n        else:\n            value, errors_ = await run_in_threadpool(\n                field.validate, response_content, {}, loc=(\"response\",)\n            )\n        if isinstance(errors_, ErrorWrapper):\n            errors.append(errors_)\n        elif isinstance(errors_, list):\n            errors.extend(errors_)\n        if errors:\n            raise ValidationError(errors, field.type_)\n        return jsonable_encoder(\n            value,\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n    else:\n        return jsonable_encoder(response_content)\n\n\nasync def run_endpoint_function(\n    *, dependant: Dependant, values: Dict[str, Any], is_coroutine: bool\n) -> Any:\n    # Only called by get_request_handler. Has been split into its own function to\n    # facilitate profiling endpoints, since inner functions are harder to profile.\n    assert dependant.call is not None, \"dependant.call must be a function\"\n\n    if is_coroutine:\n        return await dependant.call(**values)\n    else:\n        return await run_in_threadpool(dependant.call, **values)\n\n\ndef get_request_handler(\n    dependant: Dependant,\n    body_field: Optional[ModelField] = None,\n    status_code: Optional[int] = None,\n    response_class: Union[Type[Response], DefaultPlaceholder] = Default(JSONResponse),\n    response_field: Optional[ModelField] = None,\n    response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n    response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n    response_model_by_alias: bool = True,\n    response_model_exclude_unset: bool = False,\n    response_model_exclude_defaults: bool = False,\n    response_model_exclude_none: bool = False,\n    dependency_overrides_provider: Optional[Any] = None,\n) -> Callable[[Request], Coroutine[Any, Any, Response]]:\n    assert dependant.call is not None, \"dependant.call must be a function\"\n    is_coroutine = asyncio.iscoroutinefunction(dependant.call)\n    is_body_form = body_field and isinstance(body_field.field_info, params.Form)\n    if isinstance(response_class, DefaultPlaceholder):\n        actual_response_class: Type[Response] = response_class.value\n    else:\n        actual_response_class = response_class\n\n    async def app(request: Request) -> Response:\n        try:\n            body: Any = None\n            if body_field:\n                if is_body_form:\n                    body = await request.form()\n                    stack = request.scope.get(\"fastapi_astack\")\n                    assert isinstance(stack, AsyncExitStack)\n                    stack.push_async_callback(body.close)\n                else:\n                    body_bytes = await request.body()\n                    if body_bytes:\n                        json_body: Any = Undefined\n                        content_type_value = request.headers.get(\"content-type\")\n                        if not content_type_value:\n                            json_body = await request.json()\n                        else:\n                            message = email.message.Message()\n                            message[\"content-type\"] = content_type_value\n                            if message.get_content_maintype() == \"application\":\n                                subtype = message.get_content_subtype()\n                                if subtype == \"json\" or subtype.endswith(\"+json\"):\n                                    json_body = await request.json()\n                        if json_body != Undefined:\n                            body = json_body\n                        else:\n                            body = body_bytes\n        except json.JSONDecodeError as e:\n            raise RequestValidationError(\n                [ErrorWrapper(e, (\"body\", e.pos))], body=e.doc\n            ) from e\n        except HTTPException:\n            raise\n        except Exception as e:\n            raise HTTPException(\n                status_code=400, detail=\"There was an error parsing the body\"\n            ) from e\n        solved_result = await solve_dependencies(\n            request=request,\n            dependant=dependant,\n            body=body,\n            dependency_overrides_provider=dependency_overrides_provider,\n        )\n        values, errors, background_tasks, sub_response, _ = solved_result\n        if errors:\n            raise RequestValidationError(errors, body=body)\n        else:\n            raw_response = await run_endpoint_function(\n                dependant=dependant, values=values, is_coroutine=is_coroutine\n            )\n\n            if isinstance(raw_response, Response):\n                if raw_response.background is None:\n                    raw_response.background = background_tasks\n                return raw_response\n            response_args: Dict[str, Any] = {\"background\": background_tasks}\n            # If status_code was set, use it, otherwise use the default from the\n            # response class, in the case of redirect it's 307\n            current_status_code = (\n                status_code if status_code else sub_response.status_code\n            )\n            if current_status_code is not None:\n                response_args[\"status_code\"] = current_status_code\n            if sub_response.status_code:\n                response_args[\"status_code\"] = sub_response.status_code\n            content = await serialize_response(\n                field=response_field,\n                response_content=raw_response,\n                include=response_model_include,\n                exclude=response_model_exclude,\n                by_alias=response_model_by_alias,\n                exclude_unset=response_model_exclude_unset,\n                exclude_defaults=response_model_exclude_defaults,\n                exclude_none=response_model_exclude_none,\n                is_coroutine=is_coroutine,\n            )\n            response = actual_response_class(content, **response_args)\n            if not is_body_allowed_for_status_code(response.status_code):\n                response.body = b\"\"\n            response.headers.raw.extend(sub_response.headers.raw)\n            return response\n\n    return app\n\n\ndef get_websocket_app(\n    dependant: Dependant, dependency_overrides_provider: Optional[Any] = None\n) -> Callable[[WebSocket], Coroutine[Any, Any, Any]]:\n    async def app(websocket: WebSocket) -> None:\n        solved_result = await solve_dependencies(\n            request=websocket,\n            dependant=dependant,\n            dependency_overrides_provider=dependency_overrides_provider,\n        )\n        values, errors, _, _2, _3 = solved_result\n        if errors:\n            await websocket.close(code=WS_1008_POLICY_VIOLATION)\n            raise WebSocketRequestValidationError(errors)\n        assert dependant.call is not None, \"dependant.call must be a function\"\n        await dependant.call(**values)\n\n    return app\n\n\nclass APIWebSocketRoute(routing.WebSocketRoute):\n    def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        name: Optional[str] = None,\n        dependency_overrides_provider: Optional[Any] = None,\n    ) -> None:\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n        self.dependant = get_dependant(path=self.path_format, call=self.endpoint)\n        self.app = websocket_session(\n            get_websocket_app(\n                dependant=self.dependant,\n                dependency_overrides_provider=dependency_overrides_provider,\n            )\n        )\n\n    def matches(self, scope: Scope) -> Tuple[Match, Scope]:\n        match, child_scope = super().matches(scope)\n        if match != Match.NONE:\n            child_scope[\"route\"] = self\n        return match, child_scope\n\n\nclass APIRoute(routing.Route):\n    def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        name: Optional[str] = None,\n        methods: Optional[Union[Set[str], List[str]]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        dependency_overrides_provider: Optional[Any] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Union[\n            Callable[[\"APIRoute\"], str], DefaultPlaceholder\n        ] = Default(generate_unique_id),\n    ) -> None:\n        self.path = path\n        self.endpoint = endpoint\n        if isinstance(response_model, DefaultPlaceholder):\n            return_annotation = get_typed_return_annotation(endpoint)\n            if lenient_issubclass(return_annotation, Response):\n                response_model = None\n            else:\n                response_model = return_annotation\n        self.response_model = response_model\n        self.summary = summary\n        self.response_description = response_description\n        self.deprecated = deprecated\n        self.operation_id = operation_id\n        self.response_model_include = response_model_include\n        self.response_model_exclude = response_model_exclude\n        self.response_model_by_alias = response_model_by_alias\n        self.response_model_exclude_unset = response_model_exclude_unset\n        self.response_model_exclude_defaults = response_model_exclude_defaults\n        self.response_model_exclude_none = response_model_exclude_none\n        self.include_in_schema = include_in_schema\n        self.response_class = response_class\n        self.dependency_overrides_provider = dependency_overrides_provider\n        self.callbacks = callbacks\n        self.openapi_extra = openapi_extra\n        self.generate_unique_id_function = generate_unique_id_function\n        self.tags = tags or []\n        self.responses = responses or {}\n        self.name = get_name(endpoint) if name is None else name\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n        if methods is None:\n            methods = [\"GET\"]\n        self.methods: Set[str] = {method.upper() for method in methods}\n        if isinstance(generate_unique_id_function, DefaultPlaceholder):\n            current_generate_unique_id: Callable[\n                [\"APIRoute\"], str\n            ] = generate_unique_id_function.value\n        else:\n            current_generate_unique_id = generate_unique_id_function\n        self.unique_id = self.operation_id or current_generate_unique_id(self)\n        # normalize enums e.g. http.HTTPStatus\n        if isinstance(status_code, IntEnum):\n            status_code = int(status_code)\n        self.status_code = status_code\n        if self.response_model:\n            assert is_body_allowed_for_status_code(\n                status_code\n            ), f\"Status code {status_code} must not have a response body\"\n            response_name = \"Response_\" + self.unique_id\n            self.response_field = create_response_field(\n                name=response_name, type_=self.response_model\n            )\n            # Create a clone of the field, so that a Pydantic submodel is not returned\n            # as is just because it's an instance of a subclass of a more limited class\n            # e.g. UserInDB (containing hashed_password) could be a subclass of User\n            # that doesn't have the hashed_password. But because it's a subclass, it\n            # would pass the validation and be returned as is.\n            # By being a new field, no inheritance will be passed as is. A new model\n            # will be always created.\n            self.secure_cloned_response_field: Optional[\n                ModelField\n            ] = create_cloned_field(self.response_field)\n        else:\n            self.response_field = None  # type: ignore\n            self.secure_cloned_response_field = None\n        if dependencies:\n            self.dependencies = list(dependencies)\n        else:\n            self.dependencies = []\n        self.description = description or inspect.cleandoc(self.endpoint.__doc__ or \"\")\n        # if a \"form feed\" character (page break) is found in the description text,\n        # truncate description text to the content preceding the first \"form feed\"\n        self.description = self.description.split(\"\\f\")[0].strip()\n        response_fields = {}\n        for additional_status_code, response in self.responses.items():\n            assert isinstance(response, dict), \"An additional response must be a dict\"\n            model = response.get(\"model\")\n            if model:\n                assert is_body_allowed_for_status_code(\n                    additional_status_code\n                ), f\"Status code {additional_status_code} must not have a response body\"\n                response_name = f\"Response_{additional_status_code}_{self.unique_id}\"\n                response_field = create_response_field(name=response_name, type_=model)\n                response_fields[additional_status_code] = response_field\n        if response_fields:\n            self.response_fields: Dict[Union[int, str], ModelField] = response_fields\n        else:\n            self.response_fields = {}\n\n        assert callable(endpoint), \"An endpoint must be a callable\"\n        self.dependant = get_dependant(path=self.path_format, call=self.endpoint)\n        for depends in self.dependencies[::-1]:\n            self.dependant.dependencies.insert(\n                0,\n                get_parameterless_sub_dependant(depends=depends, path=self.path_format),\n            )\n        self.body_field = get_body_field(dependant=self.dependant, name=self.unique_id)\n        self.app = request_response(self.get_route_handler())\n\n    def get_route_handler(self) -> Callable[[Request], Coroutine[Any, Any, Response]]:\n        return get_request_handler(\n            dependant=self.dependant,\n            body_field=self.body_field,\n            status_code=self.status_code,\n            response_class=self.response_class,\n            response_field=self.secure_cloned_response_field,\n            response_model_include=self.response_model_include,\n            response_model_exclude=self.response_model_exclude,\n            response_model_by_alias=self.response_model_by_alias,\n            response_model_exclude_unset=self.response_model_exclude_unset,\n            response_model_exclude_defaults=self.response_model_exclude_defaults,\n            response_model_exclude_none=self.response_model_exclude_none,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n\n    def matches(self, scope: Scope) -> Tuple[Match, Scope]:\n        match, child_scope = super().matches(scope)\n        if match != Match.NONE:\n            child_scope[\"route\"] = self\n        return match, child_scope\n\n\nclass APIRouter(routing.Router):\n    def __init__(\n        self,\n        *,\n        prefix: str = \"\",\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        default_response_class: Type[Response] = Default(JSONResponse),\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        routes: Optional[List[routing.BaseRoute]] = None,\n        redirect_slashes: bool = True,\n        default: Optional[ASGIApp] = None,\n        dependency_overrides_provider: Optional[Any] = None,\n        route_class: Type[APIRoute] = APIRoute,\n        on_startup: Optional[Sequence[Callable[[], Any]]] = None,\n        on_shutdown: Optional[Sequence[Callable[[], Any]]] = None,\n        deprecated: Optional[bool] = None,\n        include_in_schema: bool = True,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        super().__init__(\n            routes=routes,\n            redirect_slashes=redirect_slashes,\n            default=default,\n            on_startup=on_startup,\n            on_shutdown=on_shutdown,\n        )\n        if prefix:\n            assert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\n            assert not prefix.endswith(\n                \"/\"\n            ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n        self.prefix = prefix\n        self.tags: List[Union[str, Enum]] = tags or []\n        self.dependencies = list(dependencies or []) or []\n        self.deprecated = deprecated\n        self.include_in_schema = include_in_schema\n        self.responses = responses or {}\n        self.callbacks = callbacks or []\n        self.dependency_overrides_provider = dependency_overrides_provider\n        self.route_class = route_class\n        self.default_response_class = default_response_class\n        self.generate_unique_id_function = generate_unique_id_function\n\n    def route(\n        self,\n        path: str,\n        methods: Optional[List[str]] = None,\n        name: Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_route(\n                path,\n                func,\n                methods=methods,\n                name=name,\n                include_in_schema=include_in_schema,\n            )\n            return func\n\n        return decorator\n\n    def add_api_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[Union[Set[str], List[str]]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        name: Optional[str] = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Union[\n            Callable[[APIRoute], str], DefaultPlaceholder\n        ] = Default(generate_unique_id),\n    ) -> None:\n        route_class = route_class_override or self.route_class\n        responses = responses or {}\n        combined_responses = {**self.responses, **responses}\n        current_response_class = get_value_or_default(\n            response_class, self.default_response_class\n        )\n        current_tags = self.tags.copy()\n        if tags:\n            current_tags.extend(tags)\n        current_dependencies = self.dependencies.copy()\n        if dependencies:\n            current_dependencies.extend(dependencies)\n        current_callbacks = self.callbacks.copy()\n        if callbacks:\n            current_callbacks.extend(callbacks)\n        current_generate_unique_id = get_value_or_default(\n            generate_unique_id_function, self.generate_unique_id_function\n        )\n        route = route_class(\n            self.prefix + path,\n            endpoint=endpoint,\n            response_model=response_model,\n            status_code=status_code,\n            tags=current_tags,\n            dependencies=current_dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=combined_responses,\n            deprecated=deprecated or self.deprecated,\n            methods=methods,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema and self.include_in_schema,\n            response_class=current_response_class,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n            callbacks=current_callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=current_generate_unique_id,\n        )\n        self.routes.append(route)\n\n    def api_route(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_route(\n                path,\n                func,\n                response_model=response_model,\n                status_code=status_code,\n                tags=tags,\n                dependencies=dependencies,\n                summary=summary,\n                description=description,\n                response_description=response_description,\n                responses=responses,\n                deprecated=deprecated,\n                methods=methods,\n                operation_id=operation_id,\n                response_model_include=response_model_include,\n                response_model_exclude=response_model_exclude,\n                response_model_by_alias=response_model_by_alias,\n                response_model_exclude_unset=response_model_exclude_unset,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                include_in_schema=include_in_schema,\n                response_class=response_class,\n                name=name,\n                callbacks=callbacks,\n                openapi_extra=openapi_extra,\n                generate_unique_id_function=generate_unique_id_function,\n            )\n            return func\n\n        return decorator\n\n    def add_api_websocket_route(\n        self, path: str, endpoint: Callable[..., Any], name: Optional[str] = None\n    ) -> None:\n        route = APIWebSocketRoute(\n            self.prefix + path,\n            endpoint=endpoint,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n        self.routes.append(route)\n\n    def websocket(\n        self, path: str, name: Optional[str] = None\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def websocket_route(\n        self, path: str, name: Union[str, None] = None\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def include_router(\n        self,\n        router: \"APIRouter\",\n        *,\n        prefix: str = \"\",\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        default_response_class: Type[Response] = Default(JSONResponse),\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        deprecated: Optional[bool] = None,\n        include_in_schema: bool = True,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        if prefix:\n            assert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\n            assert not prefix.endswith(\n                \"/\"\n            ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n        else:\n            for r in router.routes:\n                path = getattr(r, \"path\")  # noqa: B009\n                name = getattr(r, \"name\", \"unknown\")\n                if path is not None and not path:\n                    raise Exception(\n                        f\"Prefix and path cannot be both empty (path operation: {name})\"\n                    )\n        if responses is None:\n            responses = {}\n        for route in router.routes:\n            if isinstance(route, APIRoute):\n                combined_responses = {**responses, **route.responses}\n                use_response_class = get_value_or_default(\n                    route.response_class,\n                    router.default_response_class,\n                    default_response_class,\n                    self.default_response_class,\n                )\n                current_tags = []\n                if tags:\n                    current_tags.extend(tags)\n                if route.tags:\n                    current_tags.extend(route.tags)\n                current_dependencies: List[params.Depends] = []\n                if dependencies:\n                    current_dependencies.extend(dependencies)\n                if route.dependencies:\n                    current_dependencies.extend(route.dependencies)\n                current_callbacks = []\n                if callbacks:\n                    current_callbacks.extend(callbacks)\n                if route.callbacks:\n                    current_callbacks.extend(route.callbacks)\n                current_generate_unique_id = get_value_or_default(\n                    route.generate_unique_id_function,\n                    router.generate_unique_id_function,\n                    generate_unique_id_function,\n                    self.generate_unique_id_function,\n                )\n                self.add_api_route(\n                    prefix + route.path,\n                    route.endpoint,\n                    response_model=route.response_model,\n                    status_code=route.status_code,\n                    tags=current_tags,\n                    dependencies=current_dependencies,\n                    summary=route.summary,\n                    description=route.description,\n                    response_description=route.response_description,\n                    responses=combined_responses,\n                    deprecated=route.deprecated or deprecated or self.deprecated,\n                    methods=route.methods,\n                    operation_id=route.operation_id,\n                    response_model_include=route.response_model_include,\n                    response_model_exclude=route.response_model_exclude,\n                    response_model_by_alias=route.response_model_by_alias,\n                    response_model_exclude_unset=route.response_model_exclude_unset,\n                    response_model_exclude_defaults=route.response_model_exclude_defaults,\n                    response_model_exclude_none=route.response_model_exclude_none,\n                    include_in_schema=route.include_in_schema\n                    and self.include_in_schema\n                    and include_in_schema,\n                    response_class=use_response_class,\n                    name=route.name,\n                    route_class_override=type(route),\n                    callbacks=current_callbacks,\n                    openapi_extra=route.openapi_extra,\n                    generate_unique_id_function=current_generate_unique_id,\n                )\n            elif isinstance(route, routing.Route):\n                methods = list(route.methods or [])\n                self.add_route(\n                    prefix + route.path,\n                    route.endpoint,\n                    methods=methods,\n                    include_in_schema=route.include_in_schema,\n                    name=route.name,\n                )\n            elif isinstance(route, APIWebSocketRoute):\n                self.add_api_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n            elif isinstance(route, routing.WebSocketRoute):\n                self.add_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n        for handler in router.on_startup:\n            self.add_event_handler(\"startup\", handler)\n        for handler in router.on_shutdown:\n            self.add_event_handler(\"shutdown\", handler)\n\n    def get(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"GET\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def put(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"PUT\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def post(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"POST\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def delete(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"DELETE\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def options(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"OPTIONS\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def head(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"HEAD\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def patch(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"PATCH\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def trace(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"TRACE\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def on_event(\n        self, event_type: str\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_event_handler(event_type, func)\n            return func\n\n        return decorator\n", 1286], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py": ["\"\"\"\nThe typing module: Support for gradual typing as defined by PEP 484.\n\nAt large scale, the structure of the module is following:\n* Imports and exports, all public names should be explicitly added to __all__.\n* Internal helper functions: these should never be used in code outside this module.\n* _SpecialForm and its instances (special forms): Any, NoReturn, ClassVar, Union, Optional\n* Two classes whose instances can be type arguments in addition to types: ForwardRef and TypeVar\n* The core of internal generics API: _GenericAlias and _VariadicGenericAlias, the latter is\n  currently only used by Tuple and Callable. All subscripted types like X[int], Union[int, str],\n  etc., are instances of either of these classes.\n* The public counterpart of the generics API consists of two classes: Generic and Protocol\n  (the latter is currently private, but will be made public after PEP 544 acceptance).\n* Public helper functions: get_type_hints, overload, cast, no_type_check,\n  no_type_check_decorator.\n* Generic aliases for collections.abc ABCs and few additional protocols.\n* Special types: NewType, NamedTuple, TypedDict (may be added soon).\n* Wrapper submodules for re and io related types.\n\"\"\"\n\nimport abc\nfrom abc import abstractmethod, abstractproperty\nimport collections\nimport collections.abc\nimport contextlib\nimport functools\nimport operator\nimport re as stdlib_re  # Avoid confusion with the re we export.\nimport sys\nimport types\nfrom types import WrapperDescriptorType, MethodWrapperType, MethodDescriptorType\n\n# Please keep __all__ alphabetized within each category.\n__all__ = [\n    # Super-special typing primitives.\n    'Any',\n    'Callable',\n    'ClassVar',\n    'ForwardRef',\n    'Generic',\n    'Optional',\n    'Tuple',\n    'Type',\n    'TypeVar',\n    'Union',\n\n    # ABCs (from collections.abc).\n    'AbstractSet',  # collections.abc.Set.\n    'ByteString',\n    'Container',\n    'ContextManager',\n    'Hashable',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'Mapping',\n    'MappingView',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'Sequence',\n    'Sized',\n    'ValuesView',\n    'Awaitable',\n    'AsyncIterator',\n    'AsyncIterable',\n    'Coroutine',\n    'Collection',\n    'AsyncGenerator',\n    'AsyncContextManager',\n\n    # Structural checks, a.k.a. protocols.\n    'Reversible',\n    'SupportsAbs',\n    'SupportsBytes',\n    'SupportsComplex',\n    'SupportsFloat',\n    'SupportsInt',\n    'SupportsRound',\n\n    # Concrete collection types.\n    'ChainMap',\n    'Counter',\n    'Deque',\n    'Dict',\n    'DefaultDict',\n    'List',\n    'OrderedDict',\n    'Set',\n    'FrozenSet',\n    'NamedTuple',  # Not really a type.\n    'Generator',\n\n    # One-off things.\n    'AnyStr',\n    'cast',\n    'get_type_hints',\n    'NewType',\n    'no_type_check',\n    'no_type_check_decorator',\n    'NoReturn',\n    'overload',\n    'Text',\n    'TYPE_CHECKING',\n]\n\n# The pseudo-submodules 're' and 'io' are part of the public\n# namespace, but excluded from __all__ because they might stomp on\n# legitimate imports of those modules.\n\n\ndef _type_check(arg, msg, is_argument=True):\n    \"\"\"Check that the argument is a type, and return it (internal helper).\n\n    As a special case, accept None and return type(None) instead. Also wrap strings\n    into ForwardRef instances. Consider several corner cases, for example plain\n    special forms like Union are not valid, while Union[int, str] is OK, etc.\n    The msg argument is a human-readable error message, e.g::\n\n        \"Union[arg, ...]: arg should be a type.\"\n\n    We append the repr() of the actual value (truncated to 100 chars).\n    \"\"\"\n    invalid_generic_forms = (Generic, _Protocol)\n    if is_argument:\n        invalid_generic_forms = invalid_generic_forms + (ClassVar, )\n\n    if arg is None:\n        return type(None)\n    if isinstance(arg, str):\n        return ForwardRef(arg)\n    if (isinstance(arg, _GenericAlias) and\n            arg.__origin__ in invalid_generic_forms):\n        raise TypeError(f\"{arg} is not valid as type argument\")\n    if (isinstance(arg, _SpecialForm) and arg not in (Any, NoReturn) or\n            arg in (Generic, _Protocol)):\n        raise TypeError(f\"Plain {arg} is not valid as type argument\")\n    if isinstance(arg, (type, TypeVar, ForwardRef)):\n        return arg\n    if not callable(arg):\n        raise TypeError(f\"{msg} Got {arg!r:.100}.\")\n    return arg\n\n\ndef _type_repr(obj):\n    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n\n    If obj is a type, we return a shorter version than the default\n    type.__repr__, based on the module and qualified name, which is\n    typically enough to uniquely identify a type.  For everything\n    else, we fall back on repr(obj).\n    \"\"\"\n    if isinstance(obj, type):\n        if obj.__module__ == 'builtins':\n            return obj.__qualname__\n        return f'{obj.__module__}.{obj.__qualname__}'\n    if obj is ...:\n        return('...')\n    if isinstance(obj, types.FunctionType):\n        return obj.__name__\n    return repr(obj)\n\n\ndef _collect_type_vars(types):\n    \"\"\"Collect all type variable contained in types in order of\n    first appearance (lexicographic order). For example::\n\n        _collect_type_vars((T, List[S, T])) == (T, S)\n    \"\"\"\n    tvars = []\n    for t in types:\n        if isinstance(t, TypeVar) and t not in tvars:\n            tvars.append(t)\n        if isinstance(t, _GenericAlias) and not t._special:\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)\n\n\ndef _subs_tvars(tp, tvars, subs):\n    \"\"\"Substitute type variables 'tvars' with substitutions 'subs'.\n    These two must have the same length.\n    \"\"\"\n    if not isinstance(tp, _GenericAlias):\n        return tp\n    new_args = list(tp.__args__)\n    for a, arg in enumerate(tp.__args__):\n        if isinstance(arg, TypeVar):\n            for i, tvar in enumerate(tvars):\n                if arg == tvar:\n                    new_args[a] = subs[i]\n        else:\n            new_args[a] = _subs_tvars(arg, tvars, subs)\n    if tp.__origin__ is Union:\n        return Union[tuple(new_args)]\n    return tp.copy_with(tuple(new_args))\n\n\ndef _check_generic(cls, parameters):\n    \"\"\"Check correct count for parameters of a generic cls (internal helper).\n    This gives a nice error message in case of count mismatch.\n    \"\"\"\n    if not cls.__parameters__:\n        raise TypeError(f\"{cls} is not a generic class\")\n    alen = len(parameters)\n    elen = len(cls.__parameters__)\n    if alen != elen:\n        raise TypeError(f\"Too {'many' if alen > elen else 'few'} parameters for {cls};\"\n                        f\" actual {alen}, expected {elen}\")\n\n\ndef _remove_dups_flatten(parameters):\n    \"\"\"An internal helper for Union creation and substitution: flatten Unions\n    among parameters, then remove duplicates.\n    \"\"\"\n    # Flatten out Union[Union[...], ...].\n    params = []\n    for p in parameters:\n        if isinstance(p, _GenericAlias) and p.__origin__ is Union:\n            params.extend(p.__args__)\n        elif isinstance(p, tuple) and len(p) > 0 and p[0] is Union:\n            params.extend(p[1:])\n        else:\n            params.append(p)\n    # Weed out strict duplicates, preserving the first of each occurrence.\n    all_params = set(params)\n    if len(all_params) < len(params):\n        new_params = []\n        for t in params:\n            if t in all_params:\n                new_params.append(t)\n                all_params.remove(t)\n        params = new_params\n        assert not all_params, all_params\n    return tuple(params)\n\n\n_cleanups = []\n\n\ndef _tp_cache(func):\n    \"\"\"Internal wrapper caching __getitem__ of generic types with a fallback to\n    original function for non-hashable arguments.\n    \"\"\"\n    cached = functools.lru_cache()(func)\n    _cleanups.append(cached.cache_clear)\n\n    @functools.wraps(func)\n    def inner(*args, **kwds):\n        try:\n            return cached(*args, **kwds)\n        except TypeError:\n            pass  # All real errors (not unhashable args) are raised below.\n        return func(*args, **kwds)\n    return inner\n\n\ndef _eval_type(t, globalns, localns):\n    \"\"\"Evaluate all forward reverences in the given type t.\n    For use of globalns and localns see the docstring for get_type_hints().\n    \"\"\"\n    if isinstance(t, ForwardRef):\n        return t._evaluate(globalns, localns)\n    if isinstance(t, _GenericAlias):\n        ev_args = tuple(_eval_type(a, globalns, localns) for a in t.__args__)\n        if ev_args == t.__args__:\n            return t\n        res = t.copy_with(ev_args)\n        res._special = t._special\n        return res\n    return t\n\n\nclass _Final:\n    \"\"\"Mixin to prohibit subclassing\"\"\"\n\n    __slots__ = ('__weakref__',)\n\n    def __init_subclass__(self, *args, **kwds):\n        if '_root' not in kwds:\n            raise TypeError(\"Cannot subclass special typing classes\")\n\nclass _Immutable:\n    \"\"\"Mixin to indicate that object should not be copied.\"\"\"\n\n    def __copy__(self):\n        return self\n\n    def __deepcopy__(self, memo):\n        return self\n\n\nclass _SpecialForm(_Final, _Immutable, _root=True):\n    \"\"\"Internal indicator of special typing constructs.\n    See _doc instance attribute for specific docs.\n    \"\"\"\n\n    __slots__ = ('_name', '_doc')\n\n    def __new__(cls, *args, **kwds):\n        \"\"\"Constructor.\n\n        This only exists to give a better error message in case\n        someone tries to subclass a special typing object (not a good idea).\n        \"\"\"\n        if (len(args) == 3 and\n                isinstance(args[0], str) and\n                isinstance(args[1], tuple)):\n            # Close enough.\n            raise TypeError(f\"Cannot subclass {cls!r}\")\n        return super().__new__(cls)\n\n    def __init__(self, name, doc):\n        self._name = name\n        self._doc = doc\n\n    def __eq__(self, other):\n        if not isinstance(other, _SpecialForm):\n            return NotImplemented\n        return self._name == other._name\n\n    def __hash__(self):\n        return hash((self._name,))\n\n    def __repr__(self):\n        return 'typing.' + self._name\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f\"Cannot instantiate {self!r}\")\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f\"{self} cannot be used with isinstance()\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f\"{self} cannot be used with issubclass()\")\n\n    @_tp_cache\n    def __getitem__(self, parameters):\n        if self._name == 'ClassVar':\n            item = _type_check(parameters, 'ClassVar accepts only single type.')\n            return _GenericAlias(self, (item,))\n        if self._name == 'Union':\n            if parameters == ():\n                raise TypeError(\"Cannot take a Union of no types.\")\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n            msg = \"Union[arg, ...]: each arg must be a type.\"\n            parameters = tuple(_type_check(p, msg) for p in parameters)\n            parameters = _remove_dups_flatten(parameters)\n            if len(parameters) == 1:\n                return parameters[0]\n            return _GenericAlias(self, parameters)\n        if self._name == 'Optional':\n            arg = _type_check(parameters, \"Optional[t] requires a single type.\")\n            return Union[arg, type(None)]\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nAny = _SpecialForm('Any', doc=\n    \"\"\"Special type indicating an unconstrained type.\n\n    - Any is compatible with every type.\n    - Any assumed to have all methods.\n    - All values assumed to be instances of Any.\n\n    Note that all the above statements are true from the point of view of\n    static type checkers. At runtime, Any should not be used with instance\n    or class checks.\n    \"\"\")\n\nNoReturn = _SpecialForm('NoReturn', doc=\n    \"\"\"Special type indicating functions that never return.\n    Example::\n\n      from typing import NoReturn\n\n      def stop() -> NoReturn:\n          raise Exception('no way')\n\n    This type is invalid in other positions, e.g., ``List[NoReturn]``\n    will fail in static type checkers.\n    \"\"\")\n\nClassVar = _SpecialForm('ClassVar', doc=\n    \"\"\"Special type construct to mark class variables.\n\n    An annotation wrapped in ClassVar indicates that a given\n    attribute is intended to be used as a class variable and\n    should not be set on instances of that class. Usage::\n\n      class Starship:\n          stats: ClassVar[Dict[str, int]] = {} # class variable\n          damage: int = 10                     # instance variable\n\n    ClassVar accepts only types and cannot be further subscribed.\n\n    Note that ClassVar is not a class itself, and should not\n    be used with isinstance() or issubclass().\n    \"\"\")\n\nUnion = _SpecialForm('Union', doc=\n    \"\"\"Union type; Union[X, Y] means either X or Y.\n\n    To define a union, use e.g. Union[int, str].  Details:\n    - The arguments must be types and there must be at least one.\n    - None as an argument is a special case and is replaced by\n      type(None).\n    - Unions of unions are flattened, e.g.::\n\n        Union[Union[int, str], float] == Union[int, str, float]\n\n    - Unions of a single argument vanish, e.g.::\n\n        Union[int] == int  # The constructor actually returns int\n\n    - Redundant arguments are skipped, e.g.::\n\n        Union[int, str, int] == Union[int, str]\n\n    - When comparing unions, the argument order is ignored, e.g.::\n\n        Union[int, str] == Union[str, int]\n\n    - You cannot subclass or instantiate a union.\n    - You can use Optional[X] as a shorthand for Union[X, None].\n    \"\"\")\n\nOptional = _SpecialForm('Optional', doc=\n    \"\"\"Optional type.\n\n    Optional[X] is equivalent to Union[X, None].\n    \"\"\")\n\n\nclass ForwardRef(_Final, _root=True):\n    \"\"\"Internal wrapper to hold a forward reference.\"\"\"\n\n    __slots__ = ('__forward_arg__', '__forward_code__',\n                 '__forward_evaluated__', '__forward_value__',\n                 '__forward_is_argument__')\n\n    def __init__(self, arg, is_argument=True):\n        if not isinstance(arg, str):\n            raise TypeError(f\"Forward reference must be a string -- got {arg!r}\")\n        try:\n            code = compile(arg, '<string>', 'eval')\n        except SyntaxError:\n            raise SyntaxError(f\"Forward reference must be an expression -- got {arg!r}\")\n        self.__forward_arg__ = arg\n        self.__forward_code__ = code\n        self.__forward_evaluated__ = False\n        self.__forward_value__ = None\n        self.__forward_is_argument__ = is_argument\n\n    def _evaluate(self, globalns, localns):\n        if not self.__forward_evaluated__ or localns is not globalns:\n            if globalns is None and localns is None:\n                globalns = localns = {}\n            elif globalns is None:\n                globalns = localns\n            elif localns is None:\n                localns = globalns\n            self.__forward_value__ = _type_check(\n                eval(self.__forward_code__, globalns, localns),\n                \"Forward references must evaluate to types.\",\n                is_argument=self.__forward_is_argument__)\n            self.__forward_evaluated__ = True\n        return self.__forward_value__\n\n    def __eq__(self, other):\n        if not isinstance(other, ForwardRef):\n            return NotImplemented\n        if self.__forward_evaluated__ and other.__forward_evaluated__:\n            return (self.__forward_arg__ == other.__forward_arg__ and\n                    self.__forward_value__ == other.__forward_value__)\n        return self.__forward_arg__ == other.__forward_arg__\n\n    def __hash__(self):\n        return hash(self.__forward_arg__)\n\n    def __repr__(self):\n        return f'ForwardRef({self.__forward_arg__!r})'\n\n\nclass TypeVar(_Final, _Immutable, _root=True):\n    \"\"\"Type variable.\n\n    Usage::\n\n      T = TypeVar('T')  # Can be anything\n      A = TypeVar('A', str, bytes)  # Must be str or bytes\n\n    Type variables exist primarily for the benefit of static type\n    checkers.  They serve as the parameters for generic types as well\n    as for generic function definitions.  See class Generic for more\n    information on generic types.  Generic functions work as follows:\n\n      def repeat(x: T, n: int) -> List[T]:\n          '''Return a list containing n references to x.'''\n          return [x]*n\n\n      def longest(x: A, y: A) -> A:\n          '''Return the longest of two strings.'''\n          return x if len(x) >= len(y) else y\n\n    The latter example's signature is essentially the overloading\n    of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n    that if the arguments are instances of some subclass of str,\n    the return type is still plain str.\n\n    At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n\n    Type variables defined with covariant=True or contravariant=True\n    can be used to declare covariant or contravariant generic types.\n    See PEP 484 for more details. By default generic types are invariant\n    in all type variables.\n\n    Type variables can be introspected. e.g.:\n\n      T.__name__ == 'T'\n      T.__constraints__ == ()\n      T.__covariant__ == False\n      T.__contravariant__ = False\n      A.__constraints__ == (str, bytes)\n\n    Note that only type variables defined in global scope can be pickled.\n    \"\"\"\n\n    __slots__ = ('__name__', '__bound__', '__constraints__',\n                 '__covariant__', '__contravariant__')\n\n    def __init__(self, name, *constraints, bound=None,\n                 covariant=False, contravariant=False):\n        self.__name__ = name\n        if covariant and contravariant:\n            raise ValueError(\"Bivariant types are not supported.\")\n        self.__covariant__ = bool(covariant)\n        self.__contravariant__ = bool(contravariant)\n        if constraints and bound is not None:\n            raise TypeError(\"Constraints cannot be combined with bound=...\")\n        if constraints and len(constraints) == 1:\n            raise TypeError(\"A single constraint is not allowed\")\n        msg = \"TypeVar(name, constraint, ...): constraints must be types.\"\n        self.__constraints__ = tuple(_type_check(t, msg) for t in constraints)\n        if bound:\n            self.__bound__ = _type_check(bound, \"Bound must be a type.\")\n        else:\n            self.__bound__ = None\n        try:\n            def_mod = sys._getframe(1).f_globals.get('__name__', '__main__')  # for pickling\n        except (AttributeError, ValueError):\n            def_mod = None\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n    def __repr__(self):\n        if self.__covariant__:\n            prefix = '+'\n        elif self.__contravariant__:\n            prefix = '-'\n        else:\n            prefix = '~'\n        return prefix + self.__name__\n\n    def __reduce__(self):\n        return self.__name__\n\n\n# Special typing constructs Union, Optional, Generic, Callable and Tuple\n# use three special attributes for internal bookkeeping of generic types:\n# * __parameters__ is a tuple of unique free type parameters of a generic\n#   type, for example, Dict[T, T].__parameters__ == (T,);\n# * __origin__ keeps a reference to a type that was subscripted,\n#   e.g., Union[T, int].__origin__ == Union, or the non-generic version of\n#   the type.\n# * __args__ is a tuple of all arguments used in subscripting,\n#   e.g., Dict[T, int].__args__ == (T, int).\n\n\n# Mapping from non-generic type names that have a generic alias in typing\n# but with a different name.\n_normalize_alias = {'list': 'List',\n                    'tuple': 'Tuple',\n                    'dict': 'Dict',\n                    'set': 'Set',\n                    'frozenset': 'FrozenSet',\n                    'deque': 'Deque',\n                    'defaultdict': 'DefaultDict',\n                    'type': 'Type',\n                    'Set': 'AbstractSet'}\n\ndef _is_dunder(attr):\n    return attr.startswith('__') and attr.endswith('__')\n\n\nclass _GenericAlias(_Final, _root=True):\n    \"\"\"The central part of internal API.\n\n    This represents a generic version of type 'origin' with type arguments 'params'.\n    There are two kind of these aliases: user defined and special. The special ones\n    are wrappers around builtin collections and ABCs in collections.abc. These must\n    have 'name' always set. If 'inst' is False, then the alias can't be instantiated,\n    this is used by e.g. typing.List and typing.Dict.\n    \"\"\"\n    def __init__(self, origin, params, *, inst=True, special=False, name=None):\n        self._inst = inst\n        self._special = special\n        if special and name is None:\n            orig_name = origin.__name__\n            name = _normalize_alias.get(orig_name, orig_name)\n        self._name = name\n        if not isinstance(params, tuple):\n            params = (params,)\n        self.__origin__ = origin\n        self.__args__ = tuple(... if a is _TypingEllipsis else\n                              () if a is _TypingEmpty else\n                              a for a in params)\n        self.__parameters__ = _collect_type_vars(params)\n        self.__slots__ = None  # This is not documented.\n        if not name:\n            self.__module__ = origin.__module__\n\n    @_tp_cache\n    def __getitem__(self, params):\n        if self.__origin__ in (Generic, _Protocol):\n            # Can't subscript Generic[...] or _Protocol[...].\n            raise TypeError(f\"Cannot subscript already-subscripted {self}\")\n        if not isinstance(params, tuple):\n            params = (params,)\n        msg = \"Parameters to generic types must be types.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        _check_generic(self, params)\n        return _subs_tvars(self, self.__parameters__, params)\n\n    def copy_with(self, params):\n        # We don't copy self._special.\n        return _GenericAlias(self.__origin__, params, name=self._name, inst=self._inst)\n\n    def __repr__(self):\n        if (self._name != 'Callable' or\n                len(self.__args__) == 2 and self.__args__[0] is Ellipsis):\n            if self._name:\n                name = 'typing.' + self._name\n            else:\n                name = _type_repr(self.__origin__)\n            if not self._special:\n                args = f'[{\", \".join([_type_repr(a) for a in self.__args__])}]'\n            else:\n                args = ''\n            return (f'{name}{args}')\n        if self._special:\n            return 'typing.Callable'\n        return (f'typing.Callable'\n                f'[[{\", \".join([_type_repr(a) for a in self.__args__[:-1]])}], '\n                f'{_type_repr(self.__args__[-1])}]')\n\n    def __eq__(self, other):\n        if not isinstance(other, _GenericAlias):\n            return NotImplemented\n        if self.__origin__ != other.__origin__:\n            return False\n        if self.__origin__ is Union and other.__origin__ is Union:\n            return frozenset(self.__args__) == frozenset(other.__args__)\n        return self.__args__ == other.__args__\n\n    def __hash__(self):\n        if self.__origin__ is Union:\n            return hash((Union, frozenset(self.__args__)))\n        return hash((self.__origin__, self.__args__))\n\n    def __call__(self, *args, **kwargs):\n        if not self._inst:\n            raise TypeError(f\"Type {self._name} cannot be instantiated; \"\n                            f\"use {self._name.lower()}() instead\")\n        result = self.__origin__(*args, **kwargs)\n        try:\n            result.__orig_class__ = self\n        except AttributeError:\n            pass\n        return result\n\n    def __mro_entries__(self, bases):\n        if self._name:  # generic version of an ABC or built-in class\n            res = []\n            if self.__origin__ not in bases:\n                res.append(self.__origin__)\n            i = bases.index(self)\n            if not any(isinstance(b, _GenericAlias) or issubclass(b, Generic)\n                       for b in bases[i+1:]):\n                res.append(Generic)\n            return tuple(res)\n        if self.__origin__ is Generic:\n            i = bases.index(self)\n            for b in bases[i+1:]:\n                if isinstance(b, _GenericAlias) and b is not self:\n                    return ()\n        return (self.__origin__,)\n\n    def __getattr__(self, attr):\n        # We are careful for copy and pickle.\n        # Also for simplicity we just don't relay all dunder names\n        if '__origin__' in self.__dict__ and not _is_dunder(attr):\n            return getattr(self.__origin__, attr)\n        raise AttributeError(attr)\n\n    def __setattr__(self, attr, val):\n        if _is_dunder(attr) or attr in ('_name', '_inst', '_special'):\n            super().__setattr__(attr, val)\n        else:\n            setattr(self.__origin__, attr, val)\n\n    def __instancecheck__(self, obj):\n        return self.__subclasscheck__(type(obj))\n\n    def __subclasscheck__(self, cls):\n        if self._special:\n            if not isinstance(cls, _GenericAlias):\n                return issubclass(cls, self.__origin__)\n            if cls._special:\n                return issubclass(cls.__origin__, self.__origin__)\n        raise TypeError(\"Subscripted generics cannot be used with\"\n                        \" class and instance checks\")\n\n    def __reduce__(self):\n        if self._special:\n            return self._name\n\n        if self._name:\n            origin = globals()[self._name]\n        else:\n            origin = self.__origin__\n        if (origin is Callable and\n            not (len(self.__args__) == 2 and self.__args__[0] is Ellipsis)):\n            args = list(self.__args__[:-1]), self.__args__[-1]\n        else:\n            args = tuple(self.__args__)\n            if len(args) == 1 and not isinstance(args[0], tuple):\n                args, = args\n        return operator.getitem, (origin, args)\n\n\nclass _VariadicGenericAlias(_GenericAlias, _root=True):\n    \"\"\"Same as _GenericAlias above but for variadic aliases. Currently,\n    this is used only by special internal aliases: Tuple and Callable.\n    \"\"\"\n    def __getitem__(self, params):\n        if self._name != 'Callable' or not self._special:\n            return self.__getitem_inner__(params)\n        if not isinstance(params, tuple) or len(params) != 2:\n            raise TypeError(\"Callable must be used as \"\n                            \"Callable[[arg, ...], result].\")\n        args, result = params\n        if args is Ellipsis:\n            params = (Ellipsis, result)\n        else:\n            if not isinstance(args, list):\n                raise TypeError(f\"Callable[args, result]: args must be a list.\"\n                                f\" Got {args}\")\n            params = (tuple(args), result)\n        return self.__getitem_inner__(params)\n\n    @_tp_cache\n    def __getitem_inner__(self, params):\n        if self.__origin__ is tuple and self._special:\n            if params == ():\n                return self.copy_with((_TypingEmpty,))\n            if not isinstance(params, tuple):\n                params = (params,)\n            if len(params) == 2 and params[1] is ...:\n                msg = \"Tuple[t, ...]: t must be a type.\"\n                p = _type_check(params[0], msg)\n                return self.copy_with((p, _TypingEllipsis))\n            msg = \"Tuple[t0, t1, ...]: each t must be a type.\"\n            params = tuple(_type_check(p, msg) for p in params)\n            return self.copy_with(params)\n        if self.__origin__ is collections.abc.Callable and self._special:\n            args, result = params\n            msg = \"Callable[args, result]: result must be a type.\"\n            result = _type_check(result, msg)\n            if args is Ellipsis:\n                return self.copy_with((_TypingEllipsis, result))\n            msg = \"Callable[[arg, ...], result]: each arg must be a type.\"\n            args = tuple(_type_check(arg, msg) for arg in args)\n            params = args + (result,)\n            return self.copy_with(params)\n        return super().__getitem__(params)\n\n\nclass Generic:\n    \"\"\"Abstract base class for generic types.\n\n    A generic type is typically declared by inheriting from\n    this class parameterized with one or more type variables.\n    For example, a generic mapping type might be defined as::\n\n      class Mapping(Generic[KT, VT]):\n          def __getitem__(self, key: KT) -> VT:\n              ...\n          # Etc.\n\n    This class can then be used as follows::\n\n      def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT:\n          try:\n              return mapping[key]\n          except KeyError:\n              return default\n    \"\"\"\n    __slots__ = ()\n\n    def __new__(cls, *args, **kwds):\n        if cls is Generic:\n            raise TypeError(\"Type Generic cannot be instantiated; \"\n                            \"it can be used only as a base class\")\n        if super().__new__ is object.__new__ and cls.__init__ is not object.__init__:\n            obj = super().__new__(cls)\n        else:\n            obj = super().__new__(cls, *args, **kwds)\n        return obj\n\n    @_tp_cache\n    def __class_getitem__(cls, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        if not params and cls is not Tuple:\n            raise TypeError(\n                f\"Parameter list to {cls.__qualname__}[...] cannot be empty\")\n        msg = \"Parameters to generic types must be types.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        if cls is Generic:\n            # Generic can only be subscripted with unique type variables.\n            if not all(isinstance(p, TypeVar) for p in params):\n                raise TypeError(\n                    \"Parameters to Generic[...] must all be type variables\")\n            if len(set(params)) != len(params):\n                raise TypeError(\n                    \"Parameters to Generic[...] must all be unique\")\n        elif cls is _Protocol:\n            # _Protocol is internal at the moment, just skip the check\n            pass\n        else:\n            # Subscripting a regular Generic subclass.\n            _check_generic(cls, params)\n        return _GenericAlias(cls, params)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n        tvars = []\n        if '__orig_bases__' in cls.__dict__:\n            error = Generic in cls.__orig_bases__\n        else:\n            error = Generic in cls.__bases__ and cls.__name__ != '_Protocol'\n        if error:\n            raise TypeError(\"Cannot inherit from plain Generic\")\n        if '__orig_bases__' in cls.__dict__:\n            tvars = _collect_type_vars(cls.__orig_bases__)\n            # Look for Generic[T1, ..., Tn].\n            # If found, tvars must be a subset of it.\n            # If not found, tvars is it.\n            # Also check for and reject plain Generic,\n            # and reject multiple Generic[...].\n            gvars = None\n            for base in cls.__orig_bases__:\n                if (isinstance(base, _GenericAlias) and\n                        base.__origin__ is Generic):\n                    if gvars is not None:\n                        raise TypeError(\n                            \"Cannot inherit from Generic[...] multiple types.\")\n                    gvars = base.__parameters__\n            if gvars is None:\n                gvars = tvars\n            else:\n                tvarset = set(tvars)\n                gvarset = set(gvars)\n                if not tvarset <= gvarset:\n                    s_vars = ', '.join(str(t) for t in tvars if t not in gvarset)\n                    s_args = ', '.join(str(g) for g in gvars)\n                    raise TypeError(f\"Some type variables ({s_vars}) are\"\n                                    f\" not listed in Generic[{s_args}]\")\n                tvars = gvars\n        cls.__parameters__ = tuple(tvars)\n\n\nclass _TypingEmpty:\n    \"\"\"Internal placeholder for () or []. Used by TupleMeta and CallableMeta\n    to allow empty list/tuple in specific places, without allowing them\n    to sneak in where prohibited.\n    \"\"\"\n\n\nclass _TypingEllipsis:\n    \"\"\"Internal placeholder for ... (ellipsis).\"\"\"\n\n\ndef cast(typ, val):\n    \"\"\"Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    \"\"\"\n    return val\n\n\ndef _get_defaults(func):\n    \"\"\"Internal helper to extract the default arguments, by name.\"\"\"\n    try:\n        code = func.__code__\n    except AttributeError:\n        # Some built-in functions don't have __code__, __defaults__, etc.\n        return {}\n    pos_count = code.co_argcount\n    arg_names = code.co_varnames\n    arg_names = arg_names[:pos_count]\n    defaults = func.__defaults__ or ()\n    kwdefaults = func.__kwdefaults__\n    res = dict(kwdefaults) if kwdefaults else {}\n    pos_offset = pos_count - len(defaults)\n    for name, value in zip(arg_names[pos_offset:], defaults):\n        assert name not in res\n        res[name] = value\n    return res\n\n\n_allowed_types = (types.FunctionType, types.BuiltinFunctionType,\n                  types.MethodType, types.ModuleType,\n                  WrapperDescriptorType, MethodWrapperType, MethodDescriptorType)\n\n\ndef get_type_hints(obj, globalns=None, localns=None):\n    \"\"\"Return type hints for an object.\n\n    This is often the same as obj.__annotations__, but it handles\n    forward references encoded as string literals, and if necessary\n    adds Optional[t] if a default value equal to None is set.\n\n    The argument may be a module, class, method, or function. The annotations\n    are returned as a dictionary. For classes, annotations include also\n    inherited members.\n\n    TypeError is raised if the argument is not of a type that can contain\n    annotations, and an empty dictionary is returned if no annotations are\n    present.\n\n    BEWARE -- the behavior of globalns and localns is counterintuitive\n    (unless you are familiar with how eval() and exec() work).  The\n    search order is locals first, then globals.\n\n    - If no dict arguments are passed, an attempt is made to use the\n      globals from obj (or the respective module's globals for classes),\n      and these are also used as the locals.  If the object does not appear\n      to have globals, an empty dictionary is used.\n\n    - If one dict argument is passed, it is used for both globals and\n      locals.\n\n    - If two dict arguments are passed, they specify globals and\n      locals, respectively.\n    \"\"\"\n\n    if getattr(obj, '__no_type_check__', None):\n        return {}\n    # Classes require a special treatment.\n    if isinstance(obj, type):\n        hints = {}\n        for base in reversed(obj.__mro__):\n            if globalns is None:\n                base_globals = sys.modules[base.__module__].__dict__\n            else:\n                base_globals = globalns\n            ann = base.__dict__.get('__annotations__', {})\n            for name, value in ann.items():\n                if value is None:\n                    value = type(None)\n                if isinstance(value, str):\n                    value = ForwardRef(value, is_argument=False)\n                value = _eval_type(value, base_globals, localns)\n                hints[name] = value\n        return hints\n\n    if globalns is None:\n        if isinstance(obj, types.ModuleType):\n            globalns = obj.__dict__\n        else:\n            nsobj = obj\n            # Find globalns for the unwrapped object.\n            while hasattr(nsobj, '__wrapped__'):\n                nsobj = nsobj.__wrapped__\n            globalns = getattr(nsobj, '__globals__', {})\n        if localns is None:\n            localns = globalns\n    elif localns is None:\n        localns = globalns\n    hints = getattr(obj, '__annotations__', None)\n    if hints is None:\n        # Return empty annotations for something that _could_ have them.\n        if isinstance(obj, _allowed_types):\n            return {}\n        else:\n            raise TypeError('{!r} is not a module, class, method, '\n                            'or function.'.format(obj))\n    defaults = _get_defaults(obj)\n    hints = dict(hints)\n    for name, value in hints.items():\n        if value is None:\n            value = type(None)\n        if isinstance(value, str):\n            value = ForwardRef(value)\n        value = _eval_type(value, globalns, localns)\n        if name in defaults and defaults[name] is None:\n            value = Optional[value]\n        hints[name] = value\n    return hints\n\n\ndef no_type_check(arg):\n    \"\"\"Decorator to indicate that annotations are not type hints.\n\n    The argument must be a class or function; if it is a class, it\n    applies recursively to all methods and classes defined in that class\n    (but not to methods defined in its superclasses or subclasses).\n\n    This mutates the function(s) or class(es) in place.\n    \"\"\"\n    if isinstance(arg, type):\n        arg_attrs = arg.__dict__.copy()\n        for attr, val in arg.__dict__.items():\n            if val in arg.__bases__ + (arg,):\n                arg_attrs.pop(attr)\n        for obj in arg_attrs.values():\n            if isinstance(obj, types.FunctionType):\n                obj.__no_type_check__ = True\n            if isinstance(obj, type):\n                no_type_check(obj)\n    try:\n        arg.__no_type_check__ = True\n    except TypeError:  # built-in classes\n        pass\n    return arg\n\n\ndef no_type_check_decorator(decorator):\n    \"\"\"Decorator to give another decorator the @no_type_check effect.\n\n    This wraps the decorator with something that wraps the decorated\n    function in @no_type_check.\n    \"\"\"\n\n    @functools.wraps(decorator)\n    def wrapped_decorator(*args, **kwds):\n        func = decorator(*args, **kwds)\n        func = no_type_check(func)\n        return func\n\n    return wrapped_decorator\n\n\ndef _overload_dummy(*args, **kwds):\n    \"\"\"Helper for @overload to raise when called.\"\"\"\n    raise NotImplementedError(\n        \"You should not call an overloaded function. \"\n        \"A series of @overload-decorated functions \"\n        \"outside a stub module should always be followed \"\n        \"by an implementation that is not @overload-ed.\")\n\n\ndef overload(func):\n    \"\"\"Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n      def utf8(value):\n          # implementation goes here\n    \"\"\"\n    return _overload_dummy\n\n\nclass _ProtocolMeta(type):\n    \"\"\"Internal metaclass for _Protocol.\n\n    This exists so _Protocol classes can be generic without deriving\n    from Generic.\n    \"\"\"\n\n    def __instancecheck__(self, obj):\n        if _Protocol not in self.__bases__:\n            return super().__instancecheck__(obj)\n        raise TypeError(\"Protocols cannot be used with isinstance().\")\n\n    def __subclasscheck__(self, cls):\n        if not self._is_protocol:\n            # No structural checks since this isn't a protocol.\n            return NotImplemented\n\n        if self is _Protocol:\n            # Every class is a subclass of the empty protocol.\n            return True\n\n        # Find all attributes defined in the protocol.\n        attrs = self._get_protocol_attrs()\n\n        for attr in attrs:\n            if not any(attr in d.__dict__ for d in cls.__mro__):\n                return False\n        return True\n\n    def _get_protocol_attrs(self):\n        # Get all Protocol base classes.\n        protocol_bases = []\n        for c in self.__mro__:\n            if getattr(c, '_is_protocol', False) and c.__name__ != '_Protocol':\n                protocol_bases.append(c)\n\n        # Get attributes included in protocol.\n        attrs = set()\n        for base in protocol_bases:\n            for attr in base.__dict__.keys():\n                # Include attributes not defined in any non-protocol bases.\n                for c in self.__mro__:\n                    if (c is not base and attr in c.__dict__ and\n                            not getattr(c, '_is_protocol', False)):\n                        break\n                else:\n                    if (not attr.startswith('_abc_') and\n                            attr != '__abstractmethods__' and\n                            attr != '__annotations__' and\n                            attr != '__weakref__' and\n                            attr != '_is_protocol' and\n                            attr != '_gorg' and\n                            attr != '__dict__' and\n                            attr != '__args__' and\n                            attr != '__slots__' and\n                            attr != '_get_protocol_attrs' and\n                            attr != '__next_in_mro__' and\n                            attr != '__parameters__' and\n                            attr != '__origin__' and\n                            attr != '__orig_bases__' and\n                            attr != '__extra__' and\n                            attr != '__tree_hash__' and\n                            attr != '__module__'):\n                        attrs.add(attr)\n\n        return attrs\n\n\nclass _Protocol(Generic, metaclass=_ProtocolMeta):\n    \"\"\"Internal base class for protocol classes.\n\n    This implements a simple-minded structural issubclass check\n    (similar but more general than the one-offs in collections.abc\n    such as Hashable).\n    \"\"\"\n\n    __slots__ = ()\n\n    _is_protocol = True\n\n    def __class_getitem__(cls, params):\n        return super().__class_getitem__(params)\n\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = TypeVar('T')  # Any type.\nKT = TypeVar('KT')  # Key type.\nVT = TypeVar('VT')  # Value type.\nT_co = TypeVar('T_co', covariant=True)  # Any type covariant containers.\nV_co = TypeVar('V_co', covariant=True)  # Any type covariant containers.\nVT_co = TypeVar('VT_co', covariant=True)  # Value type covariant containers.\nT_contra = TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n# Internal type variable used for Type[].\nCT_co = TypeVar('CT_co', covariant=True, bound=type)\n\n# A useful type variable with constraints.  This represents string types.\n# (This one *is* for export!)\nAnyStr = TypeVar('AnyStr', bytes, str)\n\n\n# Various ABCs mimicking those in collections.abc.\ndef _alias(origin, params, inst=True):\n    return _GenericAlias(origin, params, special=True, inst=inst)\n\nHashable = _alias(collections.abc.Hashable, ())  # Not generic.\nAwaitable = _alias(collections.abc.Awaitable, T_co)\nCoroutine = _alias(collections.abc.Coroutine, (T_co, T_contra, V_co))\nAsyncIterable = _alias(collections.abc.AsyncIterable, T_co)\nAsyncIterator = _alias(collections.abc.AsyncIterator, T_co)\nIterable = _alias(collections.abc.Iterable, T_co)\nIterator = _alias(collections.abc.Iterator, T_co)\nReversible = _alias(collections.abc.Reversible, T_co)\nSized = _alias(collections.abc.Sized, ())  # Not generic.\nContainer = _alias(collections.abc.Container, T_co)\nCollection = _alias(collections.abc.Collection, T_co)\nCallable = _VariadicGenericAlias(collections.abc.Callable, (), special=True)\nCallable.__doc__ = \\\n    \"\"\"Callable type; Callable[[int], str] is a function of (int) -> str.\n\n    The subscription syntax must always be used with exactly two\n    values: the argument list and the return type.  The argument list\n    must be a list of types or ellipsis; the return type must be a single type.\n\n    There is no syntax to indicate optional or keyword arguments,\n    such function types are rarely used as callback types.\n    \"\"\"\nAbstractSet = _alias(collections.abc.Set, T_co)\nMutableSet = _alias(collections.abc.MutableSet, T)\n# NOTE: Mapping is only covariant in the value type.\nMapping = _alias(collections.abc.Mapping, (KT, VT_co))\nMutableMapping = _alias(collections.abc.MutableMapping, (KT, VT))\nSequence = _alias(collections.abc.Sequence, T_co)\nMutableSequence = _alias(collections.abc.MutableSequence, T)\nByteString = _alias(collections.abc.ByteString, ())  # Not generic\nTuple = _VariadicGenericAlias(tuple, (), inst=False, special=True)\nTuple.__doc__ = \\\n    \"\"\"Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n\n    Example: Tuple[T1, T2] is a tuple of two elements corresponding\n    to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n    of an int, a float and a string.\n\n    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n    \"\"\"\nList = _alias(list, T, inst=False)\nDeque = _alias(collections.deque, T)\nSet = _alias(set, T, inst=False)\nFrozenSet = _alias(frozenset, T_co, inst=False)\nMappingView = _alias(collections.abc.MappingView, T_co)\nKeysView = _alias(collections.abc.KeysView, KT)\nItemsView = _alias(collections.abc.ItemsView, (KT, VT_co))\nValuesView = _alias(collections.abc.ValuesView, VT_co)\nContextManager = _alias(contextlib.AbstractContextManager, T_co)\nAsyncContextManager = _alias(contextlib.AbstractAsyncContextManager, T_co)\nDict = _alias(dict, (KT, VT), inst=False)\nDefaultDict = _alias(collections.defaultdict, (KT, VT))\nOrderedDict = _alias(collections.OrderedDict, (KT, VT))\nCounter = _alias(collections.Counter, T)\nChainMap = _alias(collections.ChainMap, (KT, VT))\nGenerator = _alias(collections.abc.Generator, (T_co, T_contra, V_co))\nAsyncGenerator = _alias(collections.abc.AsyncGenerator, (T_co, T_contra))\nType = _alias(type, CT_co, inst=False)\nType.__doc__ = \\\n    \"\"\"A special construct usable to annotate class objects.\n\n    For example, suppose we have the following classes::\n\n      class User: ...  # Abstract base for User classes\n      class BasicUser(User): ...\n      class ProUser(User): ...\n      class TeamUser(User): ...\n\n    And a function that takes a class argument that's a subclass of\n    User and returns an instance of the corresponding class::\n\n      U = TypeVar('U', bound=User)\n      def new_user(user_class: Type[U]) -> U:\n          user = user_class()\n          # (Here we could write the user object to a database)\n          return user\n\n      joe = new_user(BasicUser)\n\n    At this point the type checker knows that joe has type BasicUser.\n    \"\"\"\n\n\nclass SupportsInt(_Protocol):\n    \"\"\"An ABC with one abstract method __int__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __int__(self) -> int:\n        pass\n\n\nclass SupportsFloat(_Protocol):\n    \"\"\"An ABC with one abstract method __float__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __float__(self) -> float:\n        pass\n\n\nclass SupportsComplex(_Protocol):\n    \"\"\"An ABC with one abstract method __complex__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __complex__(self) -> complex:\n        pass\n\n\nclass SupportsBytes(_Protocol):\n    \"\"\"An ABC with one abstract method __bytes__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __bytes__(self) -> bytes:\n        pass\n\n\nclass SupportsAbs(_Protocol[T_co]):\n    \"\"\"An ABC with one abstract method __abs__ that is covariant in its return type.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __abs__(self) -> T_co:\n        pass\n\n\nclass SupportsRound(_Protocol[T_co]):\n    \"\"\"An ABC with one abstract method __round__ that is covariant in its return type.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __round__(self, ndigits: int = 0) -> T_co:\n        pass\n\n\ndef _make_nmtuple(name, types):\n    msg = \"NamedTuple('Name', [(f0, t0), (f1, t1), ...]); each t must be a type\"\n    types = [(n, _type_check(t, msg)) for n, t in types]\n    nm_tpl = collections.namedtuple(name, [n for n, t in types])\n    # Prior to PEP 526, only _field_types attribute was assigned.\n    # Now, both __annotations__ and _field_types are used to maintain compatibility.\n    nm_tpl.__annotations__ = nm_tpl._field_types = collections.OrderedDict(types)\n    try:\n        nm_tpl.__module__ = sys._getframe(2).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        pass\n    return nm_tpl\n\n\n# attributes prohibited to set in NamedTuple class syntax\n_prohibited = ('__new__', '__init__', '__slots__', '__getnewargs__',\n               '_fields', '_field_defaults', '_field_types',\n               '_make', '_replace', '_asdict', '_source')\n\n_special = ('__module__', '__name__', '__annotations__')\n\n\nclass NamedTupleMeta(type):\n\n    def __new__(cls, typename, bases, ns):\n        if ns.get('_root', False):\n            return super().__new__(cls, typename, bases, ns)\n        types = ns.get('__annotations__', {})\n        nm_tpl = _make_nmtuple(typename, types.items())\n        defaults = []\n        defaults_dict = {}\n        for field_name in types:\n            if field_name in ns:\n                default_value = ns[field_name]\n                defaults.append(default_value)\n                defaults_dict[field_name] = default_value\n            elif defaults:\n                raise TypeError(\"Non-default namedtuple field {field_name} cannot \"\n                                \"follow default field(s) {default_names}\"\n                                .format(field_name=field_name,\n                                        default_names=', '.join(defaults_dict.keys())))\n        nm_tpl.__new__.__annotations__ = collections.OrderedDict(types)\n        nm_tpl.__new__.__defaults__ = tuple(defaults)\n        nm_tpl._field_defaults = defaults_dict\n        # update from user namespace without overriding special namedtuple attributes\n        for key in ns:\n            if key in _prohibited:\n                raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n            elif key not in _special and key not in nm_tpl._fields:\n                setattr(nm_tpl, key, ns[key])\n        return nm_tpl\n\n\nclass NamedTuple(metaclass=NamedTupleMeta):\n    \"\"\"Typed version of namedtuple.\n\n    Usage in Python versions >= 3.6::\n\n        class Employee(NamedTuple):\n            name: str\n            id: int\n\n    This is equivalent to::\n\n        Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n    The resulting class has extra __annotations__ and _field_types\n    attributes, giving an ordered dict mapping field names to types.\n    __annotations__ should be preferred, while _field_types\n    is kept to maintain pre PEP 526 compatibility. (The field names\n    are in the _fields attribute, which is part of the namedtuple\n    API.) Alternative equivalent keyword syntax is also accepted::\n\n        Employee = NamedTuple('Employee', name=str, id=int)\n\n    In Python versions <= 3.5 use::\n\n        Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n    \"\"\"\n    _root = True\n\n    def __new__(*args, **kwargs):\n        if not args:\n            raise TypeError('NamedTuple.__new__(): not enough arguments')\n        cls, *args = args  # allow the \"cls\" keyword be passed\n        if args:\n            typename, *args = args # allow the \"typename\" keyword be passed\n        elif 'typename' in kwargs:\n            typename = kwargs.pop('typename')\n        else:\n            raise TypeError(\"NamedTuple.__new__() missing 1 required positional \"\n                            \"argument: 'typename'\")\n        if args:\n            try:\n                fields, = args # allow the \"fields\" keyword be passed\n            except ValueError:\n                raise TypeError(f'NamedTuple.__new__() takes from 2 to 3 '\n                                f'positional arguments but {len(args) + 2} '\n                                f'were given') from None\n        elif 'fields' in kwargs and len(kwargs) == 1:\n            fields = kwargs.pop('fields')\n        else:\n            fields = None\n\n        if fields is None:\n            fields = kwargs.items()\n        elif kwargs:\n            raise TypeError(\"Either list of fields or keywords\"\n                            \" can be provided to NamedTuple, not both\")\n        return _make_nmtuple(typename, fields)\n    __new__.__text_signature__ = '($cls, typename, fields=None, /, **kwargs)'\n\n\ndef NewType(name, tp):\n    \"\"\"NewType creates simple unique types with almost zero\n    runtime overhead. NewType(name, tp) is considered a subtype of tp\n    by static type checkers. At runtime, NewType(name, tp) returns\n    a dummy function that simply returns its argument. Usage::\n\n        UserId = NewType('UserId', int)\n\n        def name_by_id(user_id: UserId) -> str:\n            ...\n\n        UserId('user')          # Fails type check\n\n        name_by_id(42)          # Fails type check\n        name_by_id(UserId(42))  # OK\n\n        num = UserId(5) + 1     # type: int\n    \"\"\"\n\n    def new_type(x):\n        return x\n\n    new_type.__name__ = name\n    new_type.__supertype__ = tp\n    return new_type\n\n\n# Python-version-specific alias (Python 2: unicode; Python 3: str)\nText = str\n\n\n# Constant that's True when type checking, but False here.\nTYPE_CHECKING = False\n\n\nclass IO(Generic[AnyStr]):\n    \"\"\"Generic base class for TextIO and BinaryIO.\n\n    This is an abstract, generic version of the return of open().\n\n    NOTE: This does not distinguish between the different possible\n    classes (text vs. binary, read vs. write vs. read/write,\n    append-only, unbuffered).  The TextIO and BinaryIO subclasses\n    below capture the distinctions between text vs. binary, which is\n    pervasive in the interface; however we currently do not offer a\n    way to track the other distinctions in the type system.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractproperty\n    def mode(self) -> str:\n        pass\n\n    @abstractproperty\n    def name(self) -> str:\n        pass\n\n    @abstractmethod\n    def close(self) -> None:\n        pass\n\n    @abstractproperty\n    def closed(self) -> bool:\n        pass\n\n    @abstractmethod\n    def fileno(self) -> int:\n        pass\n\n    @abstractmethod\n    def flush(self) -> None:\n        pass\n\n    @abstractmethod\n    def isatty(self) -> bool:\n        pass\n\n    @abstractmethod\n    def read(self, n: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def readline(self, limit: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readlines(self, hint: int = -1) -> List[AnyStr]:\n        pass\n\n    @abstractmethod\n    def seek(self, offset: int, whence: int = 0) -> int:\n        pass\n\n    @abstractmethod\n    def seekable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def tell(self) -> int:\n        pass\n\n    @abstractmethod\n    def truncate(self, size: int = None) -> int:\n        pass\n\n    @abstractmethod\n    def writable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def write(self, s: AnyStr) -> int:\n        pass\n\n    @abstractmethod\n    def writelines(self, lines: List[AnyStr]) -> None:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'IO[AnyStr]':\n        pass\n\n    @abstractmethod\n    def __exit__(self, type, value, traceback) -> None:\n        pass\n\n\nclass BinaryIO(IO[bytes]):\n    \"\"\"Typed version of the return of open() in binary mode.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def write(self, s: Union[bytes, bytearray]) -> int:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'BinaryIO':\n        pass\n\n\nclass TextIO(IO[str]):\n    \"\"\"Typed version of the return of open() in text mode.\"\"\"\n\n    __slots__ = ()\n\n    @abstractproperty\n    def buffer(self) -> BinaryIO:\n        pass\n\n    @abstractproperty\n    def encoding(self) -> str:\n        pass\n\n    @abstractproperty\n    def errors(self) -> Optional[str]:\n        pass\n\n    @abstractproperty\n    def line_buffering(self) -> bool:\n        pass\n\n    @abstractproperty\n    def newlines(self) -> Any:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'TextIO':\n        pass\n\n\nclass io:\n    \"\"\"Wrapper namespace for IO generic classes.\"\"\"\n\n    __all__ = ['IO', 'TextIO', 'BinaryIO']\n    IO = IO\n    TextIO = TextIO\n    BinaryIO = BinaryIO\n\n\nio.__name__ = __name__ + '.io'\nsys.modules[io.__name__] = io\n\nPattern = _alias(stdlib_re.Pattern, AnyStr)\nMatch = _alias(stdlib_re.Match, AnyStr)\n\nclass re:\n    \"\"\"Wrapper namespace for re type aliases.\"\"\"\n\n    __all__ = ['Pattern', 'Match']\n    Pattern = Pattern\n    Match = Match\n\n\nre.__name__ = __name__ + '.re'\nsys.modules[re.__name__] = re\n", 1654], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py": ["import json\nimport typing\nfrom http import cookies as http_cookies\n\nimport anyio\n\nfrom starlette._utils import AwaitableOrContextManager, AwaitableOrContextManagerWrapper\nfrom starlette.datastructures import URL, Address, FormData, Headers, QueryParams, State\nfrom starlette.exceptions import HTTPException\nfrom starlette.formparsers import FormParser, MultiPartException, MultiPartParser\nfrom starlette.types import Message, Receive, Scope, Send\n\ntry:\n    from multipart.multipart import parse_options_header\nexcept ImportError:  # pragma: nocover\n    parse_options_header = None\n\n\nif typing.TYPE_CHECKING:\n    from starlette.routing import Router\n\n\nSERVER_PUSH_HEADERS_TO_COPY = {\n    \"accept\",\n    \"accept-encoding\",\n    \"accept-language\",\n    \"cache-control\",\n    \"user-agent\",\n}\n\n\ndef cookie_parser(cookie_string: str) -> typing.Dict[str, str]:\n    \"\"\"\n    This function parses a ``Cookie`` HTTP header into a dict of key/value pairs.\n\n    It attempts to mimic browser cookie parsing behavior: browsers and web servers\n    frequently disregard the spec (RFC 6265) when setting and reading cookies,\n    so we attempt to suit the common scenarios here.\n\n    This function has been adapted from Django 3.1.0.\n    Note: we are explicitly _NOT_ using `SimpleCookie.load` because it is based\n    on an outdated spec and will fail on lots of input we want to support\n    \"\"\"\n    cookie_dict: typing.Dict[str, str] = {}\n    for chunk in cookie_string.split(\";\"):\n        if \"=\" in chunk:\n            key, val = chunk.split(\"=\", 1)\n        else:\n            # Assume an empty name per\n            # https://bugzilla.mozilla.org/show_bug.cgi?id=169091\n            key, val = \"\", chunk\n        key, val = key.strip(), val.strip()\n        if key or val:\n            # unquote using Python's algorithm.\n            cookie_dict[key] = http_cookies._unquote(val)\n    return cookie_dict\n\n\nclass ClientDisconnect(Exception):\n    pass\n\n\nclass HTTPConnection(typing.Mapping[str, typing.Any]):\n    \"\"\"\n    A base class for incoming HTTP connections, that is used to provide\n    any functionality that is common to both `Request` and `WebSocket`.\n    \"\"\"\n\n    def __init__(self, scope: Scope, receive: typing.Optional[Receive] = None) -> None:\n        assert scope[\"type\"] in (\"http\", \"websocket\")\n        self.scope = scope\n\n    def __getitem__(self, key: str) -> typing.Any:\n        return self.scope[key]\n\n    def __iter__(self) -> typing.Iterator[str]:\n        return iter(self.scope)\n\n    def __len__(self) -> int:\n        return len(self.scope)\n\n    # Don't use the `abc.Mapping.__eq__` implementation.\n    # Connection instances should never be considered equal\n    # unless `self is other`.\n    __eq__ = object.__eq__\n    __hash__ = object.__hash__\n\n    @property\n    def app(self) -> typing.Any:\n        return self.scope[\"app\"]\n\n    @property\n    def url(self) -> URL:\n        if not hasattr(self, \"_url\"):\n            self._url = URL(scope=self.scope)\n        return self._url\n\n    @property\n    def base_url(self) -> URL:\n        if not hasattr(self, \"_base_url\"):\n            base_url_scope = dict(self.scope)\n            base_url_scope[\"path\"] = \"/\"\n            base_url_scope[\"query_string\"] = b\"\"\n            base_url_scope[\"root_path\"] = base_url_scope.get(\n                \"app_root_path\", base_url_scope.get(\"root_path\", \"\")\n            )\n            self._base_url = URL(scope=base_url_scope)\n        return self._base_url\n\n    @property\n    def headers(self) -> Headers:\n        if not hasattr(self, \"_headers\"):\n            self._headers = Headers(scope=self.scope)\n        return self._headers\n\n    @property\n    def query_params(self) -> QueryParams:\n        if not hasattr(self, \"_query_params\"):\n            self._query_params = QueryParams(self.scope[\"query_string\"])\n        return self._query_params\n\n    @property\n    def path_params(self) -> typing.Dict[str, typing.Any]:\n        return self.scope.get(\"path_params\", {})\n\n    @property\n    def cookies(self) -> typing.Dict[str, str]:\n        if not hasattr(self, \"_cookies\"):\n            cookies: typing.Dict[str, str] = {}\n            cookie_header = self.headers.get(\"cookie\")\n\n            if cookie_header:\n                cookies = cookie_parser(cookie_header)\n            self._cookies = cookies\n        return self._cookies\n\n    @property\n    def client(self) -> typing.Optional[Address]:\n        # client is a 2 item tuple of (host, port), None or missing\n        host_port = self.scope.get(\"client\")\n        if host_port is not None:\n            return Address(*host_port)\n        return None\n\n    @property\n    def session(self) -> typing.Dict[str, typing.Any]:\n        assert (\n            \"session\" in self.scope\n        ), \"SessionMiddleware must be installed to access request.session\"\n        return self.scope[\"session\"]\n\n    @property\n    def auth(self) -> typing.Any:\n        assert (\n            \"auth\" in self.scope\n        ), \"AuthenticationMiddleware must be installed to access request.auth\"\n        return self.scope[\"auth\"]\n\n    @property\n    def user(self) -> typing.Any:\n        assert (\n            \"user\" in self.scope\n        ), \"AuthenticationMiddleware must be installed to access request.user\"\n        return self.scope[\"user\"]\n\n    @property\n    def state(self) -> State:\n        if not hasattr(self, \"_state\"):\n            # Ensure 'state' has an empty dict if it's not already populated.\n            self.scope.setdefault(\"state\", {})\n            # Create a state instance with a reference to the dict in which it should\n            # store info\n            self._state = State(self.scope[\"state\"])\n        return self._state\n\n    def url_for(self, name: str, **path_params: typing.Any) -> str:\n        router: Router = self.scope[\"router\"]\n        url_path = router.url_path_for(name, **path_params)\n        return url_path.make_absolute_url(base_url=self.base_url)\n\n\nasync def empty_receive() -> typing.NoReturn:\n    raise RuntimeError(\"Receive channel has not been made available\")\n\n\nasync def empty_send(message: Message) -> typing.NoReturn:\n    raise RuntimeError(\"Send channel has not been made available\")\n\n\nclass Request(HTTPConnection):\n    _form: typing.Optional[FormData]\n\n    def __init__(\n        self, scope: Scope, receive: Receive = empty_receive, send: Send = empty_send\n    ):\n        super().__init__(scope)\n        assert scope[\"type\"] == \"http\"\n        self._receive = receive\n        self._send = send\n        self._stream_consumed = False\n        self._is_disconnected = False\n        self._form = None\n\n    @property\n    def method(self) -> str:\n        return self.scope[\"method\"]\n\n    @property\n    def receive(self) -> Receive:\n        return self._receive\n\n    async def stream(self) -> typing.AsyncGenerator[bytes, None]:\n        if hasattr(self, \"_body\"):\n            yield self._body\n            yield b\"\"\n            return\n        if self._stream_consumed:\n            raise RuntimeError(\"Stream consumed\")\n        self._stream_consumed = True\n        while True:\n            message = await self._receive()\n            if message[\"type\"] == \"http.request\":\n                body = message.get(\"body\", b\"\")\n                if body:\n                    yield body\n                if not message.get(\"more_body\", False):\n                    break\n            elif message[\"type\"] == \"http.disconnect\":\n                self._is_disconnected = True\n                raise ClientDisconnect()\n        yield b\"\"\n\n    async def body(self) -> bytes:\n        if not hasattr(self, \"_body\"):\n            chunks: \"typing.List[bytes]\" = []\n            async for chunk in self.stream():\n                chunks.append(chunk)\n            self._body = b\"\".join(chunks)\n        return self._body\n\n    async def json(self) -> typing.Any:\n        if not hasattr(self, \"_json\"):\n            body = await self.body()\n            self._json = json.loads(body)\n        return self._json\n\n    async def _get_form(\n        self,\n        *,\n        max_files: typing.Union[int, float] = 1000,\n        max_fields: typing.Union[int, float] = 1000,\n    ) -> FormData:\n        if self._form is None:\n            assert (\n                parse_options_header is not None\n            ), \"The `python-multipart` library must be installed to use form parsing.\"\n            content_type_header = self.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header)\n            if content_type == b\"multipart/form-data\":\n                try:\n                    multipart_parser = MultiPartParser(\n                        self.headers,\n                        self.stream(),\n                        max_files=max_files,\n                        max_fields=max_fields,\n                    )\n                    self._form = await multipart_parser.parse()\n                except MultiPartException as exc:\n                    if \"app\" in self.scope:\n                        raise HTTPException(status_code=400, detail=exc.message)\n                    raise exc\n            elif content_type == b\"application/x-www-form-urlencoded\":\n                form_parser = FormParser(self.headers, self.stream())\n                self._form = await form_parser.parse()\n            else:\n                self._form = FormData()\n        return self._form\n\n    def form(\n        self,\n        *,\n        max_files: typing.Union[int, float] = 1000,\n        max_fields: typing.Union[int, float] = 1000,\n    ) -> AwaitableOrContextManager[FormData]:\n        return AwaitableOrContextManagerWrapper(\n            self._get_form(max_files=max_files, max_fields=max_fields)\n        )\n\n    async def close(self) -> None:\n        if self._form is not None:\n            await self._form.close()\n\n    async def is_disconnected(self) -> bool:\n        if not self._is_disconnected:\n            message: Message = {}\n\n            # If message isn't immediately available, move on\n            with anyio.CancelScope() as cs:\n                cs.cancel()\n                message = await self._receive()\n\n            if message.get(\"type\") == \"http.disconnect\":\n                self._is_disconnected = True\n\n        return self._is_disconnected\n\n    async def send_push_promise(self, path: str) -> None:\n        if \"http.response.push\" in self.scope.get(\"extensions\", {}):\n            raw_headers: \"typing.List[typing.Tuple[bytes, bytes]]\" = []\n            for name in SERVER_PUSH_HEADERS_TO_COPY:\n                for value in self.headers.getlist(name):\n                    raw_headers.append(\n                        (name.encode(\"latin-1\"), value.encode(\"latin-1\"))\n                    )\n            await self._send(\n                {\"type\": \"http.response.push\", \"path\": path, \"headers\": raw_headers}\n            )\n", 318], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py": ["import http.cookies\nimport json\nimport os\nimport stat\nimport sys\nimport typing\nfrom datetime import datetime\nfrom email.utils import format_datetime, formatdate\nfrom functools import partial\nfrom mimetypes import guess_type as mimetypes_guess_type\nfrom urllib.parse import quote\n\nimport anyio\n\nfrom starlette._compat import md5_hexdigest\nfrom starlette.background import BackgroundTask\nfrom starlette.concurrency import iterate_in_threadpool\nfrom starlette.datastructures import URL, MutableHeaders\nfrom starlette.types import Receive, Scope, Send\n\nif sys.version_info >= (3, 8):  # pragma: no cover\n    from typing import Literal\nelse:  # pragma: no cover\n    from typing_extensions import Literal\n\n# Workaround for adding samesite support to pre 3.8 python\nhttp.cookies.Morsel._reserved[\"samesite\"] = \"SameSite\"  # type: ignore[attr-defined]\n\n\n# Compatibility wrapper for `mimetypes.guess_type` to support `os.PathLike` on <py3.8\ndef guess_type(\n    url: typing.Union[str, \"os.PathLike[str]\"], strict: bool = True\n) -> typing.Tuple[typing.Optional[str], typing.Optional[str]]:\n    if sys.version_info < (3, 8):  # pragma: no cover\n        url = os.fspath(url)\n    return mimetypes_guess_type(url, strict)\n\n\nclass Response:\n    media_type = None\n    charset = \"utf-8\"\n\n    def __init__(\n        self,\n        content: typing.Any = None,\n        status_code: int = 200,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        media_type: typing.Optional[str] = None,\n        background: typing.Optional[BackgroundTask] = None,\n    ) -> None:\n        self.status_code = status_code\n        if media_type is not None:\n            self.media_type = media_type\n        self.background = background\n        self.body = self.render(content)\n        self.init_headers(headers)\n\n    def render(self, content: typing.Any) -> bytes:\n        if content is None:\n            return b\"\"\n        if isinstance(content, bytes):\n            return content\n        return content.encode(self.charset)\n\n    def init_headers(\n        self, headers: typing.Optional[typing.Mapping[str, str]] = None\n    ) -> None:\n        if headers is None:\n            raw_headers: typing.List[typing.Tuple[bytes, bytes]] = []\n            populate_content_length = True\n            populate_content_type = True\n        else:\n            raw_headers = [\n                (k.lower().encode(\"latin-1\"), v.encode(\"latin-1\"))\n                for k, v in headers.items()\n            ]\n            keys = [h[0] for h in raw_headers]\n            populate_content_length = b\"content-length\" not in keys\n            populate_content_type = b\"content-type\" not in keys\n\n        body = getattr(self, \"body\", None)\n        if (\n            body is not None\n            and populate_content_length\n            and not (self.status_code < 200 or self.status_code in (204, 304))\n        ):\n            content_length = str(len(body))\n            raw_headers.append((b\"content-length\", content_length.encode(\"latin-1\")))\n\n        content_type = self.media_type\n        if content_type is not None and populate_content_type:\n            if content_type.startswith(\"text/\"):\n                content_type += \"; charset=\" + self.charset\n            raw_headers.append((b\"content-type\", content_type.encode(\"latin-1\")))\n\n        self.raw_headers = raw_headers\n\n    @property\n    def headers(self) -> MutableHeaders:\n        if not hasattr(self, \"_headers\"):\n            self._headers = MutableHeaders(raw=self.raw_headers)\n        return self._headers\n\n    def set_cookie(\n        self,\n        key: str,\n        value: str = \"\",\n        max_age: typing.Optional[int] = None,\n        expires: typing.Optional[typing.Union[datetime, str, int]] = None,\n        path: str = \"/\",\n        domain: typing.Optional[str] = None,\n        secure: bool = False,\n        httponly: bool = False,\n        samesite: typing.Optional[Literal[\"lax\", \"strict\", \"none\"]] = \"lax\",\n    ) -> None:\n        cookie: \"http.cookies.BaseCookie[str]\" = http.cookies.SimpleCookie()\n        cookie[key] = value\n        if max_age is not None:\n            cookie[key][\"max-age\"] = max_age\n        if expires is not None:\n            if isinstance(expires, datetime):\n                cookie[key][\"expires\"] = format_datetime(expires, usegmt=True)\n            else:\n                cookie[key][\"expires\"] = expires\n        if path is not None:\n            cookie[key][\"path\"] = path\n        if domain is not None:\n            cookie[key][\"domain\"] = domain\n        if secure:\n            cookie[key][\"secure\"] = True\n        if httponly:\n            cookie[key][\"httponly\"] = True\n        if samesite is not None:\n            assert samesite.lower() in [\n                \"strict\",\n                \"lax\",\n                \"none\",\n            ], \"samesite must be either 'strict', 'lax' or 'none'\"\n            cookie[key][\"samesite\"] = samesite\n        cookie_val = cookie.output(header=\"\").strip()\n        self.raw_headers.append((b\"set-cookie\", cookie_val.encode(\"latin-1\")))\n\n    def delete_cookie(\n        self,\n        key: str,\n        path: str = \"/\",\n        domain: typing.Optional[str] = None,\n        secure: bool = False,\n        httponly: bool = False,\n        samesite: typing.Optional[Literal[\"lax\", \"strict\", \"none\"]] = \"lax\",\n    ) -> None:\n        self.set_cookie(\n            key,\n            max_age=0,\n            expires=0,\n            path=path,\n            domain=domain,\n            secure=secure,\n            httponly=httponly,\n            samesite=samesite,\n        )\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status_code,\n                \"headers\": self.raw_headers,\n            }\n        )\n        await send({\"type\": \"http.response.body\", \"body\": self.body})\n\n        if self.background is not None:\n            await self.background()\n\n\nclass HTMLResponse(Response):\n    media_type = \"text/html\"\n\n\nclass PlainTextResponse(Response):\n    media_type = \"text/plain\"\n\n\nclass JSONResponse(Response):\n    media_type = \"application/json\"\n\n    def __init__(\n        self,\n        content: typing.Any,\n        status_code: int = 200,\n        headers: typing.Optional[typing.Dict[str, str]] = None,\n        media_type: typing.Optional[str] = None,\n        background: typing.Optional[BackgroundTask] = None,\n    ) -> None:\n        super().__init__(content, status_code, headers, media_type, background)\n\n    def render(self, content: typing.Any) -> bytes:\n        return json.dumps(\n            content,\n            ensure_ascii=False,\n            allow_nan=False,\n            indent=None,\n            separators=(\",\", \":\"),\n        ).encode(\"utf-8\")\n\n\nclass RedirectResponse(Response):\n    def __init__(\n        self,\n        url: typing.Union[str, URL],\n        status_code: int = 307,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        background: typing.Optional[BackgroundTask] = None,\n    ) -> None:\n        super().__init__(\n            content=b\"\", status_code=status_code, headers=headers, background=background\n        )\n        self.headers[\"location\"] = quote(str(url), safe=\":/%#?=@[]!$&'()*+,;\")\n\n\nContent = typing.Union[str, bytes]\nSyncContentStream = typing.Iterator[Content]\nAsyncContentStream = typing.AsyncIterable[Content]\nContentStream = typing.Union[AsyncContentStream, SyncContentStream]\n\n\nclass StreamingResponse(Response):\n    body_iterator: AsyncContentStream\n\n    def __init__(\n        self,\n        content: ContentStream,\n        status_code: int = 200,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        media_type: typing.Optional[str] = None,\n        background: typing.Optional[BackgroundTask] = None,\n    ) -> None:\n        if isinstance(content, typing.AsyncIterable):\n            self.body_iterator = content\n        else:\n            self.body_iterator = iterate_in_threadpool(content)\n        self.status_code = status_code\n        self.media_type = self.media_type if media_type is None else media_type\n        self.background = background\n        self.init_headers(headers)\n\n    async def listen_for_disconnect(self, receive: Receive) -> None:\n        while True:\n            message = await receive()\n            if message[\"type\"] == \"http.disconnect\":\n                break\n\n    async def stream_response(self, send: Send) -> None:\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status_code,\n                \"headers\": self.raw_headers,\n            }\n        )\n        async for chunk in self.body_iterator:\n            if not isinstance(chunk, bytes):\n                chunk = chunk.encode(self.charset)\n            await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": True})\n\n        await send({\"type\": \"http.response.body\", \"body\": b\"\", \"more_body\": False})\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        async with anyio.create_task_group() as task_group:\n\n            async def wrap(func: \"typing.Callable[[], typing.Awaitable[None]]\") -> None:\n                await func()\n                task_group.cancel_scope.cancel()\n\n            task_group.start_soon(wrap, partial(self.stream_response, send))\n            await wrap(partial(self.listen_for_disconnect, receive))\n\n        if self.background is not None:\n            await self.background()\n\n\nclass FileResponse(Response):\n    chunk_size = 64 * 1024\n\n    def __init__(\n        self,\n        path: typing.Union[str, \"os.PathLike[str]\"],\n        status_code: int = 200,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        media_type: typing.Optional[str] = None,\n        background: typing.Optional[BackgroundTask] = None,\n        filename: typing.Optional[str] = None,\n        stat_result: typing.Optional[os.stat_result] = None,\n        method: typing.Optional[str] = None,\n        content_disposition_type: str = \"attachment\",\n    ) -> None:\n        self.path = path\n        self.status_code = status_code\n        self.filename = filename\n        self.send_header_only = method is not None and method.upper() == \"HEAD\"\n        if media_type is None:\n            media_type = guess_type(filename or path)[0] or \"text/plain\"\n        self.media_type = media_type\n        self.background = background\n        self.init_headers(headers)\n        if self.filename is not None:\n            content_disposition_filename = quote(self.filename)\n            if content_disposition_filename != self.filename:\n                content_disposition = \"{}; filename*=utf-8''{}\".format(\n                    content_disposition_type, content_disposition_filename\n                )\n            else:\n                content_disposition = '{}; filename=\"{}\"'.format(\n                    content_disposition_type, self.filename\n                )\n            self.headers.setdefault(\"content-disposition\", content_disposition)\n        self.stat_result = stat_result\n        if stat_result is not None:\n            self.set_stat_headers(stat_result)\n\n    def set_stat_headers(self, stat_result: os.stat_result) -> None:\n        content_length = str(stat_result.st_size)\n        last_modified = formatdate(stat_result.st_mtime, usegmt=True)\n        etag_base = str(stat_result.st_mtime) + \"-\" + str(stat_result.st_size)\n        etag = md5_hexdigest(etag_base.encode(), usedforsecurity=False)\n\n        self.headers.setdefault(\"content-length\", content_length)\n        self.headers.setdefault(\"last-modified\", last_modified)\n        self.headers.setdefault(\"etag\", etag)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if self.stat_result is None:\n            try:\n                stat_result = await anyio.to_thread.run_sync(os.stat, self.path)\n                self.set_stat_headers(stat_result)\n            except FileNotFoundError:\n                raise RuntimeError(f\"File at path {self.path} does not exist.\")\n            else:\n                mode = stat_result.st_mode\n                if not stat.S_ISREG(mode):\n                    raise RuntimeError(f\"File at path {self.path} is not a file.\")\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status_code,\n                \"headers\": self.raw_headers,\n            }\n        )\n        if self.send_header_only:\n            await send({\"type\": \"http.response.body\", \"body\": b\"\", \"more_body\": False})\n        else:\n            async with await anyio.open_file(self.path, mode=\"rb\") as file:\n                more_body = True\n                while more_body:\n                    chunk = await file.read(self.chunk_size)\n                    more_body = len(chunk) == self.chunk_size\n                    await send(\n                        {\n                            \"type\": \"http.response.body\",\n                            \"body\": chunk,\n                            \"more_body\": more_body,\n                        }\n                    )\n        if self.background is not None:\n            await self.background()\n", 366], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py": ["import typing\nfrom collections.abc import Sequence\nfrom shlex import shlex\nfrom urllib.parse import SplitResult, parse_qsl, urlencode, urlsplit\n\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.types import Scope\n\n\nclass Address(typing.NamedTuple):\n    host: str\n    port: int\n\n\n_KeyType = typing.TypeVar(\"_KeyType\")\n# Mapping keys are invariant but their values are covariant since\n# you can only read them\n# that is, you can't do `Mapping[str, Animal]()[\"fido\"] = Dog()`\n_CovariantValueType = typing.TypeVar(\"_CovariantValueType\", covariant=True)\n\n\nclass URL:\n    def __init__(\n        self,\n        url: str = \"\",\n        scope: typing.Optional[Scope] = None,\n        **components: typing.Any,\n    ) -> None:\n        if scope is not None:\n            assert not url, 'Cannot set both \"url\" and \"scope\".'\n            assert not components, 'Cannot set both \"scope\" and \"**components\".'\n            scheme = scope.get(\"scheme\", \"http\")\n            server = scope.get(\"server\", None)\n            path = scope.get(\"root_path\", \"\") + scope[\"path\"]\n            query_string = scope.get(\"query_string\", b\"\")\n\n            host_header = None\n            for key, value in scope[\"headers\"]:\n                if key == b\"host\":\n                    host_header = value.decode(\"latin-1\")\n                    break\n\n            if host_header is not None:\n                url = f\"{scheme}://{host_header}{path}\"\n            elif server is None:\n                url = path\n            else:\n                host, port = server\n                default_port = {\"http\": 80, \"https\": 443, \"ws\": 80, \"wss\": 443}[scheme]\n                if port == default_port:\n                    url = f\"{scheme}://{host}{path}\"\n                else:\n                    url = f\"{scheme}://{host}:{port}{path}\"\n\n            if query_string:\n                url += \"?\" + query_string.decode()\n        elif components:\n            assert not url, 'Cannot set both \"url\" and \"**components\".'\n            url = URL(\"\").replace(**components).components.geturl()\n\n        self._url = url\n\n    @property\n    def components(self) -> SplitResult:\n        if not hasattr(self, \"_components\"):\n            self._components = urlsplit(self._url)\n        return self._components\n\n    @property\n    def scheme(self) -> str:\n        return self.components.scheme\n\n    @property\n    def netloc(self) -> str:\n        return self.components.netloc\n\n    @property\n    def path(self) -> str:\n        return self.components.path\n\n    @property\n    def query(self) -> str:\n        return self.components.query\n\n    @property\n    def fragment(self) -> str:\n        return self.components.fragment\n\n    @property\n    def username(self) -> typing.Union[None, str]:\n        return self.components.username\n\n    @property\n    def password(self) -> typing.Union[None, str]:\n        return self.components.password\n\n    @property\n    def hostname(self) -> typing.Union[None, str]:\n        return self.components.hostname\n\n    @property\n    def port(self) -> typing.Optional[int]:\n        return self.components.port\n\n    @property\n    def is_secure(self) -> bool:\n        return self.scheme in (\"https\", \"wss\")\n\n    def replace(self, **kwargs: typing.Any) -> \"URL\":\n        if (\n            \"username\" in kwargs\n            or \"password\" in kwargs\n            or \"hostname\" in kwargs\n            or \"port\" in kwargs\n        ):\n            hostname = kwargs.pop(\"hostname\", None)\n            port = kwargs.pop(\"port\", self.port)\n            username = kwargs.pop(\"username\", self.username)\n            password = kwargs.pop(\"password\", self.password)\n\n            if hostname is None:\n                netloc = self.netloc\n                _, _, hostname = netloc.rpartition(\"@\")\n\n                if hostname[-1] != \"]\":\n                    hostname = hostname.rsplit(\":\", 1)[0]\n\n            netloc = hostname\n            if port is not None:\n                netloc += f\":{port}\"\n            if username is not None:\n                userpass = username\n                if password is not None:\n                    userpass += f\":{password}\"\n                netloc = f\"{userpass}@{netloc}\"\n\n            kwargs[\"netloc\"] = netloc\n\n        components = self.components._replace(**kwargs)\n        return self.__class__(components.geturl())\n\n    def include_query_params(self, **kwargs: typing.Any) -> \"URL\":\n        params = MultiDict(parse_qsl(self.query, keep_blank_values=True))\n        params.update({str(key): str(value) for key, value in kwargs.items()})\n        query = urlencode(params.multi_items())\n        return self.replace(query=query)\n\n    def replace_query_params(self, **kwargs: typing.Any) -> \"URL\":\n        query = urlencode([(str(key), str(value)) for key, value in kwargs.items()])\n        return self.replace(query=query)\n\n    def remove_query_params(\n        self, keys: typing.Union[str, typing.Sequence[str]]\n    ) -> \"URL\":\n        if isinstance(keys, str):\n            keys = [keys]\n        params = MultiDict(parse_qsl(self.query, keep_blank_values=True))\n        for key in keys:\n            params.pop(key, None)\n        query = urlencode(params.multi_items())\n        return self.replace(query=query)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return str(self) == str(other)\n\n    def __str__(self) -> str:\n        return self._url\n\n    def __repr__(self) -> str:\n        url = str(self)\n        if self.password:\n            url = str(self.replace(password=\"********\"))\n        return f\"{self.__class__.__name__}({repr(url)})\"\n\n\nclass URLPath(str):\n    \"\"\"\n    A URL path string that may also hold an associated protocol and/or host.\n    Used by the routing to return `url_path_for` matches.\n    \"\"\"\n\n    def __new__(cls, path: str, protocol: str = \"\", host: str = \"\") -> \"URLPath\":\n        assert protocol in (\"http\", \"websocket\", \"\")\n        return str.__new__(cls, path)\n\n    def __init__(self, path: str, protocol: str = \"\", host: str = \"\") -> None:\n        self.protocol = protocol\n        self.host = host\n\n    def make_absolute_url(self, base_url: typing.Union[str, URL]) -> str:\n        if isinstance(base_url, str):\n            base_url = URL(base_url)\n        if self.protocol:\n            scheme = {\n                \"http\": {True: \"https\", False: \"http\"},\n                \"websocket\": {True: \"wss\", False: \"ws\"},\n            }[self.protocol][base_url.is_secure]\n        else:\n            scheme = base_url.scheme\n\n        netloc = self.host or base_url.netloc\n        path = base_url.path.rstrip(\"/\") + str(self)\n        return str(URL(scheme=scheme, netloc=netloc, path=path))\n\n\nclass Secret:\n    \"\"\"\n    Holds a string value that should not be revealed in tracebacks etc.\n    You should cast the value to `str` at the point it is required.\n    \"\"\"\n\n    def __init__(self, value: str):\n        self._value = value\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        return f\"{class_name}('**********')\"\n\n    def __str__(self) -> str:\n        return self._value\n\n    def __bool__(self) -> bool:\n        return bool(self._value)\n\n\nclass CommaSeparatedStrings(Sequence):\n    def __init__(self, value: typing.Union[str, typing.Sequence[str]]):\n        if isinstance(value, str):\n            splitter = shlex(value, posix=True)\n            splitter.whitespace = \",\"\n            splitter.whitespace_split = True\n            self._items = [item.strip() for item in splitter]\n        else:\n            self._items = list(value)\n\n    def __len__(self) -> int:\n        return len(self._items)\n\n    def __getitem__(self, index: typing.Union[int, slice]) -> typing.Any:\n        return self._items[index]\n\n    def __iter__(self) -> typing.Iterator[str]:\n        return iter(self._items)\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        items = [item for item in self]\n        return f\"{class_name}({items!r})\"\n\n    def __str__(self) -> str:\n        return \", \".join(repr(item) for item in self)\n\n\nclass ImmutableMultiDict(typing.Mapping[_KeyType, _CovariantValueType]):\n    _dict: typing.Dict[_KeyType, _CovariantValueType]\n\n    def __init__(\n        self,\n        *args: typing.Union[\n            \"ImmutableMultiDict[_KeyType, _CovariantValueType]\",\n            typing.Mapping[_KeyType, _CovariantValueType],\n            typing.Iterable[typing.Tuple[_KeyType, _CovariantValueType]],\n        ],\n        **kwargs: typing.Any,\n    ) -> None:\n        assert len(args) < 2, \"Too many arguments.\"\n\n        value: typing.Any = args[0] if args else []\n        if kwargs:\n            value = (\n                ImmutableMultiDict(value).multi_items()\n                + ImmutableMultiDict(kwargs).multi_items()  # type: ignore[operator]\n            )\n\n        if not value:\n            _items: typing.List[typing.Tuple[typing.Any, typing.Any]] = []\n        elif hasattr(value, \"multi_items\"):\n            value = typing.cast(\n                ImmutableMultiDict[_KeyType, _CovariantValueType], value\n            )\n            _items = list(value.multi_items())\n        elif hasattr(value, \"items\"):\n            value = typing.cast(typing.Mapping[_KeyType, _CovariantValueType], value)\n            _items = list(value.items())\n        else:\n            value = typing.cast(\n                typing.List[typing.Tuple[typing.Any, typing.Any]], value\n            )\n            _items = list(value)\n\n        self._dict = {k: v for k, v in _items}\n        self._list = _items\n\n    def getlist(self, key: typing.Any) -> typing.List[_CovariantValueType]:\n        return [item_value for item_key, item_value in self._list if item_key == key]\n\n    def keys(self) -> typing.KeysView[_KeyType]:\n        return self._dict.keys()\n\n    def values(self) -> typing.ValuesView[_CovariantValueType]:\n        return self._dict.values()\n\n    def items(self) -> typing.ItemsView[_KeyType, _CovariantValueType]:\n        return self._dict.items()\n\n    def multi_items(self) -> typing.List[typing.Tuple[_KeyType, _CovariantValueType]]:\n        return list(self._list)\n\n    def __getitem__(self, key: _KeyType) -> _CovariantValueType:\n        return self._dict[key]\n\n    def __contains__(self, key: typing.Any) -> bool:\n        return key in self._dict\n\n    def __iter__(self) -> typing.Iterator[_KeyType]:\n        return iter(self.keys())\n\n    def __len__(self) -> int:\n        return len(self._dict)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        if not isinstance(other, self.__class__):\n            return False\n        return sorted(self._list) == sorted(other._list)\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        items = self.multi_items()\n        return f\"{class_name}({items!r})\"\n\n\nclass MultiDict(ImmutableMultiDict[typing.Any, typing.Any]):\n    def __setitem__(self, key: typing.Any, value: typing.Any) -> None:\n        self.setlist(key, [value])\n\n    def __delitem__(self, key: typing.Any) -> None:\n        self._list = [(k, v) for k, v in self._list if k != key]\n        del self._dict[key]\n\n    def pop(self, key: typing.Any, default: typing.Any = None) -> typing.Any:\n        self._list = [(k, v) for k, v in self._list if k != key]\n        return self._dict.pop(key, default)\n\n    def popitem(self) -> typing.Tuple:\n        key, value = self._dict.popitem()\n        self._list = [(k, v) for k, v in self._list if k != key]\n        return key, value\n\n    def poplist(self, key: typing.Any) -> typing.List:\n        values = [v for k, v in self._list if k == key]\n        self.pop(key)\n        return values\n\n    def clear(self) -> None:\n        self._dict.clear()\n        self._list.clear()\n\n    def setdefault(self, key: typing.Any, default: typing.Any = None) -> typing.Any:\n        if key not in self:\n            self._dict[key] = default\n            self._list.append((key, default))\n\n        return self[key]\n\n    def setlist(self, key: typing.Any, values: typing.List) -> None:\n        if not values:\n            self.pop(key, None)\n        else:\n            existing_items = [(k, v) for (k, v) in self._list if k != key]\n            self._list = existing_items + [(key, value) for value in values]\n            self._dict[key] = values[-1]\n\n    def append(self, key: typing.Any, value: typing.Any) -> None:\n        self._list.append((key, value))\n        self._dict[key] = value\n\n    def update(\n        self,\n        *args: typing.Union[\n            \"MultiDict\",\n            typing.Mapping,\n            typing.List[typing.Tuple[typing.Any, typing.Any]],\n        ],\n        **kwargs: typing.Any,\n    ) -> None:\n        value = MultiDict(*args, **kwargs)\n        existing_items = [(k, v) for (k, v) in self._list if k not in value.keys()]\n        self._list = existing_items + value.multi_items()\n        self._dict.update(value)\n\n\nclass QueryParams(ImmutableMultiDict[str, str]):\n    \"\"\"\n    An immutable multidict.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: typing.Union[\n            \"ImmutableMultiDict\",\n            typing.Mapping,\n            typing.List[typing.Tuple[typing.Any, typing.Any]],\n            str,\n            bytes,\n        ],\n        **kwargs: typing.Any,\n    ) -> None:\n        assert len(args) < 2, \"Too many arguments.\"\n\n        value = args[0] if args else []\n\n        if isinstance(value, str):\n            super().__init__(parse_qsl(value, keep_blank_values=True), **kwargs)\n        elif isinstance(value, bytes):\n            super().__init__(\n                parse_qsl(value.decode(\"latin-1\"), keep_blank_values=True), **kwargs\n            )\n        else:\n            super().__init__(*args, **kwargs)  # type: ignore[arg-type]\n        self._list = [(str(k), str(v)) for k, v in self._list]\n        self._dict = {str(k): str(v) for k, v in self._dict.items()}\n\n    def __str__(self) -> str:\n        return urlencode(self._list)\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        query_string = str(self)\n        return f\"{class_name}({query_string!r})\"\n\n\nclass UploadFile:\n    \"\"\"\n    An uploaded file included as part of the request data.\n    \"\"\"\n\n    def __init__(\n        self,\n        file: typing.BinaryIO,\n        *,\n        size: typing.Optional[int] = None,\n        filename: typing.Optional[str] = None,\n        headers: \"typing.Optional[Headers]\" = None,\n    ) -> None:\n        self.filename = filename\n        self.file = file\n        self.size = size\n        self.headers = headers or Headers()\n\n    @property\n    def content_type(self) -> typing.Optional[str]:\n        return self.headers.get(\"content-type\", None)\n\n    @property\n    def _in_memory(self) -> bool:\n        # check for SpooledTemporaryFile._rolled\n        rolled_to_disk = getattr(self.file, \"_rolled\", True)\n        return not rolled_to_disk\n\n    async def write(self, data: bytes) -> None:\n        if self.size is not None:\n            self.size += len(data)\n\n        if self._in_memory:\n            self.file.write(data)\n        else:\n            await run_in_threadpool(self.file.write, data)\n\n    async def read(self, size: int = -1) -> bytes:\n        if self._in_memory:\n            return self.file.read(size)\n        return await run_in_threadpool(self.file.read, size)\n\n    async def seek(self, offset: int) -> None:\n        if self._in_memory:\n            self.file.seek(offset)\n        else:\n            await run_in_threadpool(self.file.seek, offset)\n\n    async def close(self) -> None:\n        if self._in_memory:\n            self.file.close()\n        else:\n            await run_in_threadpool(self.file.close)\n\n\nclass FormData(ImmutableMultiDict[str, typing.Union[UploadFile, str]]):\n    \"\"\"\n    An immutable multidict, containing both file uploads and text input.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: typing.Union[\n            \"FormData\",\n            typing.Mapping[str, typing.Union[str, UploadFile]],\n            typing.List[typing.Tuple[str, typing.Union[str, UploadFile]]],\n        ],\n        **kwargs: typing.Union[str, UploadFile],\n    ) -> None:\n        super().__init__(*args, **kwargs)\n\n    async def close(self) -> None:\n        for key, value in self.multi_items():\n            if isinstance(value, UploadFile):\n                await value.close()\n\n\nclass Headers(typing.Mapping[str, str]):\n    \"\"\"\n    An immutable, case-insensitive multidict.\n    \"\"\"\n\n    def __init__(\n        self,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        raw: typing.Optional[typing.List[typing.Tuple[bytes, bytes]]] = None,\n        scope: typing.Optional[typing.MutableMapping[str, typing.Any]] = None,\n    ) -> None:\n        self._list: typing.List[typing.Tuple[bytes, bytes]] = []\n        if headers is not None:\n            assert raw is None, 'Cannot set both \"headers\" and \"raw\".'\n            assert scope is None, 'Cannot set both \"headers\" and \"scope\".'\n            self._list = [\n                (key.lower().encode(\"latin-1\"), value.encode(\"latin-1\"))\n                for key, value in headers.items()\n            ]\n        elif raw is not None:\n            assert scope is None, 'Cannot set both \"raw\" and \"scope\".'\n            self._list = raw\n        elif scope is not None:\n            # scope[\"headers\"] isn't necessarily a list\n            # it might be a tuple or other iterable\n            self._list = scope[\"headers\"] = list(scope[\"headers\"])\n\n    @property\n    def raw(self) -> typing.List[typing.Tuple[bytes, bytes]]:\n        return list(self._list)\n\n    def keys(self) -> typing.List[str]:  # type: ignore[override]\n        return [key.decode(\"latin-1\") for key, value in self._list]\n\n    def values(self) -> typing.List[str]:  # type: ignore[override]\n        return [value.decode(\"latin-1\") for key, value in self._list]\n\n    def items(self) -> typing.List[typing.Tuple[str, str]]:  # type: ignore[override]\n        return [\n            (key.decode(\"latin-1\"), value.decode(\"latin-1\"))\n            for key, value in self._list\n        ]\n\n    def getlist(self, key: str) -> typing.List[str]:\n        get_header_key = key.lower().encode(\"latin-1\")\n        return [\n            item_value.decode(\"latin-1\")\n            for item_key, item_value in self._list\n            if item_key == get_header_key\n        ]\n\n    def mutablecopy(self) -> \"MutableHeaders\":\n        return MutableHeaders(raw=self._list[:])\n\n    def __getitem__(self, key: str) -> str:\n        get_header_key = key.lower().encode(\"latin-1\")\n        for header_key, header_value in self._list:\n            if header_key == get_header_key:\n                return header_value.decode(\"latin-1\")\n        raise KeyError(key)\n\n    def __contains__(self, key: typing.Any) -> bool:\n        get_header_key = key.lower().encode(\"latin-1\")\n        for header_key, header_value in self._list:\n            if header_key == get_header_key:\n                return True\n        return False\n\n    def __iter__(self) -> typing.Iterator[typing.Any]:\n        return iter(self.keys())\n\n    def __len__(self) -> int:\n        return len(self._list)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        if not isinstance(other, Headers):\n            return False\n        return sorted(self._list) == sorted(other._list)\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        as_dict = dict(self.items())\n        if len(as_dict) == len(self):\n            return f\"{class_name}({as_dict!r})\"\n        return f\"{class_name}(raw={self.raw!r})\"\n\n\nclass MutableHeaders(Headers):\n    def __setitem__(self, key: str, value: str) -> None:\n        \"\"\"\n        Set the header `key` to `value`, removing any duplicate entries.\n        Retains insertion order.\n        \"\"\"\n        set_key = key.lower().encode(\"latin-1\")\n        set_value = value.encode(\"latin-1\")\n\n        found_indexes: \"typing.List[int]\" = []\n        for idx, (item_key, item_value) in enumerate(self._list):\n            if item_key == set_key:\n                found_indexes.append(idx)\n\n        for idx in reversed(found_indexes[1:]):\n            del self._list[idx]\n\n        if found_indexes:\n            idx = found_indexes[0]\n            self._list[idx] = (set_key, set_value)\n        else:\n            self._list.append((set_key, set_value))\n\n    def __delitem__(self, key: str) -> None:\n        \"\"\"\n        Remove the header `key`.\n        \"\"\"\n        del_key = key.lower().encode(\"latin-1\")\n\n        pop_indexes: \"typing.List[int]\" = []\n        for idx, (item_key, item_value) in enumerate(self._list):\n            if item_key == del_key:\n                pop_indexes.append(idx)\n\n        for idx in reversed(pop_indexes):\n            del self._list[idx]\n\n    def __ior__(self, other: typing.Mapping[str, str]) -> \"MutableHeaders\":\n        if not isinstance(other, typing.Mapping):\n            raise TypeError(f\"Expected a mapping but got {other.__class__.__name__}\")\n        self.update(other)\n        return self\n\n    def __or__(self, other: typing.Mapping[str, str]) -> \"MutableHeaders\":\n        if not isinstance(other, typing.Mapping):\n            raise TypeError(f\"Expected a mapping but got {other.__class__.__name__}\")\n        new = self.mutablecopy()\n        new.update(other)\n        return new\n\n    @property\n    def raw(self) -> typing.List[typing.Tuple[bytes, bytes]]:\n        return self._list\n\n    def setdefault(self, key: str, value: str) -> str:\n        \"\"\"\n        If the header `key` does not exist, then set it to `value`.\n        Returns the header value.\n        \"\"\"\n        set_key = key.lower().encode(\"latin-1\")\n        set_value = value.encode(\"latin-1\")\n\n        for idx, (item_key, item_value) in enumerate(self._list):\n            if item_key == set_key:\n                return item_value.decode(\"latin-1\")\n        self._list.append((set_key, set_value))\n        return value\n\n    def update(self, other: typing.Mapping[str, str]) -> None:\n        for key, val in other.items():\n            self[key] = val\n\n    def append(self, key: str, value: str) -> None:\n        \"\"\"\n        Append a header, preserving any duplicate entries.\n        \"\"\"\n        append_key = key.lower().encode(\"latin-1\")\n        append_value = value.encode(\"latin-1\")\n        self._list.append((append_key, append_value))\n\n    def add_vary_header(self, vary: str) -> None:\n        existing = self.get(\"vary\")\n        if existing is not None:\n            vary = \", \".join([existing, vary])\n        self[\"vary\"] = vary\n\n\nclass State:\n    \"\"\"\n    An object that can be used to store arbitrary state.\n\n    Used for `request.state` and `app.state`.\n    \"\"\"\n\n    _state: typing.Dict[str, typing.Any]\n\n    def __init__(self, state: typing.Optional[typing.Dict[str, typing.Any]] = None):\n        if state is None:\n            state = {}\n        super().__setattr__(\"_state\", state)\n\n    def __setattr__(self, key: typing.Any, value: typing.Any) -> None:\n        self._state[key] = value\n\n    def __getattr__(self, key: typing.Any) -> typing.Any:\n        try:\n            return self._state[key]\n        except KeyError:\n            message = \"'{}' object has no attribute '{}'\"\n            raise AttributeError(message.format(self.__class__.__name__, key))\n\n    def __delattr__(self, key: typing.Any) -> None:\n        del self._state[key]\n", 708], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py": ["import dataclasses\nimport inspect\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nfrom typing import (\n    Any,\n    Callable,\n    Coroutine,\n    Dict,\n    ForwardRef,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport anyio\nfrom fastapi import params\nfrom fastapi.concurrency import (\n    AsyncExitStack,\n    asynccontextmanager,\n    contextmanager_in_threadpool,\n)\nfrom fastapi.dependencies.models import Dependant, SecurityRequirement\nfrom fastapi.logger import logger\nfrom fastapi.security.base import SecurityBase\nfrom fastapi.security.oauth2 import OAuth2, SecurityScopes\nfrom fastapi.security.open_id_connect_url import OpenIdConnect\nfrom fastapi.utils import create_response_field, get_path_param_names\nfrom pydantic import BaseModel, create_model\nfrom pydantic.error_wrappers import ErrorWrapper\nfrom pydantic.errors import MissingError\nfrom pydantic.fields import (\n    SHAPE_FROZENSET,\n    SHAPE_LIST,\n    SHAPE_SEQUENCE,\n    SHAPE_SET,\n    SHAPE_SINGLETON,\n    SHAPE_TUPLE,\n    SHAPE_TUPLE_ELLIPSIS,\n    FieldInfo,\n    ModelField,\n    Required,\n    Undefined,\n)\nfrom pydantic.schema import get_annotation_from_field_info\nfrom pydantic.typing import evaluate_forwardref\nfrom pydantic.utils import lenient_issubclass\nfrom starlette.background import BackgroundTasks\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.datastructures import FormData, Headers, QueryParams, UploadFile\nfrom starlette.requests import HTTPConnection, Request\nfrom starlette.responses import Response\nfrom starlette.websockets import WebSocket\n\nsequence_shapes = {\n    SHAPE_LIST,\n    SHAPE_SET,\n    SHAPE_FROZENSET,\n    SHAPE_TUPLE,\n    SHAPE_SEQUENCE,\n    SHAPE_TUPLE_ELLIPSIS,\n}\nsequence_types = (list, set, tuple)\nsequence_shape_to_type = {\n    SHAPE_LIST: list,\n    SHAPE_SET: set,\n    SHAPE_TUPLE: tuple,\n    SHAPE_SEQUENCE: list,\n    SHAPE_TUPLE_ELLIPSIS: list,\n}\n\n\nmultipart_not_installed_error = (\n    'Form data requires \"python-multipart\" to be installed. \\n'\n    'You can install \"python-multipart\" with: \\n\\n'\n    \"pip install python-multipart\\n\"\n)\nmultipart_incorrect_install_error = (\n    'Form data requires \"python-multipart\" to be installed. '\n    'It seems you installed \"multipart\" instead. \\n'\n    'You can remove \"multipart\" with: \\n\\n'\n    \"pip uninstall multipart\\n\\n\"\n    'And then install \"python-multipart\" with: \\n\\n'\n    \"pip install python-multipart\\n\"\n)\n\n\ndef check_file_field(field: ModelField) -> None:\n    field_info = field.field_info\n    if isinstance(field_info, params.Form):\n        try:\n            # __version__ is available in both multiparts, and can be mocked\n            from multipart import __version__  # type: ignore\n\n            assert __version__\n            try:\n                # parse_options_header is only available in the right multipart\n                from multipart.multipart import parse_options_header  # type: ignore\n\n                assert parse_options_header\n            except ImportError:\n                logger.error(multipart_incorrect_install_error)\n                raise RuntimeError(multipart_incorrect_install_error) from None\n        except ImportError:\n            logger.error(multipart_not_installed_error)\n            raise RuntimeError(multipart_not_installed_error) from None\n\n\ndef get_param_sub_dependant(\n    *, param: inspect.Parameter, path: str, security_scopes: Optional[List[str]] = None\n) -> Dependant:\n    depends: params.Depends = param.default\n    if depends.dependency:\n        dependency = depends.dependency\n    else:\n        dependency = param.annotation\n    return get_sub_dependant(\n        depends=depends,\n        dependency=dependency,\n        path=path,\n        name=param.name,\n        security_scopes=security_scopes,\n    )\n\n\ndef get_parameterless_sub_dependant(*, depends: params.Depends, path: str) -> Dependant:\n    assert callable(\n        depends.dependency\n    ), \"A parameter-less dependency must have a callable dependency\"\n    return get_sub_dependant(depends=depends, dependency=depends.dependency, path=path)\n\n\ndef get_sub_dependant(\n    *,\n    depends: params.Depends,\n    dependency: Callable[..., Any],\n    path: str,\n    name: Optional[str] = None,\n    security_scopes: Optional[List[str]] = None,\n) -> Dependant:\n    security_requirement = None\n    security_scopes = security_scopes or []\n    if isinstance(depends, params.Security):\n        dependency_scopes = depends.scopes\n        security_scopes.extend(dependency_scopes)\n    if isinstance(dependency, SecurityBase):\n        use_scopes: List[str] = []\n        if isinstance(dependency, (OAuth2, OpenIdConnect)):\n            use_scopes = security_scopes\n        security_requirement = SecurityRequirement(\n            security_scheme=dependency, scopes=use_scopes\n        )\n    sub_dependant = get_dependant(\n        path=path,\n        call=dependency,\n        name=name,\n        security_scopes=security_scopes,\n        use_cache=depends.use_cache,\n    )\n    if security_requirement:\n        sub_dependant.security_requirements.append(security_requirement)\n    return sub_dependant\n\n\nCacheKey = Tuple[Optional[Callable[..., Any]], Tuple[str, ...]]\n\n\ndef get_flat_dependant(\n    dependant: Dependant,\n    *,\n    skip_repeats: bool = False,\n    visited: Optional[List[CacheKey]] = None,\n) -> Dependant:\n    if visited is None:\n        visited = []\n    visited.append(dependant.cache_key)\n\n    flat_dependant = Dependant(\n        path_params=dependant.path_params.copy(),\n        query_params=dependant.query_params.copy(),\n        header_params=dependant.header_params.copy(),\n        cookie_params=dependant.cookie_params.copy(),\n        body_params=dependant.body_params.copy(),\n        security_schemes=dependant.security_requirements.copy(),\n        use_cache=dependant.use_cache,\n        path=dependant.path,\n    )\n    for sub_dependant in dependant.dependencies:\n        if skip_repeats and sub_dependant.cache_key in visited:\n            continue\n        flat_sub = get_flat_dependant(\n            sub_dependant, skip_repeats=skip_repeats, visited=visited\n        )\n        flat_dependant.path_params.extend(flat_sub.path_params)\n        flat_dependant.query_params.extend(flat_sub.query_params)\n        flat_dependant.header_params.extend(flat_sub.header_params)\n        flat_dependant.cookie_params.extend(flat_sub.cookie_params)\n        flat_dependant.body_params.extend(flat_sub.body_params)\n        flat_dependant.security_requirements.extend(flat_sub.security_requirements)\n    return flat_dependant\n\n\ndef get_flat_params(dependant: Dependant) -> List[ModelField]:\n    flat_dependant = get_flat_dependant(dependant, skip_repeats=True)\n    return (\n        flat_dependant.path_params\n        + flat_dependant.query_params\n        + flat_dependant.header_params\n        + flat_dependant.cookie_params\n    )\n\n\ndef is_scalar_field(field: ModelField) -> bool:\n    field_info = field.field_info\n    if not (\n        field.shape == SHAPE_SINGLETON\n        and not lenient_issubclass(field.type_, BaseModel)\n        and not lenient_issubclass(field.type_, sequence_types + (dict,))\n        and not dataclasses.is_dataclass(field.type_)\n        and not isinstance(field_info, params.Body)\n    ):\n        return False\n    if field.sub_fields:\n        if not all(is_scalar_field(f) for f in field.sub_fields):\n            return False\n    return True\n\n\ndef is_scalar_sequence_field(field: ModelField) -> bool:\n    if (field.shape in sequence_shapes) and not lenient_issubclass(\n        field.type_, BaseModel\n    ):\n        if field.sub_fields is not None:\n            for sub_field in field.sub_fields:\n                if not is_scalar_field(sub_field):\n                    return False\n        return True\n    if lenient_issubclass(field.type_, sequence_types):\n        return True\n    return False\n\n\ndef get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n    signature = inspect.signature(call)\n    globalns = getattr(call, \"__globals__\", {})\n    typed_params = [\n        inspect.Parameter(\n            name=param.name,\n            kind=param.kind,\n            default=param.default,\n            annotation=get_typed_annotation(param.annotation, globalns),\n        )\n        for param in signature.parameters.values()\n    ]\n    typed_signature = inspect.Signature(typed_params)\n    return typed_signature\n\n\ndef get_typed_annotation(annotation: Any, globalns: Dict[str, Any]) -> Any:\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n        annotation = evaluate_forwardref(annotation, globalns, globalns)\n    return annotation\n\n\ndef get_typed_return_annotation(call: Callable[..., Any]) -> Any:\n    signature = inspect.signature(call)\n    annotation = signature.return_annotation\n\n    if annotation is inspect.Signature.empty:\n        return None\n\n    globalns = getattr(call, \"__globals__\", {})\n    return get_typed_annotation(annotation, globalns)\n\n\ndef get_dependant(\n    *,\n    path: str,\n    call: Callable[..., Any],\n    name: Optional[str] = None,\n    security_scopes: Optional[List[str]] = None,\n    use_cache: bool = True,\n) -> Dependant:\n    path_param_names = get_path_param_names(path)\n    endpoint_signature = get_typed_signature(call)\n    signature_params = endpoint_signature.parameters\n    dependant = Dependant(\n        call=call,\n        name=name,\n        path=path,\n        security_scopes=security_scopes,\n        use_cache=use_cache,\n    )\n    for param_name, param in signature_params.items():\n        if isinstance(param.default, params.Depends):\n            sub_dependant = get_param_sub_dependant(\n                param=param, path=path, security_scopes=security_scopes\n            )\n            dependant.dependencies.append(sub_dependant)\n            continue\n        if add_non_field_param_to_dependency(param=param, dependant=dependant):\n            continue\n        param_field = get_param_field(\n            param=param, default_field_info=params.Query, param_name=param_name\n        )\n        if param_name in path_param_names:\n            assert is_scalar_field(\n                field=param_field\n            ), \"Path params must be of one of the supported types\"\n            ignore_default = not isinstance(param.default, params.Path)\n            param_field = get_param_field(\n                param=param,\n                param_name=param_name,\n                default_field_info=params.Path,\n                force_type=params.ParamTypes.path,\n                ignore_default=ignore_default,\n            )\n            add_param_to_fields(field=param_field, dependant=dependant)\n        elif is_scalar_field(field=param_field):\n            add_param_to_fields(field=param_field, dependant=dependant)\n        elif isinstance(\n            param.default, (params.Query, params.Header)\n        ) and is_scalar_sequence_field(param_field):\n            add_param_to_fields(field=param_field, dependant=dependant)\n        else:\n            field_info = param_field.field_info\n            assert isinstance(\n                field_info, params.Body\n            ), f\"Param: {param_field.name} can only be a request body, using Body()\"\n            dependant.body_params.append(param_field)\n    return dependant\n\n\ndef add_non_field_param_to_dependency(\n    *, param: inspect.Parameter, dependant: Dependant\n) -> Optional[bool]:\n    if lenient_issubclass(param.annotation, Request):\n        dependant.request_param_name = param.name\n        return True\n    elif lenient_issubclass(param.annotation, WebSocket):\n        dependant.websocket_param_name = param.name\n        return True\n    elif lenient_issubclass(param.annotation, HTTPConnection):\n        dependant.http_connection_param_name = param.name\n        return True\n    elif lenient_issubclass(param.annotation, Response):\n        dependant.response_param_name = param.name\n        return True\n    elif lenient_issubclass(param.annotation, BackgroundTasks):\n        dependant.background_tasks_param_name = param.name\n        return True\n    elif lenient_issubclass(param.annotation, SecurityScopes):\n        dependant.security_scopes_param_name = param.name\n        return True\n    return None\n\n\ndef get_param_field(\n    *,\n    param: inspect.Parameter,\n    param_name: str,\n    default_field_info: Type[params.Param] = params.Param,\n    force_type: Optional[params.ParamTypes] = None,\n    ignore_default: bool = False,\n) -> ModelField:\n    default_value: Any = Undefined\n    had_schema = False\n    if not param.default == param.empty and ignore_default is False:\n        default_value = param.default\n    if isinstance(default_value, FieldInfo):\n        had_schema = True\n        field_info = default_value\n        default_value = field_info.default\n        if (\n            isinstance(field_info, params.Param)\n            and getattr(field_info, \"in_\", None) is None\n        ):\n            field_info.in_ = default_field_info.in_\n        if force_type:\n            field_info.in_ = force_type  # type: ignore\n    else:\n        field_info = default_field_info(default=default_value)\n    required = True\n    if default_value is Required or ignore_default:\n        required = True\n        default_value = None\n    elif default_value is not Undefined:\n        required = False\n    annotation: Any = Any\n    if not param.annotation == param.empty:\n        annotation = param.annotation\n    annotation = get_annotation_from_field_info(annotation, field_info, param_name)\n    if not field_info.alias and getattr(field_info, \"convert_underscores\", None):\n        alias = param.name.replace(\"_\", \"-\")\n    else:\n        alias = field_info.alias or param.name\n    field = create_response_field(\n        name=param.name,\n        type_=annotation,\n        default=default_value,\n        alias=alias,\n        required=required,\n        field_info=field_info,\n    )\n    if not had_schema and not is_scalar_field(field=field):\n        field.field_info = params.Body(field_info.default)\n    if not had_schema and lenient_issubclass(field.type_, UploadFile):\n        field.field_info = params.File(field_info.default)\n\n    return field\n\n\ndef add_param_to_fields(*, field: ModelField, dependant: Dependant) -> None:\n    field_info = cast(params.Param, field.field_info)\n    if field_info.in_ == params.ParamTypes.path:\n        dependant.path_params.append(field)\n    elif field_info.in_ == params.ParamTypes.query:\n        dependant.query_params.append(field)\n    elif field_info.in_ == params.ParamTypes.header:\n        dependant.header_params.append(field)\n    else:\n        assert (\n            field_info.in_ == params.ParamTypes.cookie\n        ), f\"non-body parameters must be in path, query, header or cookie: {field.name}\"\n        dependant.cookie_params.append(field)\n\n\ndef is_coroutine_callable(call: Callable[..., Any]) -> bool:\n    if inspect.isroutine(call):\n        return inspect.iscoroutinefunction(call)\n    if inspect.isclass(call):\n        return False\n    dunder_call = getattr(call, \"__call__\", None)  # noqa: B004\n    return inspect.iscoroutinefunction(dunder_call)\n\n\ndef is_async_gen_callable(call: Callable[..., Any]) -> bool:\n    if inspect.isasyncgenfunction(call):\n        return True\n    dunder_call = getattr(call, \"__call__\", None)  # noqa: B004\n    return inspect.isasyncgenfunction(dunder_call)\n\n\ndef is_gen_callable(call: Callable[..., Any]) -> bool:\n    if inspect.isgeneratorfunction(call):\n        return True\n    dunder_call = getattr(call, \"__call__\", None)  # noqa: B004\n    return inspect.isgeneratorfunction(dunder_call)\n\n\nasync def solve_generator(\n    *, call: Callable[..., Any], stack: AsyncExitStack, sub_values: Dict[str, Any]\n) -> Any:\n    if is_gen_callable(call):\n        cm = contextmanager_in_threadpool(contextmanager(call)(**sub_values))\n    elif is_async_gen_callable(call):\n        cm = asynccontextmanager(call)(**sub_values)\n    return await stack.enter_async_context(cm)\n\n\nasync def solve_dependencies(\n    *,\n    request: Union[Request, WebSocket],\n    dependant: Dependant,\n    body: Optional[Union[Dict[str, Any], FormData]] = None,\n    background_tasks: Optional[BackgroundTasks] = None,\n    response: Optional[Response] = None,\n    dependency_overrides_provider: Optional[Any] = None,\n    dependency_cache: Optional[Dict[Tuple[Callable[..., Any], Tuple[str]], Any]] = None,\n) -> Tuple[\n    Dict[str, Any],\n    List[ErrorWrapper],\n    Optional[BackgroundTasks],\n    Response,\n    Dict[Tuple[Callable[..., Any], Tuple[str]], Any],\n]:\n    values: Dict[str, Any] = {}\n    errors: List[ErrorWrapper] = []\n    if response is None:\n        response = Response()\n        del response.headers[\"content-length\"]\n        response.status_code = None  # type: ignore\n    dependency_cache = dependency_cache or {}\n    sub_dependant: Dependant\n    for sub_dependant in dependant.dependencies:\n        sub_dependant.call = cast(Callable[..., Any], sub_dependant.call)\n        sub_dependant.cache_key = cast(\n            Tuple[Callable[..., Any], Tuple[str]], sub_dependant.cache_key\n        )\n        call = sub_dependant.call\n        use_sub_dependant = sub_dependant\n        if (\n            dependency_overrides_provider\n            and dependency_overrides_provider.dependency_overrides\n        ):\n            original_call = sub_dependant.call\n            call = getattr(\n                dependency_overrides_provider, \"dependency_overrides\", {}\n            ).get(original_call, original_call)\n            use_path: str = sub_dependant.path  # type: ignore\n            use_sub_dependant = get_dependant(\n                path=use_path,\n                call=call,\n                name=sub_dependant.name,\n                security_scopes=sub_dependant.security_scopes,\n            )\n\n        solved_result = await solve_dependencies(\n            request=request,\n            dependant=use_sub_dependant,\n            body=body,\n            background_tasks=background_tasks,\n            response=response,\n            dependency_overrides_provider=dependency_overrides_provider,\n            dependency_cache=dependency_cache,\n        )\n        (\n            sub_values,\n            sub_errors,\n            background_tasks,\n            _,  # the subdependency returns the same response we have\n            sub_dependency_cache,\n        ) = solved_result\n        dependency_cache.update(sub_dependency_cache)\n        if sub_errors:\n            errors.extend(sub_errors)\n            continue\n        if sub_dependant.use_cache and sub_dependant.cache_key in dependency_cache:\n            solved = dependency_cache[sub_dependant.cache_key]\n        elif is_gen_callable(call) or is_async_gen_callable(call):\n            stack = request.scope.get(\"fastapi_astack\")\n            assert isinstance(stack, AsyncExitStack)\n            solved = await solve_generator(\n                call=call, stack=stack, sub_values=sub_values\n            )\n        elif is_coroutine_callable(call):\n            solved = await call(**sub_values)\n        else:\n            solved = await run_in_threadpool(call, **sub_values)\n        if sub_dependant.name is not None:\n            values[sub_dependant.name] = solved\n        if sub_dependant.cache_key not in dependency_cache:\n            dependency_cache[sub_dependant.cache_key] = solved\n    path_values, path_errors = request_params_to_args(\n        dependant.path_params, request.path_params\n    )\n    query_values, query_errors = request_params_to_args(\n        dependant.query_params, request.query_params\n    )\n    header_values, header_errors = request_params_to_args(\n        dependant.header_params, request.headers\n    )\n    cookie_values, cookie_errors = request_params_to_args(\n        dependant.cookie_params, request.cookies\n    )\n    values.update(path_values)\n    values.update(query_values)\n    values.update(header_values)\n    values.update(cookie_values)\n    errors += path_errors + query_errors + header_errors + cookie_errors\n    if dependant.body_params:\n        (\n            body_values,\n            body_errors,\n        ) = await request_body_to_args(  # body_params checked above\n            required_params=dependant.body_params, received_body=body\n        )\n        values.update(body_values)\n        errors.extend(body_errors)\n    if dependant.http_connection_param_name:\n        values[dependant.http_connection_param_name] = request\n    if dependant.request_param_name and isinstance(request, Request):\n        values[dependant.request_param_name] = request\n    elif dependant.websocket_param_name and isinstance(request, WebSocket):\n        values[dependant.websocket_param_name] = request\n    if dependant.background_tasks_param_name:\n        if background_tasks is None:\n            background_tasks = BackgroundTasks()\n        values[dependant.background_tasks_param_name] = background_tasks\n    if dependant.response_param_name:\n        values[dependant.response_param_name] = response\n    if dependant.security_scopes_param_name:\n        values[dependant.security_scopes_param_name] = SecurityScopes(\n            scopes=dependant.security_scopes\n        )\n    return values, errors, background_tasks, response, dependency_cache\n\n\ndef request_params_to_args(\n    required_params: Sequence[ModelField],\n    received_params: Union[Mapping[str, Any], QueryParams, Headers],\n) -> Tuple[Dict[str, Any], List[ErrorWrapper]]:\n    values = {}\n    errors = []\n    for field in required_params:\n        if is_scalar_sequence_field(field) and isinstance(\n            received_params, (QueryParams, Headers)\n        ):\n            value = received_params.getlist(field.alias) or field.default\n        else:\n            value = received_params.get(field.alias)\n        field_info = field.field_info\n        assert isinstance(\n            field_info, params.Param\n        ), \"Params must be subclasses of Param\"\n        if value is None:\n            if field.required:\n                errors.append(\n                    ErrorWrapper(\n                        MissingError(), loc=(field_info.in_.value, field.alias)\n                    )\n                )\n            else:\n                values[field.name] = deepcopy(field.default)\n            continue\n        v_, errors_ = field.validate(\n            value, values, loc=(field_info.in_.value, field.alias)\n        )\n        if isinstance(errors_, ErrorWrapper):\n            errors.append(errors_)\n        elif isinstance(errors_, list):\n            errors.extend(errors_)\n        else:\n            values[field.name] = v_\n    return values, errors\n\n\nasync def request_body_to_args(\n    required_params: List[ModelField],\n    received_body: Optional[Union[Dict[str, Any], FormData]],\n) -> Tuple[Dict[str, Any], List[ErrorWrapper]]:\n    values = {}\n    errors = []\n    if required_params:\n        field = required_params[0]\n        field_info = field.field_info\n        embed = getattr(field_info, \"embed\", None)\n        field_alias_omitted = len(required_params) == 1 and not embed\n        if field_alias_omitted:\n            received_body = {field.alias: received_body}\n\n        for field in required_params:\n            loc: Tuple[str, ...]\n            if field_alias_omitted:\n                loc = (\"body\",)\n            else:\n                loc = (\"body\", field.alias)\n\n            value: Optional[Any] = None\n            if received_body is not None:\n                if (\n                    field.shape in sequence_shapes or field.type_ in sequence_types\n                ) and isinstance(received_body, FormData):\n                    value = received_body.getlist(field.alias)\n                else:\n                    try:\n                        value = received_body.get(field.alias)\n                    except AttributeError:\n                        errors.append(get_missing_field_error(loc))\n                        continue\n            if (\n                value is None\n                or (isinstance(field_info, params.Form) and value == \"\")\n                or (\n                    isinstance(field_info, params.Form)\n                    and field.shape in sequence_shapes\n                    and len(value) == 0\n                )\n            ):\n                if field.required:\n                    errors.append(get_missing_field_error(loc))\n                else:\n                    values[field.name] = deepcopy(field.default)\n                continue\n            if (\n                isinstance(field_info, params.File)\n                and lenient_issubclass(field.type_, bytes)\n                and isinstance(value, UploadFile)\n            ):\n                value = await value.read()\n            elif (\n                field.shape in sequence_shapes\n                and isinstance(field_info, params.File)\n                and lenient_issubclass(field.type_, bytes)\n                and isinstance(value, sequence_types)\n            ):\n                results: List[Union[bytes, str]] = []\n\n                async def process_fn(\n                    fn: Callable[[], Coroutine[Any, Any, Any]]\n                ) -> None:\n                    result = await fn()\n                    results.append(result)\n\n                async with anyio.create_task_group() as tg:\n                    for sub_value in value:\n                        tg.start_soon(process_fn, sub_value.read)\n                value = sequence_shape_to_type[field.shape](results)\n\n            v_, errors_ = field.validate(value, values, loc=loc)\n\n            if isinstance(errors_, ErrorWrapper):\n                errors.append(errors_)\n            elif isinstance(errors_, list):\n                errors.extend(errors_)\n            else:\n                values[field.name] = v_\n    return values, errors\n\n\ndef get_missing_field_error(loc: Tuple[str, ...]) -> ErrorWrapper:\n    missing_field_error = ErrorWrapper(MissingError(), loc=loc)\n    return missing_field_error\n\n\ndef get_body_field(*, dependant: Dependant, name: str) -> Optional[ModelField]:\n    flat_dependant = get_flat_dependant(dependant)\n    if not flat_dependant.body_params:\n        return None\n    first_param = flat_dependant.body_params[0]\n    field_info = first_param.field_info\n    embed = getattr(field_info, \"embed\", None)\n    body_param_names_set = {param.name for param in flat_dependant.body_params}\n    if len(body_param_names_set) == 1 and not embed:\n        check_file_field(first_param)\n        return first_param\n    # If one field requires to embed, all have to be embedded\n    # in case a sub-dependency is evaluated with a single unique body field\n    # That is combined (embedded) with other body fields\n    for param in flat_dependant.body_params:\n        setattr(param.field_info, \"embed\", True)  # noqa: B010\n    model_name = \"Body_\" + name\n    BodyModel: Type[BaseModel] = create_model(model_name)\n    for f in flat_dependant.body_params:\n        BodyModel.__fields__[f.name] = f\n    required = any(True for f in flat_dependant.body_params if f.required)\n\n    BodyFieldInfo_kwargs: Dict[str, Any] = {\"default\": None}\n    if any(isinstance(f.field_info, params.File) for f in flat_dependant.body_params):\n        BodyFieldInfo: Type[params.Body] = params.File\n    elif any(isinstance(f.field_info, params.Form) for f in flat_dependant.body_params):\n        BodyFieldInfo = params.Form\n    else:\n        BodyFieldInfo = params.Body\n\n        body_param_media_types = [\n            f.field_info.media_type\n            for f in flat_dependant.body_params\n            if isinstance(f.field_info, params.Body)\n        ]\n        if len(set(body_param_media_types)) == 1:\n            BodyFieldInfo_kwargs[\"media_type\"] = body_param_media_types[0]\n    final_field = create_response_field(\n        name=\"body\",\n        type_=BodyModel,\n        required=required,\n        alias=\"body\",\n        field_info=BodyFieldInfo(**BodyFieldInfo_kwargs),\n    )\n    check_file_field(final_field)\n    return final_field\n", 767], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py": ["\"\"\"Parse (absolute and relative) URLs.\n\nurlparse module is based upon the following RFC specifications.\n\nRFC 3986 (STD66): \"Uniform Resource Identifiers\" by T. Berners-Lee, R. Fielding\nand L.  Masinter, January 2005.\n\nRFC 2732 : \"Format for Literal IPv6 Addresses in URL's by R.Hinden, B.Carpenter\nand L.Masinter, December 1999.\n\nRFC 2396:  \"Uniform Resource Identifiers (URI)\": Generic Syntax by T.\nBerners-Lee, R. Fielding, and L. Masinter, August 1998.\n\nRFC 2368: \"The mailto URL scheme\", by P.Hoffman , L Masinter, J. Zawinski, July 1998.\n\nRFC 1808: \"Relative Uniform Resource Locators\", by R. Fielding, UC Irvine, June\n1995.\n\nRFC 1738: \"Uniform Resource Locators (URL)\" by T. Berners-Lee, L. Masinter, M.\nMcCahill, December 1994\n\nRFC 3986 is considered the current standard and any future changes to\nurlparse module should conform with it.  The urlparse module is\ncurrently not entirely compliant with this RFC due to defacto\nscenarios for parsing, and for backward compatibility purposes, some\nparsing quirks from older RFCs are retained. The testcases in\ntest_urlparse.py provides a good indicator of parsing behavior.\n\"\"\"\n\nimport re\nimport sys\nimport collections\n\n__all__ = [\"urlparse\", \"urlunparse\", \"urljoin\", \"urldefrag\",\n           \"urlsplit\", \"urlunsplit\", \"urlencode\", \"parse_qs\",\n           \"parse_qsl\", \"quote\", \"quote_plus\", \"quote_from_bytes\",\n           \"unquote\", \"unquote_plus\", \"unquote_to_bytes\",\n           \"DefragResult\", \"ParseResult\", \"SplitResult\",\n           \"DefragResultBytes\", \"ParseResultBytes\", \"SplitResultBytes\"]\n\n# A classification of schemes.\n# The empty string classifies URLs with no scheme specified,\n# being the default value returned by \u201curlsplit\u201d and \u201curlparse\u201d.\n\nuses_relative = ['', 'ftp', 'http', 'gopher', 'nntp', 'imap',\n                 'wais', 'file', 'https', 'shttp', 'mms',\n                 'prospero', 'rtsp', 'rtspu', 'sftp',\n                 'svn', 'svn+ssh', 'ws', 'wss']\n\nuses_netloc = ['', 'ftp', 'http', 'gopher', 'nntp', 'telnet',\n               'imap', 'wais', 'file', 'mms', 'https', 'shttp',\n               'snews', 'prospero', 'rtsp', 'rtspu', 'rsync',\n               'svn', 'svn+ssh', 'sftp', 'nfs', 'git', 'git+ssh',\n               'ws', 'wss']\n\nuses_params = ['', 'ftp', 'hdl', 'prospero', 'http', 'imap',\n               'https', 'shttp', 'rtsp', 'rtspu', 'sip', 'sips',\n               'mms', 'sftp', 'tel']\n\n# These are not actually used anymore, but should stay for backwards\n# compatibility.  (They are undocumented, but have a public-looking name.)\n\nnon_hierarchical = ['gopher', 'hdl', 'mailto', 'news',\n                    'telnet', 'wais', 'imap', 'snews', 'sip', 'sips']\n\nuses_query = ['', 'http', 'wais', 'imap', 'https', 'shttp', 'mms',\n              'gopher', 'rtsp', 'rtspu', 'sip', 'sips']\n\nuses_fragment = ['', 'ftp', 'hdl', 'http', 'gopher', 'news',\n                 'nntp', 'wais', 'https', 'shttp', 'snews',\n                 'file', 'prospero']\n\n# Characters valid in scheme names\nscheme_chars = ('abcdefghijklmnopqrstuvwxyz'\n                'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                '0123456789'\n                '+-.')\n\n# Unsafe bytes to be removed per WHATWG spec\n_UNSAFE_URL_BYTES_TO_REMOVE = ['\\t', '\\r', '\\n']\n\n# XXX: Consider replacing with functools.lru_cache\nMAX_CACHE_SIZE = 20\n_parse_cache = {}\n\ndef clear_cache():\n    \"\"\"Clear the parse cache and the quoters cache.\"\"\"\n    _parse_cache.clear()\n    _safe_quoters.clear()\n\n\n# Helpers for bytes handling\n# For 3.2, we deliberately require applications that\n# handle improperly quoted URLs to do their own\n# decoding and encoding. If valid use cases are\n# presented, we may relax this by using latin-1\n# decoding internally for 3.3\n_implicit_encoding = 'ascii'\n_implicit_errors = 'strict'\n\ndef _noop(obj):\n    return obj\n\ndef _encode_result(obj, encoding=_implicit_encoding,\n                        errors=_implicit_errors):\n    return obj.encode(encoding, errors)\n\ndef _decode_args(args, encoding=_implicit_encoding,\n                       errors=_implicit_errors):\n    return tuple(x.decode(encoding, errors) if x else '' for x in args)\n\ndef _coerce_args(*args):\n    # Invokes decode if necessary to create str args\n    # and returns the coerced inputs along with\n    # an appropriate result coercion function\n    #   - noop for str inputs\n    #   - encoding function otherwise\n    str_input = isinstance(args[0], str)\n    for arg in args[1:]:\n        # We special-case the empty string to support the\n        # \"scheme=''\" default argument to some functions\n        if arg and isinstance(arg, str) != str_input:\n            raise TypeError(\"Cannot mix str and non-str arguments\")\n    if str_input:\n        return args + (_noop,)\n    return _decode_args(args) + (_encode_result,)\n\n# Result objects are more helpful than simple tuples\nclass _ResultMixinStr(object):\n    \"\"\"Standard approach to encoding parsed results from str to bytes\"\"\"\n    __slots__ = ()\n\n    def encode(self, encoding='ascii', errors='strict'):\n        return self._encoded_counterpart(*(x.encode(encoding, errors) for x in self))\n\n\nclass _ResultMixinBytes(object):\n    \"\"\"Standard approach to decoding parsed results from bytes to str\"\"\"\n    __slots__ = ()\n\n    def decode(self, encoding='ascii', errors='strict'):\n        return self._decoded_counterpart(*(x.decode(encoding, errors) for x in self))\n\n\nclass _NetlocResultMixinBase(object):\n    \"\"\"Shared methods for the parsed result objects containing a netloc element\"\"\"\n    __slots__ = ()\n\n    @property\n    def username(self):\n        return self._userinfo[0]\n\n    @property\n    def password(self):\n        return self._userinfo[1]\n\n    @property\n    def hostname(self):\n        hostname = self._hostinfo[0]\n        if not hostname:\n            return None\n        # Scoped IPv6 address may have zone info, which must not be lowercased\n        # like http://[fe80::822a:a8ff:fe49:470c%tESt]:1234/keys\n        separator = '%' if isinstance(hostname, str) else b'%'\n        hostname, percent, zone = hostname.partition(separator)\n        return hostname.lower() + percent + zone\n\n    @property\n    def port(self):\n        port = self._hostinfo[1]\n        if port is not None:\n            port = int(port, 10)\n            if not ( 0 <= port <= 65535):\n                raise ValueError(\"Port out of range 0-65535\")\n        return port\n\n\nclass _NetlocResultMixinStr(_NetlocResultMixinBase, _ResultMixinStr):\n    __slots__ = ()\n\n    @property\n    def _userinfo(self):\n        netloc = self.netloc\n        userinfo, have_info, hostinfo = netloc.rpartition('@')\n        if have_info:\n            username, have_password, password = userinfo.partition(':')\n            if not have_password:\n                password = None\n        else:\n            username = password = None\n        return username, password\n\n    @property\n    def _hostinfo(self):\n        netloc = self.netloc\n        _, _, hostinfo = netloc.rpartition('@')\n        _, have_open_br, bracketed = hostinfo.partition('[')\n        if have_open_br:\n            hostname, _, port = bracketed.partition(']')\n            _, _, port = port.partition(':')\n        else:\n            hostname, _, port = hostinfo.partition(':')\n        if not port:\n            port = None\n        return hostname, port\n\n\nclass _NetlocResultMixinBytes(_NetlocResultMixinBase, _ResultMixinBytes):\n    __slots__ = ()\n\n    @property\n    def _userinfo(self):\n        netloc = self.netloc\n        userinfo, have_info, hostinfo = netloc.rpartition(b'@')\n        if have_info:\n            username, have_password, password = userinfo.partition(b':')\n            if not have_password:\n                password = None\n        else:\n            username = password = None\n        return username, password\n\n    @property\n    def _hostinfo(self):\n        netloc = self.netloc\n        _, _, hostinfo = netloc.rpartition(b'@')\n        _, have_open_br, bracketed = hostinfo.partition(b'[')\n        if have_open_br:\n            hostname, _, port = bracketed.partition(b']')\n            _, _, port = port.partition(b':')\n        else:\n            hostname, _, port = hostinfo.partition(b':')\n        if not port:\n            port = None\n        return hostname, port\n\n\nfrom collections import namedtuple\n\n_DefragResultBase = namedtuple('DefragResult', 'url fragment')\n_SplitResultBase = namedtuple(\n    'SplitResult', 'scheme netloc path query fragment')\n_ParseResultBase = namedtuple(\n    'ParseResult', 'scheme netloc path params query fragment')\n\n_DefragResultBase.__doc__ = \"\"\"\nDefragResult(url, fragment)\n\nA 2-tuple that contains the url without fragment identifier and the fragment\nidentifier as a separate argument.\n\"\"\"\n\n_DefragResultBase.url.__doc__ = \"\"\"The URL with no fragment identifier.\"\"\"\n\n_DefragResultBase.fragment.__doc__ = \"\"\"\nFragment identifier separated from URL, that allows indirect identification of a\nsecondary resource by reference to a primary resource and additional identifying\ninformation.\n\"\"\"\n\n_SplitResultBase.__doc__ = \"\"\"\nSplitResult(scheme, netloc, path, query, fragment)\n\nA 5-tuple that contains the different components of a URL. Similar to\nParseResult, but does not split params.\n\"\"\"\n\n_SplitResultBase.scheme.__doc__ = \"\"\"Specifies URL scheme for the request.\"\"\"\n\n_SplitResultBase.netloc.__doc__ = \"\"\"\nNetwork location where the request is made to.\n\"\"\"\n\n_SplitResultBase.path.__doc__ = \"\"\"\nThe hierarchical path, such as the path to a file to download.\n\"\"\"\n\n_SplitResultBase.query.__doc__ = \"\"\"\nThe query component, that contains non-hierarchical data, that along with data\nin path component, identifies a resource in the scope of URI's scheme and\nnetwork location.\n\"\"\"\n\n_SplitResultBase.fragment.__doc__ = \"\"\"\nFragment identifier, that allows indirect identification of a secondary resource\nby reference to a primary resource and additional identifying information.\n\"\"\"\n\n_ParseResultBase.__doc__ = \"\"\"\nParseResult(scheme, netloc, path, params,  query, fragment)\n\nA 6-tuple that contains components of a parsed URL.\n\"\"\"\n\n_ParseResultBase.scheme.__doc__ = _SplitResultBase.scheme.__doc__\n_ParseResultBase.netloc.__doc__ = _SplitResultBase.netloc.__doc__\n_ParseResultBase.path.__doc__ = _SplitResultBase.path.__doc__\n_ParseResultBase.params.__doc__ = \"\"\"\nParameters for last path element used to dereference the URI in order to provide\naccess to perform some operation on the resource.\n\"\"\"\n\n_ParseResultBase.query.__doc__ = _SplitResultBase.query.__doc__\n_ParseResultBase.fragment.__doc__ = _SplitResultBase.fragment.__doc__\n\n\n# For backwards compatibility, alias _NetlocResultMixinStr\n# ResultBase is no longer part of the documented API, but it is\n# retained since deprecating it isn't worth the hassle\nResultBase = _NetlocResultMixinStr\n\n# Structured result objects for string data\nclass DefragResult(_DefragResultBase, _ResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        if self.fragment:\n            return self.url + '#' + self.fragment\n        else:\n            return self.url\n\nclass SplitResult(_SplitResultBase, _NetlocResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        return urlunsplit(self)\n\nclass ParseResult(_ParseResultBase, _NetlocResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        return urlunparse(self)\n\n# Structured result objects for bytes data\nclass DefragResultBytes(_DefragResultBase, _ResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        if self.fragment:\n            return self.url + b'#' + self.fragment\n        else:\n            return self.url\n\nclass SplitResultBytes(_SplitResultBase, _NetlocResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        return urlunsplit(self)\n\nclass ParseResultBytes(_ParseResultBase, _NetlocResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        return urlunparse(self)\n\n# Set up the encode/decode result pairs\ndef _fix_result_transcoding():\n    _result_pairs = (\n        (DefragResult, DefragResultBytes),\n        (SplitResult, SplitResultBytes),\n        (ParseResult, ParseResultBytes),\n    )\n    for _decoded, _encoded in _result_pairs:\n        _decoded._encoded_counterpart = _encoded\n        _encoded._decoded_counterpart = _decoded\n\n_fix_result_transcoding()\ndel _fix_result_transcoding\n\ndef urlparse(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 6 components:\n    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n    Return a 6-tuple: (scheme, netloc, path, params, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    splitresult = urlsplit(url, scheme, allow_fragments)\n    scheme, netloc, url, query, fragment = splitresult\n    if scheme in uses_params and ';' in url:\n        url, params = _splitparams(url)\n    else:\n        params = ''\n    result = ParseResult(scheme, netloc, url, params, query, fragment)\n    return _coerce_result(result)\n\ndef _splitparams(url):\n    if '/'  in url:\n        i = url.find(';', url.rfind('/'))\n        if i < 0:\n            return url, ''\n    else:\n        i = url.find(';')\n    return url[:i], url[i+1:]\n\ndef _splitnetloc(url, start=0):\n    delim = len(url)   # position of end of domain part of url, default is end\n    for c in '/?#':    # look for delimiters; the order is NOT important\n        wdelim = url.find(c, start)        # find first of this delim\n        if wdelim >= 0:                    # if found\n            delim = min(delim, wdelim)     # use earliest delim position\n    return url[start:delim], url[delim:]   # return (domain, rest)\n\ndef _checknetloc(netloc):\n    if not netloc or netloc.isascii():\n        return\n    # looking for characters like \\u2100 that expand to 'a/c'\n    # IDNA uses NFKC equivalence, so normalize for this check\n    import unicodedata\n    n = netloc.replace('@', '')   # ignore characters already included\n    n = n.replace(':', '')        # but not the surrounding text\n    n = n.replace('#', '')\n    n = n.replace('?', '')\n    netloc2 = unicodedata.normalize('NFKC', n)\n    if n == netloc2:\n        return\n    for c in '/?#@:':\n        if c in netloc2:\n            raise ValueError(\"netloc '\" + netloc + \"' contains invalid \" +\n                             \"characters under NFKC normalization\")\n\ndef _remove_unsafe_bytes_from_url(url):\n    for b in _UNSAFE_URL_BYTES_TO_REMOVE:\n        url = url.replace(b, \"\")\n    return url\n\ndef urlsplit(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 5 components:\n    <scheme>://<netloc>/<path>?<query>#<fragment>\n    Return a 5-tuple: (scheme, netloc, path, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    url = _remove_unsafe_bytes_from_url(url)\n    scheme = _remove_unsafe_bytes_from_url(scheme)\n    allow_fragments = bool(allow_fragments)\n    key = url, scheme, allow_fragments, type(url), type(scheme)\n    cached = _parse_cache.get(key, None)\n    if cached:\n        return _coerce_result(cached)\n    if len(_parse_cache) >= MAX_CACHE_SIZE: # avoid runaway growth\n        clear_cache()\n    netloc = query = fragment = ''\n    i = url.find(':')\n    if i > 0:\n        if url[:i] == 'http': # optimize the common case\n            url = url[i+1:]\n            if url[:2] == '//':\n                netloc, url = _splitnetloc(url, 2)\n                if (('[' in netloc and ']' not in netloc) or\n                        (']' in netloc and '[' not in netloc)):\n                    raise ValueError(\"Invalid IPv6 URL\")\n            if allow_fragments and '#' in url:\n                url, fragment = url.split('#', 1)\n            if '?' in url:\n                url, query = url.split('?', 1)\n            _checknetloc(netloc)\n            v = SplitResult('http', netloc, url, query, fragment)\n            _parse_cache[key] = v\n            return _coerce_result(v)\n        for c in url[:i]:\n            if c not in scheme_chars:\n                break\n        else:\n            # make sure \"url\" is not actually a port number (in which case\n            # \"scheme\" is really part of the path)\n            rest = url[i+1:]\n            if not rest or any(c not in '0123456789' for c in rest):\n                # not a port number\n                scheme, url = url[:i].lower(), rest\n\n    if url[:2] == '//':\n        netloc, url = _splitnetloc(url, 2)\n        if (('[' in netloc and ']' not in netloc) or\n                (']' in netloc and '[' not in netloc)):\n            raise ValueError(\"Invalid IPv6 URL\")\n    if allow_fragments and '#' in url:\n        url, fragment = url.split('#', 1)\n    if '?' in url:\n        url, query = url.split('?', 1)\n    _checknetloc(netloc)\n    v = SplitResult(scheme, netloc, url, query, fragment)\n    _parse_cache[key] = v\n    return _coerce_result(v)\n\ndef urlunparse(components):\n    \"\"\"Put a parsed URL back together again.  This may result in a\n    slightly different, but equivalent URL, if the URL that was parsed\n    originally had redundant delimiters, e.g. a ? with an empty query\n    (the draft states that these are equivalent).\"\"\"\n    scheme, netloc, url, params, query, fragment, _coerce_result = (\n                                                  _coerce_args(*components))\n    if params:\n        url = \"%s;%s\" % (url, params)\n    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))\n\ndef urlunsplit(components):\n    \"\"\"Combine the elements of a tuple as returned by urlsplit() into a\n    complete URL as a string. The data argument can be any five-item iterable.\n    This may result in a slightly different, but equivalent URL, if the URL that\n    was parsed originally had unnecessary delimiters (for example, a ? with an\n    empty query; the RFC states that these are equivalent).\"\"\"\n    scheme, netloc, url, query, fragment, _coerce_result = (\n                                          _coerce_args(*components))\n    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):\n        if url and url[:1] != '/': url = '/' + url\n        url = '//' + (netloc or '') + url\n    if scheme:\n        url = scheme + ':' + url\n    if query:\n        url = url + '?' + query\n    if fragment:\n        url = url + '#' + fragment\n    return _coerce_result(url)\n\ndef urljoin(base, url, allow_fragments=True):\n    \"\"\"Join a base URL and a possibly relative URL to form an absolute\n    interpretation of the latter.\"\"\"\n    if not base:\n        return url\n    if not url:\n        return base\n\n    base, url, _coerce_result = _coerce_args(base, url)\n    bscheme, bnetloc, bpath, bparams, bquery, bfragment = \\\n            urlparse(base, '', allow_fragments)\n    scheme, netloc, path, params, query, fragment = \\\n            urlparse(url, bscheme, allow_fragments)\n\n    if scheme != bscheme or scheme not in uses_relative:\n        return _coerce_result(url)\n    if scheme in uses_netloc:\n        if netloc:\n            return _coerce_result(urlunparse((scheme, netloc, path,\n                                              params, query, fragment)))\n        netloc = bnetloc\n\n    if not path and not params:\n        path = bpath\n        params = bparams\n        if not query:\n            query = bquery\n        return _coerce_result(urlunparse((scheme, netloc, path,\n                                          params, query, fragment)))\n\n    base_parts = bpath.split('/')\n    if base_parts[-1] != '':\n        # the last item is not a directory, so will not be taken into account\n        # in resolving the relative path\n        del base_parts[-1]\n\n    # for rfc3986, ignore all base path should the first character be root.\n    if path[:1] == '/':\n        segments = path.split('/')\n    else:\n        segments = base_parts + path.split('/')\n        # filter out elements that would cause redundant slashes on re-joining\n        # the resolved_path\n        segments[1:-1] = filter(None, segments[1:-1])\n\n    resolved_path = []\n\n    for seg in segments:\n        if seg == '..':\n            try:\n                resolved_path.pop()\n            except IndexError:\n                # ignore any .. segments that would otherwise cause an IndexError\n                # when popped from resolved_path if resolving for rfc3986\n                pass\n        elif seg == '.':\n            continue\n        else:\n            resolved_path.append(seg)\n\n    if segments[-1] in ('.', '..'):\n        # do some post-processing here. if the last segment was a relative dir,\n        # then we need to append the trailing '/'\n        resolved_path.append('')\n\n    return _coerce_result(urlunparse((scheme, netloc, '/'.join(\n        resolved_path) or '/', params, query, fragment)))\n\n\ndef urldefrag(url):\n    \"\"\"Removes any existing fragment from URL.\n\n    Returns a tuple of the defragmented URL and the fragment.  If\n    the URL contained no fragments, the second element is the\n    empty string.\n    \"\"\"\n    url, _coerce_result = _coerce_args(url)\n    if '#' in url:\n        s, n, p, a, q, frag = urlparse(url)\n        defrag = urlunparse((s, n, p, a, q, ''))\n    else:\n        frag = ''\n        defrag = url\n    return _coerce_result(DefragResult(defrag, frag))\n\n_hexdig = '0123456789ABCDEFabcdef'\n_hextobyte = None\n\ndef unquote_to_bytes(string):\n    \"\"\"unquote_to_bytes('abc%20def') -> b'abc def'.\"\"\"\n    # Note: strings are encoded as UTF-8. This is only an issue if it contains\n    # unescaped non-ASCII characters, which URIs should not.\n    if not string:\n        # Is it a string-like object?\n        string.split\n        return b''\n    if isinstance(string, str):\n        string = string.encode('utf-8')\n    bits = string.split(b'%')\n    if len(bits) == 1:\n        return string\n    res = [bits[0]]\n    append = res.append\n    # Delay the initialization of the table to not waste memory\n    # if the function is never called\n    global _hextobyte\n    if _hextobyte is None:\n        _hextobyte = {(a + b).encode(): bytes.fromhex(a + b)\n                      for a in _hexdig for b in _hexdig}\n    for item in bits[1:]:\n        try:\n            append(_hextobyte[item[:2]])\n            append(item[2:])\n        except KeyError:\n            append(b'%')\n            append(item)\n    return b''.join(res)\n\n_asciire = re.compile('([\\x00-\\x7f]+)')\n\ndef unquote(string, encoding='utf-8', errors='replace'):\n    \"\"\"Replace %xx escapes by their single-character equivalent. The optional\n    encoding and errors parameters specify how to decode percent-encoded\n    sequences into Unicode characters, as accepted by the bytes.decode()\n    method.\n    By default, percent-encoded sequences are decoded with UTF-8, and invalid\n    sequences are replaced by a placeholder character.\n\n    unquote('abc%20def') -> 'abc def'.\n    \"\"\"\n    if '%' not in string:\n        string.split\n        return string\n    if encoding is None:\n        encoding = 'utf-8'\n    if errors is None:\n        errors = 'replace'\n    bits = _asciire.split(string)\n    res = [bits[0]]\n    append = res.append\n    for i in range(1, len(bits), 2):\n        append(unquote_to_bytes(bits[i]).decode(encoding, errors))\n        append(bits[i + 1])\n    return ''.join(res)\n\n\ndef parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n             encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as\n            blank strings.  The default false value indicates that\n            blank values are to be ignored and treated as if they were\n            not included.\n\n        strict_parsing: flag indicating what to do with parsing errors.\n            If false (the default), errors are silently ignored.\n            If true, errors raise a ValueError exception.\n\n        encoding and errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n\n        max_num_fields: int. If set, then throws a ValueError if there\n            are more than n fields read by parse_qsl().\n\n        separator: str. The symbol to use for separating the query arguments.\n            Defaults to &.\n\n        Returns a dictionary.\n    \"\"\"\n    parsed_result = {}\n    pairs = parse_qsl(qs, keep_blank_values, strict_parsing,\n                      encoding=encoding, errors=errors,\n                      max_num_fields=max_num_fields, separator=separator)\n    for name, value in pairs:\n        if name in parsed_result:\n            parsed_result[name].append(value)\n        else:\n            parsed_result[name] = [value]\n    return parsed_result\n\n\ndef parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n              encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception.\n\n        encoding and errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n\n        max_num_fields: int. If set, then throws a ValueError\n            if there are more than n fields read by parse_qsl().\n\n        separator: str. The symbol to use for separating the query arguments.\n            Defaults to &.\n\n        Returns a list, as G-d intended.\n    \"\"\"\n    qs, _coerce_result = _coerce_args(qs)\n\n    if not separator or (not isinstance(separator, (str, bytes))):\n        raise ValueError(\"Separator must be of type string or bytes.\")\n\n    # If max_num_fields is defined then check that the number of fields\n    # is less than max_num_fields. This prevents a memory exhaustion DOS\n    # attack via post bodies with many fields.\n    if max_num_fields is not None:\n        num_fields = 1 + qs.count(separator)\n        if max_num_fields < num_fields:\n            raise ValueError('Max number of fields exceeded')\n\n    pairs = [s1 for s1 in qs.split(separator)]\n    r = []\n    for name_value in pairs:\n        if not name_value and not strict_parsing:\n            continue\n        nv = name_value.split('=', 1)\n        if len(nv) != 2:\n            if strict_parsing:\n                raise ValueError(\"bad query field: %r\" % (name_value,))\n            # Handle case of a control-name with no equal sign\n            if keep_blank_values:\n                nv.append('')\n            else:\n                continue\n        if len(nv[1]) or keep_blank_values:\n            name = nv[0].replace('+', ' ')\n            name = unquote(name, encoding=encoding, errors=errors)\n            name = _coerce_result(name)\n            value = nv[1].replace('+', ' ')\n            value = unquote(value, encoding=encoding, errors=errors)\n            value = _coerce_result(value)\n            r.append((name, value))\n    return r\n\ndef unquote_plus(string, encoding='utf-8', errors='replace'):\n    \"\"\"Like unquote(), but also replace plus signs by spaces, as required for\n    unquoting HTML form values.\n\n    unquote_plus('%7e/abc+def') -> '~/abc def'\n    \"\"\"\n    string = string.replace('+', ' ')\n    return unquote(string, encoding, errors)\n\n_ALWAYS_SAFE = frozenset(b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                         b'abcdefghijklmnopqrstuvwxyz'\n                         b'0123456789'\n                         b'_.-~')\n_ALWAYS_SAFE_BYTES = bytes(_ALWAYS_SAFE)\n_safe_quoters = {}\n\nclass Quoter(collections.defaultdict):\n    \"\"\"A mapping from bytes (in range(0,256)) to strings.\n\n    String values are percent-encoded byte values, unless the key < 128, and\n    in the \"safe\" set (either the specified safe set, or default set).\n    \"\"\"\n    # Keeps a cache internally, using defaultdict, for efficiency (lookups\n    # of cached keys don't call Python code at all).\n    def __init__(self, safe):\n        \"\"\"safe: bytes object.\"\"\"\n        self.safe = _ALWAYS_SAFE.union(safe)\n\n    def __repr__(self):\n        # Without this, will just display as a defaultdict\n        return \"<%s %r>\" % (self.__class__.__name__, dict(self))\n\n    def __missing__(self, b):\n        # Handle a cache miss. Store quoted string in cache and return.\n        res = chr(b) if b in self.safe else '%{:02X}'.format(b)\n        self[b] = res\n        return res\n\ndef quote(string, safe='/', encoding=None, errors=None):\n    \"\"\"quote('abc def') -> 'abc%20def'\n\n    Each part of a URL, e.g. the path info, the query, etc., has a\n    different set of reserved characters that must be quoted. The\n    quote function offers a cautious (not minimal) way to quote a\n    string for most of these parts.\n\n    RFC 3986 Uniform Resource Identifier (URI): Generic Syntax lists\n    the following (un)reserved characters.\n\n    unreserved    = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n    reserved      = gen-delims / sub-delims\n    gen-delims    = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\"\n    sub-delims    = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\"\n                  / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n\n    Each of the reserved characters is reserved in some component of a URL,\n    but not necessarily in all of them.\n\n    The quote function %-escapes all characters that are neither in the\n    unreserved chars (\"always safe\") nor the additional chars set via the\n    safe arg.\n\n    The default for the safe arg is '/'. The character is reserved, but in\n    typical usage the quote function is being called on a path where the\n    existing slash characters are to be preserved.\n\n    Python 3.7 updates from using RFC 2396 to RFC 3986 to quote URL strings.\n    Now, \"~\" is included in the set of unreserved characters.\n\n    string and safe may be either str or bytes objects. encoding and errors\n    must not be specified if string is a bytes object.\n\n    The optional encoding and errors parameters specify how to deal with\n    non-ASCII characters, as accepted by the str.encode method.\n    By default, encoding='utf-8' (characters are encoded with UTF-8), and\n    errors='strict' (unsupported characters raise a UnicodeEncodeError).\n    \"\"\"\n    if isinstance(string, str):\n        if not string:\n            return string\n        if encoding is None:\n            encoding = 'utf-8'\n        if errors is None:\n            errors = 'strict'\n        string = string.encode(encoding, errors)\n    else:\n        if encoding is not None:\n            raise TypeError(\"quote() doesn't support 'encoding' for bytes\")\n        if errors is not None:\n            raise TypeError(\"quote() doesn't support 'errors' for bytes\")\n    return quote_from_bytes(string, safe)\n\ndef quote_plus(string, safe='', encoding=None, errors=None):\n    \"\"\"Like quote(), but also replace ' ' with '+', as required for quoting\n    HTML form values. Plus signs in the original string are escaped unless\n    they are included in safe. It also does not have safe default to '/'.\n    \"\"\"\n    # Check if ' ' in string, where string may either be a str or bytes.  If\n    # there are no spaces, the regular quote will produce the right answer.\n    if ((isinstance(string, str) and ' ' not in string) or\n        (isinstance(string, bytes) and b' ' not in string)):\n        return quote(string, safe, encoding, errors)\n    if isinstance(safe, str):\n        space = ' '\n    else:\n        space = b' '\n    string = quote(string, safe + space, encoding, errors)\n    return string.replace(' ', '+')\n\ndef quote_from_bytes(bs, safe='/'):\n    \"\"\"Like quote(), but accepts a bytes object rather than a str, and does\n    not perform string-to-bytes encoding.  It always returns an ASCII string.\n    quote_from_bytes(b'abc def\\x3f') -> 'abc%20def%3f'\n    \"\"\"\n    if not isinstance(bs, (bytes, bytearray)):\n        raise TypeError(\"quote_from_bytes() expected bytes\")\n    if not bs:\n        return ''\n    if isinstance(safe, str):\n        # Normalize 'safe' by converting to bytes and removing non-ASCII chars\n        safe = safe.encode('ascii', 'ignore')\n    else:\n        safe = bytes([c for c in safe if c < 128])\n    if not bs.rstrip(_ALWAYS_SAFE_BYTES + safe):\n        return bs.decode()\n    try:\n        quoter = _safe_quoters[safe]\n    except KeyError:\n        _safe_quoters[safe] = quoter = Quoter(safe).__getitem__\n    return ''.join([quoter(char) for char in bs])\n\ndef urlencode(query, doseq=False, safe='', encoding=None, errors=None,\n              quote_via=quote_plus):\n    \"\"\"Encode a dict or sequence of two-element tuples into a URL query string.\n\n    If any values in the query arg are sequences and doseq is true, each\n    sequence element is converted to a separate parameter.\n\n    If the query arg is a sequence of two-element tuples, the order of the\n    parameters in the output will match the order of parameters in the\n    input.\n\n    The components of a query arg may each be either a string or a bytes type.\n\n    The safe, encoding, and errors parameters are passed down to the function\n    specified by quote_via (encoding and errors only if a component is a str).\n    \"\"\"\n\n    if hasattr(query, \"items\"):\n        query = query.items()\n    else:\n        # It's a bother at times that strings and string-like objects are\n        # sequences.\n        try:\n            # non-sequence items should not work with len()\n            # non-empty strings will fail this\n            if len(query) and not isinstance(query[0], tuple):\n                raise TypeError\n            # Zero-length sequences of all types will get here and succeed,\n            # but that's a minor nit.  Since the original implementation\n            # allowed empty dicts that type of behavior probably should be\n            # preserved for consistency\n        except TypeError:\n            ty, va, tb = sys.exc_info()\n            raise TypeError(\"not a valid non-string sequence \"\n                            \"or mapping object\").with_traceback(tb)\n\n    l = []\n    if not doseq:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_via(k, safe)\n            else:\n                k = quote_via(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_via(v, safe)\n            else:\n                v = quote_via(str(v), safe, encoding, errors)\n            l.append(k + '=' + v)\n    else:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_via(k, safe)\n            else:\n                k = quote_via(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_via(v, safe)\n                l.append(k + '=' + v)\n            elif isinstance(v, str):\n                v = quote_via(v, safe, encoding, errors)\n                l.append(k + '=' + v)\n            else:\n                try:\n                    # Is this a sufficient test for sequence-ness?\n                    x = len(v)\n                except TypeError:\n                    # not a sequence\n                    v = quote_via(str(v), safe, encoding, errors)\n                    l.append(k + '=' + v)\n                else:\n                    # loop over the sequence\n                    for elt in v:\n                        if isinstance(elt, bytes):\n                            elt = quote_via(elt, safe)\n                        else:\n                            elt = quote_via(str(elt), safe, encoding, errors)\n                        l.append(k + '=' + elt)\n    return '&'.join(l)\n\ndef to_bytes(url):\n    \"\"\"to_bytes(u\"URL\") --> 'URL'.\"\"\"\n    # Most URL schemes require ASCII. If that changes, the conversion\n    # can be relaxed.\n    # XXX get rid of to_bytes()\n    if isinstance(url, str):\n        try:\n            url = url.encode(\"ASCII\").decode()\n        except UnicodeError:\n            raise UnicodeError(\"URL \" + repr(url) +\n                               \" contains non-ASCII characters\")\n    return url\n\ndef unwrap(url):\n    \"\"\"unwrap('<URL:type://host/path>') --> 'type://host/path'.\"\"\"\n    url = str(url).strip()\n    if url[:1] == '<' and url[-1:] == '>':\n        url = url[1:-1].strip()\n    if url[:4] == 'URL:': url = url[4:].strip()\n    return url\n\n_typeprog = None\ndef splittype(url):\n    \"\"\"splittype('type:opaquestring') --> 'type', 'opaquestring'.\"\"\"\n    global _typeprog\n    if _typeprog is None:\n        _typeprog = re.compile('([^/:]+):(.*)', re.DOTALL)\n\n    match = _typeprog.match(url)\n    if match:\n        scheme, data = match.groups()\n        return scheme.lower(), data\n    return None, url\n\n_hostprog = None\ndef splithost(url):\n    \"\"\"splithost('//host[:port]/path') --> 'host[:port]', '/path'.\"\"\"\n    global _hostprog\n    if _hostprog is None:\n        _hostprog = re.compile('//([^/#?]*)(.*)', re.DOTALL)\n\n    match = _hostprog.match(url)\n    if match:\n        host_port, path = match.groups()\n        if path and path[0] != '/':\n            path = '/' + path\n        return host_port, path\n    return None, url\n\ndef splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    user, delim, host = host.rpartition('@')\n    return (user if delim else None), host\n\ndef splitpasswd(user):\n    \"\"\"splitpasswd('user:passwd') -> 'user', 'passwd'.\"\"\"\n    user, delim, passwd = user.partition(':')\n    return user, (passwd if delim else None)\n\n# splittag('/path#tag') --> '/path', 'tag'\n_portprog = None\ndef splitport(host):\n    \"\"\"splitport('host:port') --> 'host', 'port'.\"\"\"\n    global _portprog\n    if _portprog is None:\n        _portprog = re.compile('(.*):([0-9]*)', re.DOTALL)\n\n    match = _portprog.fullmatch(host)\n    if match:\n        host, port = match.groups()\n        if port:\n            return host, port\n    return host, None\n\ndef splitnport(host, defport=-1):\n    \"\"\"Split host and port, returning numeric port.\n    Return given default port if no ':' found; defaults to -1.\n    Return numerical port if a valid number are found after ':'.\n    Return None if ':' but not a valid number.\"\"\"\n    host, delim, port = host.rpartition(':')\n    if not delim:\n        host = port\n    elif port:\n        try:\n            nport = int(port)\n        except ValueError:\n            nport = None\n        return host, nport\n    return host, defport\n\ndef splitquery(url):\n    \"\"\"splitquery('/path?query') --> '/path', 'query'.\"\"\"\n    path, delim, query = url.rpartition('?')\n    if delim:\n        return path, query\n    return url, None\n\ndef splittag(url):\n    \"\"\"splittag('/path#tag') --> '/path', 'tag'.\"\"\"\n    path, delim, tag = url.rpartition('#')\n    if delim:\n        return path, tag\n    return url, None\n\ndef splitattr(url):\n    \"\"\"splitattr('/path;attr1=value1;attr2=value2;...') ->\n        '/path', ['attr1=value1', 'attr2=value2', ...].\"\"\"\n    words = url.split(';')\n    return words[0], words[1:]\n\ndef splitvalue(attr):\n    \"\"\"splitvalue('attr=value') --> 'attr', 'value'.\"\"\"\n    attr, delim, value = attr.partition('=')\n    return attr, (value if delim else None)\n", 1087], "/usr/local/opt/python@3.7/bin/../Frameworks/Python.framework/Versions/3.7/lib/python3.7/_collections_abc.py": ["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nUnit tests are in test_collections.\n\"\"\"\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\n__all__ = [\"Awaitable\", \"Coroutine\",\n           \"AsyncIterable\", \"AsyncIterator\", \"AsyncGenerator\",\n           \"Hashable\", \"Iterable\", \"Iterator\", \"Generator\", \"Reversible\",\n           \"Sized\", \"Container\", \"Callable\", \"Collection\",\n           \"Set\", \"MutableSet\",\n           \"Mapping\", \"MutableMapping\",\n           \"MappingView\", \"KeysView\", \"ItemsView\", \"ValuesView\",\n           \"Sequence\", \"MutableSequence\",\n           \"ByteString\",\n           ]\n\n# This module has been renamed from collections.abc to _collections_abc to\n# speed up interpreter startup. Some of the types such as MutableMapping are\n# required early but collections module imports a lot of other modules.\n# See issue #19218\n__name__ = \"collections.abc\"\n\n# Private list of types that we want to register with the various ABCs\n# so that they will pass tests like:\n#       it = iter(somebytearray)\n#       assert isinstance(it, Iterable)\n# Note:  in other implementations, these types might not be distinct\n# and they may have their own implementation specific types that\n# are not included on this list.\nbytes_iterator = type(iter(b''))\nbytearray_iterator = type(iter(bytearray()))\n#callable_iterator = ???\ndict_keyiterator = type(iter({}.keys()))\ndict_valueiterator = type(iter({}.values()))\ndict_itemiterator = type(iter({}.items()))\nlist_iterator = type(iter([]))\nlist_reverseiterator = type(iter(reversed([])))\nrange_iterator = type(iter(range(0)))\nlongrange_iterator = type(iter(range(1 << 1000)))\nset_iterator = type(iter(set()))\nstr_iterator = type(iter(\"\"))\ntuple_iterator = type(iter(()))\nzip_iterator = type(iter(zip()))\n## views ##\ndict_keys = type({}.keys())\ndict_values = type({}.values())\ndict_items = type({}.items())\n## misc ##\nmappingproxy = type(type.__dict__)\ngenerator = type((lambda: (yield))())\n## coroutine ##\nasync def _coro(): pass\n_coro = _coro()\ncoroutine = type(_coro)\n_coro.close()  # Prevent ResourceWarning\ndel _coro\n## asynchronous generator ##\nasync def _ag(): yield\n_ag = _ag()\nasync_generator = type(_ag)\ndel _ag\n\n\n### ONE-TRICK PONIES ###\n\ndef _check_methods(C, *methods):\n    mro = C.__mro__\n    for method in methods:\n        for B in mro:\n            if method in B.__dict__:\n                if B.__dict__[method] is None:\n                    return NotImplemented\n                break\n        else:\n            return NotImplemented\n    return True\n\nclass Hashable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            return _check_methods(C, \"__hash__\")\n        return NotImplemented\n\n\nclass Awaitable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __await__(self):\n        yield\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Awaitable:\n            return _check_methods(C, \"__await__\")\n        return NotImplemented\n\n\nclass Coroutine(Awaitable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"coroutine ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Coroutine:\n            return _check_methods(C, '__await__', 'send', 'throw', 'close')\n        return NotImplemented\n\n\nCoroutine.register(coroutine)\n\n\nclass AsyncIterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __aiter__(self):\n        return AsyncIterator()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterable:\n            return _check_methods(C, \"__aiter__\")\n        return NotImplemented\n\n\nclass AsyncIterator(AsyncIterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    async def __anext__(self):\n        \"\"\"Return the next item or raise StopAsyncIteration when exhausted.\"\"\"\n        raise StopAsyncIteration\n\n    def __aiter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterator:\n            return _check_methods(C, \"__anext__\", \"__aiter__\")\n        return NotImplemented\n\n\nclass AsyncGenerator(AsyncIterator):\n\n    __slots__ = ()\n\n    async def __anext__(self):\n        \"\"\"Return the next item from the asynchronous generator.\n        When exhausted, raise StopAsyncIteration.\n        \"\"\"\n        return await self.asend(None)\n\n    @abstractmethod\n    async def asend(self, value):\n        \"\"\"Send a value into the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        raise StopAsyncIteration\n\n    @abstractmethod\n    async def athrow(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    async def aclose(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            await self.athrow(GeneratorExit)\n        except (GeneratorExit, StopAsyncIteration):\n            pass\n        else:\n            raise RuntimeError(\"asynchronous generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncGenerator:\n            return _check_methods(C, '__aiter__', '__anext__',\n                                  'asend', 'athrow', 'aclose')\n        return NotImplemented\n\n\nAsyncGenerator.register(async_generator)\n\n\nclass Iterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            return _check_methods(C, \"__iter__\")\n        return NotImplemented\n\n\nclass Iterator(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __next__(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            return _check_methods(C, '__iter__', '__next__')\n        return NotImplemented\n\nIterator.register(bytes_iterator)\nIterator.register(bytearray_iterator)\n#Iterator.register(callable_iterator)\nIterator.register(dict_keyiterator)\nIterator.register(dict_valueiterator)\nIterator.register(dict_itemiterator)\nIterator.register(list_iterator)\nIterator.register(list_reverseiterator)\nIterator.register(range_iterator)\nIterator.register(longrange_iterator)\nIterator.register(set_iterator)\nIterator.register(str_iterator)\nIterator.register(tuple_iterator)\nIterator.register(zip_iterator)\n\n\nclass Reversible(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __reversed__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Reversible:\n            return _check_methods(C, \"__reversed__\", \"__iter__\")\n        return NotImplemented\n\n\nclass Generator(Iterator):\n\n    __slots__ = ()\n\n    def __next__(self):\n        \"\"\"Return the next item from the generator.\n        When exhausted, raise StopIteration.\n        \"\"\"\n        return self.send(None)\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside generator.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Generator:\n            return _check_methods(C, '__iter__', '__next__',\n                                  'send', 'throw', 'close')\n        return NotImplemented\n\nGenerator.register(generator)\n\n\nclass Sized(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            return _check_methods(C, \"__len__\")\n        return NotImplemented\n\n\nclass Container(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            return _check_methods(C, \"__contains__\")\n        return NotImplemented\n\nclass Collection(Sized, Iterable, Container):\n\n    __slots__ = ()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Collection:\n            return _check_methods(C,  \"__len__\", \"__iter__\", \"__contains__\")\n        return NotImplemented\n\nclass Callable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            return _check_methods(C, \"__call__\")\n        return NotImplemented\n\n\n### SETS ###\n\n\nclass Set(Collection):\n\n    \"\"\"A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), redefine __le__ and __ge__,\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        '''Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        '''\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        'Return True if two sets have a null intersection.'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    def _hash(self):\n        \"\"\"Compute the hash value of a set.\n\n        Note that we don't define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there's not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        \"\"\"\n        MAX = sys.maxsize\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    \"\"\"A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def add(self, value):\n        \"\"\"Add an element.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n        raise NotImplementedError\n\n    def remove(self, value):\n        \"\"\"Remove an element. If not a member, raise a KeyError.\"\"\"\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        \"\"\"Return the popped value.  Raise KeyError if empty.\"\"\"\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError from None\n        self.discard(value)\n        return value\n\n    def clear(self):\n        \"\"\"This is slow (creates N new iterators!) but effective.\"\"\"\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\n\nclass Mapping(Collection):\n\n    __slots__ = ()\n\n    \"\"\"A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def keys(self):\n        \"D.keys() -> a set-like object providing a view on D's keys\"\n        return KeysView(self)\n\n    def items(self):\n        \"D.items() -> a set-like object providing a view on D's items\"\n        return ItemsView(self)\n\n    def values(self):\n        \"D.values() -> an object providing a view on D's values\"\n        return ValuesView(self)\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    __reversed__ = None\n\nMapping.register(mappingproxy)\n\n\nclass MappingView(Sized):\n\n    __slots__ = '_mapping',\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0._mapping!r})'.format(self)\n\n\nclass KeysView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        yield from self._mapping\n\nKeysView.register(dict_keys)\n\n\nclass ItemsView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v is value or v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\nItemsView.register(dict_items)\n\n\nclass ValuesView(MappingView, Collection):\n\n    __slots__ = ()\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            v = self._mapping[key]\n            if v is value or v == value:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\nValuesView.register(dict_values)\n\n\nclass MutableMapping(Mapping):\n\n    __slots__ = ()\n\n    \"\"\"A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        '''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        '''\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        '''D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        '''\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError from None\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        'D.clear() -> None.  Remove all items from D.'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(*args, **kwds):\n        ''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        '''\n        if not args:\n            raise TypeError(\"descriptor 'update' of 'MutableMapping' object \"\n                            \"needs an argument\")\n        self, *args = args\n        if len(args) > 1:\n            raise TypeError('update expected at most 1 arguments, got %d' %\n                            len(args))\n        if args:\n            other = args[0]\n            if isinstance(other, Mapping):\n                for key in other:\n                    self[key] = other[key]\n            elif hasattr(other, \"keys\"):\n                for key in other.keys():\n                    self[key] = other[key]\n            else:\n                for key, value in other:\n                    self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\n\nclass Sequence(Reversible, Collection):\n\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v is value or v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value, start=0, stop=None):\n        '''S.index(value, [start, [stop]]) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n\n           Supporting start and stop arguments is optional, but\n           recommended.\n        '''\n        if start is not None and start < 0:\n            start = max(len(self) + start, 0)\n        if stop is not None and stop < 0:\n            stop += len(self)\n\n        i = start\n        while stop is None or i < stop:\n            try:\n                v = self[i]\n                if v is value or v == value:\n                    return i\n            except IndexError:\n                break\n            i += 1\n        raise ValueError\n\n    def count(self, value):\n        'S.count(value) -> integer -- return number of occurrences of value'\n        return sum(1 for v in self if v is value or v == value)\n\nSequence.register(tuple)\nSequence.register(str)\nSequence.register(range)\nSequence.register(memoryview)\n\n\nclass ByteString(Sequence):\n\n    \"\"\"This unifies bytes and bytearray.\n\n    XXX Should add all their methods.\n    \"\"\"\n\n    __slots__ = ()\n\nByteString.register(bytes)\nByteString.register(bytearray)\n\n\nclass MutableSequence(Sequence):\n\n    __slots__ = ()\n\n    \"\"\"All the operations on a read-write sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        'S.insert(index, value) -- insert value before index'\n        raise IndexError\n\n    def append(self, value):\n        'S.append(value) -- append value to the end of the sequence'\n        self.insert(len(self), value)\n\n    def clear(self):\n        'S.clear() -> None -- remove all items from S'\n        try:\n            while True:\n                self.pop()\n        except IndexError:\n            pass\n\n    def reverse(self):\n        'S.reverse() -- reverse *IN PLACE*'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        '''S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        '''\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        '''S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        '''\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\nMutableSequence.register(list)\nMutableSequence.register(bytearray)  # Multiply inheriting, see ByteString\n", 1011], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py": ["\"\"\"Support for tasks, coroutines and the scheduler.\"\"\"\n\n__all__ = (\n    'Task', 'create_task',\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\n    'wait', 'wait_for', 'as_completed', 'sleep',\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\n    'current_task', 'all_tasks',\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\n)\n\nimport concurrent.futures\nimport contextvars\nimport functools\nimport inspect\nimport types\nimport warnings\nimport weakref\n\nfrom . import base_tasks\nfrom . import coroutines\nfrom . import events\nfrom . import futures\nfrom .coroutines import coroutine\n\n\ndef current_task(loop=None):\n    \"\"\"Return a currently executed task.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    return _current_tasks.get(loop)\n\n\ndef all_tasks(loop=None):\n    \"\"\"Return a set of all tasks for the loop.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks\n            if futures._get_loop(t) is loop and not t.done()}\n\n\ndef _all_tasks_compat(loop=None):\n    # Different from \"all_task()\" by returning *all* Tasks, including\n    # the completed ones.  Used to implement deprecated \"Tasks.all_task()\"\n    # method.\n    if loop is None:\n        loop = events.get_event_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks if futures._get_loop(t) is loop}\n\n\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\n                                # from a Python Future implementation.\n\n    \"\"\"A coroutine wrapped in a Future.\"\"\"\n\n    # An important invariant maintained while a Task not done:\n    #\n    # - Either _fut_waiter is None, and _step() is scheduled;\n    # - or _fut_waiter is some Future, and _step() is *not* scheduled.\n    #\n    # The only transition from the latter to the former is through\n    # _wakeup().  When _fut_waiter is not None, one of its callbacks\n    # must be _wakeup().\n\n    # If False, don't log a message if the task is destroyed whereas its\n    # status is still pending\n    _log_destroy_pending = True\n\n    @classmethod\n    def current_task(cls, loop=None):\n        \"\"\"Return the currently running task in an event loop or None.\n\n        By default the current task for the current event loop is returned.\n\n        None is returned when called not in the context of a Task.\n        \"\"\"\n        warnings.warn(\"Task.current_task() is deprecated, \"\n                      \"use asyncio.current_task() instead\",\n                      PendingDeprecationWarning,\n                      stacklevel=2)\n        if loop is None:\n            loop = events.get_event_loop()\n        return current_task(loop)\n\n    @classmethod\n    def all_tasks(cls, loop=None):\n        \"\"\"Return a set of all tasks for an event loop.\n\n        By default all tasks for the current event loop are returned.\n        \"\"\"\n        warnings.warn(\"Task.all_tasks() is deprecated, \"\n                      \"use asyncio.all_tasks() instead\",\n                      PendingDeprecationWarning,\n                      stacklevel=2)\n        return _all_tasks_compat(loop)\n\n    def __init__(self, coro, *, loop=None):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        if not coroutines.iscoroutine(coro):\n            # raise after Future.__init__(), attrs are required for __del__\n            # prevent logging for pending task in __del__\n            self._log_destroy_pending = False\n            raise TypeError(f\"a coroutine was expected, got {coro!r}\")\n\n        self._must_cancel = False\n        self._fut_waiter = None\n        self._coro = coro\n        self._context = contextvars.copy_context()\n\n        self._loop.call_soon(self.__step, context=self._context)\n        _register_task(self)\n\n    def __del__(self):\n        if self._state == futures._PENDING and self._log_destroy_pending:\n            context = {\n                'task': self,\n                'message': 'Task was destroyed but it is pending!',\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        super().__del__()\n\n    def _repr_info(self):\n        return base_tasks._task_repr_info(self)\n\n    def set_result(self, result):\n        raise RuntimeError('Task does not support set_result operation')\n\n    def set_exception(self, exception):\n        raise RuntimeError('Task does not support set_exception operation')\n\n    def get_stack(self, *, limit=None):\n        \"\"\"Return the list of stack frames for this task's coroutine.\n\n        If the coroutine is not done, this returns the stack where it is\n        suspended.  If the coroutine has completed successfully or was\n        cancelled, this returns an empty list.  If the coroutine was\n        terminated by an exception, this returns the list of traceback\n        frames.\n\n        The frames are always ordered from oldest to newest.\n\n        The optional limit gives the maximum number of frames to\n        return; by default all available frames are returned.  Its\n        meaning differs depending on whether a stack or a traceback is\n        returned: the newest frames of a stack are returned, but the\n        oldest frames of a traceback are returned.  (This matches the\n        behavior of the traceback module.)\n\n        For reasons beyond our control, only one stack frame is\n        returned for a suspended coroutine.\n        \"\"\"\n        return base_tasks._task_get_stack(self, limit)\n\n    def print_stack(self, *, limit=None, file=None):\n        \"\"\"Print the stack or traceback for this task's coroutine.\n\n        This produces output similar to that of the traceback module,\n        for the frames retrieved by get_stack().  The limit argument\n        is passed to get_stack().  The file argument is an I/O stream\n        to which the output is written; by default output is written\n        to sys.stderr.\n        \"\"\"\n        return base_tasks._task_print_stack(self, limit, file)\n\n    def cancel(self):\n        \"\"\"Request that this task cancel itself.\n\n        This arranges for a CancelledError to be thrown into the\n        wrapped coroutine on the next cycle through the event loop.\n        The coroutine then has a chance to clean up or even deny\n        the request using try/except/finally.\n\n        Unlike Future.cancel, this does not guarantee that the\n        task will be cancelled: the exception might be caught and\n        acted upon, delaying cancellation of the task or preventing\n        cancellation completely.  The task may also return a value or\n        raise a different exception.\n\n        Immediately after this method is called, Task.cancelled() will\n        not return True (unless the task was already cancelled).  A\n        task will be marked as cancelled when the wrapped coroutine\n        terminates with a CancelledError exception (even if cancel()\n        was not called).\n        \"\"\"\n        self._log_traceback = False\n        if self.done():\n            return False\n        if self._fut_waiter is not None:\n            if self._fut_waiter.cancel():\n                # Leave self._fut_waiter; it may be a Task that\n                # catches and ignores the cancellation so we may have\n                # to cancel it again later.\n                return True\n        # It must be the case that self.__step is already scheduled.\n        self._must_cancel = True\n        return True\n\n    def __step(self, exc=None):\n        if self.done():\n            raise futures.InvalidStateError(\n                f'_step(): already done: {self!r}, {exc!r}')\n        if self._must_cancel:\n            if not isinstance(exc, futures.CancelledError):\n                exc = futures.CancelledError()\n            self._must_cancel = False\n        coro = self._coro\n        self._fut_waiter = None\n\n        _enter_task(self._loop, self)\n        # Call either coro.throw(exc) or coro.send(None).\n        try:\n            if exc is None:\n                # We use the `send` method directly, because coroutines\n                # don't have `__iter__` and `__next__` methods.\n                result = coro.send(None)\n            else:\n                result = coro.throw(exc)\n        except StopIteration as exc:\n            if self._must_cancel:\n                # Task is cancelled right before coro stops.\n                self._must_cancel = False\n                super().set_exception(futures.CancelledError())\n            else:\n                super().set_result(exc.value)\n        except futures.CancelledError:\n            super().cancel()  # I.e., Future.cancel(self).\n        except Exception as exc:\n            super().set_exception(exc)\n        except BaseException as exc:\n            super().set_exception(exc)\n            raise\n        else:\n            blocking = getattr(result, '_asyncio_future_blocking', None)\n            if blocking is not None:\n                # Yielded Future must come from Future.__iter__().\n                if futures._get_loop(result) is not self._loop:\n                    new_exc = RuntimeError(\n                        f'Task {self!r} got Future '\n                        f'{result!r} attached to a different loop')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n                elif blocking:\n                    if result is self:\n                        new_exc = RuntimeError(\n                            f'Task cannot await on itself: {self!r}')\n                        self._loop.call_soon(\n                            self.__step, new_exc, context=self._context)\n                    else:\n                        result._asyncio_future_blocking = False\n                        result.add_done_callback(\n                            self.__wakeup, context=self._context)\n                        self._fut_waiter = result\n                        if self._must_cancel:\n                            if self._fut_waiter.cancel():\n                                self._must_cancel = False\n                else:\n                    new_exc = RuntimeError(\n                        f'yield was used instead of yield from '\n                        f'in task {self!r} with {result!r}')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n\n            elif result is None:\n                # Bare yield relinquishes control for one event loop iteration.\n                self._loop.call_soon(self.__step, context=self._context)\n            elif inspect.isgenerator(result):\n                # Yielding a generator is just wrong.\n                new_exc = RuntimeError(\n                    f'yield was used instead of yield from for '\n                    f'generator in task {self!r} with {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n            else:\n                # Yielding something else is an error.\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n        finally:\n            _leave_task(self._loop, self)\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __wakeup(self, future):\n        try:\n            future.result()\n        except Exception as exc:\n            # This may also be a cancellation.\n            self.__step(exc)\n        else:\n            # Don't pass the value of `future.result()` explicitly,\n            # as `Future.__iter__` and `Future.__await__` don't need it.\n            # If we call `_step(value, None)` instead of `_step()`,\n            # Python eval loop would use `.send(value)` method call,\n            # instead of `__next__()`, which is slower for futures\n            # that return non-generator iterators from their `__iter__`.\n            self.__step()\n        self = None  # Needed to break cycles when an exception occurs.\n\n\n_PyTask = Task\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CTask is needed for tests.\n    Task = _CTask = _asyncio.Task\n\n\ndef create_task(coro):\n    \"\"\"Schedule the execution of a coroutine object in a spawn task.\n\n    Return a Task object.\n    \"\"\"\n    loop = events.get_running_loop()\n    return loop.create_task(coro)\n\n\n# wait() and as_completed() similar to those in PEP 3148.\n\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\n\n\nasync def wait(fs, *, loop=None, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the Futures and coroutines given by fs to complete.\n\n    The sequence futures must not be empty.\n\n    Coroutines will be wrapped in Tasks.\n\n    Returns two sets of Future: (done, pending).\n\n    Usage:\n\n        done, pending = await asyncio.wait(fs)\n\n    Note: This does not raise TimeoutError! Futures that aren't done\n    when the timeout occurs are returned in the second set.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    if not fs:\n        raise ValueError('Set of coroutines/Futures is empty.')\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\n        raise ValueError(f'Invalid return_when value: {return_when}')\n\n    if loop is None:\n        loop = events.get_event_loop()\n\n    fs = {ensure_future(f, loop=loop) for f in set(fs)}\n\n    return await _wait(fs, timeout, return_when, loop)\n\n\ndef _release_waiter(waiter, *args):\n    if not waiter.done():\n        waiter.set_result(None)\n\n\nasync def wait_for(fut, timeout, *, loop=None):\n    \"\"\"Wait for the single Future or coroutine to complete, with timeout.\n\n    Coroutine will be wrapped in Task.\n\n    Returns result of the Future or coroutine.  When a timeout occurs,\n    it cancels the task and raises TimeoutError.  To avoid the task\n    cancellation, wrap it in shield().\n\n    If the wait is cancelled, the task is also cancelled.\n\n    This function is a coroutine.\n    \"\"\"\n    if loop is None:\n        loop = events.get_event_loop()\n\n    if timeout is None:\n        return await fut\n\n    if timeout <= 0:\n        fut = ensure_future(fut, loop=loop)\n\n        if fut.done():\n            return fut.result()\n\n        fut.cancel()\n        raise futures.TimeoutError()\n\n    waiter = loop.create_future()\n    timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    cb = functools.partial(_release_waiter, waiter)\n\n    fut = ensure_future(fut, loop=loop)\n    fut.add_done_callback(cb)\n\n    try:\n        # wait until the future completes or the timeout\n        try:\n            await waiter\n        except futures.CancelledError:\n            fut.remove_done_callback(cb)\n            fut.cancel()\n            raise\n\n        if fut.done():\n            return fut.result()\n        else:\n            fut.remove_done_callback(cb)\n            # We must ensure that the task is not running\n            # after wait_for() returns.\n            # See https://bugs.python.org/issue32751\n            await _cancel_and_wait(fut, loop=loop)\n            raise futures.TimeoutError()\n    finally:\n        timeout_handle.cancel()\n\n\nasync def _wait(fs, timeout, return_when, loop):\n    \"\"\"Internal helper for wait().\n\n    The fs argument must be a collection of Futures.\n    \"\"\"\n    assert fs, 'Set of Futures is empty.'\n    waiter = loop.create_future()\n    timeout_handle = None\n    if timeout is not None:\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    counter = len(fs)\n\n    def _on_completion(f):\n        nonlocal counter\n        counter -= 1\n        if (counter <= 0 or\n            return_when == FIRST_COMPLETED or\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\n                                                f.exception() is not None)):\n            if timeout_handle is not None:\n                timeout_handle.cancel()\n            if not waiter.done():\n                waiter.set_result(None)\n\n    for f in fs:\n        f.add_done_callback(_on_completion)\n\n    try:\n        await waiter\n    finally:\n        if timeout_handle is not None:\n            timeout_handle.cancel()\n        for f in fs:\n            f.remove_done_callback(_on_completion)\n\n    done, pending = set(), set()\n    for f in fs:\n        if f.done():\n            done.add(f)\n        else:\n            pending.add(f)\n    return done, pending\n\n\nasync def _cancel_and_wait(fut, loop):\n    \"\"\"Cancel the *fut* future or task and wait until it completes.\"\"\"\n\n    waiter = loop.create_future()\n    cb = functools.partial(_release_waiter, waiter)\n    fut.add_done_callback(cb)\n\n    try:\n        fut.cancel()\n        # We cannot wait on *fut* directly to make\n        # sure _cancel_and_wait itself is reliably cancellable.\n        await waiter\n    finally:\n        fut.remove_done_callback(cb)\n\n\n# This is *not* a @coroutine!  It is just an iterator (yielding Futures).\ndef as_completed(fs, *, loop=None, timeout=None):\n    \"\"\"Return an iterator whose values are coroutines.\n\n    When waiting for the yielded coroutines you'll get the results (or\n    exceptions!) of the original Futures (or coroutines), in the order\n    in which and as soon as they complete.\n\n    This differs from PEP 3148; the proper way to use this is:\n\n        for f in as_completed(fs):\n            result = await f  # The 'await' may raise.\n            # Use result.\n\n    If a timeout is specified, the 'await' will raise\n    TimeoutError when the timeout occurs before all Futures are done.\n\n    Note: The futures 'f' are not necessarily members of fs.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    loop = loop if loop is not None else events.get_event_loop()\n    todo = {ensure_future(f, loop=loop) for f in set(fs)}\n    from .queues import Queue  # Import here to avoid circular import problem.\n    done = Queue(loop=loop)\n    timeout_handle = None\n\n    def _on_timeout():\n        for f in todo:\n            f.remove_done_callback(_on_completion)\n            done.put_nowait(None)  # Queue a dummy value for _wait_for_one().\n        todo.clear()  # Can't do todo.remove(f) in the loop.\n\n    def _on_completion(f):\n        if not todo:\n            return  # _on_timeout() was here first.\n        todo.remove(f)\n        done.put_nowait(f)\n        if not todo and timeout_handle is not None:\n            timeout_handle.cancel()\n\n    async def _wait_for_one():\n        f = await done.get()\n        if f is None:\n            # Dummy value from _on_timeout().\n            raise futures.TimeoutError\n        return f.result()  # May raise f.exception().\n\n    for f in todo:\n        f.add_done_callback(_on_completion)\n    if todo and timeout is not None:\n        timeout_handle = loop.call_later(timeout, _on_timeout)\n    for _ in range(len(todo)):\n        yield _wait_for_one()\n\n\n@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\n\n\nasync def sleep(delay, result=None, *, loop=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n\n    if loop is None:\n        loop = events.get_event_loop()\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()\n\n\ndef ensure_future(coro_or_future, *, loop=None):\n    \"\"\"Wrap a coroutine or an awaitable in a future.\n\n    If the argument is a Future, it is returned directly.\n    \"\"\"\n    if coroutines.iscoroutine(coro_or_future):\n        if loop is None:\n            loop = events.get_event_loop()\n        task = loop.create_task(coro_or_future)\n        if task._source_traceback:\n            del task._source_traceback[-1]\n        return task\n    elif futures.isfuture(coro_or_future):\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\n            raise ValueError('loop argument must agree with Future')\n        return coro_or_future\n    elif inspect.isawaitable(coro_or_future):\n        return ensure_future(_wrap_awaitable(coro_or_future), loop=loop)\n    else:\n        raise TypeError('An asyncio.Future, a coroutine or an awaitable is '\n                        'required')\n\n\n@coroutine\ndef _wrap_awaitable(awaitable):\n    \"\"\"Helper for asyncio.ensure_future().\n\n    Wraps awaitable (an object with __await__) into a coroutine\n    that will later be wrapped in a Task by ensure_future().\n    \"\"\"\n    return (yield from awaitable.__await__())\n\n\nclass _GatheringFuture(futures.Future):\n    \"\"\"Helper for gather().\n\n    This overrides cancel() to cancel all the children and act more\n    like Task.cancel(), which doesn't immediately mark itself as\n    cancelled.\n    \"\"\"\n\n    def __init__(self, children, *, loop=None):\n        super().__init__(loop=loop)\n        self._children = children\n        self._cancel_requested = False\n\n    def cancel(self):\n        if self.done():\n            return False\n        ret = False\n        for child in self._children:\n            if child.cancel():\n                ret = True\n        if ret:\n            # If any child tasks were actually cancelled, we should\n            # propagate the cancellation request regardless of\n            # *return_exceptions* argument.  See issue 32684.\n            self._cancel_requested = True\n        return ret\n\n\ndef gather(*coros_or_futures, loop=None, return_exceptions=False):\n    \"\"\"Return a future aggregating results from the given coroutines/futures.\n\n    Coroutines will be wrapped in a future and scheduled in the event\n    loop. They will not necessarily be scheduled in the same order as\n    passed in.\n\n    All futures must share the same event loop.  If all the tasks are\n    done successfully, the returned future's result is the list of\n    results (in the order of the original sequence, not necessarily\n    the order of results arrival).  If *return_exceptions* is True,\n    exceptions in the tasks are treated the same as successful\n    results, and gathered in the result list; otherwise, the first\n    raised exception will be immediately propagated to the returned\n    future.\n\n    Cancellation: if the outer Future is cancelled, all children (that\n    have not completed yet) are also cancelled.  If any child is\n    cancelled, this is treated as if it raised CancelledError --\n    the outer Future is *not* cancelled in this case.  (This is to\n    prevent the cancellation of one child to cause other children to\n    be cancelled.)\n    \"\"\"\n    if not coros_or_futures:\n        if loop is None:\n            loop = events.get_event_loop()\n        outer = loop.create_future()\n        outer.set_result([])\n        return outer\n\n    def _done_callback(fut):\n        nonlocal nfinished\n        nfinished += 1\n\n        if outer.done():\n            if not fut.cancelled():\n                # Mark exception retrieved.\n                fut.exception()\n            return\n\n        if not return_exceptions:\n            if fut.cancelled():\n                # Check if 'fut' is cancelled first, as\n                # 'fut.exception()' will *raise* a CancelledError\n                # instead of returning it.\n                exc = futures.CancelledError()\n                outer.set_exception(exc)\n                return\n            else:\n                exc = fut.exception()\n                if exc is not None:\n                    outer.set_exception(exc)\n                    return\n\n        if nfinished == nfuts:\n            # All futures are done; create a list of results\n            # and set it to the 'outer' future.\n            results = []\n\n            for fut in children:\n                if fut.cancelled():\n                    # Check if 'fut' is cancelled first, as\n                    # 'fut.exception()' will *raise* a CancelledError\n                    # instead of returning it.\n                    res = futures.CancelledError()\n                else:\n                    res = fut.exception()\n                    if res is None:\n                        res = fut.result()\n                results.append(res)\n\n            if outer._cancel_requested:\n                # If gather is being cancelled we must propagate the\n                # cancellation regardless of *return_exceptions* argument.\n                # See issue 32684.\n                outer.set_exception(futures.CancelledError())\n            else:\n                outer.set_result(results)\n\n    arg_to_fut = {}\n    children = []\n    nfuts = 0\n    nfinished = 0\n    for arg in coros_or_futures:\n        if arg not in arg_to_fut:\n            fut = ensure_future(arg, loop=loop)\n            if loop is None:\n                loop = futures._get_loop(fut)\n            if fut is not arg:\n                # 'arg' was not a Future, therefore, 'fut' is a new\n                # Future created specifically for 'arg'.  Since the caller\n                # can't control it, disable the \"destroy pending task\"\n                # warning.\n                fut._log_destroy_pending = False\n\n            nfuts += 1\n            arg_to_fut[arg] = fut\n            fut.add_done_callback(_done_callback)\n\n        else:\n            # There's a duplicate Future object in coros_or_futures.\n            fut = arg_to_fut[arg]\n\n        children.append(fut)\n\n    outer = _GatheringFuture(children, loop=loop)\n    return outer\n\n\ndef shield(arg, *, loop=None):\n    \"\"\"Wait for a future, shielding it from cancellation.\n\n    The statement\n\n        res = await shield(something())\n\n    is exactly equivalent to the statement\n\n        res = await something()\n\n    *except* that if the coroutine containing it is cancelled, the\n    task running in something() is not cancelled.  From the POV of\n    something(), the cancellation did not happen.  But its caller is\n    still cancelled, so the yield-from expression still raises\n    CancelledError.  Note: If something() is cancelled by other means\n    this will still cancel shield().\n\n    If you want to completely ignore cancellation (not recommended)\n    you can combine shield() with a try/except clause, as follows:\n\n        try:\n            res = await shield(something())\n        except CancelledError:\n            res = None\n    \"\"\"\n    inner = ensure_future(arg, loop=loop)\n    if inner.done():\n        # Shortcut.\n        return inner\n    loop = futures._get_loop(inner)\n    outer = loop.create_future()\n\n    def _inner_done_callback(inner):\n        if outer.cancelled():\n            if not inner.cancelled():\n                # Mark inner's result as retrieved.\n                inner.exception()\n            return\n\n        if inner.cancelled():\n            outer.cancel()\n        else:\n            exc = inner.exception()\n            if exc is not None:\n                outer.set_exception(exc)\n            else:\n                outer.set_result(inner.result())\n\n\n    def _outer_done_callback(outer):\n        if not inner.done():\n            inner.remove_done_callback(_inner_done_callback)\n\n    inner.add_done_callback(_inner_done_callback)\n    outer.add_done_callback(_outer_done_callback)\n    return outer\n\n\ndef run_coroutine_threadsafe(coro, loop):\n    \"\"\"Submit a coroutine object to a given event loop.\n\n    Return a concurrent.futures.Future to access the result.\n    \"\"\"\n    if not coroutines.iscoroutine(coro):\n        raise TypeError('A coroutine object is required')\n    future = concurrent.futures.Future()\n\n    def callback():\n        try:\n            futures._chain_future(ensure_future(coro, loop=loop), future)\n        except Exception as exc:\n            if future.set_running_or_notify_cancel():\n                future.set_exception(exc)\n            raise\n\n    loop.call_soon_threadsafe(callback)\n    return future\n\n\n# WeakSet containing all alive tasks.\n_all_tasks = weakref.WeakSet()\n\n# Dictionary containing tasks that are currently active in\n# all running event loops.  {EventLoop: Task}\n_current_tasks = {}\n\n\ndef _register_task(task):\n    \"\"\"Register a new task in asyncio as executed by loop.\"\"\"\n    _all_tasks.add(task)\n\n\ndef _enter_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not None:\n        raise RuntimeError(f\"Cannot enter into task {task!r} while another \"\n                           f\"task {current_task!r} is being executed.\")\n    _current_tasks[loop] = task\n\n\ndef _leave_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not task:\n        raise RuntimeError(f\"Leaving task {task!r} does not match \"\n                           f\"the current task {current_task!r}.\")\n    del _current_tasks[loop]\n\n\ndef _unregister_task(task):\n    \"\"\"Unregister a task.\"\"\"\n    _all_tasks.discard(task)\n\n\n_py_register_task = _register_task\n_py_unregister_task = _unregister_task\n_py_enter_task = _enter_task\n_py_leave_task = _leave_task\n\n\ntry:\n    from _asyncio import (_register_task, _unregister_task,\n                          _enter_task, _leave_task,\n                          _all_tasks, _current_tasks)\nexcept ImportError:\n    pass\nelse:\n    _c_register_task = _register_task\n    _c_unregister_task = _unregister_task\n    _c_enter_task = _enter_task\n    _c_leave_task = _leave_task\n", 900], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py": ["from contextvars import ContextVar\nfrom typing import Optional\nimport sys\nimport threading\n\ncurrent_async_library_cvar = ContextVar(\n    \"current_async_library_cvar\", default=None\n)  # type: ContextVar[Optional[str]]\n\n\nclass _ThreadLocal(threading.local):\n    # Since threading.local provides no explicit mechanism is for setting\n    # a default for a value, a custom class with a class attribute is used\n    # instead.\n    name = None  # type: Optional[str]\n\n\nthread_local = _ThreadLocal()\n\n\nclass AsyncLibraryNotFoundError(RuntimeError):\n    pass\n\n\ndef current_async_library() -> str:\n    \"\"\"Detect which async library is currently running.\n\n    The following libraries are currently supported:\n\n    ================   ===========  ============================\n    Library             Requires     Magic string\n    ================   ===========  ============================\n    **Trio**            Trio v0.6+   ``\"trio\"``\n    **Curio**           -            ``\"curio\"``\n    **asyncio**                      ``\"asyncio\"``\n    **Trio-asyncio**    v0.8.2+     ``\"trio\"`` or ``\"asyncio\"``,\n                                    depending on current mode\n    ================   ===========  ============================\n\n    Returns:\n      A string like ``\"trio\"``.\n\n    Raises:\n      AsyncLibraryNotFoundError: if called from synchronous context,\n        or if the current async library was not recognized.\n\n    Examples:\n\n        .. code-block:: python3\n\n           from sniffio import current_async_library\n\n           async def generic_sleep(seconds):\n               library = current_async_library()\n               if library == \"trio\":\n                   import trio\n                   await trio.sleep(seconds)\n               elif library == \"asyncio\":\n                   import asyncio\n                   await asyncio.sleep(seconds)\n               # ... and so on ...\n               else:\n                   raise RuntimeError(f\"Unsupported library {library!r}\")\n\n    \"\"\"\n    value = thread_local.name\n    if value is not None:\n        return value\n\n    value = current_async_library_cvar.get()\n    if value is not None:\n        return value\n\n    # Need to sniff for asyncio\n    if \"asyncio\" in sys.modules:\n        import asyncio\n        try:\n            current_task = asyncio.current_task  # type: ignore[attr-defined]\n        except AttributeError:\n            current_task = asyncio.Task.current_task  # type: ignore[attr-defined]\n        try:\n            if current_task() is not None:\n                return \"asyncio\"\n        except RuntimeError:\n            pass\n\n    # Sniff for curio (for now)\n    if 'curio' in sys.modules:\n        from curio.meta import curio_running\n        if curio_running():\n            return 'curio'\n\n    raise AsyncLibraryNotFoundError(\n        \"unknown async library, or not in async context\"\n    )\n", 95], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py": ["import math\nimport sys\nimport threading\nfrom contextlib import contextmanager\nfrom importlib import import_module\nfrom typing import (\n    Any,\n    Callable,\n    Coroutine,\n    Dict,\n    Generator,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n)\n\nimport sniffio\n\n# This must be updated when new backends are introduced\nfrom ._compat import DeprecatedAwaitableFloat\n\nBACKENDS = \"asyncio\", \"trio\"\n\nT_Retval = TypeVar(\"T_Retval\")\nthreadlocals = threading.local()\n\n\ndef run(\n    func: Callable[..., Coroutine[Any, Any, T_Retval]],\n    *args: object,\n    backend: str = \"asyncio\",\n    backend_options: Optional[Dict[str, Any]] = None,\n) -> T_Retval:\n    \"\"\"\n    Run the given coroutine function in an asynchronous event loop.\n\n    The current thread must not be already running an event loop.\n\n    :param func: a coroutine function\n    :param args: positional arguments to ``func``\n    :param backend: name of the asynchronous event loop implementation \u2013 currently either\n        ``asyncio`` or ``trio``\n    :param backend_options: keyword arguments to call the backend ``run()`` implementation with\n        (documented :ref:`here <backend options>`)\n    :return: the return value of the coroutine function\n    :raises RuntimeError: if an asynchronous event loop is already running in this thread\n    :raises LookupError: if the named backend is not found\n\n    \"\"\"\n    try:\n        asynclib_name = sniffio.current_async_library()\n    except sniffio.AsyncLibraryNotFoundError:\n        pass\n    else:\n        raise RuntimeError(f\"Already running {asynclib_name} in this thread\")\n\n    try:\n        asynclib = import_module(f\"..._backends._{backend}\", package=__name__)\n    except ImportError as exc:\n        raise LookupError(f\"No such backend: {backend}\") from exc\n\n    token = None\n    if sniffio.current_async_library_cvar.get(None) is None:\n        # Since we're in control of the event loop, we can cache the name of the async library\n        token = sniffio.current_async_library_cvar.set(backend)\n\n    try:\n        backend_options = backend_options or {}\n        return asynclib.run(func, *args, **backend_options)\n    finally:\n        if token:\n            sniffio.current_async_library_cvar.reset(token)\n\n\nasync def sleep(delay: float) -> None:\n    \"\"\"\n    Pause the current task for the specified duration.\n\n    :param delay: the duration, in seconds\n\n    \"\"\"\n    return await get_asynclib().sleep(delay)\n\n\nasync def sleep_forever() -> None:\n    \"\"\"\n    Pause the current task until it's cancelled.\n\n    This is a shortcut for ``sleep(math.inf)``.\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    await sleep(math.inf)\n\n\nasync def sleep_until(deadline: float) -> None:\n    \"\"\"\n    Pause the current task until the given time.\n\n    :param deadline: the absolute time to wake up at (according to the internal monotonic clock of\n        the event loop)\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    now = current_time()\n    await sleep(max(deadline - now, 0))\n\n\ndef current_time() -> DeprecatedAwaitableFloat:\n    \"\"\"\n    Return the current value of the event loop's internal clock.\n\n    :return: the clock value (seconds)\n\n    \"\"\"\n    return DeprecatedAwaitableFloat(get_asynclib().current_time(), current_time)\n\n\ndef get_all_backends() -> Tuple[str, ...]:\n    \"\"\"Return a tuple of the names of all built-in backends.\"\"\"\n    return BACKENDS\n\n\ndef get_cancelled_exc_class() -> Type[BaseException]:\n    \"\"\"Return the current async library's cancellation exception class.\"\"\"\n    return get_asynclib().CancelledError\n\n\n#\n# Private API\n#\n\n\n@contextmanager\ndef claim_worker_thread(backend: str) -> Generator[Any, None, None]:\n    module = sys.modules[\"anyio._backends._\" + backend]\n    threadlocals.current_async_module = module\n    try:\n        yield\n    finally:\n        del threadlocals.current_async_module\n\n\ndef get_asynclib(asynclib_name: Optional[str] = None) -> Any:\n    if asynclib_name is None:\n        asynclib_name = sniffio.current_async_library()\n\n    modulename = \"anyio._backends._\" + asynclib_name\n    try:\n        return sys.modules[modulename]\n    except KeyError:\n        return import_module(modulename)\n", 155], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py": ["import array\nimport asyncio\nimport concurrent.futures\nimport math\nimport socket\nimport sys\nfrom asyncio.base_events import _run_until_complete_cb  # type: ignore[attr-defined]\nfrom collections import OrderedDict, deque\nfrom concurrent.futures import Future\nfrom contextvars import Context, copy_context\nfrom dataclasses import dataclass\nfrom functools import partial, wraps\nfrom inspect import (\n    CORO_RUNNING,\n    CORO_SUSPENDED,\n    GEN_RUNNING,\n    GEN_SUSPENDED,\n    getcoroutinestate,\n    getgeneratorstate,\n)\nfrom io import IOBase\nfrom os import PathLike\nfrom queue import Queue\nfrom socket import AddressFamily, SocketKind\nfrom threading import Thread\nfrom types import TracebackType\nfrom typing import (\n    IO,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    Collection,\n    Coroutine,\n    Deque,\n    Dict,\n    Generator,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom weakref import WeakKeyDictionary\n\nimport sniffio\n\nfrom .. import CapacityLimiterStatistics, EventStatistics, TaskInfo, abc\nfrom .._core._compat import DeprecatedAsyncContextManager, DeprecatedAwaitable\nfrom .._core._eventloop import claim_worker_thread, threadlocals\nfrom .._core._exceptions import (\n    BrokenResourceError,\n    BusyResourceError,\n    ClosedResourceError,\n    EndOfStream,\n)\nfrom .._core._exceptions import ExceptionGroup as BaseExceptionGroup\nfrom .._core._exceptions import WouldBlock\nfrom .._core._sockets import GetAddrInfoReturnType, convert_ipv6_sockaddr\nfrom .._core._synchronization import CapacityLimiter as BaseCapacityLimiter\nfrom .._core._synchronization import Event as BaseEvent\nfrom .._core._synchronization import ResourceGuard\nfrom .._core._tasks import CancelScope as BaseCancelScope\nfrom ..abc import IPSockAddrType, UDPPacketType\nfrom ..lowlevel import RunVar\n\nif sys.version_info >= (3, 8):\n\n    def get_coro(task: asyncio.Task) -> Union[Generator, Awaitable[Any]]:\n        return task.get_coro()\n\nelse:\n\n    def get_coro(task: asyncio.Task) -> Union[Generator, Awaitable[Any]]:\n        return task._coro\n\n\nif sys.version_info >= (3, 7):\n    from asyncio import all_tasks, create_task, current_task, get_running_loop\n    from asyncio import run as native_run\n\n    def _get_task_callbacks(task: asyncio.Task) -> Iterable[Callable]:\n        return [cb for cb, context in task._callbacks]  # type: ignore[attr-defined]\n\nelse:\n    _T = TypeVar(\"_T\")\n\n    def _get_task_callbacks(task: asyncio.Task) -> Iterable[Callable]:\n        return task._callbacks\n\n    def native_run(main, *, debug=False):\n        # Snatched from Python 3.7\n        from asyncio import coroutines, events, tasks\n\n        def _cancel_all_tasks(loop):\n            to_cancel = all_tasks(loop)\n            if not to_cancel:\n                return\n\n            for task in to_cancel:\n                task.cancel()\n\n            loop.run_until_complete(\n                tasks.gather(*to_cancel, loop=loop, return_exceptions=True)\n            )\n\n            for task in to_cancel:\n                if task.cancelled():\n                    continue\n                if task.exception() is not None:\n                    loop.call_exception_handler(\n                        {\n                            \"message\": \"unhandled exception during asyncio.run() shutdown\",\n                            \"exception\": task.exception(),\n                            \"task\": task,\n                        }\n                    )\n\n        if events._get_running_loop() is not None:\n            raise RuntimeError(\n                \"asyncio.run() cannot be called from a running event loop\"\n            )\n\n        if not coroutines.iscoroutine(main):\n            raise ValueError(f\"a coroutine was expected, got {main!r}\")\n\n        loop = events.new_event_loop()\n        try:\n            events.set_event_loop(loop)\n            loop.set_debug(debug)\n            return loop.run_until_complete(main)\n        finally:\n            try:\n                _cancel_all_tasks(loop)\n                loop.run_until_complete(loop.shutdown_asyncgens())\n            finally:\n                events.set_event_loop(None)\n                loop.close()\n\n    def create_task(\n        coro: Union[Generator[Any, None, _T], Awaitable[_T]], *, name: object = None\n    ) -> asyncio.Task:\n        return get_running_loop().create_task(coro)\n\n    def get_running_loop() -> asyncio.AbstractEventLoop:\n        loop = asyncio._get_running_loop()\n        if loop is not None:\n            return loop\n        else:\n            raise RuntimeError(\"no running event loop\")\n\n    def all_tasks(\n        loop: Optional[asyncio.AbstractEventLoop] = None,\n    ) -> Set[asyncio.Task]:\n        \"\"\"Return a set of all tasks for the loop.\"\"\"\n        from asyncio import Task\n\n        if loop is None:\n            loop = get_running_loop()\n\n        return {t for t in Task.all_tasks(loop) if not t.done()}\n\n    def current_task(\n        loop: Optional[asyncio.AbstractEventLoop] = None,\n    ) -> Optional[asyncio.Task]:\n        if loop is None:\n            loop = get_running_loop()\n\n        return asyncio.Task.current_task(loop)\n\n\nT_Retval = TypeVar(\"T_Retval\")\n\n# Check whether there is native support for task names in asyncio (3.8+)\n_native_task_names = hasattr(asyncio.Task, \"get_name\")\n\n\n_root_task: RunVar[Optional[asyncio.Task]] = RunVar(\"_root_task\")\n\n\ndef find_root_task() -> asyncio.Task:\n    root_task = _root_task.get(None)\n    if root_task is not None and not root_task.done():\n        return root_task\n\n    # Look for a task that has been started via run_until_complete()\n    for task in all_tasks():\n        if task._callbacks and not task.done():\n            for cb in _get_task_callbacks(task):\n                if (\n                    cb is _run_until_complete_cb\n                    or getattr(cb, \"__module__\", None) == \"uvloop.loop\"\n                ):\n                    _root_task.set(task)\n                    return task\n\n    # Look up the topmost task in the AnyIO task tree, if possible\n    task = cast(asyncio.Task, current_task())\n    state = _task_states.get(task)\n    if state:\n        cancel_scope = state.cancel_scope\n        while cancel_scope and cancel_scope._parent_scope is not None:\n            cancel_scope = cancel_scope._parent_scope\n\n        if cancel_scope is not None:\n            return cast(asyncio.Task, cancel_scope._host_task)\n\n    return task\n\n\ndef get_callable_name(func: Callable) -> str:\n    module = getattr(func, \"__module__\", None)\n    qualname = getattr(func, \"__qualname__\", None)\n    return \".\".join([x for x in (module, qualname) if x])\n\n\n#\n# Event loop\n#\n\n_run_vars = (\n    WeakKeyDictionary()\n)  # type: WeakKeyDictionary[asyncio.AbstractEventLoop, Any]\n\ncurrent_token = get_running_loop\n\n\ndef _task_started(task: asyncio.Task) -> bool:\n    \"\"\"Return ``True`` if the task has been started and has not finished.\"\"\"\n    coro = cast(Coroutine[Any, Any, Any], get_coro(task))\n    try:\n        return getcoroutinestate(coro) in (CORO_RUNNING, CORO_SUSPENDED)\n    except AttributeError:\n        try:\n            return getgeneratorstate(cast(Generator, coro)) in (\n                GEN_RUNNING,\n                GEN_SUSPENDED,\n            )\n        except AttributeError:\n            # task coro is async_genenerator_asend https://bugs.python.org/issue37771\n            raise Exception(f\"Cannot determine if task {task} has started or not\")\n\n\ndef _maybe_set_event_loop_policy(\n    policy: Optional[asyncio.AbstractEventLoopPolicy], use_uvloop: bool\n) -> None:\n    # On CPython, use uvloop when possible if no other policy has been given and if not\n    # explicitly disabled\n    if policy is None and use_uvloop and sys.implementation.name == \"cpython\":\n        try:\n            import uvloop\n        except ImportError:\n            pass\n        else:\n            # Test for missing shutdown_default_executor() (uvloop 0.14.0 and earlier)\n            if not hasattr(\n                asyncio.AbstractEventLoop, \"shutdown_default_executor\"\n            ) or hasattr(uvloop.loop.Loop, \"shutdown_default_executor\"):\n                policy = uvloop.EventLoopPolicy()\n\n    if policy is not None:\n        asyncio.set_event_loop_policy(policy)\n\n\ndef run(\n    func: Callable[..., Awaitable[T_Retval]],\n    *args: object,\n    debug: bool = False,\n    use_uvloop: bool = False,\n    policy: Optional[asyncio.AbstractEventLoopPolicy] = None,\n) -> T_Retval:\n    @wraps(func)\n    async def wrapper() -> T_Retval:\n        task = cast(asyncio.Task, current_task())\n        task_state = TaskState(None, get_callable_name(func), None)\n        _task_states[task] = task_state\n        if _native_task_names:\n            task.set_name(task_state.name)\n\n        try:\n            return await func(*args)\n        finally:\n            del _task_states[task]\n\n    _maybe_set_event_loop_policy(policy, use_uvloop)\n    return native_run(wrapper(), debug=debug)\n\n\n#\n# Miscellaneous\n#\n\nsleep = asyncio.sleep\n\n\n#\n# Timeouts and cancellation\n#\n\nCancelledError = asyncio.CancelledError\n\n\nclass CancelScope(BaseCancelScope):\n    def __new__(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> \"CancelScope\":\n        return object.__new__(cls)\n\n    def __init__(self, deadline: float = math.inf, shield: bool = False):\n        self._deadline = deadline\n        self._shield = shield\n        self._parent_scope: Optional[CancelScope] = None\n        self._cancel_called = False\n        self._active = False\n        self._timeout_handle: Optional[asyncio.TimerHandle] = None\n        self._cancel_handle: Optional[asyncio.Handle] = None\n        self._tasks: Set[asyncio.Task] = set()\n        self._host_task: Optional[asyncio.Task] = None\n        self._timeout_expired = False\n\n    def __enter__(self) -> \"CancelScope\":\n        if self._active:\n            raise RuntimeError(\n                \"Each CancelScope may only be used for a single 'with' block\"\n            )\n\n        self._host_task = host_task = cast(asyncio.Task, current_task())\n        self._tasks.add(host_task)\n        try:\n            task_state = _task_states[host_task]\n        except KeyError:\n            task_name = host_task.get_name() if _native_task_names else None\n            task_state = TaskState(None, task_name, self)\n            _task_states[host_task] = task_state\n        else:\n            self._parent_scope = task_state.cancel_scope\n            task_state.cancel_scope = self\n\n        self._timeout()\n        self._active = True\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        if not self._active:\n            raise RuntimeError(\"This cancel scope is not active\")\n        if current_task() is not self._host_task:\n            raise RuntimeError(\n                \"Attempted to exit cancel scope in a different task than it was \"\n                \"entered in\"\n            )\n\n        assert self._host_task is not None\n        host_task_state = _task_states.get(self._host_task)\n        if host_task_state is None or host_task_state.cancel_scope is not self:\n            raise RuntimeError(\n                \"Attempted to exit a cancel scope that isn't the current tasks's \"\n                \"current cancel scope\"\n            )\n\n        self._active = False\n        if self._timeout_handle:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        self._tasks.remove(self._host_task)\n\n        host_task_state.cancel_scope = self._parent_scope\n\n        # Restart the cancellation effort in the farthest directly cancelled parent scope if this\n        # one was shielded\n        if self._shield:\n            self._deliver_cancellation_to_parent()\n\n        if exc_val is not None:\n            exceptions = (\n                exc_val.exceptions if isinstance(exc_val, ExceptionGroup) else [exc_val]\n            )\n            if all(isinstance(exc, CancelledError) for exc in exceptions):\n                if self._timeout_expired:\n                    return True\n                elif not self._cancel_called:\n                    # Task was cancelled natively\n                    return None\n                elif not self._parent_cancelled():\n                    # This scope was directly cancelled\n                    return True\n\n        return None\n\n    def _timeout(self) -> None:\n        if self._deadline != math.inf:\n            loop = get_running_loop()\n            if loop.time() >= self._deadline:\n                self._timeout_expired = True\n                self.cancel()\n            else:\n                self._timeout_handle = loop.call_at(self._deadline, self._timeout)\n\n    def _deliver_cancellation(self) -> None:\n        \"\"\"\n        Deliver cancellation to directly contained tasks and nested cancel scopes.\n\n        Schedule another run at the end if we still have tasks eligible for cancellation.\n        \"\"\"\n        should_retry = False\n        current = current_task()\n        for task in self._tasks:\n            if task._must_cancel:  # type: ignore[attr-defined]\n                continue\n\n            # The task is eligible for cancellation if it has started and is not in a cancel\n            # scope shielded from this one\n            cancel_scope = _task_states[task].cancel_scope\n            while cancel_scope is not self:\n                if cancel_scope is None or cancel_scope._shield:\n                    break\n                else:\n                    cancel_scope = cancel_scope._parent_scope\n            else:\n                should_retry = True\n                if task is not current and (\n                    task is self._host_task or _task_started(task)\n                ):\n                    task.cancel()\n\n        # Schedule another callback if there are still tasks left\n        if should_retry:\n            self._cancel_handle = get_running_loop().call_soon(\n                self._deliver_cancellation\n            )\n        else:\n            self._cancel_handle = None\n\n    def _deliver_cancellation_to_parent(self) -> None:\n        \"\"\"Start cancellation effort in the farthest directly cancelled parent scope\"\"\"\n        scope = self._parent_scope\n        scope_to_cancel: Optional[CancelScope] = None\n        while scope is not None:\n            if scope._cancel_called and scope._cancel_handle is None:\n                scope_to_cancel = scope\n\n            # No point in looking beyond any shielded scope\n            if scope._shield:\n                break\n\n            scope = scope._parent_scope\n\n        if scope_to_cancel is not None:\n            scope_to_cancel._deliver_cancellation()\n\n    def _parent_cancelled(self) -> bool:\n        # Check whether any parent has been cancelled\n        cancel_scope = self._parent_scope\n        while cancel_scope is not None and not cancel_scope._shield:\n            if cancel_scope._cancel_called:\n                return True\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n        return False\n\n    def cancel(self) -> DeprecatedAwaitable:\n        if not self._cancel_called:\n            if self._timeout_handle:\n                self._timeout_handle.cancel()\n                self._timeout_handle = None\n\n            self._cancel_called = True\n            self._deliver_cancellation()\n\n        return DeprecatedAwaitable(self.cancel)\n\n    @property\n    def deadline(self) -> float:\n        return self._deadline\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        self._deadline = float(value)\n        if self._timeout_handle is not None:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        if self._active and not self._cancel_called:\n            self._timeout()\n\n    @property\n    def cancel_called(self) -> bool:\n        return self._cancel_called\n\n    @property\n    def shield(self) -> bool:\n        return self._shield\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        if self._shield != value:\n            self._shield = value\n            if not value:\n                self._deliver_cancellation_to_parent()\n\n\nasync def checkpoint() -> None:\n    await sleep(0)\n\n\nasync def checkpoint_if_cancelled() -> None:\n    task = current_task()\n    if task is None:\n        return\n\n    try:\n        cancel_scope = _task_states[task].cancel_scope\n    except KeyError:\n        return\n\n    while cancel_scope:\n        if cancel_scope.cancel_called:\n            await sleep(0)\n        elif cancel_scope.shield:\n            break\n        else:\n            cancel_scope = cancel_scope._parent_scope\n\n\nasync def cancel_shielded_checkpoint() -> None:\n    with CancelScope(shield=True):\n        await sleep(0)\n\n\ndef current_effective_deadline() -> float:\n    try:\n        cancel_scope = _task_states[current_task()].cancel_scope  # type: ignore[index]\n    except KeyError:\n        return math.inf\n\n    deadline = math.inf\n    while cancel_scope:\n        deadline = min(deadline, cancel_scope.deadline)\n        if cancel_scope.shield:\n            break\n        else:\n            cancel_scope = cancel_scope._parent_scope\n\n    return deadline\n\n\ndef current_time() -> float:\n    return get_running_loop().time()\n\n\n#\n# Task states\n#\n\n\nclass TaskState:\n    \"\"\"\n    Encapsulates auxiliary task information that cannot be added to the Task instance itself\n    because there are no guarantees about its implementation.\n    \"\"\"\n\n    __slots__ = \"parent_id\", \"name\", \"cancel_scope\"\n\n    def __init__(\n        self,\n        parent_id: Optional[int],\n        name: Optional[str],\n        cancel_scope: Optional[CancelScope],\n    ):\n        self.parent_id = parent_id\n        self.name = name\n        self.cancel_scope = cancel_scope\n\n\n_task_states = WeakKeyDictionary()  # type: WeakKeyDictionary[asyncio.Task, TaskState]\n\n\n#\n# Task groups\n#\n\n\nclass ExceptionGroup(BaseExceptionGroup):\n    def __init__(self, exceptions: List[BaseException]):\n        super().__init__()\n        self.exceptions = exceptions\n\n\nclass _AsyncioTaskStatus(abc.TaskStatus):\n    def __init__(self, future: asyncio.Future, parent_id: int):\n        self._future = future\n        self._parent_id = parent_id\n\n    def started(self, value: object = None) -> None:\n        try:\n            self._future.set_result(value)\n        except asyncio.InvalidStateError:\n            raise RuntimeError(\n                \"called 'started' twice on the same task status\"\n            ) from None\n\n        task = cast(asyncio.Task, current_task())\n        _task_states[task].parent_id = self._parent_id\n\n\nclass TaskGroup(abc.TaskGroup):\n    def __init__(self) -> None:\n        self.cancel_scope: CancelScope = CancelScope()\n        self._active = False\n        self._exceptions: List[BaseException] = []\n\n    async def __aenter__(self) -> \"TaskGroup\":\n        self.cancel_scope.__enter__()\n        self._active = True\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        ignore_exception = self.cancel_scope.__exit__(exc_type, exc_val, exc_tb)\n        if exc_val is not None:\n            self.cancel_scope.cancel()\n            self._exceptions.append(exc_val)\n\n        while self.cancel_scope._tasks:\n            try:\n                await asyncio.wait(self.cancel_scope._tasks)\n            except asyncio.CancelledError:\n                self.cancel_scope.cancel()\n\n        self._active = False\n        if not self.cancel_scope._parent_cancelled():\n            exceptions = self._filter_cancellation_errors(self._exceptions)\n        else:\n            exceptions = self._exceptions\n\n        try:\n            if len(exceptions) > 1:\n                if all(\n                    isinstance(e, CancelledError) and not e.args for e in exceptions\n                ):\n                    # Tasks were cancelled natively, without a cancellation message\n                    raise CancelledError\n                else:\n                    raise ExceptionGroup(exceptions)\n            elif exceptions and exceptions[0] is not exc_val:\n                raise exceptions[0]\n        except BaseException as exc:\n            # Clear the context here, as it can only be done in-flight.\n            # If the context is not cleared, it can result in recursive tracebacks (see #145).\n            exc.__context__ = None\n            raise\n\n        return ignore_exception\n\n    @staticmethod\n    def _filter_cancellation_errors(\n        exceptions: Sequence[BaseException],\n    ) -> List[BaseException]:\n        filtered_exceptions: List[BaseException] = []\n        for exc in exceptions:\n            if isinstance(exc, ExceptionGroup):\n                new_exceptions = TaskGroup._filter_cancellation_errors(exc.exceptions)\n                if len(new_exceptions) > 1:\n                    filtered_exceptions.append(exc)\n                elif len(new_exceptions) == 1:\n                    filtered_exceptions.append(new_exceptions[0])\n                elif new_exceptions:\n                    new_exc = ExceptionGroup(new_exceptions)\n                    new_exc.__cause__ = exc.__cause__\n                    new_exc.__context__ = exc.__context__\n                    new_exc.__traceback__ = exc.__traceback__\n                    filtered_exceptions.append(new_exc)\n            elif not isinstance(exc, CancelledError) or exc.args:\n                filtered_exceptions.append(exc)\n\n        return filtered_exceptions\n\n    async def _run_wrapped_task(\n        self, coro: Coroutine, task_status_future: Optional[asyncio.Future]\n    ) -> None:\n        # This is the code path for Python 3.6 and 3.7 on which asyncio freaks out if a task raises\n        # a BaseException.\n        __traceback_hide__ = __tracebackhide__ = True  # noqa: F841\n        task = cast(asyncio.Task, current_task())\n        try:\n            await coro\n        except BaseException as exc:\n            if task_status_future is None or task_status_future.done():\n                self._exceptions.append(exc)\n                self.cancel_scope.cancel()\n            else:\n                task_status_future.set_exception(exc)\n        else:\n            if task_status_future is not None and not task_status_future.done():\n                task_status_future.set_exception(\n                    RuntimeError(\"Child exited without calling task_status.started()\")\n                )\n        finally:\n            if task in self.cancel_scope._tasks:\n                self.cancel_scope._tasks.remove(task)\n                del _task_states[task]\n\n    def _spawn(\n        self,\n        func: Callable[..., Coroutine],\n        args: tuple,\n        name: object,\n        task_status_future: Optional[asyncio.Future] = None,\n    ) -> asyncio.Task:\n        def task_done(_task: asyncio.Task) -> None:\n            # This is the code path for Python 3.8+\n            assert _task in self.cancel_scope._tasks\n            self.cancel_scope._tasks.remove(_task)\n            del _task_states[_task]\n\n            try:\n                exc = _task.exception()\n            except CancelledError as e:\n                while isinstance(e.__context__, CancelledError):\n                    e = e.__context__\n\n                exc = e\n\n            if exc is not None:\n                if task_status_future is None or task_status_future.done():\n                    self._exceptions.append(exc)\n                    self.cancel_scope.cancel()\n                else:\n                    task_status_future.set_exception(exc)\n            elif task_status_future is not None and not task_status_future.done():\n                task_status_future.set_exception(\n                    RuntimeError(\"Child exited without calling task_status.started()\")\n                )\n\n        if not self._active:\n            raise RuntimeError(\n                \"This task group is not active; no new tasks can be started.\"\n            )\n\n        options = {}\n        name = get_callable_name(func) if name is None else str(name)\n        if _native_task_names:\n            options[\"name\"] = name\n\n        kwargs = {}\n        if task_status_future:\n            parent_id = id(current_task())\n            kwargs[\"task_status\"] = _AsyncioTaskStatus(\n                task_status_future, id(self.cancel_scope._host_task)\n            )\n        else:\n            parent_id = id(self.cancel_scope._host_task)\n\n        coro = func(*args, **kwargs)\n        if not asyncio.iscoroutine(coro):\n            raise TypeError(\n                f\"Expected an async function, but {func} appears to be synchronous\"\n            )\n\n        foreign_coro = not hasattr(coro, \"cr_frame\") and not hasattr(coro, \"gi_frame\")\n        if foreign_coro or sys.version_info < (3, 8):\n            coro = self._run_wrapped_task(coro, task_status_future)\n\n        task = create_task(coro, **options)\n        if not foreign_coro and sys.version_info >= (3, 8):\n            task.add_done_callback(task_done)\n\n        # Make the spawned task inherit the task group's cancel scope\n        _task_states[task] = TaskState(\n            parent_id=parent_id, name=name, cancel_scope=self.cancel_scope\n        )\n        self.cancel_scope._tasks.add(task)\n        return task\n\n    def start_soon(\n        self, func: Callable[..., Coroutine], *args: object, name: object = None\n    ) -> None:\n        self._spawn(func, args, name)\n\n    async def start(\n        self, func: Callable[..., Coroutine], *args: object, name: object = None\n    ) -> None:\n        future: asyncio.Future = asyncio.Future()\n        task = self._spawn(func, args, name, future)\n\n        # If the task raises an exception after sending a start value without a switch point\n        # between, the task group is cancelled and this method never proceeds to process the\n        # completed future. That's why we have to have a shielded cancel scope here.\n        with CancelScope(shield=True):\n            try:\n                return await future\n            except CancelledError:\n                task.cancel()\n                raise\n\n\n#\n# Threads\n#\n\n_Retval_Queue_Type = Tuple[Optional[T_Retval], Optional[BaseException]]\n\n\nclass WorkerThread(Thread):\n    MAX_IDLE_TIME = 10  # seconds\n\n    def __init__(\n        self,\n        root_task: asyncio.Task,\n        workers: Set[\"WorkerThread\"],\n        idle_workers: Deque[\"WorkerThread\"],\n    ):\n        super().__init__(name=\"AnyIO worker thread\")\n        self.root_task = root_task\n        self.workers = workers\n        self.idle_workers = idle_workers\n        self.loop = root_task._loop\n        self.queue: Queue[\n            Union[Tuple[Context, Callable, tuple, asyncio.Future], None]\n        ] = Queue(2)\n        self.idle_since = current_time()\n        self.stopping = False\n\n    def _report_result(\n        self, future: asyncio.Future, result: Any, exc: Optional[BaseException]\n    ) -> None:\n        self.idle_since = current_time()\n        if not self.stopping:\n            self.idle_workers.append(self)\n\n        if not future.cancelled():\n            if exc is not None:\n                future.set_exception(exc)\n            else:\n                future.set_result(result)\n\n    def run(self) -> None:\n        with claim_worker_thread(\"asyncio\"):\n            threadlocals.loop = self.loop\n            while True:\n                item = self.queue.get()\n                if item is None:\n                    # Shutdown command received\n                    return\n\n                context, func, args, future = item\n                if not future.cancelled():\n                    result = None\n                    exception: Optional[BaseException] = None\n                    try:\n                        result = context.run(func, *args)\n                    except BaseException as exc:\n                        exception = exc\n\n                    if not self.loop.is_closed():\n                        self.loop.call_soon_threadsafe(\n                            self._report_result, future, result, exception\n                        )\n\n                self.queue.task_done()\n\n    def stop(self, f: Optional[asyncio.Task] = None) -> None:\n        self.stopping = True\n        self.queue.put_nowait(None)\n        self.workers.discard(self)\n        try:\n            self.idle_workers.remove(self)\n        except ValueError:\n            pass\n\n\n_threadpool_idle_workers: RunVar[Deque[WorkerThread]] = RunVar(\n    \"_threadpool_idle_workers\"\n)\n_threadpool_workers: RunVar[Set[WorkerThread]] = RunVar(\"_threadpool_workers\")\n\n\nasync def run_sync_in_worker_thread(\n    func: Callable[..., T_Retval],\n    *args: object,\n    cancellable: bool = False,\n    limiter: Optional[\"CapacityLimiter\"] = None,\n) -> T_Retval:\n    await checkpoint()\n\n    # If this is the first run in this event loop thread, set up the necessary variables\n    try:\n        idle_workers = _threadpool_idle_workers.get()\n        workers = _threadpool_workers.get()\n    except LookupError:\n        idle_workers = deque()\n        workers = set()\n        _threadpool_idle_workers.set(idle_workers)\n        _threadpool_workers.set(workers)\n\n    async with (limiter or current_default_thread_limiter()):\n        with CancelScope(shield=not cancellable):\n            future: asyncio.Future = asyncio.Future()\n            root_task = find_root_task()\n            if not idle_workers:\n                worker = WorkerThread(root_task, workers, idle_workers)\n                worker.start()\n                workers.add(worker)\n                root_task.add_done_callback(worker.stop)\n            else:\n                worker = idle_workers.pop()\n\n                # Prune any other workers that have been idle for MAX_IDLE_TIME seconds or longer\n                now = current_time()\n                while idle_workers:\n                    if now - idle_workers[0].idle_since < WorkerThread.MAX_IDLE_TIME:\n                        break\n\n                    expired_worker = idle_workers.popleft()\n                    expired_worker.root_task.remove_done_callback(expired_worker.stop)\n                    expired_worker.stop()\n\n            context = copy_context()\n            context.run(sniffio.current_async_library_cvar.set, None)\n            worker.queue.put_nowait((context, func, args, future))\n            return await future\n\n\ndef run_sync_from_thread(\n    func: Callable[..., T_Retval],\n    *args: object,\n    loop: Optional[asyncio.AbstractEventLoop] = None,\n) -> T_Retval:\n    @wraps(func)\n    def wrapper() -> None:\n        try:\n            f.set_result(func(*args))\n        except BaseException as exc:\n            f.set_exception(exc)\n            if not isinstance(exc, Exception):\n                raise\n\n    f: concurrent.futures.Future[T_Retval] = Future()\n    loop = loop or threadlocals.loop\n    if sys.version_info < (3, 7):\n        loop.call_soon_threadsafe(copy_context().run, wrapper)\n    else:\n        loop.call_soon_threadsafe(wrapper)\n\n    return f.result()\n\n\ndef run_async_from_thread(\n    func: Callable[..., Coroutine[Any, Any, T_Retval]], *args: object\n) -> T_Retval:\n    f: concurrent.futures.Future[T_Retval] = asyncio.run_coroutine_threadsafe(\n        func(*args), threadlocals.loop\n    )\n    return f.result()\n\n\nclass BlockingPortal(abc.BlockingPortal):\n    def __new__(cls) -> \"BlockingPortal\":\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._loop = get_running_loop()\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable,\n        args: tuple,\n        kwargs: Dict[str, Any],\n        name: object,\n        future: Future,\n    ) -> None:\n        run_sync_from_thread(\n            partial(self._task_group.start_soon, name=name),\n            self._call_func,\n            func,\n            args,\n            kwargs,\n            future,\n            loop=self._loop,\n        )\n\n\n#\n# Subprocesses\n#\n\n\n@dataclass(eq=False)\nclass StreamReaderWrapper(abc.ByteReceiveStream):\n    _stream: asyncio.StreamReader\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        data = await self._stream.read(max_bytes)\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def aclose(self) -> None:\n        self._stream.feed_eof()\n\n\n@dataclass(eq=False)\nclass StreamWriterWrapper(abc.ByteSendStream):\n    _stream: asyncio.StreamWriter\n\n    async def send(self, item: bytes) -> None:\n        self._stream.write(item)\n        await self._stream.drain()\n\n    async def aclose(self) -> None:\n        self._stream.close()\n\n\n@dataclass(eq=False)\nclass Process(abc.Process):\n    _process: asyncio.subprocess.Process\n    _stdin: Optional[StreamWriterWrapper]\n    _stdout: Optional[StreamReaderWrapper]\n    _stderr: Optional[StreamReaderWrapper]\n\n    async def aclose(self) -> None:\n        if self._stdin:\n            await self._stdin.aclose()\n        if self._stdout:\n            await self._stdout.aclose()\n        if self._stderr:\n            await self._stderr.aclose()\n\n        await self.wait()\n\n    async def wait(self) -> int:\n        return await self._process.wait()\n\n    def terminate(self) -> None:\n        self._process.terminate()\n\n    def kill(self) -> None:\n        self._process.kill()\n\n    def send_signal(self, signal: int) -> None:\n        self._process.send_signal(signal)\n\n    @property\n    def pid(self) -> int:\n        return self._process.pid\n\n    @property\n    def returncode(self) -> Optional[int]:\n        return self._process.returncode\n\n    @property\n    def stdin(self) -> Optional[abc.ByteSendStream]:\n        return self._stdin\n\n    @property\n    def stdout(self) -> Optional[abc.ByteReceiveStream]:\n        return self._stdout\n\n    @property\n    def stderr(self) -> Optional[abc.ByteReceiveStream]:\n        return self._stderr\n\n\nasync def open_process(\n    command: Union[str, bytes, Sequence[Union[str, bytes]]],\n    *,\n    shell: bool,\n    stdin: Union[int, IO[Any], None],\n    stdout: Union[int, IO[Any], None],\n    stderr: Union[int, IO[Any], None],\n    cwd: Union[str, bytes, PathLike, None] = None,\n    env: Optional[Mapping[str, str]] = None,\n    start_new_session: bool = False,\n) -> Process:\n    await checkpoint()\n    if shell:\n        process = await asyncio.create_subprocess_shell(\n            cast(Union[str, bytes], command),\n            stdin=stdin,\n            stdout=stdout,\n            stderr=stderr,\n            cwd=cwd,\n            env=env,\n            start_new_session=start_new_session,\n        )\n    else:\n        process = await asyncio.create_subprocess_exec(\n            *command,\n            stdin=stdin,\n            stdout=stdout,\n            stderr=stderr,\n            cwd=cwd,\n            env=env,\n            start_new_session=start_new_session,\n        )\n\n    stdin_stream = StreamWriterWrapper(process.stdin) if process.stdin else None\n    stdout_stream = StreamReaderWrapper(process.stdout) if process.stdout else None\n    stderr_stream = StreamReaderWrapper(process.stderr) if process.stderr else None\n    return Process(process, stdin_stream, stdout_stream, stderr_stream)\n\n\ndef _forcibly_shutdown_process_pool_on_exit(\n    workers: Set[Process], _task: object\n) -> None:\n    \"\"\"\n    Forcibly shuts down worker processes belonging to this event loop.\"\"\"\n    child_watcher: Optional[asyncio.AbstractChildWatcher]\n    try:\n        child_watcher = asyncio.get_event_loop_policy().get_child_watcher()\n    except NotImplementedError:\n        child_watcher = None\n\n    # Close as much as possible (w/o async/await) to avoid warnings\n    for process in workers:\n        if process.returncode is None:\n            continue\n\n        process._stdin._stream._transport.close()  # type: ignore[union-attr]\n        process._stdout._stream._transport.close()  # type: ignore[union-attr]\n        process._stderr._stream._transport.close()  # type: ignore[union-attr]\n        process.kill()\n        if child_watcher:\n            child_watcher.remove_child_handler(process.pid)\n\n\nasync def _shutdown_process_pool_on_exit(workers: Set[Process]) -> None:\n    \"\"\"\n    Shuts down worker processes belonging to this event loop.\n\n    NOTE: this only works when the event loop was started using asyncio.run() or anyio.run().\n\n    \"\"\"\n    process: Process\n    try:\n        await sleep(math.inf)\n    except asyncio.CancelledError:\n        for process in workers:\n            if process.returncode is None:\n                process.kill()\n\n        for process in workers:\n            await process.aclose()\n\n\ndef setup_process_pool_exit_at_shutdown(workers: Set[Process]) -> None:\n    kwargs = {\"name\": \"AnyIO process pool shutdown task\"} if _native_task_names else {}\n    create_task(_shutdown_process_pool_on_exit(workers), **kwargs)\n    find_root_task().add_done_callback(\n        partial(_forcibly_shutdown_process_pool_on_exit, workers)\n    )\n\n\n#\n# Sockets and networking\n#\n\n\nclass StreamProtocol(asyncio.Protocol):\n    read_queue: Deque[bytes]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Optional[Exception] = None\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque()\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n        cast(asyncio.Transport, transport).set_write_buffer_limits(0)\n\n    def connection_lost(self, exc: Optional[Exception]) -> None:\n        if exc:\n            self.exception = BrokenResourceError()\n            self.exception.__cause__ = exc\n\n        self.read_event.set()\n        self.write_event.set()\n\n    def data_received(self, data: bytes) -> None:\n        self.read_queue.append(data)\n        self.read_event.set()\n\n    def eof_received(self) -> Optional[bool]:\n        self.read_event.set()\n        return True\n\n    def pause_writing(self) -> None:\n        self.write_event = asyncio.Event()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass DatagramProtocol(asyncio.DatagramProtocol):\n    read_queue: Deque[Tuple[bytes, IPSockAddrType]]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Optional[Exception] = None\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque(maxlen=100)  # arbitrary value\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n\n    def connection_lost(self, exc: Optional[Exception]) -> None:\n        self.read_event.set()\n        self.write_event.set()\n\n    def datagram_received(self, data: bytes, addr: IPSockAddrType) -> None:\n        addr = convert_ipv6_sockaddr(addr)\n        self.read_queue.append((data, addr))\n        self.read_event.set()\n\n    def error_received(self, exc: Exception) -> None:\n        self.exception = exc\n\n    def pause_writing(self) -> None:\n        self.write_event.clear()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass SocketStream(abc.SocketStream):\n    def __init__(self, transport: asyncio.Transport, protocol: StreamProtocol):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        with self._receive_guard:\n            await checkpoint()\n\n            if (\n                not self._protocol.read_event.is_set()\n                and not self._transport.is_closing()\n            ):\n                self._transport.resume_reading()\n                await self._protocol.read_event.wait()\n                self._transport.pause_reading()\n\n            try:\n                chunk = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                elif self._protocol.exception:\n                    raise self._protocol.exception\n                else:\n                    raise EndOfStream from None\n\n            if len(chunk) > max_bytes:\n                # Split the oversized chunk\n                chunk, leftover = chunk[:max_bytes], chunk[max_bytes:]\n                self._protocol.read_queue.appendleft(leftover)\n\n            # If the read queue is empty, clear the flag so that the next call will block until\n            # data is available\n            if not self._protocol.read_queue:\n                self._protocol.read_event.clear()\n\n        return chunk\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await checkpoint()\n\n            if self._closed:\n                raise ClosedResourceError\n            elif self._protocol.exception is not None:\n                raise self._protocol.exception\n\n            try:\n                self._transport.write(item)\n            except RuntimeError as exc:\n                if self._transport.is_closing():\n                    raise BrokenResourceError from exc\n                else:\n                    raise\n\n            await self._protocol.write_event.wait()\n\n    async def send_eof(self) -> None:\n        try:\n            self._transport.write_eof()\n        except OSError:\n            pass\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            try:\n                self._transport.write_eof()\n            except OSError:\n                pass\n\n            self._transport.close()\n            await sleep(0)\n            self._transport.abort()\n\n\nclass UNIXSocketStream(abc.SocketStream):\n    _receive_future: Optional[asyncio.Future] = None\n    _send_future: Optional[asyncio.Future] = None\n    _closing = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = get_running_loop()\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    def _wait_until_readable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._receive_future\n            loop.remove_reader(self.__raw_socket)\n\n        f = self._receive_future = asyncio.Future()\n        self._loop.add_reader(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    def _wait_until_writable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._send_future\n            loop.remove_writer(self.__raw_socket)\n\n        f = self._send_future = asyncio.Future()\n        self._loop.add_writer(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    async def send_eof(self) -> None:\n        with self._send_guard:\n            self._raw_socket.shutdown(socket.SHUT_WR)\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        loop = get_running_loop()\n        await checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self.__raw_socket.recv(max_bytes)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not data:\n                        raise EndOfStream\n\n                    return data\n\n    async def send(self, item: bytes) -> None:\n        loop = get_running_loop()\n        await checkpoint()\n        with self._send_guard:\n            view = memoryview(item)\n            while view:\n                try:\n                    bytes_sent = self.__raw_socket.send(item)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    view = view[bytes_sent:]\n\n    async def receive_fds(self, msglen: int, maxfds: int) -> Tuple[bytes, List[int]]:\n        if not isinstance(msglen, int) or msglen < 0:\n            raise ValueError(\"msglen must be a non-negative integer\")\n        if not isinstance(maxfds, int) or maxfds < 1:\n            raise ValueError(\"maxfds must be a positive integer\")\n\n        loop = get_running_loop()\n        fds = array.array(\"i\")\n        await checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    message, ancdata, flags, addr = self.__raw_socket.recvmsg(\n                        msglen, socket.CMSG_LEN(maxfds * fds.itemsize)\n                    )\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not message and not ancdata:\n                        raise EndOfStream\n\n                    break\n\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level != socket.SOL_SOCKET or cmsg_type != socket.SCM_RIGHTS:\n                raise RuntimeError(\n                    f\"Received unexpected ancillary data; message = {message!r}, \"\n                    f\"cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}\"\n                )\n\n            fds.frombytes(cmsg_data[: len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return message, list(fds)\n\n    async def send_fds(\n        self, message: bytes, fds: Collection[Union[int, IOBase]]\n    ) -> None:\n        if not message:\n            raise ValueError(\"message must not be empty\")\n        if not fds:\n            raise ValueError(\"fds must not be empty\")\n\n        loop = get_running_loop()\n        filenos: List[int] = []\n        for fd in fds:\n            if isinstance(fd, int):\n                filenos.append(fd)\n            elif isinstance(fd, IOBase):\n                filenos.append(fd.fileno())\n\n        fdarray = array.array(\"i\", filenos)\n        await checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    # The ignore can be removed after mypy picks up\n                    # https://github.com/python/typeshed/pull/5545\n                    self.__raw_socket.sendmsg(\n                        [message], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, fdarray)]\n                    )\n                    break\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n    async def aclose(self) -> None:\n        if not self._closing:\n            self._closing = True\n            if self.__raw_socket.fileno() != -1:\n                self.__raw_socket.close()\n\n            if self._receive_future:\n                self._receive_future.set_result(None)\n            if self._send_future:\n                self._send_future.set_result(None)\n\n\nclass TCPSocketListener(abc.SocketListener):\n    _accept_scope: Optional[CancelScope] = None\n    _closed = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = cast(asyncio.BaseEventLoop, get_running_loop())\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    async def accept(self) -> abc.SocketStream:\n        if self._closed:\n            raise ClosedResourceError\n\n        with self._accept_guard:\n            await checkpoint()\n            with CancelScope() as self._accept_scope:\n                try:\n                    client_sock, _addr = await self._loop.sock_accept(self._raw_socket)\n                except asyncio.CancelledError:\n                    # Workaround for https://bugs.python.org/issue41317\n                    try:\n                        self._loop.remove_reader(self._raw_socket)\n                    except (ValueError, NotImplementedError):\n                        pass\n\n                    if self._closed:\n                        raise ClosedResourceError from None\n\n                    raise\n                finally:\n                    self._accept_scope = None\n\n        client_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        transport, protocol = await self._loop.connect_accepted_socket(\n            StreamProtocol, client_sock\n        )\n        return SocketStream(cast(asyncio.Transport, transport), protocol)\n\n    async def aclose(self) -> None:\n        if self._closed:\n            return\n\n        self._closed = True\n        if self._accept_scope:\n            # Workaround for https://bugs.python.org/issue41317\n            try:\n                self._loop.remove_reader(self._raw_socket)\n            except (ValueError, NotImplementedError):\n                pass\n\n            self._accept_scope.cancel()\n            await sleep(0)\n\n        self._raw_socket.close()\n\n\nclass UNIXSocketListener(abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = get_running_loop()\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n        self._closed = False\n\n    async def accept(self) -> abc.SocketStream:\n        await checkpoint()\n        with self._accept_guard:\n            while True:\n                try:\n                    client_sock, _ = self.__raw_socket.accept()\n                    client_sock.setblocking(False)\n                    return UNIXSocketStream(client_sock)\n                except BlockingIOError:\n                    f: asyncio.Future = asyncio.Future()\n                    self._loop.add_reader(self.__raw_socket, f.set_result, None)\n                    f.add_done_callback(\n                        lambda _: self._loop.remove_reader(self.__raw_socket)\n                    )\n                    await f\n                except OSError as exc:\n                    if self._closed:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n    async def aclose(self) -> None:\n        self._closed = True\n        self.__raw_socket.close()\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n\nclass UDPSocket(abc.UDPSocket):\n    def __init__(\n        self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol\n    ):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> Tuple[bytes, IPSockAddrType]:\n        with self._receive_guard:\n            await checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                return self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n    async def send(self, item: UDPPacketType) -> None:\n        with self._send_guard:\n            await checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(*item)\n\n\nclass ConnectedUDPSocket(abc.ConnectedUDPSocket):\n    def __init__(\n        self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol\n    ):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            await checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                packet = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n            return packet[0]\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(item)\n\n\nasync def connect_tcp(\n    host: str, port: int, local_addr: Optional[Tuple[str, int]] = None\n) -> SocketStream:\n    transport, protocol = cast(\n        Tuple[asyncio.Transport, StreamProtocol],\n        await get_running_loop().create_connection(\n            StreamProtocol, host, port, local_addr=local_addr\n        ),\n    )\n    transport.pause_reading()\n    return SocketStream(transport, protocol)\n\n\nasync def connect_unix(path: str) -> UNIXSocketStream:\n    await checkpoint()\n    loop = get_running_loop()\n    raw_socket = socket.socket(socket.AF_UNIX)\n    raw_socket.setblocking(False)\n    while True:\n        try:\n            raw_socket.connect(path)\n        except BlockingIOError:\n            f: asyncio.Future = asyncio.Future()\n            loop.add_writer(raw_socket, f.set_result, None)\n            f.add_done_callback(lambda _: loop.remove_writer(raw_socket))\n            await f\n        except BaseException:\n            raw_socket.close()\n            raise\n        else:\n            return UNIXSocketStream(raw_socket)\n\n\nasync def create_udp_socket(\n    family: socket.AddressFamily,\n    local_address: Optional[IPSockAddrType],\n    remote_address: Optional[IPSockAddrType],\n    reuse_port: bool,\n) -> Union[UDPSocket, ConnectedUDPSocket]:\n    result = await get_running_loop().create_datagram_endpoint(\n        DatagramProtocol,\n        local_addr=local_address,\n        remote_addr=remote_address,\n        family=family,\n        reuse_port=reuse_port,\n    )\n    transport = cast(asyncio.DatagramTransport, result[0])\n    protocol = result[1]\n    if protocol.exception:\n        transport.close()\n        raise protocol.exception\n\n    if not remote_address:\n        return UDPSocket(transport, protocol)\n    else:\n        return ConnectedUDPSocket(transport, protocol)\n\n\nasync def getaddrinfo(\n    host: Union[bytes, str],\n    port: Union[str, int, None],\n    *,\n    family: Union[int, AddressFamily] = 0,\n    type: Union[int, SocketKind] = 0,\n    proto: int = 0,\n    flags: int = 0,\n) -> GetAddrInfoReturnType:\n    # https://github.com/python/typeshed/pull/4304\n    result = await get_running_loop().getaddrinfo(\n        host, port, family=family, type=type, proto=proto, flags=flags\n    )\n    return cast(GetAddrInfoReturnType, result)\n\n\nasync def getnameinfo(sockaddr: IPSockAddrType, flags: int = 0) -> Tuple[str, str]:\n    return await get_running_loop().getnameinfo(sockaddr, flags)\n\n\n_read_events: RunVar[Dict[Any, asyncio.Event]] = RunVar(\"read_events\")\n_write_events: RunVar[Dict[Any, asyncio.Event]] = RunVar(\"write_events\")\n\n\nasync def wait_socket_readable(sock: socket.socket) -> None:\n    await checkpoint()\n    try:\n        read_events = _read_events.get()\n    except LookupError:\n        read_events = {}\n        _read_events.set(read_events)\n\n    if read_events.get(sock):\n        raise BusyResourceError(\"reading from\") from None\n\n    loop = get_running_loop()\n    event = read_events[sock] = asyncio.Event()\n    loop.add_reader(sock, event.set)\n    try:\n        await event.wait()\n    finally:\n        if read_events.pop(sock, None) is not None:\n            loop.remove_reader(sock)\n            readable = True\n        else:\n            readable = False\n\n    if not readable:\n        raise ClosedResourceError\n\n\nasync def wait_socket_writable(sock: socket.socket) -> None:\n    await checkpoint()\n    try:\n        write_events = _write_events.get()\n    except LookupError:\n        write_events = {}\n        _write_events.set(write_events)\n\n    if write_events.get(sock):\n        raise BusyResourceError(\"writing to\") from None\n\n    loop = get_running_loop()\n    event = write_events[sock] = asyncio.Event()\n    loop.add_writer(sock.fileno(), event.set)\n    try:\n        await event.wait()\n    finally:\n        if write_events.pop(sock, None) is not None:\n            loop.remove_writer(sock)\n            writable = True\n        else:\n            writable = False\n\n    if not writable:\n        raise ClosedResourceError\n\n\n#\n# Synchronization\n#\n\n\nclass Event(BaseEvent):\n    def __new__(cls) -> \"Event\":\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        self._event = asyncio.Event()\n\n    def set(self) -> DeprecatedAwaitable:\n        self._event.set()\n        return DeprecatedAwaitable(self.set)\n\n    def is_set(self) -> bool:\n        return self._event.is_set()\n\n    async def wait(self) -> None:\n        if await self._event.wait():\n            await checkpoint()\n\n    def statistics(self) -> EventStatistics:\n        return EventStatistics(len(self._event._waiters))  # type: ignore[attr-defined]\n\n\nclass CapacityLimiter(BaseCapacityLimiter):\n    _total_tokens: float = 0\n\n    def __new__(cls, total_tokens: float) -> \"CapacityLimiter\":\n        return object.__new__(cls)\n\n    def __init__(self, total_tokens: float):\n        self._borrowers: Set[Any] = set()\n        self._wait_queue: Dict[Any, asyncio.Event] = OrderedDict()\n        self.total_tokens = total_tokens\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        self.release()\n\n    @property\n    def total_tokens(self) -> float:\n        return self._total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        if not isinstance(value, int) and not math.isinf(value):\n            raise TypeError(\"total_tokens must be an int or math.inf\")\n        if value < 1:\n            raise ValueError(\"total_tokens must be >= 1\")\n\n        old_value = self._total_tokens\n        self._total_tokens = value\n        events = []\n        for event in self._wait_queue.values():\n            if value <= old_value:\n                break\n\n            if not event.is_set():\n                events.append(event)\n                old_value += 1\n\n        for event in events:\n            event.set()\n\n    @property\n    def borrowed_tokens(self) -> int:\n        return len(self._borrowers)\n\n    @property\n    def available_tokens(self) -> float:\n        return self._total_tokens - len(self._borrowers)\n\n    def acquire_nowait(self) -> DeprecatedAwaitable:\n        self.acquire_on_behalf_of_nowait(current_task())\n        return DeprecatedAwaitable(self.acquire_nowait)\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> DeprecatedAwaitable:\n        if borrower in self._borrowers:\n            raise RuntimeError(\n                \"this borrower is already holding one of this CapacityLimiter's \"\n                \"tokens\"\n            )\n\n        if self._wait_queue or len(self._borrowers) >= self._total_tokens:\n            raise WouldBlock\n\n        self._borrowers.add(borrower)\n        return DeprecatedAwaitable(self.acquire_on_behalf_of_nowait)\n\n    async def acquire(self) -> None:\n        return await self.acquire_on_behalf_of(current_task())\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await checkpoint_if_cancelled()\n        try:\n            self.acquire_on_behalf_of_nowait(borrower)\n        except WouldBlock:\n            event = asyncio.Event()\n            self._wait_queue[borrower] = event\n            try:\n                await event.wait()\n            except BaseException:\n                self._wait_queue.pop(borrower, None)\n                raise\n\n            self._borrowers.add(borrower)\n        else:\n            try:\n                await cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def release(self) -> None:\n        self.release_on_behalf_of(current_task())\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        try:\n            self._borrowers.remove(borrower)\n        except KeyError:\n            raise RuntimeError(\n                \"this borrower isn't holding any of this CapacityLimiter's \" \"tokens\"\n            ) from None\n\n        # Notify the next task in line if this limiter has free capacity now\n        if self._wait_queue and len(self._borrowers) < self._total_tokens:\n            event = self._wait_queue.popitem()[1]\n            event.set()\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        return CapacityLimiterStatistics(\n            self.borrowed_tokens,\n            self.total_tokens,\n            tuple(self._borrowers),\n            len(self._wait_queue),\n        )\n\n\n_default_thread_limiter: RunVar[CapacityLimiter] = RunVar(\"_default_thread_limiter\")\n\n\ndef current_default_thread_limiter() -> CapacityLimiter:\n    try:\n        return _default_thread_limiter.get()\n    except LookupError:\n        limiter = CapacityLimiter(40)\n        _default_thread_limiter.set(limiter)\n        return limiter\n\n\n#\n# Operating system signals\n#\n\n\nclass _SignalReceiver(DeprecatedAsyncContextManager[\"_SignalReceiver\"]):\n    def __init__(self, signals: Tuple[int, ...]):\n        self._signals = signals\n        self._loop = get_running_loop()\n        self._signal_queue: Deque[int] = deque()\n        self._future: asyncio.Future = asyncio.Future()\n        self._handled_signals: Set[int] = set()\n\n    def _deliver(self, signum: int) -> None:\n        self._signal_queue.append(signum)\n        if not self._future.done():\n            self._future.set_result(None)\n\n    def __enter__(self) -> \"_SignalReceiver\":\n        for sig in set(self._signals):\n            self._loop.add_signal_handler(sig, self._deliver, sig)\n            self._handled_signals.add(sig)\n\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        for sig in self._handled_signals:\n            self._loop.remove_signal_handler(sig)\n        return None\n\n    def __aiter__(self) -> \"_SignalReceiver\":\n        return self\n\n    async def __anext__(self) -> int:\n        await checkpoint()\n        if not self._signal_queue:\n            self._future = asyncio.Future()\n            await self._future\n\n        return self._signal_queue.popleft()\n\n\ndef open_signal_receiver(*signals: int) -> _SignalReceiver:\n    return _SignalReceiver(signals)\n\n\n#\n# Testing and debugging\n#\n\n\ndef _create_task_info(task: asyncio.Task) -> TaskInfo:\n    task_state = _task_states.get(task)\n    if task_state is None:\n        name = task.get_name() if _native_task_names else None\n        parent_id = None\n    else:\n        name = task_state.name\n        parent_id = task_state.parent_id\n\n    return TaskInfo(id(task), parent_id, name, get_coro(task))\n\n\ndef get_current_task() -> TaskInfo:\n    return _create_task_info(current_task())  # type: ignore[arg-type]\n\n\ndef get_running_tasks() -> List[TaskInfo]:\n    return [_create_task_info(task) for task in all_tasks() if not task.done()]\n\n\nasync def wait_all_tasks_blocked() -> None:\n    await checkpoint()\n    this_task = current_task()\n    while True:\n        for task in all_tasks():\n            if task is this_task:\n                continue\n\n            if task._fut_waiter is None or task._fut_waiter.done():  # type: ignore[attr-defined]\n                await sleep(0.1)\n                break\n        else:\n            return\n\n\nclass TestRunner(abc.TestRunner):\n    def __init__(\n        self,\n        debug: bool = False,\n        use_uvloop: bool = False,\n        policy: Optional[asyncio.AbstractEventLoopPolicy] = None,\n    ):\n        self._exceptions: List[BaseException] = []\n        _maybe_set_event_loop_policy(policy, use_uvloop)\n        self._loop = asyncio.new_event_loop()\n        self._loop.set_debug(debug)\n        self._loop.set_exception_handler(self._exception_handler)\n        asyncio.set_event_loop(self._loop)\n\n    def _cancel_all_tasks(self) -> None:\n        to_cancel = all_tasks(self._loop)\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n\n        self._loop.run_until_complete(\n            asyncio.gather(*to_cancel, return_exceptions=True)\n        )\n\n        for task in to_cancel:\n            if task.cancelled():\n                continue\n            if task.exception() is not None:\n                raise cast(BaseException, task.exception())\n\n    def _exception_handler(\n        self, loop: asyncio.AbstractEventLoop, context: Dict[str, Any]\n    ) -> None:\n        if isinstance(context.get(\"exception\"), Exception):\n            self._exceptions.append(context[\"exception\"])\n        else:\n            loop.default_exception_handler(context)\n\n    def _raise_async_exceptions(self) -> None:\n        # Re-raise any exceptions raised in asynchronous callbacks\n        if self._exceptions:\n            exceptions, self._exceptions = self._exceptions, []\n            if len(exceptions) == 1:\n                raise exceptions[0]\n            elif exceptions:\n                raise ExceptionGroup(exceptions)\n\n    def close(self) -> None:\n        try:\n            self._cancel_all_tasks()\n            self._loop.run_until_complete(self._loop.shutdown_asyncgens())\n        finally:\n            asyncio.set_event_loop(None)\n            self._loop.close()\n\n    def run_asyncgen_fixture(\n        self,\n        fixture_func: Callable[..., AsyncGenerator[T_Retval, Any]],\n        kwargs: Dict[str, Any],\n    ) -> Iterable[T_Retval]:\n        async def fixture_runner() -> None:\n            agen = fixture_func(**kwargs)\n            try:\n                retval = await agen.asend(None)\n                self._raise_async_exceptions()\n            except BaseException as exc:\n                f.set_exception(exc)\n                return\n            else:\n                f.set_result(retval)\n\n            await event.wait()\n            try:\n                await agen.asend(None)\n            except StopAsyncIteration:\n                pass\n            else:\n                await agen.aclose()\n                raise RuntimeError(\"Async generator fixture did not stop\")\n\n        f = self._loop.create_future()\n        event = asyncio.Event()\n        fixture_task = self._loop.create_task(fixture_runner())\n        self._loop.run_until_complete(f)\n        yield f.result()\n        event.set()\n        self._loop.run_until_complete(fixture_task)\n        self._raise_async_exceptions()\n\n    def run_fixture(\n        self,\n        fixture_func: Callable[..., Coroutine[Any, Any, T_Retval]],\n        kwargs: Dict[str, Any],\n    ) -> T_Retval:\n        retval = self._loop.run_until_complete(fixture_func(**kwargs))\n        self._raise_async_exceptions()\n        return retval\n\n    def run_test(\n        self, test_func: Callable[..., Coroutine[Any, Any, Any]], kwargs: Dict[str, Any]\n    ) -> None:\n        try:\n            self._loop.run_until_complete(test_func(**kwargs))\n        except Exception as exc:\n            self._exceptions.append(exc)\n\n        self._raise_async_exceptions()\n", 2181], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/to_thread.py": ["from typing import Callable, Optional, TypeVar\nfrom warnings import warn\n\nfrom ._core._eventloop import get_asynclib\nfrom .abc import CapacityLimiter\n\nT_Retval = TypeVar(\"T_Retval\")\n\n\nasync def run_sync(\n    func: Callable[..., T_Retval],\n    *args: object,\n    cancellable: bool = False,\n    limiter: Optional[CapacityLimiter] = None\n) -> T_Retval:\n    \"\"\"\n    Call the given function with the given arguments in a worker thread.\n\n    If the ``cancellable`` option is enabled and the task waiting for its completion is cancelled,\n    the thread will still run its course but its return value (or any raised exception) will be\n    ignored.\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :param cancellable: ``True`` to allow cancellation of the operation\n    :param limiter: capacity limiter to use to limit the total amount of threads running\n        (if omitted, the default limiter is used)\n    :return: an awaitable that yields the return value of the function.\n\n    \"\"\"\n    return await get_asynclib().run_sync_in_worker_thread(\n        func, *args, cancellable=cancellable, limiter=limiter\n    )\n\n\nasync def run_sync_in_worker_thread(\n    func: Callable[..., T_Retval],\n    *args: object,\n    cancellable: bool = False,\n    limiter: Optional[CapacityLimiter] = None\n) -> T_Retval:\n    warn(\n        \"run_sync_in_worker_thread() has been deprecated, use anyio.to_thread.run_sync() instead\",\n        DeprecationWarning,\n    )\n    return await run_sync(func, *args, cancellable=cancellable, limiter=limiter)\n\n\ndef current_default_thread_limiter() -> CapacityLimiter:\n    \"\"\"\n    Return the capacity limiter that is used by default to limit the number of concurrent threads.\n\n    :return: a capacity limiter object\n\n    \"\"\"\n    return get_asynclib().current_default_thread_limiter()\n\n\ndef current_default_worker_thread_limiter() -> CapacityLimiter:\n    warn(\n        \"current_default_worker_thread_limiter() has been deprecated, \"\n        \"use anyio.to_thread.current_default_thread_limiter() instead\",\n        DeprecationWarning,\n    )\n    return current_default_thread_limiter()\n", 65], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/concurrency.py": ["import functools\nimport sys\nimport typing\nimport warnings\n\nimport anyio\n\nif sys.version_info >= (3, 10):  # pragma: no cover\n    from typing import ParamSpec\nelse:  # pragma: no cover\n    from typing_extensions import ParamSpec\n\n\nT = typing.TypeVar(\"T\")\nP = ParamSpec(\"P\")\n\n\nasync def run_until_first_complete(*args: typing.Tuple[typing.Callable, dict]) -> None:\n    warnings.warn(\n        \"run_until_first_complete is deprecated \"\n        \"and will be removed in a future version.\",\n        DeprecationWarning,\n    )\n\n    async with anyio.create_task_group() as task_group:\n\n        async def run(func: typing.Callable[[], typing.Coroutine]) -> None:\n            await func()\n            task_group.cancel_scope.cancel()\n\n        for func, kwargs in args:\n            task_group.start_soon(run, functools.partial(func, **kwargs))\n\n\nasync def run_in_threadpool(\n    func: typing.Callable[P, T], *args: P.args, **kwargs: P.kwargs\n) -> T:\n    if kwargs:  # pragma: no cover\n        # run_sync doesn't accept 'kwargs', so bind them in here\n        func = functools.partial(func, **kwargs)\n    return await anyio.to_thread.run_sync(func, *args)\n\n\nclass _StopIteration(Exception):\n    pass\n\n\ndef _next(iterator: typing.Iterator[T]) -> T:\n    # We can't raise `StopIteration` from within the threadpool iterator\n    # and catch it outside that context, so we coerce them into a different\n    # exception type.\n    try:\n        return next(iterator)\n    except StopIteration:\n        raise _StopIteration\n\n\nasync def iterate_in_threadpool(\n    iterator: typing.Iterator[T],\n) -> typing.AsyncIterator[T]:\n    while True:\n        try:\n            yield await anyio.to_thread.run_sync(_next, iterator)\n        except _StopIteration:\n            break\n", 65], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py": ["from typing import Optional\n\nfrom fastapi.concurrency import AsyncExitStack\nfrom starlette.types import ASGIApp, Receive, Scope, Send\n\n\nclass AsyncExitStackMiddleware:\n    def __init__(self, app: ASGIApp, context_name: str = \"fastapi_astack\") -> None:\n        self.app = app\n        self.context_name = context_name\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if AsyncExitStack:\n            dependency_exception: Optional[Exception] = None\n            async with AsyncExitStack() as stack:\n                scope[self.context_name] = stack\n                try:\n                    await self.app(scope, receive, send)\n                except Exception as e:\n                    dependency_exception = e\n                    raise e\n            if dependency_exception:\n                # This exception was possibly handled by the dependency but it should\n                # still bubble up so that the ServerErrorMiddleware can return a 500\n                # or the ExceptionMiddleware can catch and handle any other exceptions\n                raise dependency_exception\n        else:\n            await self.app(scope, receive, send)  # pragma: no cover\n", 28], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py": ["import typing\n\nfrom starlette._utils import is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.exceptions import HTTPException, WebSocketException\nfrom starlette.requests import Request\nfrom starlette.responses import PlainTextResponse, Response\nfrom starlette.types import ASGIApp, Message, Receive, Scope, Send\nfrom starlette.websockets import WebSocket\n\n\nclass ExceptionMiddleware:\n    def __init__(\n        self,\n        app: ASGIApp,\n        handlers: typing.Optional[\n            typing.Mapping[typing.Any, typing.Callable[[Request, Exception], Response]]\n        ] = None,\n        debug: bool = False,\n    ) -> None:\n        self.app = app\n        self.debug = debug  # TODO: We ought to handle 404 cases if debug is set.\n        self._status_handlers: typing.Dict[int, typing.Callable] = {}\n        self._exception_handlers: typing.Dict[\n            typing.Type[Exception], typing.Callable\n        ] = {\n            HTTPException: self.http_exception,\n            WebSocketException: self.websocket_exception,\n        }\n        if handlers is not None:\n            for key, value in handlers.items():\n                self.add_exception_handler(key, value)\n\n    def add_exception_handler(\n        self,\n        exc_class_or_status_code: typing.Union[int, typing.Type[Exception]],\n        handler: typing.Callable[[Request, Exception], Response],\n    ) -> None:\n        if isinstance(exc_class_or_status_code, int):\n            self._status_handlers[exc_class_or_status_code] = handler\n        else:\n            assert issubclass(exc_class_or_status_code, Exception)\n            self._exception_handlers[exc_class_or_status_code] = handler\n\n    def _lookup_exception_handler(\n        self, exc: Exception\n    ) -> typing.Optional[typing.Callable]:\n        for cls in type(exc).__mro__:\n            if cls in self._exception_handlers:\n                return self._exception_handlers[cls]\n        return None\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] not in (\"http\", \"websocket\"):\n            await self.app(scope, receive, send)\n            return\n\n        response_started = False\n\n        async def sender(message: Message) -> None:\n            nonlocal response_started\n\n            if message[\"type\"] == \"http.response.start\":\n                response_started = True\n            await send(message)\n\n        try:\n            await self.app(scope, receive, sender)\n        except Exception as exc:\n            handler = None\n\n            if isinstance(exc, HTTPException):\n                handler = self._status_handlers.get(exc.status_code)\n\n            if handler is None:\n                handler = self._lookup_exception_handler(exc)\n\n            if handler is None:\n                raise exc\n\n            if response_started:\n                msg = \"Caught handled exception, but response already started.\"\n                raise RuntimeError(msg) from exc\n\n            if scope[\"type\"] == \"http\":\n                request = Request(scope, receive=receive)\n                if is_async_callable(handler):\n                    response = await handler(request, exc)\n                else:\n                    response = await run_in_threadpool(handler, request, exc)\n                await response(scope, receive, sender)\n            elif scope[\"type\"] == \"websocket\":\n                websocket = WebSocket(scope, receive=receive, send=send)\n                if is_async_callable(handler):\n                    await handler(websocket, exc)\n                else:\n                    await run_in_threadpool(handler, websocket, exc)\n\n    def http_exception(self, request: Request, exc: HTTPException) -> Response:\n        if exc.status_code in {204, 304}:\n            return Response(status_code=exc.status_code, headers=exc.headers)\n        return PlainTextResponse(\n            exc.detail, status_code=exc.status_code, headers=exc.headers\n        )\n\n    async def websocket_exception(\n        self, websocket: WebSocket, exc: WebSocketException\n    ) -> None:\n        await websocket.close(code=exc.code, reason=exc.reason)\n", 109], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py": ["\"\"\"Base implementation of event loop.\n\nThe event loop can be broken up into a multiplexer (the part\nresponsible for notifying us of I/O events) and the event loop proper,\nwhich wraps a multiplexer with functionality for scheduling callbacks,\nimmediately or at a given time in the future.\n\nWhenever a public API takes a callback, subsequent positional\narguments will be passed to the callback if/when it is called.  This\navoids the proliferation of trivial lambdas implementing closures.\nKeyword arguments for the callback are not supported; this is a\nconscious design decision, leaving the door open for keyword arguments\nto modify the meaning of the API call itself.\n\"\"\"\n\nimport collections\nimport collections.abc\nimport concurrent.futures\nimport heapq\nimport itertools\nimport logging\nimport os\nimport socket\nimport subprocess\nimport threading\nimport time\nimport traceback\nimport sys\nimport warnings\nimport weakref\n\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import tasks\nfrom . import transports\nfrom .log import logger\n\n\n__all__ = 'BaseEventLoop',\n\n\n# Minimum number of _scheduled timer handles before cleanup of\n# cancelled handles is performed.\n_MIN_SCHEDULED_TIMER_HANDLES = 100\n\n# Minimum fraction of _scheduled timer handles that are cancelled\n# before cleanup of cancelled handles is performed.\n_MIN_CANCELLED_TIMER_HANDLES_FRACTION = 0.5\n\n_HAS_IPv6 = hasattr(socket, 'AF_INET6')\n\n# Maximum timeout passed to select to avoid OS limitations\nMAXIMUM_SELECT_TIMEOUT = 24 * 3600\n\n# Used for deprecation and removal of `loop.create_datagram_endpoint()`'s\n# *reuse_address* parameter\n_unset = object()\n\n\ndef _format_handle(handle):\n    cb = handle._callback\n    if isinstance(getattr(cb, '__self__', None), tasks.Task):\n        # format the task\n        return repr(cb.__self__)\n    else:\n        return str(handle)\n\n\ndef _format_pipe(fd):\n    if fd == subprocess.PIPE:\n        return '<pipe>'\n    elif fd == subprocess.STDOUT:\n        return '<stdout>'\n    else:\n        return repr(fd)\n\n\ndef _set_reuseport(sock):\n    if not hasattr(socket, 'SO_REUSEPORT'):\n        raise ValueError('reuse_port not supported by socket module')\n    else:\n        try:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n        except OSError:\n            raise ValueError('reuse_port not supported by socket module, '\n                             'SO_REUSEPORT defined but not implemented.')\n\n\ndef _ipaddr_info(host, port, family, type, proto, flowinfo=0, scopeid=0):\n    # Try to skip getaddrinfo if \"host\" is already an IP. Users might have\n    # handled name resolution in their own code and pass in resolved IPs.\n    if not hasattr(socket, 'inet_pton'):\n        return\n\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or \\\n            host is None:\n        return None\n\n    if type == socket.SOCK_STREAM:\n        proto = socket.IPPROTO_TCP\n    elif type == socket.SOCK_DGRAM:\n        proto = socket.IPPROTO_UDP\n    else:\n        return None\n\n    if port is None:\n        port = 0\n    elif isinstance(port, bytes) and port == b'':\n        port = 0\n    elif isinstance(port, str) and port == '':\n        port = 0\n    else:\n        # If port's a service name like \"http\", don't skip getaddrinfo.\n        try:\n            port = int(port)\n        except (TypeError, ValueError):\n            return None\n\n    if family == socket.AF_UNSPEC:\n        afs = [socket.AF_INET]\n        if _HAS_IPv6:\n            afs.append(socket.AF_INET6)\n    else:\n        afs = [family]\n\n    if isinstance(host, bytes):\n        host = host.decode('idna')\n    if '%' in host:\n        # Linux's inet_pton doesn't accept an IPv6 zone index after host,\n        # like '::1%lo0'.\n        return None\n\n    for af in afs:\n        try:\n            socket.inet_pton(af, host)\n            # The host has already been resolved.\n            if _HAS_IPv6 and af == socket.AF_INET6:\n                return af, type, proto, '', (host, port, flowinfo, scopeid)\n            else:\n                return af, type, proto, '', (host, port)\n        except OSError:\n            pass\n\n    # \"host\" is not an IP address.\n    return None\n\n\ndef _run_until_complete_cb(fut):\n    if not fut.cancelled():\n        exc = fut.exception()\n        if isinstance(exc, BaseException) and not isinstance(exc, Exception):\n            # Issue #22429: run_forever() already finished, no need to\n            # stop it.\n            return\n    futures._get_loop(fut).stop()\n\n\nif hasattr(socket, 'TCP_NODELAY'):\n    def _set_nodelay(sock):\n        if (sock.family in {socket.AF_INET, socket.AF_INET6} and\n                sock.type == socket.SOCK_STREAM and\n                sock.proto == socket.IPPROTO_TCP):\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\nelse:\n    def _set_nodelay(sock):\n        pass\n\n\nclass _SendfileFallbackProtocol(protocols.Protocol):\n    def __init__(self, transp):\n        if not isinstance(transp, transports._FlowControlMixin):\n            raise TypeError(\"transport should be _FlowControlMixin instance\")\n        self._transport = transp\n        self._proto = transp.get_protocol()\n        self._should_resume_reading = transp.is_reading()\n        self._should_resume_writing = transp._protocol_paused\n        transp.pause_reading()\n        transp.set_protocol(self)\n        if self._should_resume_writing:\n            self._write_ready_fut = self._transport._loop.create_future()\n        else:\n            self._write_ready_fut = None\n\n    async def drain(self):\n        if self._transport.is_closing():\n            raise ConnectionError(\"Connection closed by peer\")\n        fut = self._write_ready_fut\n        if fut is None:\n            return\n        await fut\n\n    def connection_made(self, transport):\n        raise RuntimeError(\"Invalid state: \"\n                           \"connection should have been established already.\")\n\n    def connection_lost(self, exc):\n        if self._write_ready_fut is not None:\n            # Never happens if peer disconnects after sending the whole content\n            # Thus disconnection is always an exception from user perspective\n            if exc is None:\n                self._write_ready_fut.set_exception(\n                    ConnectionError(\"Connection is closed by peer\"))\n            else:\n                self._write_ready_fut.set_exception(exc)\n        self._proto.connection_lost(exc)\n\n    def pause_writing(self):\n        if self._write_ready_fut is not None:\n            return\n        self._write_ready_fut = self._transport._loop.create_future()\n\n    def resume_writing(self):\n        if self._write_ready_fut is None:\n            return\n        self._write_ready_fut.set_result(False)\n        self._write_ready_fut = None\n\n    def data_received(self, data):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    def eof_received(self):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    async def restore(self):\n        self._transport.set_protocol(self._proto)\n        if self._should_resume_reading:\n            self._transport.resume_reading()\n        if self._write_ready_fut is not None:\n            # Cancel the future.\n            # Basically it has no effect because protocol is switched back,\n            # no code should wait for it anymore.\n            self._write_ready_fut.cancel()\n        if self._should_resume_writing:\n            self._proto.resume_writing()\n\n\nclass Server(events.AbstractServer):\n\n    def __init__(self, loop, sockets, protocol_factory, ssl_context, backlog,\n                 ssl_handshake_timeout):\n        self._loop = loop\n        self._sockets = sockets\n        self._active_count = 0\n        self._waiters = []\n        self._protocol_factory = protocol_factory\n        self._backlog = backlog\n        self._ssl_context = ssl_context\n        self._ssl_handshake_timeout = ssl_handshake_timeout\n        self._serving = False\n        self._serving_forever_fut = None\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__} sockets={self.sockets!r}>'\n\n    def _attach(self):\n        assert self._sockets is not None\n        self._active_count += 1\n\n    def _detach(self):\n        assert self._active_count > 0\n        self._active_count -= 1\n        if self._active_count == 0 and self._sockets is None:\n            self._wakeup()\n\n    def _wakeup(self):\n        waiters = self._waiters\n        self._waiters = None\n        for waiter in waiters:\n            if not waiter.done():\n                waiter.set_result(waiter)\n\n    def _start_serving(self):\n        if self._serving:\n            return\n        self._serving = True\n        for sock in self._sockets:\n            sock.listen(self._backlog)\n            self._loop._start_serving(\n                self._protocol_factory, sock, self._ssl_context,\n                self, self._backlog, self._ssl_handshake_timeout)\n\n    def get_loop(self):\n        return self._loop\n\n    def is_serving(self):\n        return self._serving\n\n    @property\n    def sockets(self):\n        if self._sockets is None:\n            return []\n        return list(self._sockets)\n\n    def close(self):\n        sockets = self._sockets\n        if sockets is None:\n            return\n        self._sockets = None\n\n        for sock in sockets:\n            self._loop._stop_serving(sock)\n\n        self._serving = False\n\n        if (self._serving_forever_fut is not None and\n                not self._serving_forever_fut.done()):\n            self._serving_forever_fut.cancel()\n            self._serving_forever_fut = None\n\n        if self._active_count == 0:\n            self._wakeup()\n\n    async def start_serving(self):\n        self._start_serving()\n        # Skip one loop iteration so that all 'loop.add_reader'\n        # go through.\n        await tasks.sleep(0, loop=self._loop)\n\n    async def serve_forever(self):\n        if self._serving_forever_fut is not None:\n            raise RuntimeError(\n                f'server {self!r} is already being awaited on serve_forever()')\n        if self._sockets is None:\n            raise RuntimeError(f'server {self!r} is closed')\n\n        self._start_serving()\n        self._serving_forever_fut = self._loop.create_future()\n\n        try:\n            await self._serving_forever_fut\n        except futures.CancelledError:\n            try:\n                self.close()\n                await self.wait_closed()\n            finally:\n                raise\n        finally:\n            self._serving_forever_fut = None\n\n    async def wait_closed(self):\n        if self._sockets is None or self._waiters is None:\n            return\n        waiter = self._loop.create_future()\n        self._waiters.append(waiter)\n        await waiter\n\n\nclass BaseEventLoop(events.AbstractEventLoop):\n\n    def __init__(self):\n        self._timer_cancelled_count = 0\n        self._closed = False\n        self._stopping = False\n        self._ready = collections.deque()\n        self._scheduled = []\n        self._default_executor = None\n        self._internal_fds = 0\n        # Identifier of the thread running the event loop, or None if the\n        # event loop is not running\n        self._thread_id = None\n        self._clock_resolution = time.get_clock_info('monotonic').resolution\n        self._exception_handler = None\n        self.set_debug(coroutines._is_debug_mode())\n        # In debug mode, if the execution of a callback or a step of a task\n        # exceed this duration in seconds, the slow callback/task is logged.\n        self.slow_callback_duration = 0.1\n        self._current_handle = None\n        self._task_factory = None\n        self._coroutine_origin_tracking_enabled = False\n        self._coroutine_origin_tracking_saved_depth = None\n\n        # A weak set of all asynchronous generators that are\n        # being iterated by the loop.\n        self._asyncgens = weakref.WeakSet()\n        # Set to True when `loop.shutdown_asyncgens` is called.\n        self._asyncgens_shutdown_called = False\n\n    def __repr__(self):\n        return (\n            f'<{self.__class__.__name__} running={self.is_running()} '\n            f'closed={self.is_closed()} debug={self.get_debug()}>'\n        )\n\n    def create_future(self):\n        \"\"\"Create a Future object attached to the loop.\"\"\"\n        return futures.Future(loop=self)\n\n    def create_task(self, coro):\n        \"\"\"Schedule a coroutine object.\n\n        Return a task object.\n        \"\"\"\n        self._check_closed()\n        if self._task_factory is None:\n            task = tasks.Task(coro, loop=self)\n            if task._source_traceback:\n                del task._source_traceback[-1]\n        else:\n            task = self._task_factory(self, coro)\n        return task\n\n    def set_task_factory(self, factory):\n        \"\"\"Set a task factory that will be used by loop.create_task().\n\n        If factory is None the default task factory will be set.\n\n        If factory is a callable, it should have a signature matching\n        '(loop, coro)', where 'loop' will be a reference to the active\n        event loop, 'coro' will be a coroutine object.  The callable\n        must return a Future.\n        \"\"\"\n        if factory is not None and not callable(factory):\n            raise TypeError('task factory must be a callable or None')\n        self._task_factory = factory\n\n    def get_task_factory(self):\n        \"\"\"Return a task factory, or None if the default one is in use.\"\"\"\n        return self._task_factory\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        \"\"\"Create socket transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=None,\n            call_connection_made=True):\n        \"\"\"Create SSL transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        \"\"\"Create datagram transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        \"\"\"Create read pipe transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        \"\"\"Create write pipe transport.\"\"\"\n        raise NotImplementedError\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        \"\"\"Create subprocess transport.\"\"\"\n        raise NotImplementedError\n\n    def _write_to_self(self):\n        \"\"\"Write a byte to self-pipe, to wake up the event loop.\n\n        This may be called from a different thread.\n\n        The subclass is responsible for implementing the self-pipe.\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_events(self, event_list):\n        \"\"\"Process selector events.\"\"\"\n        raise NotImplementedError\n\n    def _check_closed(self):\n        if self._closed:\n            raise RuntimeError('Event loop is closed')\n\n    def _asyncgen_finalizer_hook(self, agen):\n        self._asyncgens.discard(agen)\n        if not self.is_closed():\n            self.call_soon_threadsafe(self.create_task, agen.aclose())\n\n    def _asyncgen_firstiter_hook(self, agen):\n        if self._asyncgens_shutdown_called:\n            warnings.warn(\n                f\"asynchronous generator {agen!r} was scheduled after \"\n                f\"loop.shutdown_asyncgens() call\",\n                ResourceWarning, source=self)\n\n        self._asyncgens.add(agen)\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        self._asyncgens_shutdown_called = True\n\n        if not len(self._asyncgens):\n            # If Python version is <3.6 or we don't have any asynchronous\n            # generators alive.\n            return\n\n        closing_agens = list(self._asyncgens)\n        self._asyncgens.clear()\n\n        results = await tasks.gather(\n            *[ag.aclose() for ag in closing_agens],\n            return_exceptions=True,\n            loop=self)\n\n        for result, agen in zip(results, closing_agens):\n            if isinstance(result, Exception):\n                self.call_exception_handler({\n                    'message': f'an error occurred during closing of '\n                               f'asynchronous generator {agen!r}',\n                    'exception': result,\n                    'asyncgen': agen\n                })\n\n    def _check_runnung(self):\n        if self.is_running():\n            raise RuntimeError('This event loop is already running')\n        if events._get_running_loop() is not None:\n            raise RuntimeError(\n                'Cannot run the event loop while another loop is running')\n\n    def run_forever(self):\n        \"\"\"Run until stop() is called.\"\"\"\n        self._check_closed()\n        self._check_runnung()\n        self._set_coroutine_origin_tracking(self._debug)\n        self._thread_id = threading.get_ident()\n\n        old_agen_hooks = sys.get_asyncgen_hooks()\n        sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n                               finalizer=self._asyncgen_finalizer_hook)\n        try:\n            events._set_running_loop(self)\n            while True:\n                self._run_once()\n                if self._stopping:\n                    break\n        finally:\n            self._stopping = False\n            self._thread_id = None\n            events._set_running_loop(None)\n            self._set_coroutine_origin_tracking(False)\n            sys.set_asyncgen_hooks(*old_agen_hooks)\n\n    def run_until_complete(self, future):\n        \"\"\"Run until the Future is done.\n\n        If the argument is a coroutine, it is wrapped in a Task.\n\n        WARNING: It would be disastrous to call run_until_complete()\n        with the same coroutine twice -- it would wrap it in two\n        different Tasks and that can't be good.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        self._check_closed()\n        self._check_runnung()\n\n        new_task = not futures.isfuture(future)\n        future = tasks.ensure_future(future, loop=self)\n        if new_task:\n            # An exception is raised if the future didn't complete, so there\n            # is no need to log the \"destroy pending task\" message\n            future._log_destroy_pending = False\n\n        future.add_done_callback(_run_until_complete_cb)\n        try:\n            self.run_forever()\n        except:\n            if new_task and future.done() and not future.cancelled():\n                # The coroutine raised a BaseException. Consume the exception\n                # to not log a warning, the caller doesn't have access to the\n                # local task.\n                future.exception()\n            raise\n        finally:\n            future.remove_done_callback(_run_until_complete_cb)\n        if not future.done():\n            raise RuntimeError('Event loop stopped before Future completed.')\n\n        return future.result()\n\n    def stop(self):\n        \"\"\"Stop running the event loop.\n\n        Every callback already scheduled will still run.  This simply informs\n        run_forever to stop looping after a complete iteration.\n        \"\"\"\n        self._stopping = True\n\n    def close(self):\n        \"\"\"Close the event loop.\n\n        This clears the queues and shuts down the executor,\n        but does not wait for the executor to finish.\n\n        The event loop must not be running.\n        \"\"\"\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self._closed:\n            return\n        if self._debug:\n            logger.debug(\"Close %r\", self)\n        self._closed = True\n        self._ready.clear()\n        self._scheduled.clear()\n        executor = self._default_executor\n        if executor is not None:\n            self._default_executor = None\n            executor.shutdown(wait=False)\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        return self._closed\n\n    def __del__(self):\n        if not self.is_closed():\n            warnings.warn(f\"unclosed event loop {self!r}\", ResourceWarning,\n                          source=self)\n            if not self.is_running():\n                self.close()\n\n    def is_running(self):\n        \"\"\"Returns True if the event loop is running.\"\"\"\n        return (self._thread_id is not None)\n\n    def time(self):\n        \"\"\"Return the time according to the event loop's clock.\n\n        This is a float expressed in seconds since an epoch, but the\n        epoch, precision, accuracy and drift are unspecified and may\n        differ per event loop.\n        \"\"\"\n        return time.monotonic()\n\n    def call_later(self, delay, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called at a given time.\n\n        Return a Handle: an opaque object with a cancel() method that\n        can be used to cancel the call.\n\n        The delay can be an int or float, expressed in seconds.  It is\n        always relative to the current time.\n\n        Each callback will be called exactly once.  If two callbacks\n        are scheduled for exactly the same time, it undefined which\n        will be called first.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        timer = self.call_at(self.time() + delay, callback, *args,\n                             context=context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        return timer\n\n    def call_at(self, when, callback, *args, context=None):\n        \"\"\"Like call_later(), but uses an absolute time.\n\n        Absolute time corresponds to the event loop's time() method.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_at')\n        timer = events.TimerHandle(when, callback, args, self, context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        heapq.heappush(self._scheduled, timer)\n        timer._scheduled = True\n        return timer\n\n    def call_soon(self, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called as soon as possible.\n\n        This operates as a FIFO queue: callbacks are called in the\n        order in which they are registered.  Each callback will be\n        called exactly once.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_soon')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        return handle\n\n    def _check_callback(self, callback, method):\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\n                f\"coroutines cannot be used with {method}()\")\n        if not callable(callback):\n            raise TypeError(\n                f'a callable object was expected by {method}(), '\n                f'got {callback!r}')\n\n    def _call_soon(self, callback, args, context):\n        handle = events.Handle(callback, args, self, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._ready.append(handle)\n        return handle\n\n    def _check_thread(self):\n        \"\"\"Check that the current thread is the thread running the event loop.\n\n        Non-thread-safe methods of this class make this assumption and will\n        likely behave incorrectly when the assumption is violated.\n\n        Should only be called when (self._debug == True).  The caller is\n        responsible for checking this condition for performance reasons.\n        \"\"\"\n        if self._thread_id is None:\n            return\n        thread_id = threading.get_ident()\n        if thread_id != self._thread_id:\n            raise RuntimeError(\n                \"Non-thread-safe operation invoked on an event loop other \"\n                \"than the current one\")\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        \"\"\"Like call_soon(), but thread-safe.\"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_callback(callback, 'call_soon_threadsafe')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._write_to_self()\n        return handle\n\n    def run_in_executor(self, executor, func, *args):\n        self._check_closed()\n        if self._debug:\n            self._check_callback(func, 'run_in_executor')\n        if executor is None:\n            executor = self._default_executor\n            if executor is None:\n                executor = concurrent.futures.ThreadPoolExecutor()\n                self._default_executor = executor\n        return futures.wrap_future(\n            executor.submit(func, *args), loop=self)\n\n    def set_default_executor(self, executor):\n        self._default_executor = executor\n\n    def _getaddrinfo_debug(self, host, port, family, type, proto, flags):\n        msg = [f\"{host}:{port!r}\"]\n        if family:\n            msg.append(f'family={family!r}')\n        if type:\n            msg.append(f'type={type!r}')\n        if proto:\n            msg.append(f'proto={proto!r}')\n        if flags:\n            msg.append(f'flags={flags!r}')\n        msg = ', '.join(msg)\n        logger.debug('Get address info %s', msg)\n\n        t0 = self.time()\n        addrinfo = socket.getaddrinfo(host, port, family, type, proto, flags)\n        dt = self.time() - t0\n\n        msg = f'Getting address info {msg} took {dt * 1e3:.3f}ms: {addrinfo!r}'\n        if dt >= self.slow_callback_duration:\n            logger.info(msg)\n        else:\n            logger.debug(msg)\n        return addrinfo\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        if self._debug:\n            getaddr_func = self._getaddrinfo_debug\n        else:\n            getaddr_func = socket.getaddrinfo\n\n        return await self.run_in_executor(\n            None, getaddr_func, host, port, family, type, proto, flags)\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        return await self.run_in_executor(\n            None, socket.getnameinfo, sockaddr, flags)\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=True):\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        self._check_sendfile_params(sock, file, offset, count)\n        try:\n            return await self._sock_sendfile_native(sock, file,\n                                                    offset, count)\n        except events.SendfileNotAvailableError as exc:\n            if not fallback:\n                raise\n        return await self._sock_sendfile_fallback(sock, file,\n                                                  offset, count)\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        # NB: sendfile syscall is not supported for SSL sockets and\n        # non-mmap files even if sendfile is supported by OS\n        raise events.SendfileNotAvailableError(\n            f\"syscall sendfile is not available for socket {sock!r} \"\n            \"and file {file!r} combination\")\n\n    async def _sock_sendfile_fallback(self, sock, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = (\n            min(count, constants.SENDFILE_FALLBACK_READBUFFER_SIZE)\n            if count else constants.SENDFILE_FALLBACK_READBUFFER_SIZE\n        )\n        buf = bytearray(blocksize)\n        total_sent = 0\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    break  # EOF\n                await self.sock_sendall(sock, view[:read])\n                total_sent += read\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, sock, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not sock.type == socket.SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n        if not isinstance(offset, int):\n            raise TypeError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n        if offset < 0:\n            raise ValueError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0,\n            proto=0, flags=0, sock=None,\n            local_addr=None, server_hostname=None,\n            ssl_handshake_timeout=None):\n        \"\"\"Connect to a TCP server.\n\n        Create a streaming transport connection to a given Internet host and\n        port: socket family AF_INET or socket.AF_INET6 depending on host (or\n        family if specified), socket type SOCK_STREAM. protocol_factory must be\n        a callable returning a protocol instance.\n\n        This method is a coroutine which will try to establish the connection\n        in the background.  When successful, the coroutine returns a\n        (transport, protocol) pair.\n        \"\"\"\n        if server_hostname is not None and not ssl:\n            raise ValueError('server_hostname is only meaningful with ssl')\n\n        if server_hostname is None and ssl:\n            # Use host as default for server_hostname.  It is an error\n            # if host is empty or not set, e.g. when an\n            # already-connected socket was passed or when only a port\n            # is given.  To avoid this error, you can pass\n            # server_hostname='' -- this will bypass the hostname\n            # check.  (This also means that if host is a numeric\n            # IP/IPv6 address, we will attempt to verify that exact\n            # address; this will probably fail, but it is possible to\n            # create a certificate for a specific IP address, so we\n            # don't judge it here.)\n            if not host:\n                raise ValueError('You must set server_hostname '\n                                 'when using ssl without a host')\n            server_hostname = host\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            infos = await self._ensure_resolved(\n                (host, port), family=family,\n                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)\n            if not infos:\n                raise OSError('getaddrinfo() returned empty list')\n\n            if local_addr is not None:\n                laddr_infos = await self._ensure_resolved(\n                    local_addr, family=family,\n                    type=socket.SOCK_STREAM, proto=proto,\n                    flags=flags, loop=self)\n                if not laddr_infos:\n                    raise OSError('getaddrinfo() returned empty list')\n\n            exceptions = []\n            for family, type, proto, cname, address in infos:\n                try:\n                    sock = socket.socket(family=family, type=type, proto=proto)\n                    sock.setblocking(False)\n                    if local_addr is not None:\n                        for _, _, _, _, laddr in laddr_infos:\n                            try:\n                                sock.bind(laddr)\n                                break\n                            except OSError as exc:\n                                msg = (\n                                    f'error while attempting to bind on '\n                                    f'address {laddr!r}: '\n                                    f'{exc.strerror.lower()}'\n                                )\n                                exc = OSError(exc.errno, msg)\n                                exceptions.append(exc)\n                        else:\n                            sock.close()\n                            sock = None\n                            continue\n                    if self._debug:\n                        logger.debug(\"connect %r to %r\", sock, address)\n                    await self.sock_connect(sock, address)\n                except OSError as exc:\n                    if sock is not None:\n                        sock.close()\n                    exceptions.append(exc)\n                except:\n                    if sock is not None:\n                        sock.close()\n                    raise\n                else:\n                    break\n            else:\n                if len(exceptions) == 1:\n                    raise exceptions[0]\n                else:\n                    # If they all have the same str(), raise one.\n                    model = str(exceptions[0])\n                    if all(str(exc) == model for exc in exceptions):\n                        raise exceptions[0]\n                    # Raise a combined exception so the user can see all\n                    # the various error messages.\n                    raise OSError('Multiple exceptions: {}'.format(\n                        ', '.join(str(exc) for exc in exceptions)))\n\n        else:\n            if sock is None:\n                raise ValueError(\n                    'host and port was not specified and no sock specified')\n            if sock.type != socket.SOCK_STREAM:\n                # We allow AF_INET, AF_INET6, AF_UNIX as long as they\n                # are SOCK_STREAM.\n                # We support passing AF_UNIX sockets even though we have\n                # a dedicated API for that: create_unix_connection.\n                # Disallowing AF_UNIX in this method, breaks backwards\n                # compatibility.\n                raise ValueError(\n                    f'A Stream Socket was expected, got {sock!r}')\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r connected to %s:%r: (%r, %r)\",\n                         sock, host, port, transport, protocol)\n        return transport, protocol\n\n    async def _create_connection_transport(\n            self, sock, protocol_factory, ssl,\n            server_hostname, server_side=False,\n            ssl_handshake_timeout=None):\n\n        sock.setblocking(False)\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        if ssl:\n            sslcontext = None if isinstance(ssl, bool) else ssl\n            transport = self._make_ssl_transport(\n                sock, protocol, sslcontext, waiter,\n                server_side=server_side, server_hostname=server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout)\n        else:\n            transport = self._make_socket_transport(sock, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file to transport.\n\n        Return the total number of bytes which were sent.\n\n        The method uses high-performance os.sendfile if available.\n\n        file must be a regular file object opened in binary mode.\n\n        offset tells from where to start reading the file. If specified,\n        count is the total number of bytes to transmit as opposed to\n        sending the file until EOF is reached. File position is updated on\n        return or also in case of error in which case file.tell()\n        can be used to figure out the number of bytes\n        which were sent.\n\n        fallback set to True makes asyncio to manually read and send\n        the file when the platform does not support the sendfile syscall\n        (e.g. Windows or SSL socket on Unix).\n\n        Raise SendfileNotAvailableError if the system does not support\n        sendfile syscall and fallback is False.\n        \"\"\"\n        if transport.is_closing():\n            raise RuntimeError(\"Transport is closing\")\n        mode = getattr(transport, '_sendfile_compatible',\n                       constants._SendfileMode.UNSUPPORTED)\n        if mode is constants._SendfileMode.UNSUPPORTED:\n            raise RuntimeError(\n                f\"sendfile is not supported for transport {transport!r}\")\n        if mode is constants._SendfileMode.TRY_NATIVE:\n            try:\n                return await self._sendfile_native(transport, file,\n                                                   offset, count)\n            except events.SendfileNotAvailableError as exc:\n                if not fallback:\n                    raise\n\n        if not fallback:\n            raise RuntimeError(\n                f\"fallback is disabled and native sendfile is not \"\n                f\"supported for transport {transport!r}\")\n\n        return await self._sendfile_fallback(transport, file,\n                                             offset, count)\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        raise events.SendfileNotAvailableError(\n            \"sendfile syscall is not supported\")\n\n    async def _sendfile_fallback(self, transp, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 16384) if count else 16384\n        buf = bytearray(blocksize)\n        total_sent = 0\n        proto = _SendfileFallbackProtocol(transp)\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        return total_sent\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    return total_sent  # EOF\n                await proto.drain()\n                transp.write(view[:read])\n                total_sent += read\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n            await proto.restore()\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        if ssl is None:\n            raise RuntimeError('Python ssl module is not available')\n\n        if not isinstance(sslcontext, ssl.SSLContext):\n            raise TypeError(\n                f'sslcontext is expected to be an instance of ssl.SSLContext, '\n                f'got {sslcontext!r}')\n\n        if not getattr(transport, '_start_tls_compatible', False):\n            raise TypeError(\n                f'transport {transport!r} is not supported by start_tls()')\n\n        waiter = self.create_future()\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            call_connection_made=False)\n\n        # Pause early so that \"ssl_protocol.data_received()\" doesn't\n        # have a chance to get called before \"ssl_protocol.connection_made()\".\n        transport.pause_reading()\n\n        transport.set_protocol(ssl_protocol)\n        conmade_cb = self.call_soon(ssl_protocol.connection_made, transport)\n        resume_cb = self.call_soon(transport.resume_reading)\n\n        try:\n            await waiter\n        except Exception:\n            transport.close()\n            conmade_cb.cancel()\n            resume_cb.cancel()\n            raise\n\n        return ssl_protocol._app_transport\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=_unset, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"Create datagram connection.\"\"\"\n        if sock is not None:\n            if sock.type != socket.SOCK_DGRAM:\n                raise ValueError(\n                    f'A UDP Socket was expected, got {sock!r}')\n            if (local_addr or remote_addr or\n                    family or proto or flags or\n                    reuse_port or allow_broadcast):\n                # show the problematic kwargs in exception msg\n                opts = dict(local_addr=local_addr, remote_addr=remote_addr,\n                            family=family, proto=proto, flags=flags,\n                            reuse_address=reuse_address, reuse_port=reuse_port,\n                            allow_broadcast=allow_broadcast)\n                problems = ', '.join(f'{k}={v}' for k, v in opts.items() if v)\n                raise ValueError(\n                    f'socket modifier keyword arguments can not be used '\n                    f'when sock is specified. ({problems})')\n            sock.setblocking(False)\n            r_addr = None\n        else:\n            if not (local_addr or remote_addr):\n                if family == 0:\n                    raise ValueError('unexpected address family')\n                addr_pairs_info = (((family, proto), (None, None)),)\n            elif hasattr(socket, 'AF_UNIX') and family == socket.AF_UNIX:\n                for addr in (local_addr, remote_addr):\n                    if addr is not None and not isinstance(addr, str):\n                        raise TypeError('string is expected')\n                addr_pairs_info = (((family, proto),\n                                    (local_addr, remote_addr)), )\n            else:\n                # join address by (family, protocol)\n                addr_infos = collections.OrderedDict()\n                for idx, addr in ((0, local_addr), (1, remote_addr)):\n                    if addr is not None:\n                        assert isinstance(addr, tuple) and len(addr) == 2, (\n                            '2-tuple is expected')\n\n                        infos = await self._ensure_resolved(\n                            addr, family=family, type=socket.SOCK_DGRAM,\n                            proto=proto, flags=flags, loop=self)\n                        if not infos:\n                            raise OSError('getaddrinfo() returned empty list')\n\n                        for fam, _, pro, _, address in infos:\n                            key = (fam, pro)\n                            if key not in addr_infos:\n                                addr_infos[key] = [None, None]\n                            addr_infos[key][idx] = address\n\n                # each addr has to have info for each (family, proto) pair\n                addr_pairs_info = [\n                    (key, addr_pair) for key, addr_pair in addr_infos.items()\n                    if not ((local_addr and addr_pair[0] is None) or\n                            (remote_addr and addr_pair[1] is None))]\n\n                if not addr_pairs_info:\n                    raise ValueError('can not get address information')\n\n            exceptions = []\n\n            # bpo-37228\n            if reuse_address is not _unset:\n                if reuse_address:\n                    raise ValueError(\"Passing `reuse_address=True` is no \"\n                                     \"longer supported, as the usage of \"\n                                     \"SO_REUSEPORT in UDP poses a significant \"\n                                     \"security concern.\")\n                else:\n                    warnings.warn(\"The *reuse_address* parameter has been \"\n                                  \"deprecated as of 3.7.6 and is scheduled \"\n                                  \"for removal in 3.11.\", DeprecationWarning,\n                                  stacklevel=2)\n\n            for ((family, proto),\n                 (local_address, remote_address)) in addr_pairs_info:\n                sock = None\n                r_addr = None\n                try:\n                    sock = socket.socket(\n                        family=family, type=socket.SOCK_DGRAM, proto=proto)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    if allow_broadcast:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n                    sock.setblocking(False)\n\n                    if local_addr:\n                        sock.bind(local_address)\n                    if remote_addr:\n                        if not allow_broadcast:\n                            await self.sock_connect(sock, remote_address)\n                        r_addr = remote_address\n                except OSError as exc:\n                    if sock is not None:\n                        sock.close()\n                    exceptions.append(exc)\n                except:\n                    if sock is not None:\n                        sock.close()\n                    raise\n                else:\n                    break\n            else:\n                raise exceptions[0]\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_datagram_transport(\n            sock, protocol, r_addr, waiter)\n        if self._debug:\n            if local_addr:\n                logger.info(\"Datagram endpoint local_addr=%r remote_addr=%r \"\n                            \"created: (%r, %r)\",\n                            local_addr, remote_addr, transport, protocol)\n            else:\n                logger.debug(\"Datagram endpoint remote_addr=%r created: \"\n                             \"(%r, %r)\",\n                             remote_addr, transport, protocol)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def _ensure_resolved(self, address, *,\n                               family=0, type=socket.SOCK_STREAM,\n                               proto=0, flags=0, loop):\n        host, port = address[:2]\n        info = _ipaddr_info(host, port, family, type, proto, *address[2:])\n        if info is not None:\n            # \"host\" is already a resolved IP.\n            return [info]\n        else:\n            return await loop.getaddrinfo(host, port, family=family, type=type,\n                                          proto=proto, flags=flags)\n\n    async def _create_server_getaddrinfo(self, host, port, family, flags):\n        infos = await self._ensure_resolved((host, port), family=family,\n                                            type=socket.SOCK_STREAM,\n                                            flags=flags, loop=self)\n        if not infos:\n            raise OSError(f'getaddrinfo({host!r}) returned empty list')\n        return infos\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *,\n            family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE,\n            sock=None,\n            backlog=100,\n            ssl=None,\n            reuse_address=None,\n            reuse_port=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"Create a TCP server.\n\n        The host parameter can be a string, in that case the TCP server is\n        bound to host and port.\n\n        The host parameter can also be a sequence of strings and in that case\n        the TCP server is bound to all hosts of the sequence. If a host\n        appears multiple times (possibly indirectly e.g. when hostnames\n        resolve to the same IP address), the server is only bound once to that\n        host.\n\n        Return a Server object which can be used to stop the service.\n\n        This method is a coroutine.\n        \"\"\"\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            if reuse_address is None:\n                reuse_address = os.name == 'posix' and sys.platform != 'cygwin'\n            sockets = []\n            if host == '':\n                hosts = [None]\n            elif (isinstance(host, str) or\n                  not isinstance(host, collections.abc.Iterable)):\n                hosts = [host]\n            else:\n                hosts = host\n\n            fs = [self._create_server_getaddrinfo(host, port, family=family,\n                                                  flags=flags)\n                  for host in hosts]\n            infos = await tasks.gather(*fs, loop=self)\n            infos = set(itertools.chain.from_iterable(infos))\n\n            completed = False\n            try:\n                for res in infos:\n                    af, socktype, proto, canonname, sa = res\n                    try:\n                        sock = socket.socket(af, socktype, proto)\n                    except socket.error:\n                        # Assume it's a bad family/type/protocol combination.\n                        if self._debug:\n                            logger.warning('create_server() failed to create '\n                                           'socket.socket(%r, %r, %r)',\n                                           af, socktype, proto, exc_info=True)\n                        continue\n                    sockets.append(sock)\n                    if reuse_address:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    # Disable IPv4/IPv6 dual stack support (enabled by\n                    # default on Linux) which makes a single socket\n                    # listen on both address families.\n                    if (_HAS_IPv6 and\n                            af == socket.AF_INET6 and\n                            hasattr(socket, 'IPPROTO_IPV6')):\n                        sock.setsockopt(socket.IPPROTO_IPV6,\n                                        socket.IPV6_V6ONLY,\n                                        True)\n                    try:\n                        sock.bind(sa)\n                    except OSError as err:\n                        raise OSError(err.errno, 'error while attempting '\n                                      'to bind on address %r: %s'\n                                      % (sa, err.strerror.lower())) from None\n                completed = True\n            finally:\n                if not completed:\n                    for sock in sockets:\n                        sock.close()\n        else:\n            if sock is None:\n                raise ValueError('Neither host/port nor sock were specified')\n            if sock.type != socket.SOCK_STREAM:\n                raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n            sockets = [sock]\n\n        for sock in sockets:\n            sock.setblocking(False)\n\n        server = Server(self, sockets, protocol_factory,\n                        ssl, backlog, ssl_handshake_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0, loop=self)\n\n        if self._debug:\n            logger.info(\"%r is serving\", server)\n        return server\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None):\n        \"\"\"Handle an accepted connection.\n\n        This is used by servers that accept connections outside of\n        asyncio but that use asyncio to handle connections.\n\n        This method is a coroutine.  When completed, the coroutine\n        returns a (transport, protocol) pair.\n        \"\"\"\n        if sock.type != socket.SOCK_STREAM:\n            raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, '', server_side=True,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r handled: (%r, %r)\", sock, transport, protocol)\n        return transport, protocol\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_read_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Read pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_write_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Write pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    def _log_subprocess(self, msg, stdin, stdout, stderr):\n        info = [msg]\n        if stdin is not None:\n            info.append(f'stdin={_format_pipe(stdin)}')\n        if stdout is not None and stderr == subprocess.STDOUT:\n            info.append(f'stdout=stderr={_format_pipe(stdout)}')\n        else:\n            if stdout is not None:\n                info.append(f'stdout={_format_pipe(stdout)}')\n            if stderr is not None:\n                info.append(f'stderr={_format_pipe(stderr)}')\n        logger.debug(' '.join(info))\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               universal_newlines=False,\n                               shell=True, bufsize=0,\n                               **kwargs):\n        if not isinstance(cmd, (bytes, str)):\n            raise ValueError(\"cmd must be a string\")\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if not shell:\n            raise ValueError(\"shell must be True\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = 'run shell command %r' % cmd\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, cmd, True, stdin, stdout, stderr, bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    async def subprocess_exec(self, protocol_factory, program, *args,\n                              stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE, universal_newlines=False,\n                              shell=False, bufsize=0, **kwargs):\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if shell:\n            raise ValueError(\"shell must be False\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        popen_args = (program,) + args\n        for arg in popen_args:\n            if not isinstance(arg, (str, bytes)):\n                raise TypeError(\n                    f\"program arguments must be a bytes or text string, \"\n                    f\"not {type(arg).__name__}\")\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = f'execute program {program!r}'\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, popen_args, False, stdin, stdout, stderr,\n            bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    def get_exception_handler(self):\n        \"\"\"Return an exception handler, or None if the default one is in use.\n        \"\"\"\n        return self._exception_handler\n\n    def set_exception_handler(self, handler):\n        \"\"\"Set handler as the new event loop exception handler.\n\n        If handler is None, the default exception handler will\n        be set.\n\n        If handler is a callable object, it should have a\n        signature matching '(loop, context)', where 'loop'\n        will be a reference to the active event loop, 'context'\n        will be a dict object (see `call_exception_handler()`\n        documentation for details about context).\n        \"\"\"\n        if handler is not None and not callable(handler):\n            raise TypeError(f'A callable object or None is expected, '\n                            f'got {handler!r}')\n        self._exception_handler = handler\n\n    def default_exception_handler(self, context):\n        \"\"\"Default exception handler.\n\n        This is called when an exception occurs and no exception\n        handler is set, and can be called by a custom exception\n        handler that wants to defer to the default behavior.\n\n        This default handler logs the error message and other\n        context-dependent information.  In debug mode, a truncated\n        stack trace is also appended showing where the given object\n        (e.g. a handle or future or task) was created, if any.\n\n        The context parameter has the same meaning as in\n        `call_exception_handler()`.\n        \"\"\"\n        message = context.get('message')\n        if not message:\n            message = 'Unhandled exception in event loop'\n\n        exception = context.get('exception')\n        if exception is not None:\n            exc_info = (type(exception), exception, exception.__traceback__)\n        else:\n            exc_info = False\n\n        if ('source_traceback' not in context and\n                self._current_handle is not None and\n                self._current_handle._source_traceback):\n            context['handle_traceback'] = \\\n                self._current_handle._source_traceback\n\n        log_lines = [message]\n        for key in sorted(context):\n            if key in {'message', 'exception'}:\n                continue\n            value = context[key]\n            if key == 'source_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Object created at (most recent call last):\\n'\n                value += tb.rstrip()\n            elif key == 'handle_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Handle created at (most recent call last):\\n'\n                value += tb.rstrip()\n            else:\n                value = repr(value)\n            log_lines.append(f'{key}: {value}')\n\n        logger.error('\\n'.join(log_lines), exc_info=exc_info)\n\n    def call_exception_handler(self, context):\n        \"\"\"Call the current event loop's exception handler.\n\n        The context argument is a dict containing the following keys:\n\n        - 'message': Error message;\n        - 'exception' (optional): Exception object;\n        - 'future' (optional): Future instance;\n        - 'task' (optional): Task instance;\n        - 'handle' (optional): Handle instance;\n        - 'protocol' (optional): Protocol instance;\n        - 'transport' (optional): Transport instance;\n        - 'socket' (optional): Socket instance;\n        - 'asyncgen' (optional): Asynchronous generator that caused\n                                 the exception.\n\n        New keys maybe introduced in the future.\n\n        Note: do not overload this method in an event loop subclass.\n        For custom exception handling, use the\n        `set_exception_handler()` method.\n        \"\"\"\n        if self._exception_handler is None:\n            try:\n                self.default_exception_handler(context)\n            except Exception:\n                # Second protection layer for unexpected errors\n                # in the default implementation, as well as for subclassed\n                # event loops with overloaded \"default_exception_handler\".\n                logger.error('Exception in default exception handler',\n                             exc_info=True)\n        else:\n            try:\n                self._exception_handler(self, context)\n            except Exception as exc:\n                # Exception in the user set custom exception handler.\n                try:\n                    # Let's try default handler.\n                    self.default_exception_handler({\n                        'message': 'Unhandled error in exception handler',\n                        'exception': exc,\n                        'context': context,\n                    })\n                except Exception:\n                    # Guard 'default_exception_handler' in case it is\n                    # overloaded.\n                    logger.error('Exception in default exception handler '\n                                 'while handling an unexpected error '\n                                 'in custom exception handler',\n                                 exc_info=True)\n\n    def _add_callback(self, handle):\n        \"\"\"Add a Handle to _scheduled (TimerHandle) or _ready.\"\"\"\n        assert isinstance(handle, events.Handle), 'A Handle is required here'\n        if handle._cancelled:\n            return\n        assert not isinstance(handle, events.TimerHandle)\n        self._ready.append(handle)\n\n    def _add_callback_signalsafe(self, handle):\n        \"\"\"Like _add_callback() but called from a signal handler.\"\"\"\n        self._add_callback(handle)\n        self._write_to_self()\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        if handle._scheduled:\n            self._timer_cancelled_count += 1\n\n    def _run_once(self):\n        \"\"\"Run one full iteration of the event loop.\n\n        This calls all currently ready callbacks, polls for I/O,\n        schedules the resulting callbacks, and finally schedules\n        'call_later' callbacks.\n        \"\"\"\n\n        sched_count = len(self._scheduled)\n        if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and\n            self._timer_cancelled_count / sched_count >\n                _MIN_CANCELLED_TIMER_HANDLES_FRACTION):\n            # Remove delayed calls that were cancelled if their number\n            # is too high\n            new_scheduled = []\n            for handle in self._scheduled:\n                if handle._cancelled:\n                    handle._scheduled = False\n                else:\n                    new_scheduled.append(handle)\n\n            heapq.heapify(new_scheduled)\n            self._scheduled = new_scheduled\n            self._timer_cancelled_count = 0\n        else:\n            # Remove delayed calls that were cancelled from head of queue.\n            while self._scheduled and self._scheduled[0]._cancelled:\n                self._timer_cancelled_count -= 1\n                handle = heapq.heappop(self._scheduled)\n                handle._scheduled = False\n\n        timeout = None\n        if self._ready or self._stopping:\n            timeout = 0\n        elif self._scheduled:\n            # Compute the desired timeout.\n            when = self._scheduled[0]._when\n            timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT)\n\n        if self._debug and timeout != 0:\n            t0 = self.time()\n            event_list = self._selector.select(timeout)\n            dt = self.time() - t0\n            if dt >= 1.0:\n                level = logging.INFO\n            else:\n                level = logging.DEBUG\n            nevent = len(event_list)\n            if timeout is None:\n                logger.log(level, 'poll took %.3f ms: %s events',\n                           dt * 1e3, nevent)\n            elif nevent:\n                logger.log(level,\n                           'poll %.3f ms took %.3f ms: %s events',\n                           timeout * 1e3, dt * 1e3, nevent)\n            elif dt >= 1.0:\n                logger.log(level,\n                           'poll %.3f ms took %.3f ms: timeout',\n                           timeout * 1e3, dt * 1e3)\n        else:\n            event_list = self._selector.select(timeout)\n        self._process_events(event_list)\n\n        # Handle 'later' callbacks that are ready.\n        end_time = self.time() + self._clock_resolution\n        while self._scheduled:\n            handle = self._scheduled[0]\n            if handle._when >= end_time:\n                break\n            handle = heapq.heappop(self._scheduled)\n            handle._scheduled = False\n            self._ready.append(handle)\n\n        # This is the only place where callbacks are actually *called*.\n        # All other places just add them to ready.\n        # Note: We run all currently scheduled callbacks, but not any\n        # callbacks scheduled by callbacks run this time around --\n        # they will be run the next time (after another I/O poll).\n        # Use an idiom that is thread-safe without using locks.\n        ntodo = len(self._ready)\n        for i in range(ntodo):\n            handle = self._ready.popleft()\n            if handle._cancelled:\n                continue\n            if self._debug:\n                try:\n                    self._current_handle = handle\n                    t0 = self.time()\n                    handle._run()\n                    dt = self.time() - t0\n                    if dt >= self.slow_callback_duration:\n                        logger.warning('Executing %s took %.3f seconds',\n                                       _format_handle(handle), dt)\n                finally:\n                    self._current_handle = None\n            else:\n                handle._run()\n        handle = None  # Needed to break cycles when an exception occurs.\n\n    def _set_coroutine_origin_tracking(self, enabled):\n        if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):\n            return\n\n        if enabled:\n            self._coroutine_origin_tracking_saved_depth = (\n                sys.get_coroutine_origin_tracking_depth())\n            sys.set_coroutine_origin_tracking_depth(\n                constants.DEBUG_STACK_DEPTH)\n        else:\n            sys.set_coroutine_origin_tracking_depth(\n                self._coroutine_origin_tracking_saved_depth)\n\n        self._coroutine_origin_tracking_enabled = enabled\n\n    def get_debug(self):\n        return self._debug\n\n    def set_debug(self, enabled):\n        self._debug = enabled\n\n        if self.is_running():\n            self.call_soon_threadsafe(self._set_coroutine_origin_tracking, enabled)\n", 1811], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py": ["\"\"\"Event loop and event loop policy.\"\"\"\n\n__all__ = (\n    'AbstractEventLoopPolicy',\n    'AbstractEventLoop', 'AbstractServer',\n    'Handle', 'TimerHandle', 'SendfileNotAvailableError',\n    'get_event_loop_policy', 'set_event_loop_policy',\n    'get_event_loop', 'set_event_loop', 'new_event_loop',\n    'get_child_watcher', 'set_child_watcher',\n    '_set_running_loop', 'get_running_loop',\n    '_get_running_loop',\n)\n\nimport contextvars\nimport os\nimport socket\nimport subprocess\nimport sys\nimport threading\n\nfrom . import format_helpers\n\n\nclass SendfileNotAvailableError(RuntimeError):\n    \"\"\"Sendfile syscall is not available.\n\n    Raised if OS does not support sendfile syscall for given socket or\n    file type.\n    \"\"\"\n\n\nclass Handle:\n    \"\"\"Object returned by callback registration methods.\"\"\"\n\n    __slots__ = ('_callback', '_args', '_cancelled', '_loop',\n                 '_source_traceback', '_repr', '__weakref__',\n                 '_context')\n\n    def __init__(self, callback, args, loop, context=None):\n        if context is None:\n            context = contextvars.copy_context()\n        self._context = context\n        self._loop = loop\n        self._callback = callback\n        self._args = args\n        self._cancelled = False\n        self._repr = None\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n        else:\n            self._source_traceback = None\n\n    def _repr_info(self):\n        info = [self.__class__.__name__]\n        if self._cancelled:\n            info.append('cancelled')\n        if self._callback is not None:\n            info.append(format_helpers._format_callback_source(\n                self._callback, self._args))\n        if self._source_traceback:\n            frame = self._source_traceback[-1]\n            info.append(f'created at {frame[0]}:{frame[1]}')\n        return info\n\n    def __repr__(self):\n        if self._repr is not None:\n            return self._repr\n        info = self._repr_info()\n        return '<{}>'.format(' '.join(info))\n\n    def cancel(self):\n        if not self._cancelled:\n            self._cancelled = True\n            if self._loop.get_debug():\n                # Keep a representation in debug mode to keep callback and\n                # parameters. For example, to log the warning\n                # \"Executing <Handle...> took 2.5 second\"\n                self._repr = repr(self)\n            self._callback = None\n            self._args = None\n\n    def cancelled(self):\n        return self._cancelled\n\n    def _run(self):\n        try:\n            self._context.run(self._callback, *self._args)\n        except Exception as exc:\n            cb = format_helpers._format_callback_source(\n                self._callback, self._args)\n            msg = f'Exception in callback {cb}'\n            context = {\n                'message': msg,\n                'exception': exc,\n                'handle': self,\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        self = None  # Needed to break cycles when an exception occurs.\n\n\nclass TimerHandle(Handle):\n    \"\"\"Object returned by timed callback registration methods.\"\"\"\n\n    __slots__ = ['_scheduled', '_when']\n\n    def __init__(self, when, callback, args, loop, context=None):\n        assert when is not None\n        super().__init__(callback, args, loop, context)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        self._when = when\n        self._scheduled = False\n\n    def _repr_info(self):\n        info = super()._repr_info()\n        pos = 2 if self._cancelled else 1\n        info.insert(pos, f'when={self._when}')\n        return info\n\n    def __hash__(self):\n        return hash(self._when)\n\n    def __lt__(self, other):\n        return self._when < other._when\n\n    def __le__(self, other):\n        if self._when < other._when:\n            return True\n        return self.__eq__(other)\n\n    def __gt__(self, other):\n        return self._when > other._when\n\n    def __ge__(self, other):\n        if self._when > other._when:\n            return True\n        return self.__eq__(other)\n\n    def __eq__(self, other):\n        if isinstance(other, TimerHandle):\n            return (self._when == other._when and\n                    self._callback == other._callback and\n                    self._args == other._args and\n                    self._cancelled == other._cancelled)\n        return NotImplemented\n\n    def __ne__(self, other):\n        equal = self.__eq__(other)\n        return NotImplemented if equal is NotImplemented else not equal\n\n    def cancel(self):\n        if not self._cancelled:\n            self._loop._timer_handle_cancelled(self)\n        super().cancel()\n\n    def when(self):\n        \"\"\"Return a scheduled callback time.\n\n        The time is an absolute timestamp, using the same time\n        reference as loop.time().\n        \"\"\"\n        return self._when\n\n\nclass AbstractServer:\n    \"\"\"Abstract server returned by create_server().\"\"\"\n\n    def close(self):\n        \"\"\"Stop serving.  This leaves existing connections open.\"\"\"\n        raise NotImplementedError\n\n    def get_loop(self):\n        \"\"\"Get the event loop the Server object is attached to.\"\"\"\n        raise NotImplementedError\n\n    def is_serving(self):\n        \"\"\"Return True if the server is accepting connections.\"\"\"\n        raise NotImplementedError\n\n    async def start_serving(self):\n        \"\"\"Start accepting connections.\n\n        This method is idempotent, so it can be called when\n        the server is already being serving.\n        \"\"\"\n        raise NotImplementedError\n\n    async def serve_forever(self):\n        \"\"\"Start accepting connections until the coroutine is cancelled.\n\n        The server is closed when the coroutine is cancelled.\n        \"\"\"\n        raise NotImplementedError\n\n    async def wait_closed(self):\n        \"\"\"Coroutine to wait until service is closed.\"\"\"\n        raise NotImplementedError\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc):\n        self.close()\n        await self.wait_closed()\n\n\nclass AbstractEventLoop:\n    \"\"\"Abstract event loop.\"\"\"\n\n    # Running and stopping the event loop.\n\n    def run_forever(self):\n        \"\"\"Run the event loop until stop() is called.\"\"\"\n        raise NotImplementedError\n\n    def run_until_complete(self, future):\n        \"\"\"Run the event loop until a Future is done.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        raise NotImplementedError\n\n    def stop(self):\n        \"\"\"Stop the event loop as soon as reasonable.\n\n        Exactly how soon that is may depend on the implementation, but\n        no more I/O callbacks should be scheduled.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_running(self):\n        \"\"\"Return whether the event loop is currently running.\"\"\"\n        raise NotImplementedError\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the loop.\n\n        The loop should not be running.\n\n        This is idempotent and irreversible.\n\n        No other methods should be called after this one.\n        \"\"\"\n        raise NotImplementedError\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        raise NotImplementedError\n\n    # Methods scheduling callbacks.  All these return Handles.\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        raise NotImplementedError\n\n    def call_soon(self, callback, *args):\n        return self.call_later(0, callback, *args)\n\n    def call_later(self, delay, callback, *args):\n        raise NotImplementedError\n\n    def call_at(self, when, callback, *args):\n        raise NotImplementedError\n\n    def time(self):\n        raise NotImplementedError\n\n    def create_future(self):\n        raise NotImplementedError\n\n    # Method scheduling a coroutine object: create a task.\n\n    def create_task(self, coro):\n        raise NotImplementedError\n\n    # Methods for interacting with threads.\n\n    def call_soon_threadsafe(self, callback, *args):\n        raise NotImplementedError\n\n    async def run_in_executor(self, executor, func, *args):\n        raise NotImplementedError\n\n    def set_default_executor(self, executor):\n        raise NotImplementedError\n\n    # Network I/O methods returning Futures.\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        raise NotImplementedError\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        raise NotImplementedError\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0, proto=0,\n            flags=0, sock=None, local_addr=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None):\n        raise NotImplementedError\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *, family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE, sock=None, backlog=100,\n            ssl=None, reuse_address=None, reuse_port=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a TCP server bound to host and port.\n\n        The return value is a Server object which can be used to stop\n        the service.\n\n        If host is an empty string or None all interfaces are assumed\n        and a list of multiple sockets will be returned (most likely\n        one for IPv4 and another one for IPv6). The host parameter can also be\n        a sequence (e.g. list) of hosts to bind to.\n\n        family can be set to either AF_INET or AF_INET6 to force the\n        socket to use IPv4 or IPv6. If not set it will be determined\n        from host (defaults to AF_UNSPEC).\n\n        flags is a bitmask for getaddrinfo().\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL handshake before aborting the\n        connection. Default is 60s.\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file through a transport.\n\n        Return an amount of sent bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade a transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None):\n        raise NotImplementedError\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a UNIX Domain Socket server.\n\n        The return value is a Server object, which can be used to stop\n        the service.\n\n        path is a str, representing a file systsem path to bind the\n        server socket to.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for the SSL handshake to complete (defaults to 60s).\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=None, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"A coroutine which creates a datagram endpoint.\n\n        This method will try to establish the endpoint in the background.\n        When successful, the coroutine returns a (transport, protocol) pair.\n\n        protocol_factory must be a callable returning a protocol instance.\n\n        socket family AF_INET, socket.AF_INET6 or socket.AF_UNIX depending on\n        host (or family if specified), socket type SOCK_DGRAM.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified it will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows and some UNIX's. If the\n        :py:data:`~socket.SO_REUSEPORT` constant is not defined then this\n        capability is unsupported.\n\n        allow_broadcast tells the kernel to allow this endpoint to send\n        messages to the broadcast address.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n        \"\"\"\n        raise NotImplementedError\n\n    # Pipes and subprocesses.\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        \"\"\"Register read pipe in event loop. Set the pipe to non-blocking mode.\n\n        protocol_factory should instantiate object with Protocol interface.\n        pipe is a file-like object.\n        Return pair (transport, protocol), where transport supports the\n        ReadTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vise versa.\n        raise NotImplementedError\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        \"\"\"Register write pipe in event loop.\n\n        protocol_factory should instantiate object with BaseProtocol interface.\n        Pipe is file-like object already switched to nonblocking.\n        Return pair (transport, protocol), where transport support\n        WriteTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vise versa.\n        raise NotImplementedError\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               **kwargs):\n        raise NotImplementedError\n\n    async def subprocess_exec(self, protocol_factory, *args,\n                              stdin=subprocess.PIPE,\n                              stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE,\n                              **kwargs):\n        raise NotImplementedError\n\n    # Ready-based callback registration methods.\n    # The add_*() methods return None.\n    # The remove_*() methods return True if something was removed,\n    # False if there was nothing to delete.\n\n    def add_reader(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_reader(self, fd):\n        raise NotImplementedError\n\n    def add_writer(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_writer(self, fd):\n        raise NotImplementedError\n\n    # Completion based I/O methods returning Futures.\n\n    async def sock_recv(self, sock, nbytes):\n        raise NotImplementedError\n\n    async def sock_recv_into(self, sock, buf):\n        raise NotImplementedError\n\n    async def sock_sendall(self, sock, data):\n        raise NotImplementedError\n\n    async def sock_connect(self, sock, address):\n        raise NotImplementedError\n\n    async def sock_accept(self, sock):\n        raise NotImplementedError\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=None):\n        raise NotImplementedError\n\n    # Signal handling.\n\n    def add_signal_handler(self, sig, callback, *args):\n        raise NotImplementedError\n\n    def remove_signal_handler(self, sig):\n        raise NotImplementedError\n\n    # Task factory.\n\n    def set_task_factory(self, factory):\n        raise NotImplementedError\n\n    def get_task_factory(self):\n        raise NotImplementedError\n\n    # Error handlers.\n\n    def get_exception_handler(self):\n        raise NotImplementedError\n\n    def set_exception_handler(self, handler):\n        raise NotImplementedError\n\n    def default_exception_handler(self, context):\n        raise NotImplementedError\n\n    def call_exception_handler(self, context):\n        raise NotImplementedError\n\n    # Debug flag management.\n\n    def get_debug(self):\n        raise NotImplementedError\n\n    def set_debug(self, enabled):\n        raise NotImplementedError\n\n\nclass AbstractEventLoopPolicy:\n    \"\"\"Abstract policy for accessing the event loop.\"\"\"\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an event loop object implementing the BaseEventLoop interface,\n        or raises an exception in case no event loop has been set for the\n        current context and the current policy does not specify to create one.\n\n        It should never return None.\"\"\"\n        raise NotImplementedError\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop for the current context to loop.\"\"\"\n        raise NotImplementedError\n\n    def new_event_loop(self):\n        \"\"\"Create and return a new event loop object according to this\n        policy's rules. If there's need to set this loop as the event loop for\n        the current context, set_event_loop must be called explicitly.\"\"\"\n        raise NotImplementedError\n\n    # Child processes handling (Unix only).\n\n    def get_child_watcher(self):\n        \"Get the watcher for child processes.\"\n        raise NotImplementedError\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n        raise NotImplementedError\n\n\nclass BaseDefaultEventLoopPolicy(AbstractEventLoopPolicy):\n    \"\"\"Default policy implementation for accessing the event loop.\n\n    In this policy, each thread has its own event loop.  However, we\n    only automatically create an event loop by default for the main\n    thread; other threads by default have no event loop.\n\n    Other policies may have different rules (e.g. a single global\n    event loop, or automatically creating an event loop per thread, or\n    using some other notion of context to which an event loop is\n    associated).\n    \"\"\"\n\n    _loop_factory = None\n\n    class _Local(threading.local):\n        _loop = None\n        _set_called = False\n\n    def __init__(self):\n        self._local = self._Local()\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an instance of EventLoop or raises an exception.\n        \"\"\"\n        if (self._local._loop is None and\n                not self._local._set_called and\n                isinstance(threading.current_thread(), threading._MainThread)):\n            self.set_event_loop(self.new_event_loop())\n\n        if self._local._loop is None:\n            raise RuntimeError('There is no current event loop in thread %r.'\n                               % threading.current_thread().name)\n\n        return self._local._loop\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\"\"\"\n        self._local._set_called = True\n        assert loop is None or isinstance(loop, AbstractEventLoop)\n        self._local._loop = loop\n\n    def new_event_loop(self):\n        \"\"\"Create a new event loop.\n\n        You must call set_event_loop() to make this the current event\n        loop.\n        \"\"\"\n        return self._loop_factory()\n\n\n# Event loop policy.  The policy itself is always global, even if the\n# policy's rules say that there is an event loop per thread (or other\n# notion of context).  The default policy is installed by the first\n# call to get_event_loop_policy().\n_event_loop_policy = None\n\n# Lock for protecting the on-the-fly creation of the event loop policy.\n_lock = threading.Lock()\n\n\n# A TLS for the running event loop, used by _get_running_loop.\nclass _RunningLoop(threading.local):\n    loop_pid = (None, None)\n\n\n_running_loop = _RunningLoop()\n\n\ndef get_running_loop():\n    \"\"\"Return the running event loop.  Raise a RuntimeError if there is none.\n\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    loop = _get_running_loop()\n    if loop is None:\n        raise RuntimeError('no running event loop')\n    return loop\n\n\ndef _get_running_loop():\n    \"\"\"Return the running event loop or None.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    running_loop, pid = _running_loop.loop_pid\n    if running_loop is not None and pid == os.getpid():\n        return running_loop\n\n\ndef _set_running_loop(loop):\n    \"\"\"Set the running event loop.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    _running_loop.loop_pid = (loop, os.getpid())\n\n\ndef _init_event_loop_policy():\n    global _event_loop_policy\n    with _lock:\n        if _event_loop_policy is None:  # pragma: no branch\n            from . import DefaultEventLoopPolicy\n            _event_loop_policy = DefaultEventLoopPolicy()\n\n\ndef get_event_loop_policy():\n    \"\"\"Get the current event loop policy.\"\"\"\n    if _event_loop_policy is None:\n        _init_event_loop_policy()\n    return _event_loop_policy\n\n\ndef set_event_loop_policy(policy):\n    \"\"\"Set the current event loop policy.\n\n    If policy is None, the default policy is restored.\"\"\"\n    global _event_loop_policy\n    assert policy is None or isinstance(policy, AbstractEventLoopPolicy)\n    _event_loop_policy = policy\n\n\ndef get_event_loop():\n    \"\"\"Return an asyncio event loop.\n\n    When called from a coroutine or a callback (e.g. scheduled with call_soon\n    or similar API), this function will always return the running event loop.\n\n    If there is no running event loop set, the function will return\n    the result of `get_event_loop_policy().get_event_loop()` call.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    current_loop = _get_running_loop()\n    if current_loop is not None:\n        return current_loop\n    return get_event_loop_policy().get_event_loop()\n\n\ndef set_event_loop(loop):\n    \"\"\"Equivalent to calling get_event_loop_policy().set_event_loop(loop).\"\"\"\n    get_event_loop_policy().set_event_loop(loop)\n\n\ndef new_event_loop():\n    \"\"\"Equivalent to calling get_event_loop_policy().new_event_loop().\"\"\"\n    return get_event_loop_policy().new_event_loop()\n\n\ndef get_child_watcher():\n    \"\"\"Equivalent to calling get_event_loop_policy().get_child_watcher().\"\"\"\n    return get_event_loop_policy().get_child_watcher()\n\n\ndef set_child_watcher(watcher):\n    \"\"\"Equivalent to calling\n    get_event_loop_policy().set_child_watcher(watcher).\"\"\"\n    return get_event_loop_policy().set_child_watcher(watcher)\n\n\n# Alias pure-Python implementations for testing purposes.\n_py__get_running_loop = _get_running_loop\n_py__set_running_loop = _set_running_loop\n_py_get_running_loop = get_running_loop\n_py_get_event_loop = get_event_loop\n\n\ntry:\n    # get_event_loop() is one of the most frequently called\n    # functions in asyncio.  Pure Python implementation is\n    # about 4 times slower than C-accelerated.\n    from _asyncio import (_get_running_loop, _set_running_loop,\n                          get_running_loop, get_event_loop)\nexcept ImportError:\n    pass\nelse:\n    # Alias C implementations for testing purposes.\n    _c__get_running_loop = _get_running_loop\n    _c__set_running_loop = _set_running_loop\n    _c_get_running_loop = get_running_loop\n    _c_get_event_loop = get_event_loop\n", 796], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py": ["\"\"\"Selectors module.\n\nThis module allows high-level and efficient I/O multiplexing, built upon the\n`select` module primitives.\n\"\"\"\n\n\nfrom abc import ABCMeta, abstractmethod\nfrom collections import namedtuple\nfrom collections.abc import Mapping\nimport math\nimport select\nimport sys\n\n\n# generic events, that must be mapped to implementation-specific ones\nEVENT_READ = (1 << 0)\nEVENT_WRITE = (1 << 1)\n\n\ndef _fileobj_to_fd(fileobj):\n    \"\"\"Return a file descriptor from a file object.\n\n    Parameters:\n    fileobj -- file object or file descriptor\n\n    Returns:\n    corresponding file descriptor\n\n    Raises:\n    ValueError if the object is invalid\n    \"\"\"\n    if isinstance(fileobj, int):\n        fd = fileobj\n    else:\n        try:\n            fd = int(fileobj.fileno())\n        except (AttributeError, TypeError, ValueError):\n            raise ValueError(\"Invalid file object: \"\n                             \"{!r}\".format(fileobj)) from None\n    if fd < 0:\n        raise ValueError(\"Invalid file descriptor: {}\".format(fd))\n    return fd\n\n\nSelectorKey = namedtuple('SelectorKey', ['fileobj', 'fd', 'events', 'data'])\n\nSelectorKey.__doc__ = \"\"\"SelectorKey(fileobj, fd, events, data)\n\n    Object used to associate a file object to its backing\n    file descriptor, selected event mask, and attached data.\n\"\"\"\nif sys.version_info >= (3, 5):\n    SelectorKey.fileobj.__doc__ = 'File object registered.'\n    SelectorKey.fd.__doc__ = 'Underlying file descriptor.'\n    SelectorKey.events.__doc__ = 'Events that must be waited for on this file object.'\n    SelectorKey.data.__doc__ = ('''Optional opaque data associated to this file object.\n    For example, this could be used to store a per-client session ID.''')\n\nclass _SelectorMapping(Mapping):\n    \"\"\"Mapping of file objects to selector keys.\"\"\"\n\n    def __init__(self, selector):\n        self._selector = selector\n\n    def __len__(self):\n        return len(self._selector._fd_to_key)\n\n    def __getitem__(self, fileobj):\n        try:\n            fd = self._selector._fileobj_lookup(fileobj)\n            return self._selector._fd_to_key[fd]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    def __iter__(self):\n        return iter(self._selector._fd_to_key)\n\n\nclass BaseSelector(metaclass=ABCMeta):\n    \"\"\"Selector abstract base class.\n\n    A selector supports registering file objects to be monitored for specific\n    I/O events.\n\n    A file object is a file descriptor or any object with a `fileno()` method.\n    An arbitrary object can be attached to the file object, which can be used\n    for example to store context information, a callback, etc.\n\n    A selector can use various implementations (select(), poll(), epoll()...)\n    depending on the platform. The default `Selector` class uses the most\n    efficient implementation on the current platform.\n    \"\"\"\n\n    @abstractmethod\n    def register(self, fileobj, events, data=None):\n        \"\"\"Register a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        ValueError if events is invalid\n        KeyError if fileobj is already registered\n        OSError if fileobj is closed or otherwise is unacceptable to\n                the underlying system call (if a system call is made)\n\n        Note:\n        OSError may or may not be raised\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def unregister(self, fileobj):\n        \"\"\"Unregister a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        KeyError if fileobj is not registered\n\n        Note:\n        If fileobj is registered but has since been closed this does\n        *not* raise OSError (even if the wrapped syscall does)\n        \"\"\"\n        raise NotImplementedError\n\n    def modify(self, fileobj, events, data=None):\n        \"\"\"Change a registered file object monitored events or attached data.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        Anything that unregister() or register() raises\n        \"\"\"\n        self.unregister(fileobj)\n        return self.register(fileobj, events, data)\n\n    @abstractmethod\n    def select(self, timeout=None):\n        \"\"\"Perform the actual selection, until some monitored file objects are\n        ready or a timeout expires.\n\n        Parameters:\n        timeout -- if timeout > 0, this specifies the maximum wait time, in\n                   seconds\n                   if timeout <= 0, the select() call won't block, and will\n                   report the currently ready file objects\n                   if timeout is None, select() will block until a monitored\n                   file object becomes ready\n\n        Returns:\n        list of (key, events) for ready file objects\n        `events` is a bitwise mask of EVENT_READ|EVENT_WRITE\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the selector.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        pass\n\n    def get_key(self, fileobj):\n        \"\"\"Return the key associated to a registered file object.\n\n        Returns:\n        SelectorKey for this file object\n        \"\"\"\n        mapping = self.get_map()\n        if mapping is None:\n            raise RuntimeError('Selector is closed')\n        try:\n            return mapping[fileobj]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    @abstractmethod\n    def get_map(self):\n        \"\"\"Return a mapping of file objects to selector keys.\"\"\"\n        raise NotImplementedError\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n\nclass _BaseSelectorImpl(BaseSelector):\n    \"\"\"Base selector implementation.\"\"\"\n\n    def __init__(self):\n        # this maps file descriptors to keys\n        self._fd_to_key = {}\n        # read-only mapping returned by get_map()\n        self._map = _SelectorMapping(self)\n\n    def _fileobj_lookup(self, fileobj):\n        \"\"\"Return a file descriptor from a file object.\n\n        This wraps _fileobj_to_fd() to do an exhaustive search in case\n        the object is invalid but we still have it in our map.  This\n        is used by unregister() so we can unregister an object that\n        was previously registered even if it is closed.  It is also\n        used by _SelectorMapping.\n        \"\"\"\n        try:\n            return _fileobj_to_fd(fileobj)\n        except ValueError:\n            # Do an exhaustive search.\n            for key in self._fd_to_key.values():\n                if key.fileobj is fileobj:\n                    return key.fd\n            # Raise ValueError after all.\n            raise\n\n    def register(self, fileobj, events, data=None):\n        if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):\n            raise ValueError(\"Invalid events: {!r}\".format(events))\n\n        key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)\n\n        if key.fd in self._fd_to_key:\n            raise KeyError(\"{!r} (FD {}) is already registered\"\n                           .format(fileobj, key.fd))\n\n        self._fd_to_key[key.fd] = key\n        return key\n\n    def unregister(self, fileobj):\n        try:\n            key = self._fd_to_key.pop(self._fileobj_lookup(fileobj))\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        if events != key.events:\n            self.unregister(fileobj)\n            key = self.register(fileobj, events, data)\n        elif data != key.data:\n            # Use a shortcut to update the data.\n            key = key._replace(data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def close(self):\n        self._fd_to_key.clear()\n        self._map = None\n\n    def get_map(self):\n        return self._map\n\n    def _key_from_fd(self, fd):\n        \"\"\"Return the key associated to a given file descriptor.\n\n        Parameters:\n        fd -- file descriptor\n\n        Returns:\n        corresponding key, or None if not found\n        \"\"\"\n        try:\n            return self._fd_to_key[fd]\n        except KeyError:\n            return None\n\n\nclass SelectSelector(_BaseSelectorImpl):\n    \"\"\"Select-based selector.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._readers = set()\n        self._writers = set()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        if events & EVENT_READ:\n            self._readers.add(key.fd)\n        if events & EVENT_WRITE:\n            self._writers.add(key.fd)\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        self._readers.discard(key.fd)\n        self._writers.discard(key.fd)\n        return key\n\n    if sys.platform == 'win32':\n        def _select(self, r, w, _, timeout=None):\n            r, w, x = select.select(r, w, w, timeout)\n            return r, w + x, []\n    else:\n        _select = select.select\n\n    def select(self, timeout=None):\n        timeout = None if timeout is None else max(timeout, 0)\n        ready = []\n        try:\n            r, w, _ = self._select(self._readers, self._writers, [], timeout)\n        except InterruptedError:\n            return ready\n        r = set(r)\n        w = set(w)\n        for fd in r | w:\n            events = 0\n            if fd in r:\n                events |= EVENT_READ\n            if fd in w:\n                events |= EVENT_WRITE\n\n            key = self._key_from_fd(fd)\n            if key:\n                ready.append((key, events & key.events))\n        return ready\n\n\nclass _PollLikeSelector(_BaseSelectorImpl):\n    \"\"\"Base class shared between poll, epoll and devpoll selectors.\"\"\"\n    _selector_cls = None\n    _EVENT_READ = None\n    _EVENT_WRITE = None\n\n    def __init__(self):\n        super().__init__()\n        self._selector = self._selector_cls()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        poller_events = 0\n        if events & EVENT_READ:\n            poller_events |= self._EVENT_READ\n        if events & EVENT_WRITE:\n            poller_events |= self._EVENT_WRITE\n        try:\n            self._selector.register(key.fd, poller_events)\n        except:\n            super().unregister(fileobj)\n            raise\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        try:\n            self._selector.unregister(key.fd)\n        except OSError:\n            # This can happen if the FD was closed since it\n            # was registered.\n            pass\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(f\"{fileobj!r} is not registered\") from None\n\n        changed = False\n        if events != key.events:\n            selector_events = 0\n            if events & EVENT_READ:\n                selector_events |= self._EVENT_READ\n            if events & EVENT_WRITE:\n                selector_events |= self._EVENT_WRITE\n            try:\n                self._selector.modify(key.fd, selector_events)\n            except:\n                super().unregister(fileobj)\n                raise\n            changed = True\n        if data != key.data:\n            changed = True\n\n        if changed:\n            key = key._replace(events=events, data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def select(self, timeout=None):\n        # This is shared between poll() and epoll().\n        # epoll() has a different signature and handling of timeout parameter.\n        if timeout is None:\n            timeout = None\n        elif timeout <= 0:\n            timeout = 0\n        else:\n            # poll() has a resolution of 1 millisecond, round away from\n            # zero to wait *at least* timeout seconds.\n            timeout = math.ceil(timeout * 1e3)\n        ready = []\n        try:\n            fd_event_list = self._selector.poll(timeout)\n        except InterruptedError:\n            return ready\n        for fd, event in fd_event_list:\n            events = 0\n            if event & ~self._EVENT_READ:\n                events |= EVENT_WRITE\n            if event & ~self._EVENT_WRITE:\n                events |= EVENT_READ\n\n            key = self._key_from_fd(fd)\n            if key:\n                ready.append((key, events & key.events))\n        return ready\n\n\nif hasattr(select, 'poll'):\n\n    class PollSelector(_PollLikeSelector):\n        \"\"\"Poll-based selector.\"\"\"\n        _selector_cls = select.poll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n\nif hasattr(select, 'epoll'):\n\n    class EpollSelector(_PollLikeSelector):\n        \"\"\"Epoll-based selector.\"\"\"\n        _selector_cls = select.epoll\n        _EVENT_READ = select.EPOLLIN\n        _EVENT_WRITE = select.EPOLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def select(self, timeout=None):\n            if timeout is None:\n                timeout = -1\n            elif timeout <= 0:\n                timeout = 0\n            else:\n                # epoll_wait() has a resolution of 1 millisecond, round away\n                # from zero to wait *at least* timeout seconds.\n                timeout = math.ceil(timeout * 1e3) * 1e-3\n\n            # epoll_wait() expects `maxevents` to be greater than zero;\n            # we want to make sure that `select()` can be called when no\n            # FD is registered.\n            max_ev = max(len(self._fd_to_key), 1)\n\n            ready = []\n            try:\n                fd_event_list = self._selector.poll(timeout, max_ev)\n            except InterruptedError:\n                return ready\n            for fd, event in fd_event_list:\n                events = 0\n                if event & ~select.EPOLLIN:\n                    events |= EVENT_WRITE\n                if event & ~select.EPOLLOUT:\n                    events |= EVENT_READ\n\n                key = self._key_from_fd(fd)\n                if key:\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'devpoll'):\n\n    class DevpollSelector(_PollLikeSelector):\n        \"\"\"Solaris /dev/poll selector.\"\"\"\n        _selector_cls = select.devpoll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'kqueue'):\n\n    class KqueueSelector(_BaseSelectorImpl):\n        \"\"\"Kqueue-based selector.\"\"\"\n\n        def __init__(self):\n            super().__init__()\n            self._selector = select.kqueue()\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def register(self, fileobj, events, data=None):\n            key = super().register(fileobj, events, data)\n            try:\n                if events & EVENT_READ:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n                if events & EVENT_WRITE:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n            except:\n                super().unregister(fileobj)\n                raise\n            return key\n\n        def unregister(self, fileobj):\n            key = super().unregister(fileobj)\n            if key.events & EVENT_READ:\n                kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                    select.KQ_EV_DELETE)\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # This can happen if the FD was closed since it\n                    # was registered.\n                    pass\n            if key.events & EVENT_WRITE:\n                kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                    select.KQ_EV_DELETE)\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # See comment above.\n                    pass\n            return key\n\n        def select(self, timeout=None):\n            timeout = None if timeout is None else max(timeout, 0)\n            max_ev = len(self._fd_to_key)\n            ready = []\n            try:\n                kev_list = self._selector.control(None, max_ev, timeout)\n            except InterruptedError:\n                return ready\n            for kev in kev_list:\n                fd = kev.ident\n                flag = kev.filter\n                events = 0\n                if flag == select.KQ_FILTER_READ:\n                    events |= EVENT_READ\n                if flag == select.KQ_FILTER_WRITE:\n                    events |= EVENT_WRITE\n\n                key = self._key_from_fd(fd)\n                if key:\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\n# Choose the best implementation, roughly:\n#    epoll|kqueue|devpoll > poll > select.\n# select() also can't accept a FD > FD_SETSIZE (usually around 1024)\nif 'KqueueSelector' in globals():\n    DefaultSelector = KqueueSelector\nelif 'EpollSelector' in globals():\n    DefaultSelector = EpollSelector\nelif 'DevpollSelector' in globals():\n    DefaultSelector = DevpollSelector\nelif 'PollSelector' in globals():\n    DefaultSelector = PollSelector\nelse:\n    DefaultSelector = SelectSelector\n", 592], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py": ["\"\"\"Event loop using a selector and related classes.\n\nA selector is a \"notify-when-ready\" multiplexer.  For a subclass which\nalso includes support for signal handling, see the unix_events sub-module.\n\"\"\"\n\n__all__ = 'BaseSelectorEventLoop',\n\nimport collections\nimport errno\nimport functools\nimport selectors\nimport socket\nimport warnings\nimport weakref\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import base_events\nfrom . import constants\nfrom . import events\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import transports\nfrom .log import logger\n\n\ndef _test_selector_event(selector, fd, event):\n    # Test if the selector is monitoring 'event' events\n    # for the file descriptor 'fd'.\n    try:\n        key = selector.get_key(fd)\n    except KeyError:\n        return False\n    else:\n        return bool(key.events & event)\n\n\ndef _check_ssl_socket(sock):\n    if ssl is not None and isinstance(sock, ssl.SSLSocket):\n        raise TypeError(\"Socket cannot be of type SSLSocket\")\n\n\nclass BaseSelectorEventLoop(base_events.BaseEventLoop):\n    \"\"\"Selector event loop.\n\n    See events.EventLoop for API specification.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__()\n\n        if selector is None:\n            selector = selectors.DefaultSelector()\n        logger.debug('Using selector: %s', selector.__class__.__name__)\n        self._selector = selector\n        self._make_self_pipe()\n        self._transports = weakref.WeakValueDictionary()\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        return _SelectorSocketTransport(self, sock, protocol, waiter,\n                                        extra, server)\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        ssl_protocol = sslproto.SSLProtocol(\n                self, protocol, sslcontext, waiter,\n                server_side, server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout)\n        _SelectorSocketTransport(self, rawsock, ssl_protocol,\n                                 extra=extra, server=server)\n        return ssl_protocol._app_transport\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        return _SelectorDatagramTransport(self, sock, protocol,\n                                          address, waiter, extra)\n\n    def close(self):\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self.is_closed():\n            return\n        self._close_self_pipe()\n        super().close()\n        if self._selector is not None:\n            self._selector.close()\n            self._selector = None\n\n    def _close_self_pipe(self):\n        self._remove_reader(self._ssock.fileno())\n        self._ssock.close()\n        self._ssock = None\n        self._csock.close()\n        self._csock = None\n        self._internal_fds -= 1\n\n    def _make_self_pipe(self):\n        # A self-socket, really. :-)\n        self._ssock, self._csock = socket.socketpair()\n        self._ssock.setblocking(False)\n        self._csock.setblocking(False)\n        self._internal_fds += 1\n        self._add_reader(self._ssock.fileno(), self._read_from_self)\n\n    def _process_self_data(self, data):\n        pass\n\n    def _read_from_self(self):\n        while True:\n            try:\n                data = self._ssock.recv(4096)\n                if not data:\n                    break\n                self._process_self_data(data)\n            except InterruptedError:\n                continue\n            except BlockingIOError:\n                break\n\n    def _write_to_self(self):\n        # This may be called from a different thread, possibly after\n        # _close_self_pipe() has been called or even while it is\n        # running.  Guard for self._csock being None or closed.  When\n        # a socket is closed, send() raises OSError (with errno set to\n        # EBADF, but let's not rely on the exact error code).\n        csock = self._csock\n        if csock is not None:\n            try:\n                csock.send(b'\\0')\n            except OSError:\n                if self._debug:\n                    logger.debug(\"Fail to write a null byte into the \"\n                                 \"self-pipe socket\",\n                                 exc_info=True)\n\n    def _start_serving(self, protocol_factory, sock,\n                       sslcontext=None, server=None, backlog=100,\n                       ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        self._add_reader(sock.fileno(), self._accept_connection,\n                         protocol_factory, sock, sslcontext, server, backlog,\n                         ssl_handshake_timeout)\n\n    def _accept_connection(\n            self, protocol_factory, sock,\n            sslcontext=None, server=None, backlog=100,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        # This method is only called once for each event loop tick where the\n        # listening socket has triggered an EVENT_READ. There may be multiple\n        # connections waiting for an .accept() so it is called in a loop.\n        # See https://bugs.python.org/issue27906 for more details.\n        for _ in range(backlog):\n            try:\n                conn, addr = sock.accept()\n                if self._debug:\n                    logger.debug(\"%r got a new connection from %r: %r\",\n                                 server, addr, conn)\n                conn.setblocking(False)\n            except (BlockingIOError, InterruptedError, ConnectionAbortedError):\n                # Early exit because the socket accept buffer is empty.\n                return None\n            except OSError as exc:\n                # There's nowhere to send the error, so just log it.\n                if exc.errno in (errno.EMFILE, errno.ENFILE,\n                                 errno.ENOBUFS, errno.ENOMEM):\n                    # Some platforms (e.g. Linux keep reporting the FD as\n                    # ready, so we remove the read handler temporarily.\n                    # We'll try again in a while.\n                    self.call_exception_handler({\n                        'message': 'socket.accept() out of system resource',\n                        'exception': exc,\n                        'socket': sock,\n                    })\n                    self._remove_reader(sock.fileno())\n                    self.call_later(constants.ACCEPT_RETRY_DELAY,\n                                    self._start_serving,\n                                    protocol_factory, sock, sslcontext, server,\n                                    backlog, ssl_handshake_timeout)\n                else:\n                    raise  # The event loop will catch, log and ignore it.\n            else:\n                extra = {'peername': addr}\n                accept = self._accept_connection2(\n                    protocol_factory, conn, extra, sslcontext, server,\n                    ssl_handshake_timeout)\n                self.create_task(accept)\n\n    async def _accept_connection2(\n            self, protocol_factory, conn, extra,\n            sslcontext=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        protocol = None\n        transport = None\n        try:\n            protocol = protocol_factory()\n            waiter = self.create_future()\n            if sslcontext:\n                transport = self._make_ssl_transport(\n                    conn, protocol, sslcontext, waiter=waiter,\n                    server_side=True, extra=extra, server=server,\n                    ssl_handshake_timeout=ssl_handshake_timeout)\n            else:\n                transport = self._make_socket_transport(\n                    conn, protocol, waiter=waiter, extra=extra,\n                    server=server)\n\n            try:\n                await waiter\n            except:\n                transport.close()\n                raise\n\n            # It's now up to the protocol to handle the connection.\n        except Exception as exc:\n            if self._debug:\n                context = {\n                    'message':\n                        'Error on transport creation for incoming connection',\n                    'exception': exc,\n                }\n                if protocol is not None:\n                    context['protocol'] = protocol\n                if transport is not None:\n                    context['transport'] = transport\n                self.call_exception_handler(context)\n\n    def _ensure_fd_no_transport(self, fd):\n        fileno = fd\n        if not isinstance(fileno, int):\n            try:\n                fileno = int(fileno.fileno())\n            except (AttributeError, TypeError, ValueError):\n                # This code matches selectors._fileobj_to_fd function.\n                raise ValueError(f\"Invalid file object: {fd!r}\") from None\n        try:\n            transport = self._transports[fileno]\n        except KeyError:\n            pass\n        else:\n            if not transport.is_closing():\n                raise RuntimeError(\n                    f'File descriptor {fd!r} is used by transport '\n                    f'{transport!r}')\n\n    def _add_reader(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            self._selector.register(fd, selectors.EVENT_READ,\n                                    (handle, None))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_READ,\n                                  (handle, writer))\n            if reader is not None:\n                reader.cancel()\n\n    def _remove_reader(self, fd):\n        if self.is_closed():\n            return False\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            return False\n        else:\n            mask, (reader, writer) = key.events, key.data\n            mask &= ~selectors.EVENT_READ\n            if not mask:\n                self._selector.unregister(fd)\n            else:\n                self._selector.modify(fd, mask, (None, writer))\n\n            if reader is not None:\n                reader.cancel()\n                return True\n            else:\n                return False\n\n    def _add_writer(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            self._selector.register(fd, selectors.EVENT_WRITE,\n                                    (None, handle))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_WRITE,\n                                  (reader, handle))\n            if writer is not None:\n                writer.cancel()\n\n    def _remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        if self.is_closed():\n            return False\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            return False\n        else:\n            mask, (reader, writer) = key.events, key.data\n            # Remove both writer and connector.\n            mask &= ~selectors.EVENT_WRITE\n            if not mask:\n                self._selector.unregister(fd)\n            else:\n                self._selector.modify(fd, mask, (reader, None))\n\n            if writer is not None:\n                writer.cancel()\n                return True\n            else:\n                return False\n\n    def add_reader(self, fd, callback, *args):\n        \"\"\"Add a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._add_reader(fd, callback, *args)\n\n    def remove_reader(self, fd):\n        \"\"\"Remove a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_reader(fd)\n\n    def add_writer(self, fd, callback, *args):\n        \"\"\"Add a writer callback..\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._add_writer(fd, callback, *args)\n\n    def remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_writer(fd)\n\n    async def sock_recv(self, sock, n):\n        \"\"\"Receive data from the socket.\n\n        The return value is a bytes object representing the data received.\n        The maximum amount of data to be received at once is specified by\n        nbytes.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        fut = self.create_future()\n        self._sock_recv(fut, None, sock, n)\n        return await fut\n\n    def _sock_recv(self, fut, registered_fd, sock, n):\n        # _sock_recv() can add itself as an I/O callback if the operation can't\n        # be done immediately. Don't use it directly, call sock_recv().\n        if registered_fd is not None:\n            # Remove the callback early.  It should be rare that the\n            # selector says the fd is ready but the call still returns\n            # EAGAIN, and I am willing to take a hit in that case in\n            # order to simplify the common case.\n            self.remove_reader(registered_fd)\n        if fut.cancelled():\n            return\n        try:\n            data = sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            fd = sock.fileno()\n            self.add_reader(fd, self._sock_recv, fut, fd, sock, n)\n        except Exception as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(data)\n\n    async def sock_recv_into(self, sock, buf):\n        \"\"\"Receive data from the socket.\n\n        The received data is written into *buf* (a writable buffer).\n        The return value is the number of bytes written.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        fut = self.create_future()\n        self._sock_recv_into(fut, None, sock, buf)\n        return await fut\n\n    def _sock_recv_into(self, fut, registered_fd, sock, buf):\n        # _sock_recv_into() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recv_into().\n        if registered_fd is not None:\n            # Remove the callback early.  It should be rare that the\n            # selector says the FD is ready but the call still returns\n            # EAGAIN, and I am willing to take a hit in that case in\n            # order to simplify the common case.\n            self.remove_reader(registered_fd)\n        if fut.cancelled():\n            return\n        try:\n            nbytes = sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            fd = sock.fileno()\n            self.add_reader(fd, self._sock_recv_into, fut, fd, sock, buf)\n        except Exception as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(nbytes)\n\n    async def sock_sendall(self, sock, data):\n        \"\"\"Send data to the socket.\n\n        The socket must be connected to a remote socket. This method continues\n        to send data from data until either all data has been sent or an\n        error occurs. None is returned on success. On error, an exception is\n        raised, and there is no way to determine how much data, if any, was\n        successfully processed by the receiving end of the connection.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        fut = self.create_future()\n        if data:\n            self._sock_sendall(fut, None, sock, data)\n        else:\n            fut.set_result(None)\n        return await fut\n\n    def _sock_sendall(self, fut, registered_fd, sock, data):\n        if registered_fd is not None:\n            self.remove_writer(registered_fd)\n        if fut.cancelled():\n            return\n\n        try:\n            n = sock.send(data)\n        except (BlockingIOError, InterruptedError):\n            n = 0\n        except Exception as exc:\n            fut.set_exception(exc)\n            return\n\n        if n == len(data):\n            fut.set_result(None)\n        else:\n            if n:\n                data = data[n:]\n            fd = sock.fileno()\n            self.add_writer(fd, self._sock_sendall, fut, fd, sock, data)\n\n    async def sock_connect(self, sock, address):\n        \"\"\"Connect to a remote socket at address.\n\n        This method is a coroutine.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n\n        if not hasattr(socket, 'AF_UNIX') or sock.family != socket.AF_UNIX:\n            resolved = await self._ensure_resolved(\n                address, family=sock.family, proto=sock.proto, loop=self)\n            _, _, _, _, address = resolved[0]\n\n        fut = self.create_future()\n        self._sock_connect(fut, sock, address)\n        return await fut\n\n    def _sock_connect(self, fut, sock, address):\n        fd = sock.fileno()\n        try:\n            sock.connect(address)\n        except (BlockingIOError, InterruptedError):\n            # Issue #23618: When the C function connect() fails with EINTR, the\n            # connection runs in background. We have to wait until the socket\n            # becomes writable to be notified when the connection succeed or\n            # fails.\n            fut.add_done_callback(\n                functools.partial(self._sock_connect_done, fd))\n            self.add_writer(fd, self._sock_connect_cb, fut, sock, address)\n        except Exception as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n\n    def _sock_connect_done(self, fd, fut):\n        self.remove_writer(fd)\n\n    def _sock_connect_cb(self, fut, sock, address):\n        if fut.cancelled():\n            return\n\n        try:\n            err = sock.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)\n            if err != 0:\n                # Jump to any except clause below.\n                raise OSError(err, f'Connect call failed {address}')\n        except (BlockingIOError, InterruptedError):\n            # socket is still registered, the callback will be retried later\n            pass\n        except Exception as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n\n    async def sock_accept(self, sock):\n        \"\"\"Accept a connection.\n\n        The socket must be bound to an address and listening for connections.\n        The return value is a pair (conn, address) where conn is a new socket\n        object usable to send and receive data on the connection, and address\n        is the address bound to the socket on the other end of the connection.\n        \"\"\"\n        _check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        fut = self.create_future()\n        self._sock_accept(fut, False, sock)\n        return await fut\n\n    def _sock_accept(self, fut, registered, sock):\n        fd = sock.fileno()\n        if registered:\n            self.remove_reader(fd)\n        if fut.cancelled():\n            return\n        try:\n            conn, address = sock.accept()\n            conn.setblocking(False)\n        except (BlockingIOError, InterruptedError):\n            self.add_reader(fd, self._sock_accept, fut, True, sock)\n        except Exception as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result((conn, address))\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        del self._transports[transp._sock_fd]\n        resume_reading = transp.is_reading()\n        transp.pause_reading()\n        await transp._make_empty_waiter()\n        try:\n            return await self.sock_sendfile(transp._sock, file, offset, count,\n                                            fallback=False)\n        finally:\n            transp._reset_empty_waiter()\n            if resume_reading:\n                transp.resume_reading()\n            self._transports[transp._sock_fd] = transp\n\n    def _process_events(self, event_list):\n        for key, mask in event_list:\n            fileobj, (reader, writer) = key.fileobj, key.data\n            if mask & selectors.EVENT_READ and reader is not None:\n                if reader._cancelled:\n                    self._remove_reader(fileobj)\n                else:\n                    self._add_callback(reader)\n            if mask & selectors.EVENT_WRITE and writer is not None:\n                if writer._cancelled:\n                    self._remove_writer(fileobj)\n                else:\n                    self._add_callback(writer)\n\n    def _stop_serving(self, sock):\n        self._remove_reader(sock.fileno())\n        sock.close()\n\n\nclass _SelectorTransport(transports._FlowControlMixin,\n                         transports.Transport):\n\n    max_size = 256 * 1024  # Buffer size passed to recv().\n\n    _buffer_factory = bytearray  # Constructs initial value for self._buffer.\n\n    # Attribute used in the destructor: it must be set even if the constructor\n    # is not called (see _SelectorSslTransport which may start by raising an\n    # exception)\n    _sock = None\n\n    def __init__(self, loop, sock, protocol, extra=None, server=None):\n        super().__init__(extra, loop)\n        self._extra['socket'] = sock\n        try:\n            self._extra['sockname'] = sock.getsockname()\n        except OSError:\n            self._extra['sockname'] = None\n        if 'peername' not in self._extra:\n            try:\n                self._extra['peername'] = sock.getpeername()\n            except socket.error:\n                self._extra['peername'] = None\n        self._sock = sock\n        self._sock_fd = sock.fileno()\n\n        self._protocol_connected = False\n        self.set_protocol(protocol)\n\n        self._server = server\n        self._buffer = self._buffer_factory()\n        self._conn_lost = 0  # Set when call to connection_lost scheduled.\n        self._closing = False  # Set when close() called.\n        if self._server is not None:\n            self._server._attach()\n        loop._transports[self._sock_fd] = self\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._sock is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._sock_fd}')\n        # test if the transport was closed\n        if self._loop is not None and not self._loop.is_closed():\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd, selectors.EVENT_READ)\n            if polling:\n                info.append('read=polling')\n            else:\n                info.append('read=idle')\n\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd,\n                                           selectors.EVENT_WRITE)\n            if polling:\n                state = 'polling'\n            else:\n                state = 'idle'\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'write=<{state}, bufsize={bufsize}>')\n        return '<{}>'.format(' '.join(info))\n\n    def abort(self):\n        self._force_close(None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n        self._protocol_connected = True\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if self._closing:\n            return\n        self._closing = True\n        self._loop._remove_reader(self._sock_fd)\n        if not self._buffer:\n            self._conn_lost += 1\n            self._loop._remove_writer(self._sock_fd)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def __del__(self):\n        if self._sock is not None:\n            warnings.warn(f\"unclosed transport {self!r}\", ResourceWarning,\n                          source=self)\n            self._sock.close()\n\n    def _fatal_error(self, exc, message='Fatal error on transport'):\n        # Should be called from exception handler only.\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._force_close(exc)\n\n    def _force_close(self, exc):\n        if self._conn_lost:\n            return\n        if self._buffer:\n            self._buffer.clear()\n            self._loop._remove_writer(self._sock_fd)\n        if not self._closing:\n            self._closing = True\n            self._loop._remove_reader(self._sock_fd)\n        self._conn_lost += 1\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            if self._protocol_connected:\n                self._protocol.connection_lost(exc)\n        finally:\n            self._sock.close()\n            self._sock = None\n            self._protocol = None\n            self._loop = None\n            server = self._server\n            if server is not None:\n                server._detach()\n                self._server = None\n\n    def get_write_buffer_size(self):\n        return len(self._buffer)\n\n    def _add_reader(self, fd, callback, *args):\n        if self._closing:\n            return\n\n        self._loop._add_reader(fd, callback, *args)\n\n\nclass _SelectorSocketTransport(_SelectorTransport):\n\n    _start_tls_compatible = True\n    _sendfile_compatible = constants._SendfileMode.TRY_NATIVE\n\n    def __init__(self, loop, sock, protocol, waiter=None,\n                 extra=None, server=None):\n\n        self._read_ready_cb = None\n        super().__init__(loop, sock, protocol, extra, server)\n        self._eof = False\n        self._paused = False\n        self._empty_waiter = None\n\n        # Disable the Nagle algorithm -- small writes will be\n        # sent without waiting for the TCP ACK.  This generally\n        # decreases the latency (in some cases significantly.)\n        base_events._set_nodelay(self._sock)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def set_protocol(self, protocol):\n        if isinstance(protocol, protocols.BufferedProtocol):\n            self._read_ready_cb = self._read_ready__get_buffer\n        else:\n            self._read_ready_cb = self._read_ready__data_received\n\n        super().set_protocol(protocol)\n\n    def is_reading(self):\n        return not self._paused and not self._closing\n\n    def pause_reading(self):\n        if self._closing or self._paused:\n            return\n        self._paused = True\n        self._loop._remove_reader(self._sock_fd)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._add_reader(self._sock_fd, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def _read_ready(self):\n        self._read_ready_cb()\n\n    def _read_ready__get_buffer(self):\n        if self._conn_lost:\n            return\n\n        try:\n            buf = self._protocol.get_buffer(-1)\n            if not len(buf):\n                raise RuntimeError('get_buffer() returned an empty buffer')\n        except Exception as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.get_buffer() call failed.')\n            return\n\n        try:\n            nbytes = self._sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return\n        except Exception as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not nbytes:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.buffer_updated(nbytes)\n        except Exception as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.buffer_updated() call failed.')\n\n    def _read_ready__data_received(self):\n        if self._conn_lost:\n            return\n        try:\n            data = self._sock.recv(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            return\n        except Exception as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not data:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.data_received(data)\n        except Exception as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.data_received() call failed.')\n\n    def _read_ready__on_eof(self):\n        if self._loop.get_debug():\n            logger.debug(\"%r received EOF\", self)\n\n        try:\n            keep_open = self._protocol.eof_received()\n        except Exception as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.eof_received() call failed.')\n            return\n\n        if keep_open:\n            # We're keeping the connection open so the\n            # protocol can write more, but we still can't\n            # receive more, so remove the reader callback.\n            self._loop._remove_reader(self._sock_fd)\n        else:\n            self.close()\n\n    def write(self, data):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if self._eof:\n            raise RuntimeError('Cannot call write() after write_eof()')\n        if self._empty_waiter is not None:\n            raise RuntimeError('unable to write; sendfile is in progress')\n        if not data:\n            return\n\n        if self._conn_lost:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Optimization: try to send now.\n            try:\n                n = self._sock.send(data)\n            except (BlockingIOError, InterruptedError):\n                pass\n            except Exception as exc:\n                self._fatal_error(exc, 'Fatal write error on socket transport')\n                return\n            else:\n                data = data[n:]\n                if not data:\n                    return\n            # Not all was written; register write handler.\n            self._loop._add_writer(self._sock_fd, self._write_ready)\n\n        # Add it to the buffer.\n        self._buffer.extend(data)\n        self._maybe_pause_protocol()\n\n    def _write_ready(self):\n        assert self._buffer, 'Data should not be empty'\n\n        if self._conn_lost:\n            return\n        try:\n            n = self._sock.send(self._buffer)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except Exception as exc:\n            self._loop._remove_writer(self._sock_fd)\n            self._buffer.clear()\n            self._fatal_error(exc, 'Fatal write error on socket transport')\n            if self._empty_waiter is not None:\n                self._empty_waiter.set_exception(exc)\n        else:\n            if n:\n                del self._buffer[:n]\n            self._maybe_resume_protocol()  # May append to buffer.\n            if not self._buffer:\n                self._loop._remove_writer(self._sock_fd)\n                if self._empty_waiter is not None:\n                    self._empty_waiter.set_result(None)\n                if self._closing:\n                    self._call_connection_lost(None)\n                elif self._eof:\n                    self._sock.shutdown(socket.SHUT_WR)\n\n    def write_eof(self):\n        if self._closing or self._eof:\n            return\n        self._eof = True\n        if not self._buffer:\n            self._sock.shutdown(socket.SHUT_WR)\n\n    def can_write_eof(self):\n        return True\n\n    def _call_connection_lost(self, exc):\n        super()._call_connection_lost(exc)\n        if self._empty_waiter is not None:\n            self._empty_waiter.set_exception(\n                ConnectionError(\"Connection is closed by peer\"))\n\n    def _make_empty_waiter(self):\n        if self._empty_waiter is not None:\n            raise RuntimeError(\"Empty waiter is already set\")\n        self._empty_waiter = self._loop.create_future()\n        if not self._buffer:\n            self._empty_waiter.set_result(None)\n        return self._empty_waiter\n\n    def _reset_empty_waiter(self):\n        self._empty_waiter = None\n\n\nclass _SelectorDatagramTransport(_SelectorTransport):\n\n    _buffer_factory = collections.deque\n\n    def __init__(self, loop, sock, protocol, address=None,\n                 waiter=None, extra=None):\n        super().__init__(loop, sock, protocol, extra)\n        self._address = address\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def get_write_buffer_size(self):\n        return sum(len(data) for data, _ in self._buffer)\n\n    def _read_ready(self):\n        if self._conn_lost:\n            return\n        try:\n            data, addr = self._sock.recvfrom(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._protocol.error_received(exc)\n        except Exception as exc:\n            self._fatal_error(exc, 'Fatal read error on datagram transport')\n        else:\n            self._protocol.datagram_received(data, addr)\n\n    def sendto(self, data, addr=None):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if not data:\n            return\n\n        if self._address:\n            if addr not in (None, self._address):\n                raise ValueError(\n                    f'Invalid address: must be None or {self._address}')\n            addr = self._address\n\n        if self._conn_lost and self._address:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n                return\n            except (BlockingIOError, InterruptedError):\n                self._loop._add_writer(self._sock_fd, self._sendto_ready)\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except Exception as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        # Ensure that what we buffer is immutable.\n        self._buffer.append((bytes(data), addr))\n        self._maybe_pause_protocol()\n\n    def _sendto_ready(self):\n        while self._buffer:\n            data, addr = self._buffer.popleft()\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n            except (BlockingIOError, InterruptedError):\n                self._buffer.appendleft((data, addr))  # Try again later.\n                break\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except Exception as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        self._maybe_resume_protocol()  # May append to buffer.\n        if not self._buffer:\n            self._loop._remove_writer(self._sock_fd)\n            if self._closing:\n                self._call_connection_lost(None)\n", 1041], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py": ["import enum\nimport sys\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Generic, Set, TypeVar, Union, overload\nfrom weakref import WeakKeyDictionary\n\nfrom ._core._eventloop import get_asynclib\n\nif sys.version_info >= (3, 8):\n    from typing import Literal\nelse:\n    from typing_extensions import Literal\n\nT = TypeVar(\"T\")\nD = TypeVar(\"D\")\n\n\nasync def checkpoint() -> None:\n    \"\"\"\n    Check for cancellation and allow the scheduler to switch to another task.\n\n    Equivalent to (but more efficient than)::\n\n        await checkpoint_if_cancelled()\n        await cancel_shielded_checkpoint()\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_asynclib().checkpoint()\n\n\nasync def checkpoint_if_cancelled() -> None:\n    \"\"\"\n    Enter a checkpoint if the enclosing cancel scope has been cancelled.\n\n    This does not allow the scheduler to switch to a different task.\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_asynclib().checkpoint_if_cancelled()\n\n\nasync def cancel_shielded_checkpoint() -> None:\n    \"\"\"\n    Allow the scheduler to switch to another task but without checking for cancellation.\n\n    Equivalent to (but potentially more efficient than)::\n\n        with CancelScope(shield=True):\n            await checkpoint()\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_asynclib().cancel_shielded_checkpoint()\n\n\ndef current_token() -> object:\n    \"\"\"Return a backend specific token object that can be used to get back to the event loop.\"\"\"\n    return get_asynclib().current_token()\n\n\n_run_vars = WeakKeyDictionary()  # type: WeakKeyDictionary[Any, Dict[str, Any]]\n_token_wrappers: Dict[Any, \"_TokenWrapper\"] = {}\n\n\n@dataclass(frozen=True)\nclass _TokenWrapper:\n    __slots__ = \"_token\", \"__weakref__\"\n    _token: object\n\n\nclass _NoValueSet(enum.Enum):\n    NO_VALUE_SET = enum.auto()\n\n\nclass RunvarToken(Generic[T]):\n    __slots__ = \"_var\", \"_value\", \"_redeemed\"\n\n    def __init__(\n        self, var: \"RunVar[T]\", value: Union[T, Literal[_NoValueSet.NO_VALUE_SET]]\n    ):\n        self._var = var\n        self._value: Union[T, Literal[_NoValueSet.NO_VALUE_SET]] = value\n        self._redeemed = False\n\n\nclass RunVar(Generic[T]):\n    \"\"\"Like a :class:`~contextvars.ContextVar`, expect scoped to the running event loop.\"\"\"\n\n    __slots__ = \"_name\", \"_default\"\n\n    NO_VALUE_SET: Literal[_NoValueSet.NO_VALUE_SET] = _NoValueSet.NO_VALUE_SET\n\n    _token_wrappers: Set[_TokenWrapper] = set()\n\n    def __init__(\n        self,\n        name: str,\n        default: Union[T, Literal[_NoValueSet.NO_VALUE_SET]] = NO_VALUE_SET,\n    ):\n        self._name = name\n        self._default = default\n\n    @property\n    def _current_vars(self) -> Dict[str, T]:\n        token = current_token()\n        while True:\n            try:\n                return _run_vars[token]\n            except TypeError:\n                # Happens when token isn't weak referable (TrioToken).\n                # This workaround does mean that some memory will leak on Trio until the problem\n                # is fixed on their end.\n                token = _TokenWrapper(token)\n                self._token_wrappers.add(token)\n            except KeyError:\n                run_vars = _run_vars[token] = {}\n                return run_vars\n\n    @overload\n    def get(self, default: D) -> Union[T, D]:\n        ...\n\n    @overload\n    def get(self) -> T:\n        ...\n\n    def get(\n        self, default: Union[D, Literal[_NoValueSet.NO_VALUE_SET]] = NO_VALUE_SET\n    ) -> Union[T, D]:\n        try:\n            return self._current_vars[self._name]\n        except KeyError:\n            if default is not RunVar.NO_VALUE_SET:\n                return default\n            elif self._default is not RunVar.NO_VALUE_SET:\n                return self._default\n\n        raise LookupError(\n            f'Run variable \"{self._name}\" has no value and no default set'\n        )\n\n    def set(self, value: T) -> RunvarToken[T]:\n        current_vars = self._current_vars\n        token = RunvarToken(self, current_vars.get(self._name, RunVar.NO_VALUE_SET))\n        current_vars[self._name] = value\n        return token\n\n    def reset(self, token: RunvarToken[T]) -> None:\n        if token._var is not self:\n            raise ValueError(\"This token does not belong to this RunVar\")\n\n        if token._redeemed:\n            raise ValueError(\"This token has already been used\")\n\n        if token._value is _NoValueSet.NO_VALUE_SET:\n            try:\n                del self._current_vars[self._name]\n            except KeyError:\n                pass\n        else:\n            self._current_vars[self._name] = token._value\n\n        token._redeemed = True\n\n    def __repr__(self) -> str:\n        return f\"<RunVar name={self._name!r}>\"\n", 170], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py": ["\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttp://www.python.org/dev/peps/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType,\n     _remove_dead_weakref)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nimport _collections_abc  # Import after _weakref to avoid circular import.\nimport sys\nimport itertools\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\",\n           \"WeakSet\", \"WeakMethod\", \"finalize\"]\n\n\nclass WeakMethod(ref):\n    \"\"\"\n    A custom `weakref.ref` subclass which simulates a weak reference to\n    a bound method, working around the lifetime problem of bound methods.\n    \"\"\"\n\n    __slots__ = \"_func_ref\", \"_meth_type\", \"_alive\", \"__weakref__\"\n\n    def __new__(cls, meth, callback=None):\n        try:\n            obj = meth.__self__\n            func = meth.__func__\n        except AttributeError:\n            raise TypeError(\"argument should be a bound method, not {}\"\n                            .format(type(meth))) from None\n        def _cb(arg):\n            # The self-weakref trick is needed to avoid creating a reference\n            # cycle.\n            self = self_wr()\n            if self._alive:\n                self._alive = False\n                if callback is not None:\n                    callback(self)\n        self = ref.__new__(cls, obj, _cb)\n        self._func_ref = ref(func, _cb)\n        self._meth_type = type(meth)\n        self._alive = True\n        self_wr = ref(self)\n        return self\n\n    def __call__(self):\n        obj = super().__call__()\n        func = self._func_ref()\n        if obj is None or func is None:\n            return None\n        return self._meth_type(func, obj)\n\n    def __eq__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is other\n            return ref.__eq__(self, other) and self._func_ref == other._func_ref\n        return False\n\n    def __ne__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is not other\n            return ref.__ne__(self, other) or self._func_ref != other._func_ref\n        return True\n\n    __hash__ = ref.__hash__\n\n\nclass WeakValueDictionary(_collections_abc.MutableMapping):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(*args, **kw):\n        if not args:\n            raise TypeError(\"descriptor '__init__' of 'WeakValueDictionary' \"\n                            \"object needs an argument\")\n        self, *args = args\n        if len(args) > 1:\n            raise TypeError('expected at most 1 arguments, got %d' % len(args))\n        def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Atomic removal is necessary since this function\n                    # can be called asynchronously by the GC\n                    _atomic_removal(self.data, wr.key)\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n        self.update(*args, **kw)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while l:\n            key = l.pop()\n            _remove_dead_weakref(d, key)\n\n    def __getitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        o = self.data[key]()\n        if o is None:\n            raise KeyError(key)\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __len__(self):\n        if self._pending_removals:\n            self._commit_removals()\n        return len(self.data)\n\n    def __contains__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def copy(self):\n        if self._pending_removals:\n            self._commit_removals()\n        new = WeakValueDictionary()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        if self._pending_removals:\n            self._commit_removals()\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                v = wr()\n                if v is not None:\n                    yield k, v\n\n    def keys(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                if wr() is not None:\n                    yield k\n\n    __iter__ = keys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            yield from self.data.values()\n\n    def values(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for wr in self.data.values():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            else:\n                raise KeyError(key)\n        else:\n            return o\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n\n    def update(*args, **kwargs):\n        if not args:\n            raise TypeError(\"descriptor 'update' of 'WeakValueDictionary' \"\n                            \"object needs an argument\")\n        self, *args = args\n        if len(args) > 1:\n            raise TypeError('expected at most 1 arguments, got %d' % len(args))\n        dict = args[0] if args else None\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, o in dict.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        if len(kwargs):\n            self.update(kwargs)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        return list(self.data.values())\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super().__init__(ob, callback)\n\n\nclass WeakKeyDictionary(_collections_abc.MutableMapping):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    del self.data[k]\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        self._dirty_len = False\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        l = self._pending_removals\n        d = self.data\n        while l:\n            try:\n                del d[l.pop()]\n            except KeyError:\n                pass\n\n    def _scrub_removals(self):\n        d = self.data\n        self._pending_removals = [k for k in self._pending_removals if k in d]\n        self._dirty_len = False\n\n    def __delitem__(self, key):\n        self._dirty_len = True\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __len__(self):\n        if self._dirty_len and self._pending_removals:\n            # self._pending_removals may still contain keys which were\n            # explicitly removed, we have to scrub them (see issue #21173).\n            self._scrub_removals()\n        return len(self.data) - len(self._pending_removals)\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def items(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def keys(self):\n        with _IterationGuard(self):\n            for wr in self.data:\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = keys\n\n    def values(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                if wr() is not None:\n                    yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return list(self.data)\n\n    def popitem(self):\n        self._dirty_len = True\n        while True:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        self._dirty_len = True\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n\n\nclass finalize:\n    \"\"\"Class for finalization of weakrefable objects\n\n    finalize(obj, func, *args, **kwargs) returns a callable finalizer\n    object which will be called when obj is garbage collected. The\n    first time the finalizer is called it evaluates func(*arg, **kwargs)\n    and returns the result. After this the finalizer is dead, and\n    calling it just returns None.\n\n    When the program exits any remaining finalizers for which the\n    atexit attribute is true will be run in reverse order of creation.\n    By default atexit is true.\n    \"\"\"\n\n    # Finalizer objects don't have any state of their own.  They are\n    # just used as keys to lookup _Info objects in the registry.  This\n    # ensures that they cannot be part of a ref-cycle.\n\n    __slots__ = ()\n    _registry = {}\n    _shutdown = False\n    _index_iter = itertools.count()\n    _dirty = False\n    _registered_with_atexit = False\n\n    class _Info:\n        __slots__ = (\"weakref\", \"func\", \"args\", \"kwargs\", \"atexit\", \"index\")\n\n    def __init__(*args, **kwargs):\n        if len(args) >= 3:\n            self, obj, func, *args = args\n        elif not args:\n            raise TypeError(\"descriptor '__init__' of 'finalize' object \"\n                            \"needs an argument\")\n        else:\n            if 'func' not in kwargs:\n                raise TypeError('finalize expected at least 2 positional '\n                                'arguments, got %d' % (len(args)-1))\n            func = kwargs.pop('func')\n            if len(args) >= 2:\n                self, obj, *args = args\n            else:\n                if 'obj' not in kwargs:\n                    raise TypeError('finalize expected at least 2 positional '\n                                    'arguments, got %d' % (len(args)-1))\n                obj = kwargs.pop('obj')\n                self, *args = args\n        args = tuple(args)\n\n        if not self._registered_with_atexit:\n            # We may register the exit function more than once because\n            # of a thread race, but that is harmless\n            import atexit\n            atexit.register(self._exitfunc)\n            finalize._registered_with_atexit = True\n        info = self._Info()\n        info.weakref = ref(obj, self)\n        info.func = func\n        info.args = args\n        info.kwargs = kwargs or None\n        info.atexit = True\n        info.index = next(self._index_iter)\n        self._registry[self] = info\n        finalize._dirty = True\n\n    def __call__(self, _=None):\n        \"\"\"If alive then mark as dead and return func(*args, **kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.pop(self, None)\n        if info and not self._shutdown:\n            return info.func(*info.args, **(info.kwargs or {}))\n\n    def detach(self):\n        \"\"\"If alive then mark as dead and return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None and self._registry.pop(self, None):\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    def peek(self):\n        \"\"\"If alive then return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None:\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    @property\n    def alive(self):\n        \"\"\"Whether finalizer is alive\"\"\"\n        return self in self._registry\n\n    @property\n    def atexit(self):\n        \"\"\"Whether finalizer should be called at exit\"\"\"\n        info = self._registry.get(self)\n        return bool(info) and info.atexit\n\n    @atexit.setter\n    def atexit(self, value):\n        info = self._registry.get(self)\n        if info:\n            info.atexit = bool(value)\n\n    def __repr__(self):\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is None:\n            return '<%s object at %#x; dead>' % (type(self).__name__, id(self))\n        else:\n            return '<%s object at %#x; for %r at %#x>' % \\\n                (type(self).__name__, id(self), type(obj).__name__, id(obj))\n\n    @classmethod\n    def _select_for_exit(cls):\n        # Return live finalizers marked for exit, oldest first\n        L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]\n        L.sort(key=lambda item:item[1].index)\n        return [f for (f,i) in L]\n\n    @classmethod\n    def _exitfunc(cls):\n        # At shutdown invoke finalizers for which atexit is true.\n        # This is called once all other non-daemonic threads have been\n        # joined.\n        reenable_gc = False\n        try:\n            if cls._registry:\n                import gc\n                if gc.isenabled():\n                    reenable_gc = True\n                    gc.disable()\n                pending = None\n                while True:\n                    if pending is None or finalize._dirty:\n                        pending = cls._select_for_exit()\n                        finalize._dirty = False\n                    if not pending:\n                        break\n                    f = pending.pop()\n                    try:\n                        # gc is disabled, so (assuming no daemonic\n                        # threads) the following is the only line in\n                        # this function which might trigger creation\n                        # of a new finalizer\n                        f()\n                    except Exception:\n                        sys.excepthook(*sys.exc_info())\n                    assert f not in cls._registry\n        finally:\n            # prevent any more finalizers from executing during shutdown\n            finalize._shutdown = True\n            if reenable_gc:\n                gc.enable()\n", 656], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_compat.py": ["from abc import ABCMeta, abstractmethod\nfrom contextlib import AbstractContextManager\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncContextManager,\n    Callable,\n    ContextManager,\n    Generator,\n    Generic,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    overload,\n)\nfrom warnings import warn\n\nif TYPE_CHECKING:\n    from ._testing import TaskInfo\nelse:\n    TaskInfo = object\n\nT = TypeVar(\"T\")\nAnyDeprecatedAwaitable = Union[\n    \"DeprecatedAwaitable\",\n    \"DeprecatedAwaitableFloat\",\n    \"DeprecatedAwaitableList[T]\",\n    TaskInfo,\n]\n\n\n@overload\nasync def maybe_async(__obj: TaskInfo) -> TaskInfo:\n    ...\n\n\n@overload\nasync def maybe_async(__obj: \"DeprecatedAwaitableFloat\") -> float:\n    ...\n\n\n@overload\nasync def maybe_async(__obj: \"DeprecatedAwaitableList[T]\") -> List[T]:\n    ...\n\n\n@overload\nasync def maybe_async(__obj: \"DeprecatedAwaitable\") -> None:\n    ...\n\n\nasync def maybe_async(\n    __obj: \"AnyDeprecatedAwaitable[T]\",\n) -> Union[TaskInfo, float, List[T], None]:\n    \"\"\"\n    Await on the given object if necessary.\n\n    This function is intended to bridge the gap between AnyIO 2.x and 3.x where some functions and\n    methods were converted from coroutine functions into regular functions.\n\n    Do **not** try to use this for any other purpose!\n\n    :return: the result of awaiting on the object if coroutine, or the object itself otherwise\n\n    .. versionadded:: 2.2\n\n    \"\"\"\n    return __obj._unwrap()\n\n\nclass _ContextManagerWrapper:\n    def __init__(self, cm: ContextManager[T]):\n        self._cm = cm\n\n    async def __aenter__(self) -> T:\n        return self._cm.__enter__()\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        return self._cm.__exit__(exc_type, exc_val, exc_tb)\n\n\ndef maybe_async_cm(\n    cm: Union[ContextManager[T], AsyncContextManager[T]]\n) -> AsyncContextManager[T]:\n    \"\"\"\n    Wrap a regular context manager as an async one if necessary.\n\n    This function is intended to bridge the gap between AnyIO 2.x and 3.x where some functions and\n    methods were changed to return regular context managers instead of async ones.\n\n    :param cm: a regular or async context manager\n    :return: an async context manager\n\n    .. versionadded:: 2.2\n\n    \"\"\"\n    if not isinstance(cm, AbstractContextManager):\n        raise TypeError(\"Given object is not an context manager\")\n\n    return _ContextManagerWrapper(cm)\n\n\ndef _warn_deprecation(\n    awaitable: \"AnyDeprecatedAwaitable[Any]\", stacklevel: int = 1\n) -> None:\n    warn(\n        f'Awaiting on {awaitable._name}() is deprecated. Use \"await '\n        f\"anyio.maybe_async({awaitable._name}(...)) if you have to support both AnyIO 2.x \"\n        f'and 3.x, or just remove the \"await\" if you are completely migrating to AnyIO 3+.',\n        DeprecationWarning,\n        stacklevel=stacklevel + 1,\n    )\n\n\nclass DeprecatedAwaitable:\n    def __init__(self, func: Callable[..., \"DeprecatedAwaitable\"]):\n        self._name = f\"{func.__module__}.{func.__qualname__}\"\n\n    def __await__(self) -> Generator[None, None, None]:\n        _warn_deprecation(self)\n        if False:\n            yield\n\n    def __reduce__(self) -> Tuple[Type[None], Tuple[()]]:\n        return type(None), ()\n\n    def _unwrap(self) -> None:\n        return None\n\n\nclass DeprecatedAwaitableFloat(float):\n    def __new__(\n        cls, x: float, func: Callable[..., \"DeprecatedAwaitableFloat\"]\n    ) -> \"DeprecatedAwaitableFloat\":\n        return super().__new__(cls, x)\n\n    def __init__(self, x: float, func: Callable[..., \"DeprecatedAwaitableFloat\"]):\n        self._name = f\"{func.__module__}.{func.__qualname__}\"\n\n    def __await__(self) -> Generator[None, None, float]:\n        _warn_deprecation(self)\n        if False:\n            yield\n\n        return float(self)\n\n    def __reduce__(self) -> Tuple[Type[float], Tuple[float]]:\n        return float, (float(self),)\n\n    def _unwrap(self) -> float:\n        return float(self)\n\n\nclass DeprecatedAwaitableList(List[T]):\n    def __init__(\n        self,\n        iterable: Iterable[T] = (),\n        *,\n        func: Callable[..., \"DeprecatedAwaitableList[T]\"],\n    ):\n        super().__init__(iterable)\n        self._name = f\"{func.__module__}.{func.__qualname__}\"\n\n    def __await__(self) -> Generator[None, None, List[T]]:\n        _warn_deprecation(self)\n        if False:\n            yield\n\n        return list(self)\n\n    def __reduce__(self) -> Tuple[Type[List[T]], Tuple[List[T]]]:\n        return list, (list(self),)\n\n    def _unwrap(self) -> List[T]:\n        return list(self)\n\n\nclass DeprecatedAsyncContextManager(Generic[T], metaclass=ABCMeta):\n    @abstractmethod\n    def __enter__(self) -> T:\n        pass\n\n    @abstractmethod\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        pass\n\n    async def __aenter__(self) -> T:\n        warn(\n            f\"Using {self.__class__.__name__} as an async context manager has been deprecated. \"\n            f'Use \"async with anyio.maybe_async_cm(yourcontextmanager) as foo:\" if you have to '\n            f'support both AnyIO 2.x and 3.x, or just remove the \"async\" from \"async with\" if '\n            f\"you are completely migrating to AnyIO 3+.\",\n            DeprecationWarning,\n        )\n        return self.__enter__()\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        return self.__exit__(exc_type, exc_val, exc_tb)\n", 218], "/Users/shopbox/projects/profyle/profyle/middleware/fastapi.py": ["import cProfile\nimport pstats\n\nfrom starlette.types import ASGIApp, Scope, Receive, Send\n\nfrom profyle.database.sql_lite import store_trace\nfrom profyle.models.trace import Trace\nfrom profyle.deps.get_connection import get_connection\nfrom viztracer import VizTracer\n\nclass ProfileMiddleware:\n    def __init__(\n        self,\n        app: ASGIApp,\n        enable: bool = True,\n        sort_by: str = 'cumulative',\n    ):\n\n        self.app = app\n        self.enable = enable\n        if enable:\n            self.tracer = VizTracer()\n            self.sort_by = sort_by\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n\n        if scope['type'] != 'http' or not self.enable:\n            await self.app(scope, receive, send)\n            return\n\n        self.tracer.start()\n        await self.app(scope, receive, send)\n        self.tracer.stop()\n        self.tracer.save(output_file='result1.json')\n\n\n", 36], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/errors.py": ["import html\nimport inspect\nimport traceback\nimport typing\n\nfrom starlette._utils import is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.requests import Request\nfrom starlette.responses import HTMLResponse, PlainTextResponse, Response\nfrom starlette.types import ASGIApp, Message, Receive, Scope, Send\n\nSTYLES = \"\"\"\np {\n    color: #211c1c;\n}\n.traceback-container {\n    border: 1px solid #038BB8;\n}\n.traceback-title {\n    background-color: #038BB8;\n    color: lemonchiffon;\n    padding: 12px;\n    font-size: 20px;\n    margin-top: 0px;\n}\n.frame-line {\n    padding-left: 10px;\n    font-family: monospace;\n}\n.frame-filename {\n    font-family: monospace;\n}\n.center-line {\n    background-color: #038BB8;\n    color: #f9f6e1;\n    padding: 5px 0px 5px 5px;\n}\n.lineno {\n    margin-right: 5px;\n}\n.frame-title {\n    font-weight: unset;\n    padding: 10px 10px 10px 10px;\n    background-color: #E4F4FD;\n    margin-right: 10px;\n    color: #191f21;\n    font-size: 17px;\n    border: 1px solid #c7dce8;\n}\n.collapse-btn {\n    float: right;\n    padding: 0px 5px 1px 5px;\n    border: solid 1px #96aebb;\n    cursor: pointer;\n}\n.collapsed {\n  display: none;\n}\n.source-code {\n  font-family: courier;\n  font-size: small;\n  padding-bottom: 10px;\n}\n\"\"\"\n\nJS = \"\"\"\n<script type=\"text/javascript\">\n    function collapse(element){\n        const frameId = element.getAttribute(\"data-frame-id\");\n        const frame = document.getElementById(frameId);\n\n        if (frame.classList.contains(\"collapsed\")){\n            element.innerHTML = \"&#8210;\";\n            frame.classList.remove(\"collapsed\");\n        } else {\n            element.innerHTML = \"+\";\n            frame.classList.add(\"collapsed\");\n        }\n    }\n</script>\n\"\"\"\n\nTEMPLATE = \"\"\"\n<html>\n    <head>\n        <style type='text/css'>\n            {styles}\n        </style>\n        <title>Starlette Debugger</title>\n    </head>\n    <body>\n        <h1>500 Server Error</h1>\n        <h2>{error}</h2>\n        <div class=\"traceback-container\">\n            <p class=\"traceback-title\">Traceback</p>\n            <div>{exc_html}</div>\n        </div>\n        {js}\n    </body>\n</html>\n\"\"\"\n\nFRAME_TEMPLATE = \"\"\"\n<div>\n    <p class=\"frame-title\">File <span class=\"frame-filename\">{frame_filename}</span>,\n    line <i>{frame_lineno}</i>,\n    in <b>{frame_name}</b>\n    <span class=\"collapse-btn\" data-frame-id=\"{frame_filename}-{frame_lineno}\" onclick=\"collapse(this)\">{collapse_button}</span>\n    </p>\n    <div id=\"{frame_filename}-{frame_lineno}\" class=\"source-code {collapsed}\">{code_context}</div>\n</div>\n\"\"\"  # noqa: E501\n\nLINE = \"\"\"\n<p><span class=\"frame-line\">\n<span class=\"lineno\">{lineno}.</span> {line}</span></p>\n\"\"\"\n\nCENTER_LINE = \"\"\"\n<p class=\"center-line\"><span class=\"frame-line center-line\">\n<span class=\"lineno\">{lineno}.</span> {line}</span></p>\n\"\"\"\n\n\nclass ServerErrorMiddleware:\n    \"\"\"\n    Handles returning 500 responses when a server error occurs.\n\n    If 'debug' is set, then traceback responses will be returned,\n    otherwise the designated 'handler' will be called.\n\n    This middleware class should generally be used to wrap *everything*\n    else up, so that unhandled exceptions anywhere in the stack\n    always result in an appropriate 500 response.\n    \"\"\"\n\n    def __init__(\n        self,\n        app: ASGIApp,\n        handler: typing.Optional[typing.Callable] = None,\n        debug: bool = False,\n    ) -> None:\n        self.app = app\n        self.handler = handler\n        self.debug = debug\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] != \"http\":\n            await self.app(scope, receive, send)\n            return\n\n        response_started = False\n\n        async def _send(message: Message) -> None:\n            nonlocal response_started, send\n\n            if message[\"type\"] == \"http.response.start\":\n                response_started = True\n            await send(message)\n\n        try:\n            await self.app(scope, receive, _send)\n        except Exception as exc:\n            request = Request(scope)\n            if self.debug:\n                # In debug mode, return traceback responses.\n                response = self.debug_response(request, exc)\n            elif self.handler is None:\n                # Use our default 500 error handler.\n                response = self.error_response(request, exc)\n            else:\n                # Use an installed 500 error handler.\n                if is_async_callable(self.handler):\n                    response = await self.handler(request, exc)\n                else:\n                    response = await run_in_threadpool(self.handler, request, exc)\n\n            if not response_started:\n                await response(scope, receive, send)\n\n            # We always continue to raise the exception.\n            # This allows servers to log the error, or allows test clients\n            # to optionally raise the error within the test case.\n            raise exc\n\n    def format_line(\n        self, index: int, line: str, frame_lineno: int, frame_index: int\n    ) -> str:\n        values = {\n            # HTML escape - line could contain < or >\n            \"line\": html.escape(line).replace(\" \", \"&nbsp\"),\n            \"lineno\": (frame_lineno - frame_index) + index,\n        }\n\n        if index != frame_index:\n            return LINE.format(**values)\n        return CENTER_LINE.format(**values)\n\n    def generate_frame_html(self, frame: inspect.FrameInfo, is_collapsed: bool) -> str:\n        code_context = \"\".join(\n            self.format_line(\n                index, line, frame.lineno, frame.index  # type: ignore[arg-type]\n            )\n            for index, line in enumerate(frame.code_context or [])\n        )\n\n        values = {\n            # HTML escape - filename could contain < or >, especially if it's a virtual\n            # file e.g. <stdin> in the REPL\n            \"frame_filename\": html.escape(frame.filename),\n            \"frame_lineno\": frame.lineno,\n            # HTML escape - if you try very hard it's possible to name a function with <\n            # or >\n            \"frame_name\": html.escape(frame.function),\n            \"code_context\": code_context,\n            \"collapsed\": \"collapsed\" if is_collapsed else \"\",\n            \"collapse_button\": \"+\" if is_collapsed else \"&#8210;\",\n        }\n        return FRAME_TEMPLATE.format(**values)\n\n    def generate_html(self, exc: Exception, limit: int = 7) -> str:\n        traceback_obj = traceback.TracebackException.from_exception(\n            exc, capture_locals=True\n        )\n\n        exc_html = \"\"\n        is_collapsed = False\n        exc_traceback = exc.__traceback__\n        if exc_traceback is not None:\n            frames = inspect.getinnerframes(exc_traceback, limit)\n            for frame in reversed(frames):\n                exc_html += self.generate_frame_html(frame, is_collapsed)\n                is_collapsed = True\n\n        # escape error class and text\n        error = (\n            f\"{html.escape(traceback_obj.exc_type.__name__)}: \"\n            f\"{html.escape(str(traceback_obj))}\"\n        )\n\n        return TEMPLATE.format(styles=STYLES, js=JS, error=error, exc_html=exc_html)\n\n    def generate_plain_text(self, exc: Exception) -> str:\n        return \"\".join(traceback.format_exception(type(exc), exc, exc.__traceback__))\n\n    def debug_response(self, request: Request, exc: Exception) -> Response:\n        accept = request.headers.get(\"accept\", \"\")\n\n        if \"text/html\" in accept:\n            content = self.generate_html(exc)\n            return HTMLResponse(content, status_code=500)\n        content = self.generate_plain_text(exc)\n        return PlainTextResponse(content, status_code=500)\n\n    def error_response(self, request: Request, exc: Exception) -> Response:\n        return PlainTextResponse(\"Internal Server Error\", status_code=500)\n", 256], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/applications.py": ["import typing\nimport warnings\n\nfrom starlette.datastructures import State, URLPath\nfrom starlette.middleware import Middleware\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.middleware.errors import ServerErrorMiddleware\nfrom starlette.middleware.exceptions import ExceptionMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom starlette.routing import BaseRoute, Router\nfrom starlette.types import ASGIApp, Receive, Scope, Send\n\n\nclass Starlette:\n    \"\"\"\n    Creates an application instance.\n\n    **Parameters:**\n\n    * **debug** - Boolean indicating if debug tracebacks should be returned on errors.\n    * **routes** - A list of routes to serve incoming HTTP and WebSocket requests.\n    * **middleware** - A list of middleware to run for every request. A starlette\n    application will always automatically include two middleware classes.\n    `ServerErrorMiddleware` is added as the very outermost middleware, to handle\n    any uncaught errors occurring anywhere in the entire stack.\n    `ExceptionMiddleware` is added as the very innermost middleware, to deal\n    with handled exception cases occurring in the routing or endpoints.\n    * **exception_handlers** - A mapping of either integer status codes,\n    or exception class types onto callables which handle the exceptions.\n    Exception handler callables should be of the form\n    `handler(request, exc) -> response` and may be be either standard functions, or\n    async functions.\n    * **on_startup** - A list of callables to run on application startup.\n    Startup handler callables do not take any arguments, and may be be either\n    standard functions, or async functions.\n    * **on_shutdown** - A list of callables to run on application shutdown.\n    Shutdown handler callables do not take any arguments, and may be be either\n    standard functions, or async functions.\n    \"\"\"\n\n    def __init__(\n        self,\n        debug: bool = False,\n        routes: typing.Optional[typing.Sequence[BaseRoute]] = None,\n        middleware: typing.Optional[typing.Sequence[Middleware]] = None,\n        exception_handlers: typing.Optional[\n            typing.Mapping[\n                typing.Any,\n                typing.Callable[\n                    [Request, Exception],\n                    typing.Union[Response, typing.Awaitable[Response]],\n                ],\n            ]\n        ] = None,\n        on_startup: typing.Optional[typing.Sequence[typing.Callable]] = None,\n        on_shutdown: typing.Optional[typing.Sequence[typing.Callable]] = None,\n        lifespan: typing.Optional[\n            typing.Callable[[\"Starlette\"], typing.AsyncContextManager]\n        ] = None,\n    ) -> None:\n        # The lifespan context function is a newer style that replaces\n        # on_startup / on_shutdown handlers. Use one or the other, not both.\n        assert lifespan is None or (\n            on_startup is None and on_shutdown is None\n        ), \"Use either 'lifespan' or 'on_startup'/'on_shutdown', not both.\"\n\n        self.debug = debug\n        self.state = State()\n        self.router = Router(\n            routes, on_startup=on_startup, on_shutdown=on_shutdown, lifespan=lifespan\n        )\n        self.exception_handlers = (\n            {} if exception_handlers is None else dict(exception_handlers)\n        )\n        self.user_middleware = [] if middleware is None else list(middleware)\n        self.middleware_stack: typing.Optional[ASGIApp] = None\n\n    def build_middleware_stack(self) -> ASGIApp:\n        debug = self.debug\n        error_handler = None\n        exception_handlers: typing.Dict[\n            typing.Any, typing.Callable[[Request, Exception], Response]\n        ] = {}\n\n        for key, value in self.exception_handlers.items():\n            if key in (500, Exception):\n                error_handler = value\n            else:\n                exception_handlers[key] = value\n\n        middleware = (\n            [Middleware(ServerErrorMiddleware, handler=error_handler, debug=debug)]\n            + self.user_middleware\n            + [\n                Middleware(\n                    ExceptionMiddleware, handlers=exception_handlers, debug=debug\n                )\n            ]\n        )\n\n        app = self.router\n        for cls, options in reversed(middleware):\n            app = cls(app=app, **options)\n        return app\n\n    @property\n    def routes(self) -> typing.List[BaseRoute]:\n        return self.router.routes\n\n    def url_path_for(self, name: str, **path_params: typing.Any) -> URLPath:\n        return self.router.url_path_for(name, **path_params)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        scope[\"app\"] = self\n        if self.middleware_stack is None:\n            self.middleware_stack = self.build_middleware_stack()\n        await self.middleware_stack(scope, receive, send)\n\n    def on_event(self, event_type: str) -> typing.Callable:  # pragma: nocover\n        return self.router.on_event(event_type)\n\n    def mount(\n        self, path: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:  # pragma: nocover\n        self.router.mount(path, app=app, name=name)\n\n    def host(\n        self, host: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:  # pragma: no cover\n        self.router.host(host, app=app, name=name)\n\n    def add_middleware(self, middleware_class: type, **options: typing.Any) -> None:\n        if self.middleware_stack is not None:  # pragma: no cover\n            raise RuntimeError(\"Cannot add middleware after an application has started\")\n        self.user_middleware.insert(0, Middleware(middleware_class, **options))\n\n    def add_exception_handler(\n        self,\n        exc_class_or_status_code: typing.Union[int, typing.Type[Exception]],\n        handler: typing.Callable,\n    ) -> None:  # pragma: no cover\n        self.exception_handlers[exc_class_or_status_code] = handler\n\n    def add_event_handler(\n        self, event_type: str, func: typing.Callable\n    ) -> None:  # pragma: no cover\n        self.router.add_event_handler(event_type, func)\n\n    def add_route(\n        self,\n        path: str,\n        route: typing.Callable,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> None:  # pragma: no cover\n        self.router.add_route(\n            path, route, methods=methods, name=name, include_in_schema=include_in_schema\n        )\n\n    def add_websocket_route(\n        self, path: str, route: typing.Callable, name: typing.Optional[str] = None\n    ) -> None:  # pragma: no cover\n        self.router.add_websocket_route(path, route, name=name)\n\n    def exception_handler(\n        self, exc_class_or_status_code: typing.Union[int, typing.Type[Exception]]\n    ) -> typing.Callable:\n        warnings.warn(\n            \"The `exception_handler` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/exceptions/ for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_exception_handler(exc_class_or_status_code, func)\n            return func\n\n        return decorator\n\n    def route(\n        self,\n        path: str,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [Route(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `route` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/routing/ for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.router.add_route(\n                path,\n                func,\n                methods=methods,\n                name=name,\n                include_in_schema=include_in_schema,\n            )\n            return func\n\n        return decorator\n\n    def websocket_route(\n        self, path: str, name: typing.Optional[str] = None\n    ) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [WebSocketRoute(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `websocket_route` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/routing/#websocket-routing for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.router.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def middleware(self, middleware_type: str) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> middleware = [Middleware(...), ...]\n        >>> app = Starlette(middleware=middleware)\n        \"\"\"\n        warnings.warn(\n            \"The `middleware` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/middleware/#using-middleware for recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n        assert (\n            middleware_type == \"http\"\n        ), 'Currently only middleware(\"http\") is supported.'\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_middleware(BaseHTTPMiddleware, dispatch=func)\n            return func\n\n        return decorator\n", 257], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/applications.py": ["from enum import Enum\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    Union,\n)\n\nfrom fastapi import routing\nfrom fastapi.datastructures import Default, DefaultPlaceholder\nfrom fastapi.encoders import DictIntStrAny, SetIntStr\nfrom fastapi.exception_handlers import (\n    http_exception_handler,\n    request_validation_exception_handler,\n)\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.logger import logger\nfrom fastapi.middleware.asyncexitstack import AsyncExitStackMiddleware\nfrom fastapi.openapi.docs import (\n    get_redoc_html,\n    get_swagger_ui_html,\n    get_swagger_ui_oauth2_redirect_html,\n)\nfrom fastapi.openapi.utils import get_openapi\nfrom fastapi.params import Depends\nfrom fastapi.types import DecoratedCallable\nfrom fastapi.utils import generate_unique_id\nfrom starlette.applications import Starlette\nfrom starlette.datastructures import State\nfrom starlette.exceptions import HTTPException\nfrom starlette.middleware import Middleware\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.middleware.errors import ServerErrorMiddleware\nfrom starlette.middleware.exceptions import ExceptionMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import HTMLResponse, JSONResponse, Response\nfrom starlette.routing import BaseRoute\nfrom starlette.types import ASGIApp, Receive, Scope, Send\n\n\nclass FastAPI(Starlette):\n    def __init__(\n        self,\n        *,\n        debug: bool = False,\n        routes: Optional[List[BaseRoute]] = None,\n        title: str = \"FastAPI\",\n        description: str = \"\",\n        version: str = \"0.1.0\",\n        openapi_url: Optional[str] = \"/openapi.json\",\n        openapi_tags: Optional[List[Dict[str, Any]]] = None,\n        servers: Optional[List[Dict[str, Union[str, Any]]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        default_response_class: Type[Response] = Default(JSONResponse),\n        docs_url: Optional[str] = \"/docs\",\n        redoc_url: Optional[str] = \"/redoc\",\n        swagger_ui_oauth2_redirect_url: Optional[str] = \"/docs/oauth2-redirect\",\n        swagger_ui_init_oauth: Optional[Dict[str, Any]] = None,\n        middleware: Optional[Sequence[Middleware]] = None,\n        exception_handlers: Optional[\n            Dict[\n                Union[int, Type[Exception]],\n                Callable[[Request, Any], Coroutine[Any, Any, Response]],\n            ]\n        ] = None,\n        on_startup: Optional[Sequence[Callable[[], Any]]] = None,\n        on_shutdown: Optional[Sequence[Callable[[], Any]]] = None,\n        terms_of_service: Optional[str] = None,\n        contact: Optional[Dict[str, Union[str, Any]]] = None,\n        license_info: Optional[Dict[str, Union[str, Any]]] = None,\n        openapi_prefix: str = \"\",\n        root_path: str = \"\",\n        root_path_in_servers: bool = True,\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        deprecated: Optional[bool] = None,\n        include_in_schema: bool = True,\n        swagger_ui_parameters: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n        **extra: Any,\n    ) -> None:\n        self.debug = debug\n        self.title = title\n        self.description = description\n        self.version = version\n        self.terms_of_service = terms_of_service\n        self.contact = contact\n        self.license_info = license_info\n        self.openapi_url = openapi_url\n        self.openapi_tags = openapi_tags\n        self.root_path_in_servers = root_path_in_servers\n        self.docs_url = docs_url\n        self.redoc_url = redoc_url\n        self.swagger_ui_oauth2_redirect_url = swagger_ui_oauth2_redirect_url\n        self.swagger_ui_init_oauth = swagger_ui_init_oauth\n        self.swagger_ui_parameters = swagger_ui_parameters\n        self.servers = servers or []\n        self.extra = extra\n        self.openapi_version = \"3.0.2\"\n        self.openapi_schema: Optional[Dict[str, Any]] = None\n        if self.openapi_url:\n            assert self.title, \"A title must be provided for OpenAPI, e.g.: 'My API'\"\n            assert self.version, \"A version must be provided for OpenAPI, e.g.: '2.1.0'\"\n        # TODO: remove when discarding the openapi_prefix parameter\n        if openapi_prefix:\n            logger.warning(\n                '\"openapi_prefix\" has been deprecated in favor of \"root_path\", which '\n                \"follows more closely the ASGI standard, is simpler, and more \"\n                \"automatic. Check the docs at \"\n                \"https://fastapi.tiangolo.com/advanced/sub-applications/\"\n            )\n        self.root_path = root_path or openapi_prefix\n        self.state: State = State()\n        self.dependency_overrides: Dict[Callable[..., Any], Callable[..., Any]] = {}\n        self.router: routing.APIRouter = routing.APIRouter(\n            routes=routes,\n            dependency_overrides_provider=self,\n            on_startup=on_startup,\n            on_shutdown=on_shutdown,\n            default_response_class=default_response_class,\n            dependencies=dependencies,\n            callbacks=callbacks,\n            deprecated=deprecated,\n            include_in_schema=include_in_schema,\n            responses=responses,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n        self.exception_handlers: Dict[\n            Any, Callable[[Request, Any], Union[Response, Awaitable[Response]]]\n        ] = ({} if exception_handlers is None else dict(exception_handlers))\n        self.exception_handlers.setdefault(HTTPException, http_exception_handler)\n        self.exception_handlers.setdefault(\n            RequestValidationError, request_validation_exception_handler\n        )\n\n        self.user_middleware: List[Middleware] = (\n            [] if middleware is None else list(middleware)\n        )\n        self.middleware_stack: Union[ASGIApp, None] = None\n        self.setup()\n\n    def build_middleware_stack(self) -> ASGIApp:\n        # Duplicate/override from Starlette to add AsyncExitStackMiddleware\n        # inside of ExceptionMiddleware, inside of custom user middlewares\n        debug = self.debug\n        error_handler = None\n        exception_handlers = {}\n\n        for key, value in self.exception_handlers.items():\n            if key in (500, Exception):\n                error_handler = value\n            else:\n                exception_handlers[key] = value\n\n        middleware = (\n            [Middleware(ServerErrorMiddleware, handler=error_handler, debug=debug)]\n            + self.user_middleware\n            + [\n                Middleware(\n                    ExceptionMiddleware, handlers=exception_handlers, debug=debug\n                ),\n                # Add FastAPI-specific AsyncExitStackMiddleware for dependencies with\n                # contextvars.\n                # This needs to happen after user middlewares because those create a\n                # new contextvars context copy by using a new AnyIO task group.\n                # The initial part of dependencies with yield is executed in the\n                # FastAPI code, inside all the middlewares, but the teardown part\n                # (after yield) is executed in the AsyncExitStack in this middleware,\n                # if the AsyncExitStack lived outside of the custom middlewares and\n                # contextvars were set in a dependency with yield in that internal\n                # contextvars context, the values would not be available in the\n                # outside context of the AsyncExitStack.\n                # By putting the middleware and the AsyncExitStack here, inside all\n                # user middlewares, the code before and after yield in dependencies\n                # with yield is executed in the same contextvars context, so all values\n                # set in contextvars before yield is still available after yield as\n                # would be expected.\n                # Additionally, by having this AsyncExitStack here, after the\n                # ExceptionMiddleware, now dependencies can catch handled exceptions,\n                # e.g. HTTPException, to customize the teardown code (e.g. DB session\n                # rollback).\n                Middleware(AsyncExitStackMiddleware),\n            ]\n        )\n\n        app = self.router\n        for cls, options in reversed(middleware):\n            app = cls(app=app, **options)\n        return app\n\n    def openapi(self) -> Dict[str, Any]:\n        if not self.openapi_schema:\n            self.openapi_schema = get_openapi(\n                title=self.title,\n                version=self.version,\n                openapi_version=self.openapi_version,\n                description=self.description,\n                terms_of_service=self.terms_of_service,\n                contact=self.contact,\n                license_info=self.license_info,\n                routes=self.routes,\n                tags=self.openapi_tags,\n                servers=self.servers,\n            )\n        return self.openapi_schema\n\n    def setup(self) -> None:\n        if self.openapi_url:\n            urls = (server_data.get(\"url\") for server_data in self.servers)\n            server_urls = {url for url in urls if url}\n\n            async def openapi(req: Request) -> JSONResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                if root_path not in server_urls:\n                    if root_path and self.root_path_in_servers:\n                        self.servers.insert(0, {\"url\": root_path})\n                        server_urls.add(root_path)\n                return JSONResponse(self.openapi())\n\n            self.add_route(self.openapi_url, openapi, include_in_schema=False)\n        if self.openapi_url and self.docs_url:\n\n            async def swagger_ui_html(req: Request) -> HTMLResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                openapi_url = root_path + self.openapi_url\n                oauth2_redirect_url = self.swagger_ui_oauth2_redirect_url\n                if oauth2_redirect_url:\n                    oauth2_redirect_url = root_path + oauth2_redirect_url\n                return get_swagger_ui_html(\n                    openapi_url=openapi_url,\n                    title=self.title + \" - Swagger UI\",\n                    oauth2_redirect_url=oauth2_redirect_url,\n                    init_oauth=self.swagger_ui_init_oauth,\n                    swagger_ui_parameters=self.swagger_ui_parameters,\n                )\n\n            self.add_route(self.docs_url, swagger_ui_html, include_in_schema=False)\n\n            if self.swagger_ui_oauth2_redirect_url:\n\n                async def swagger_ui_redirect(req: Request) -> HTMLResponse:\n                    return get_swagger_ui_oauth2_redirect_html()\n\n                self.add_route(\n                    self.swagger_ui_oauth2_redirect_url,\n                    swagger_ui_redirect,\n                    include_in_schema=False,\n                )\n        if self.openapi_url and self.redoc_url:\n\n            async def redoc_html(req: Request) -> HTMLResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                openapi_url = root_path + self.openapi_url\n                return get_redoc_html(\n                    openapi_url=openapi_url, title=self.title + \" - ReDoc\"\n                )\n\n            self.add_route(self.redoc_url, redoc_html, include_in_schema=False)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if self.root_path:\n            scope[\"root_path\"] = self.root_path\n        await super().__call__(scope, receive, send)\n\n    def add_api_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Coroutine[Any, Any, Response]],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        name: Optional[str] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        self.router.add_api_route(\n            path,\n            endpoint=endpoint,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=methods,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def api_route(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.router.add_api_route(\n                path,\n                func,\n                response_model=response_model,\n                status_code=status_code,\n                tags=tags,\n                dependencies=dependencies,\n                summary=summary,\n                description=description,\n                response_description=response_description,\n                responses=responses,\n                deprecated=deprecated,\n                methods=methods,\n                operation_id=operation_id,\n                response_model_include=response_model_include,\n                response_model_exclude=response_model_exclude,\n                response_model_by_alias=response_model_by_alias,\n                response_model_exclude_unset=response_model_exclude_unset,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                include_in_schema=include_in_schema,\n                response_class=response_class,\n                name=name,\n                openapi_extra=openapi_extra,\n                generate_unique_id_function=generate_unique_id_function,\n            )\n            return func\n\n        return decorator\n\n    def add_api_websocket_route(\n        self, path: str, endpoint: Callable[..., Any], name: Optional[str] = None\n    ) -> None:\n        self.router.add_api_websocket_route(path, endpoint, name=name)\n\n    def websocket(\n        self, path: str, name: Optional[str] = None\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def include_router(\n        self,\n        router: routing.APIRouter,\n        *,\n        prefix: str = \"\",\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        include_in_schema: bool = True,\n        default_response_class: Type[Response] = Default(JSONResponse),\n        callbacks: Optional[List[BaseRoute]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        self.router.include_router(\n            router,\n            prefix=prefix,\n            tags=tags,\n            dependencies=dependencies,\n            responses=responses,\n            deprecated=deprecated,\n            include_in_schema=include_in_schema,\n            default_response_class=default_response_class,\n            callbacks=callbacks,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def get(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.get(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def put(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.put(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def post(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.post(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def delete(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.delete(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def options(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.options(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def head(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.head(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def patch(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.patch(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def trace(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.trace(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def websocket_route(\n        self, path: str, name: Union[str, None] = None\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.router.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def on_event(\n        self, event_type: str\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.on_event(event_type)\n\n    def middleware(\n        self, middleware_type: str\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_middleware(BaseHTTPMiddleware, dispatch=func)\n            return func\n\n        return decorator\n\n    def exception_handler(\n        self, exc_class_or_status_code: Union[int, Type[Exception]]\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_exception_handler(exc_class_or_status_code, func)\n            return func\n\n        return decorator\n", 905], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/from_thread.py": ["import threading\nfrom asyncio import iscoroutine\nfrom concurrent.futures import FIRST_COMPLETED, Future, ThreadPoolExecutor, wait\nfrom contextlib import AbstractContextManager, contextmanager\nfrom types import TracebackType\nfrom typing import (\n    Any,\n    AsyncContextManager,\n    Callable,\n    ContextManager,\n    Coroutine,\n    Dict,\n    Generator,\n    Iterable,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\nfrom warnings import warn\n\nfrom ._core import _eventloop\nfrom ._core._eventloop import get_asynclib, get_cancelled_exc_class, threadlocals\nfrom ._core._synchronization import Event\nfrom ._core._tasks import CancelScope, create_task_group\nfrom .abc._tasks import TaskStatus\n\nT_Retval = TypeVar(\"T_Retval\")\nT_co = TypeVar(\"T_co\")\n\n\ndef run(func: Callable[..., Coroutine[Any, Any, T_Retval]], *args: object) -> T_Retval:\n    \"\"\"\n    Call a coroutine function from a worker thread.\n\n    :param func: a coroutine function\n    :param args: positional arguments for the callable\n    :return: the return value of the coroutine function\n\n    \"\"\"\n    try:\n        asynclib = threadlocals.current_async_module\n    except AttributeError:\n        raise RuntimeError(\"This function can only be run from an AnyIO worker thread\")\n\n    return asynclib.run_async_from_thread(func, *args)\n\n\ndef run_async_from_thread(\n    func: Callable[..., Coroutine[Any, Any, T_Retval]], *args: object\n) -> T_Retval:\n    warn(\n        \"run_async_from_thread() has been deprecated, use anyio.from_thread.run() instead\",\n        DeprecationWarning,\n    )\n    return run(func, *args)\n\n\ndef run_sync(func: Callable[..., T_Retval], *args: object) -> T_Retval:\n    \"\"\"\n    Call a function in the event loop thread from a worker thread.\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :return: the return value of the callable\n\n    \"\"\"\n    try:\n        asynclib = threadlocals.current_async_module\n    except AttributeError:\n        raise RuntimeError(\"This function can only be run from an AnyIO worker thread\")\n\n    return asynclib.run_sync_from_thread(func, *args)\n\n\ndef run_sync_from_thread(func: Callable[..., T_Retval], *args: object) -> T_Retval:\n    warn(\n        \"run_sync_from_thread() has been deprecated, use anyio.from_thread.run_sync() instead\",\n        DeprecationWarning,\n    )\n    return run_sync(func, *args)\n\n\nclass _BlockingAsyncContextManager(AbstractContextManager):\n    _enter_future: Future\n    _exit_future: Future\n    _exit_event: Event\n    _exit_exc_info: Tuple[\n        Optional[Type[BaseException]], Optional[BaseException], Optional[TracebackType]\n    ] = (None, None, None)\n\n    def __init__(self, async_cm: AsyncContextManager[T_co], portal: \"BlockingPortal\"):\n        self._async_cm = async_cm\n        self._portal = portal\n\n    async def run_async_cm(self) -> Optional[bool]:\n        try:\n            self._exit_event = Event()\n            value = await self._async_cm.__aenter__()\n        except BaseException as exc:\n            self._enter_future.set_exception(exc)\n            raise\n        else:\n            self._enter_future.set_result(value)\n\n        try:\n            # Wait for the sync context manager to exit.\n            # This next statement can raise `get_cancelled_exc_class()` if\n            # something went wrong in a task group in this async context\n            # manager.\n            await self._exit_event.wait()\n        finally:\n            # In case of cancellation, it could be that we end up here before\n            # `_BlockingAsyncContextManager.__exit__` is called, and an\n            # `_exit_exc_info` has been set.\n            result = await self._async_cm.__aexit__(*self._exit_exc_info)\n            return result\n\n    def __enter__(self) -> T_co:\n        self._enter_future = Future()\n        self._exit_future = self._portal.start_task_soon(self.run_async_cm)\n        cm = self._enter_future.result()\n        return cast(T_co, cm)\n\n    def __exit__(\n        self,\n        __exc_type: Optional[Type[BaseException]],\n        __exc_value: Optional[BaseException],\n        __traceback: Optional[TracebackType],\n    ) -> Optional[bool]:\n        self._exit_exc_info = __exc_type, __exc_value, __traceback\n        self._portal.call(self._exit_event.set)\n        return self._exit_future.result()\n\n\nclass _BlockingPortalTaskStatus(TaskStatus):\n    def __init__(self, future: Future):\n        self._future = future\n\n    def started(self, value: object = None) -> None:\n        self._future.set_result(value)\n\n\nclass BlockingPortal:\n    \"\"\"An object that lets external threads run code in an asynchronous event loop.\"\"\"\n\n    def __new__(cls) -> \"BlockingPortal\":\n        return get_asynclib().BlockingPortal()\n\n    def __init__(self) -> None:\n        self._event_loop_thread_id: Optional[int] = threading.get_ident()\n        self._stop_event = Event()\n        self._task_group = create_task_group()\n        self._cancelled_exc_class = get_cancelled_exc_class()\n\n    async def __aenter__(self) -> \"BlockingPortal\":\n        await self._task_group.__aenter__()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        await self.stop()\n        return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n\n    def _check_running(self) -> None:\n        if self._event_loop_thread_id is None:\n            raise RuntimeError(\"This portal is not running\")\n        if self._event_loop_thread_id == threading.get_ident():\n            raise RuntimeError(\n                \"This method cannot be called from the event loop thread\"\n            )\n\n    async def sleep_until_stopped(self) -> None:\n        \"\"\"Sleep until :meth:`stop` is called.\"\"\"\n        await self._stop_event.wait()\n\n    async def stop(self, cancel_remaining: bool = False) -> None:\n        \"\"\"\n        Signal the portal to shut down.\n\n        This marks the portal as no longer accepting new calls and exits from\n        :meth:`sleep_until_stopped`.\n\n        :param cancel_remaining: ``True`` to cancel all the remaining tasks, ``False`` to let them\n            finish before returning\n\n        \"\"\"\n        self._event_loop_thread_id = None\n        self._stop_event.set()\n        if cancel_remaining:\n            self._task_group.cancel_scope.cancel()\n\n    async def _call_func(\n        self, func: Callable, args: tuple, kwargs: Dict[str, Any], future: Future\n    ) -> None:\n        def callback(f: Future) -> None:\n            if f.cancelled() and self._event_loop_thread_id not in (\n                None,\n                threading.get_ident(),\n            ):\n                self.call(scope.cancel)\n\n        try:\n            retval = func(*args, **kwargs)\n            if iscoroutine(retval):\n                with CancelScope() as scope:\n                    if future.cancelled():\n                        scope.cancel()\n                    else:\n                        future.add_done_callback(callback)\n\n                    retval = await retval\n        except self._cancelled_exc_class:\n            future.cancel()\n        except BaseException as exc:\n            if not future.cancelled():\n                future.set_exception(exc)\n\n            # Let base exceptions fall through\n            if not isinstance(exc, Exception):\n                raise\n        else:\n            if not future.cancelled():\n                future.set_result(retval)\n        finally:\n            scope = None  # type: ignore[assignment]\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable,\n        args: tuple,\n        kwargs: Dict[str, Any],\n        name: object,\n        future: Future,\n    ) -> None:\n        \"\"\"\n        Spawn a new task using the given callable.\n\n        Implementors must ensure that the future is resolved when the task finishes.\n\n        :param func: a callable\n        :param args: positional arguments to be passed to the callable\n        :param kwargs: keyword arguments to be passed to the callable\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :param future: a future that will resolve to the return value of the callable, or the\n            exception raised during its execution\n\n        \"\"\"\n        raise NotImplementedError\n\n    @overload\n    def call(\n        self, func: Callable[..., Coroutine[Any, Any, T_Retval]], *args: object\n    ) -> T_Retval:\n        ...\n\n    @overload\n    def call(self, func: Callable[..., T_Retval], *args: object) -> T_Retval:\n        ...\n\n    def call(\n        self,\n        func: Callable[..., Union[Coroutine[Any, Any, T_Retval], T_Retval]],\n        *args: object\n    ) -> T_Retval:\n        \"\"\"\n        Call the given function in the event loop thread.\n\n        If the callable returns a coroutine object, it is awaited on.\n\n        :param func: any callable\n        :raises RuntimeError: if the portal is not running or if this method is called from within\n            the event loop thread\n\n        \"\"\"\n        return cast(T_Retval, self.start_task_soon(func, *args).result())\n\n    @overload\n    def spawn_task(\n        self,\n        func: Callable[..., Coroutine[Any, Any, T_Retval]],\n        *args: object,\n        name: object = None\n    ) -> \"Future[T_Retval]\":\n        ...\n\n    @overload\n    def spawn_task(\n        self, func: Callable[..., T_Retval], *args: object, name: object = None\n    ) -> \"Future[T_Retval]\":\n        ...\n\n    def spawn_task(\n        self,\n        func: Callable[..., Union[Coroutine[Any, Any, T_Retval], T_Retval]],\n        *args: object,\n        name: object = None\n    ) -> \"Future[T_Retval]\":\n        \"\"\"\n        Start a task in the portal's task group.\n\n        :param func: the target coroutine function\n        :param args: positional arguments passed to ``func``\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :return: a future that resolves with the return value of the callable if the task completes\n            successfully, or with the exception raised in the task\n        :raises RuntimeError: if the portal is not running or if this method is called from within\n            the event loop thread\n\n        .. versionadded:: 2.1\n        .. deprecated:: 3.0\n           Use :meth:`start_task_soon` instead. If your code needs AnyIO 2 compatibility, you\n           can keep using this until AnyIO 4.\n\n        \"\"\"\n        warn(\n            \"spawn_task() is deprecated -- use start_task_soon() instead\",\n            DeprecationWarning,\n        )\n        return self.start_task_soon(func, *args, name=name)  # type: ignore[arg-type]\n\n    @overload\n    def start_task_soon(\n        self,\n        func: Callable[..., Coroutine[Any, Any, T_Retval]],\n        *args: object,\n        name: object = None\n    ) -> \"Future[T_Retval]\":\n        ...\n\n    @overload\n    def start_task_soon(\n        self, func: Callable[..., T_Retval], *args: object, name: object = None\n    ) -> \"Future[T_Retval]\":\n        ...\n\n    def start_task_soon(\n        self,\n        func: Callable[..., Union[Coroutine[Any, Any, T_Retval], T_Retval]],\n        *args: object,\n        name: object = None\n    ) -> \"Future[T_Retval]\":\n        \"\"\"\n        Start a task in the portal's task group.\n\n        The task will be run inside a cancel scope which can be cancelled by cancelling the\n        returned future.\n\n        :param func: the target coroutine function\n        :param args: positional arguments passed to ``func``\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :return: a future that resolves with the return value of the callable if the task completes\n            successfully, or with the exception raised in the task\n        :raises RuntimeError: if the portal is not running or if this method is called from within\n            the event loop thread\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        self._check_running()\n        f: Future = Future()\n        self._spawn_task_from_thread(func, args, {}, name, f)\n        return f\n\n    def start_task(\n        self,\n        func: Callable[..., Coroutine[Any, Any, Any]],\n        *args: object,\n        name: object = None\n    ) -> Tuple[\"Future[Any]\", Any]:\n        \"\"\"\n        Start a task in the portal's task group and wait until it signals for readiness.\n\n        This method works the same way as :meth:`TaskGroup.start`.\n\n        :param func: the target coroutine function\n        :param args: positional arguments passed to ``func``\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :return: a tuple of (future, task_status_value) where the ``task_status_value`` is the\n            value passed to ``task_status.started()`` from within the target function\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n\n        def task_done(future: Future) -> None:\n            if not task_status_future.done():\n                if future.cancelled():\n                    task_status_future.cancel()\n                elif future.exception():\n                    task_status_future.set_exception(future.exception())\n                else:\n                    exc = RuntimeError(\n                        \"Task exited without calling task_status.started()\"\n                    )\n                    task_status_future.set_exception(exc)\n\n        self._check_running()\n        task_status_future: Future = Future()\n        task_status = _BlockingPortalTaskStatus(task_status_future)\n        f: Future = Future()\n        f.add_done_callback(task_done)\n        self._spawn_task_from_thread(func, args, {\"task_status\": task_status}, name, f)\n        return f, task_status_future.result()\n\n    def wrap_async_context_manager(\n        self, cm: AsyncContextManager[T_co]\n    ) -> ContextManager[T_co]:\n        \"\"\"\n        Wrap an async context manager as a synchronous context manager via this portal.\n\n        Spawns a task that will call both ``__aenter__()`` and ``__aexit__()``, stopping in the\n        middle until the synchronous context manager exits.\n\n        :param cm: an asynchronous context manager\n        :return: a synchronous context manager\n\n        .. versionadded:: 2.1\n\n        \"\"\"\n        return _BlockingAsyncContextManager(cm, self)\n\n\ndef create_blocking_portal() -> BlockingPortal:\n    \"\"\"\n    Create a portal for running functions in the event loop thread from external threads.\n\n    Use this function in asynchronous code when you need to allow external threads access to the\n    event loop where your asynchronous code is currently running.\n\n    .. deprecated:: 3.0\n        Use :class:`.BlockingPortal` directly.\n\n    \"\"\"\n    warn(\n        \"create_blocking_portal() has been deprecated -- use anyio.from_thread.BlockingPortal() \"\n        \"directly\",\n        DeprecationWarning,\n    )\n    return BlockingPortal()\n\n\n@contextmanager\ndef start_blocking_portal(\n    backend: str = \"asyncio\", backend_options: Optional[Dict[str, Any]] = None\n) -> Generator[BlockingPortal, Any, None]:\n    \"\"\"\n    Start a new event loop in a new thread and run a blocking portal in its main task.\n\n    The parameters are the same as for :func:`~anyio.run`.\n\n    :param backend: name of the backend\n    :param backend_options: backend options\n    :return: a context manager that yields a blocking portal\n\n    .. versionchanged:: 3.0\n        Usage as a context manager is now required.\n\n    \"\"\"\n\n    async def run_portal() -> None:\n        async with BlockingPortal() as portal_:\n            if future.set_running_or_notify_cancel():\n                future.set_result(portal_)\n                await portal_.sleep_until_stopped()\n\n    future: Future[BlockingPortal] = Future()\n    with ThreadPoolExecutor(1) as executor:\n        run_future = executor.submit(\n            _eventloop.run,\n            run_portal,  # type: ignore[arg-type]\n            backend=backend,\n            backend_options=backend_options,\n        )\n        try:\n            wait(\n                cast(Iterable[Future], [run_future, future]),\n                return_when=FIRST_COMPLETED,\n            )\n        except BaseException:\n            future.cancel()\n            run_future.cancel()\n            raise\n\n        if future.done():\n            portal = future.result()\n            try:\n                yield portal\n            except BaseException:\n                portal.call(portal.stop, True)\n                raise\n\n            portal.call(portal.stop, False)\n\n        run_future.result()\n", 502], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py": ["# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard:\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet:\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        discard = self.data.discard\n        while l:\n            discard(l.pop())\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),),\n                getattr(self, '__dict__', None))\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet') from None\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(map(ref, other))\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(map(ref, other))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(map(ref, other))\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n", 196], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py": ["\"\"\"A Future class similar to the one in PEP 3148.\"\"\"\n\n__all__ = (\n    'CancelledError', 'TimeoutError', 'InvalidStateError',\n    'Future', 'wrap_future', 'isfuture',\n)\n\nimport concurrent.futures\nimport contextvars\nimport logging\nimport sys\n\nfrom . import base_futures\nfrom . import events\nfrom . import format_helpers\n\n\nCancelledError = base_futures.CancelledError\nInvalidStateError = base_futures.InvalidStateError\nTimeoutError = base_futures.TimeoutError\nisfuture = base_futures.isfuture\n\n\n_PENDING = base_futures._PENDING\n_CANCELLED = base_futures._CANCELLED\n_FINISHED = base_futures._FINISHED\n\n\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\n\n\nclass Future:\n    \"\"\"This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - This class is not thread-safe.\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.\n\n    (In Python 3.4 or later we may be able to unify the implementations.)\n    \"\"\"\n\n    # Class variables serving as defaults for instance variables.\n    _state = _PENDING\n    _result = None\n    _exception = None\n    _loop = None\n    _source_traceback = None\n\n    # This field is used for a dual purpose:\n    # - Its presence is a marker to declare that a class implements\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\n    #   The value must also be not-None, to enable a subclass to declare\n    #   that it is not compatible by setting this to None.\n    # - It is set by __iter__() below so that Task._step() can tell\n    #   the difference between\n    #   `await Future()` or`yield from Future()` (correct) vs.\n    #   `yield Future()` (incorrect).\n    _asyncio_future_blocking = False\n\n    __log_traceback = False\n\n    def __init__(self, *, loop=None):\n        \"\"\"Initialize the future.\n\n        The optional event_loop argument allows explicitly setting the event\n        loop object used by the future. If it's not provided, the future uses\n        the default event loop.\n        \"\"\"\n        if loop is None:\n            self._loop = events.get_event_loop()\n        else:\n            self._loop = loop\n        self._callbacks = []\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    _repr_info = base_futures._future_repr_info\n\n    def __repr__(self):\n        return '<{} {}>'.format(self.__class__.__name__,\n                                ' '.join(self._repr_info()))\n\n    def __del__(self):\n        if not self.__log_traceback:\n            # set_exception() was not called, or result() or exception()\n            # has consumed the exception\n            return\n        exc = self._exception\n        context = {\n            'message':\n                f'{self.__class__.__name__} exception was never retrieved',\n            'exception': exc,\n            'future': self,\n        }\n        if self._source_traceback:\n            context['source_traceback'] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    @property\n    def _log_traceback(self):\n        return self.__log_traceback\n\n    @_log_traceback.setter\n    def _log_traceback(self, val):\n        if bool(val):\n            raise ValueError('_log_traceback can only be set to False')\n        self.__log_traceback = False\n\n    def get_loop(self):\n        \"\"\"Return the event loop the Future is bound to.\"\"\"\n        loop = self._loop\n        if loop is None:\n            raise RuntimeError(\"Future object is not initialized.\")\n        return loop\n\n    def cancel(self):\n        \"\"\"Cancel the future and schedule callbacks.\n\n        If the future is already done or cancelled, return False.  Otherwise,\n        change the future's state to cancelled, schedule the callbacks and\n        return True.\n        \"\"\"\n        self.__log_traceback = False\n        if self._state != _PENDING:\n            return False\n        self._state = _CANCELLED\n        self.__schedule_callbacks()\n        return True\n\n    def __schedule_callbacks(self):\n        \"\"\"Internal: Ask the event loop to call all callbacks.\n\n        The callbacks are scheduled to be called as soon as possible. Also\n        clears the callback list.\n        \"\"\"\n        callbacks = self._callbacks[:]\n        if not callbacks:\n            return\n\n        self._callbacks[:] = []\n        for callback, ctx in callbacks:\n            self._loop.call_soon(callback, self, context=ctx)\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        return self._state == _CANCELLED\n\n    # Don't implement running(); see http://bugs.python.org/issue18699\n\n    def done(self):\n        \"\"\"Return True if the future is done.\n\n        Done means either that a result / exception are available, or that the\n        future was cancelled.\n        \"\"\"\n        return self._state != _PENDING\n\n    def result(self):\n        \"\"\"Return the result this future represents.\n\n        If the future has been cancelled, raises CancelledError.  If the\n        future's result isn't yet available, raises InvalidStateError.  If\n        the future is done and has an exception set, this exception is raised.\n        \"\"\"\n        if self._state == _CANCELLED:\n            raise CancelledError\n        if self._state != _FINISHED:\n            raise InvalidStateError('Result is not ready.')\n        self.__log_traceback = False\n        if self._exception is not None:\n            raise self._exception\n        return self._result\n\n    def exception(self):\n        \"\"\"Return the exception that was set on this future.\n\n        The exception (or None if no exception was set) is returned only if\n        the future is done.  If the future has been cancelled, raises\n        CancelledError.  If the future isn't done yet, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state == _CANCELLED:\n            raise CancelledError\n        if self._state != _FINISHED:\n            raise InvalidStateError('Exception is not set.')\n        self.__log_traceback = False\n        return self._exception\n\n    def add_done_callback(self, fn, *, context=None):\n        \"\"\"Add a callback to be run when the future becomes done.\n\n        The callback is called with a single argument - the future object. If\n        the future is already done when this is called, the callback is\n        scheduled with call_soon.\n        \"\"\"\n        if self._state != _PENDING:\n            self._loop.call_soon(fn, self, context=context)\n        else:\n            if context is None:\n                context = contextvars.copy_context()\n            self._callbacks.append((fn, context))\n\n    # New method not in PEP 3148.\n\n    def remove_done_callback(self, fn):\n        \"\"\"Remove all instances of a callback from the \"call when done\" list.\n\n        Returns the number of callbacks removed.\n        \"\"\"\n        filtered_callbacks = [(f, ctx)\n                              for (f, ctx) in self._callbacks\n                              if f != fn]\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\n        if removed_count:\n            self._callbacks[:] = filtered_callbacks\n        return removed_count\n\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\n\n    def set_result(self, result):\n        \"\"\"Mark the future done and set its result.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise InvalidStateError('{}: {!r}'.format(self._state, self))\n        self._result = result\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Mark the future done and set an exception.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise InvalidStateError('{}: {!r}'.format(self._state, self))\n        if isinstance(exception, type):\n            exception = exception()\n        if type(exception) is StopIteration:\n            raise TypeError(\"StopIteration interacts badly with generators \"\n                            \"and cannot be raised into a Future\")\n        self._exception = exception\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n        self.__log_traceback = True\n\n    def __await__(self):\n        if not self.done():\n            self._asyncio_future_blocking = True\n            yield self  # This tells Task to wait for completion.\n        if not self.done():\n            raise RuntimeError(\"await wasn't used with future\")\n        return self.result()  # May raise too.\n\n    __iter__ = __await__  # make compatible with 'yield from'.\n\n\n# Needed for testing purposes.\n_PyFuture = Future\n\n\ndef _get_loop(fut):\n    # Tries to call Future.get_loop() if it's available.\n    # Otherwise fallbacks to using the old '_loop' property.\n    try:\n        get_loop = fut.get_loop\n    except AttributeError:\n        pass\n    else:\n        return get_loop()\n    return fut._loop\n\n\ndef _set_result_unless_cancelled(fut, result):\n    \"\"\"Helper setting the result only if the future was not cancelled.\"\"\"\n    if fut.cancelled():\n        return\n    fut.set_result(result)\n\n\ndef _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(exception)\n    else:\n        result = source.result()\n        concurrent.set_result(result)\n\n\ndef _copy_future_state(source, dest):\n    \"\"\"Internal helper to copy state from another Future.\n\n    The other Future may be a concurrent.futures.Future.\n    \"\"\"\n    assert source.done()\n    if dest.cancelled():\n        return\n    assert not dest.done()\n    if source.cancelled():\n        dest.cancel()\n    else:\n        exception = source.exception()\n        if exception is not None:\n            dest.set_exception(exception)\n        else:\n            result = source.result()\n            dest.set_result(result)\n\n\ndef _chain_future(source, destination):\n    \"\"\"Chain two futures so that when one completes, so does the other.\n\n    The result (or exception) of source will be copied to destination.\n    If destination is cancelled, source gets cancelled too.\n    Compatible with both asyncio.Future and concurrent.futures.Future.\n    \"\"\"\n    if not isfuture(source) and not isinstance(source,\n                                               concurrent.futures.Future):\n        raise TypeError('A future is required for source argument')\n    if not isfuture(destination) and not isinstance(destination,\n                                                    concurrent.futures.Future):\n        raise TypeError('A future is required for destination argument')\n    source_loop = _get_loop(source) if isfuture(source) else None\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\n\n    def _set_state(future, other):\n        if isfuture(future):\n            _copy_future_state(other, future)\n        else:\n            _set_concurrent_future_state(future, other)\n\n    def _call_check_cancel(destination):\n        if destination.cancelled():\n            if source_loop is None or source_loop is dest_loop:\n                source.cancel()\n            else:\n                source_loop.call_soon_threadsafe(source.cancel)\n\n    def _call_set_state(source):\n        if (destination.cancelled() and\n                dest_loop is not None and dest_loop.is_closed()):\n            return\n        if dest_loop is None or dest_loop is source_loop:\n            _set_state(destination, source)\n        else:\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\n\n    destination.add_done_callback(_call_check_cancel)\n    source.add_done_callback(_call_set_state)\n\n\ndef wrap_future(future, *, loop=None):\n    \"\"\"Wrap concurrent.futures.Future object.\"\"\"\n    if isfuture(future):\n        return future\n    assert isinstance(future, concurrent.futures.Future), \\\n        f'concurrent.futures.Future is expected, got {future!r}'\n    if loop is None:\n        loop = events.get_event_loop()\n    new_future = loop.create_future()\n    _chain_future(future, new_future)\n    return new_future\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CFuture is needed for tests.\n    Future = _CFuture = _asyncio.Future\n", 390], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py": ["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\n\nfrom time import monotonic as _time\nfrom traceback import format_exc as _format_exc\nfrom _weakrefset import WeakSet\nfrom itertools import islice as _islice, count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_new_thread = _thread.start_new_thread\n_allocate_lock = _thread.allocate_lock\n_set_sentinel = _thread._set_sentinel\nget_ident = _thread.get_ident\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\n# Synchronization classes\n\nLock = _allocate_lock\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self._waiters = _deque()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(0):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        all_waiters = self._waiters\n        waiters_to_notify = _deque(_islice(all_waiters, n))\n        if not waiters_to_notify:\n            return\n        for waiter in waiters_to_notify:\n            waiter.release()\n            try:\n                all_waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    notifyAll = notify_all\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        with self._cond:\n            self._value += 1\n            self._cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        Semaphore.__init__(self, value)\n        self._initial_value = value\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        with self._cond:\n            if self._value >= self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += 1\n            self._cond.notify()\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def _reset_internal_locks(self):\n        # private!  called by Thread._reset_internal_locks by _after_fork()\n        self._cond.__init__(Lock())\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    isSet = is_set\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0 #0 filling, 1, draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count().__next__\n_counter() # Consume 0 so first non-main thread has id 1.\ndef _newname(template=\"Thread-%d\"):\n    return template % _counter()\n\n# Active thread administration\n_active_limbo_lock = _allocate_lock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()\n# to wait until all Python thread states get deleted:\n# see Thread._set_tstate_lock().\n_shutdown_locks_lock = _allocate_lock()\n_shutdown_locks = set()\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n    # Need to store a reference to sys.exc_info for printing\n    # out exceptions when a thread tries to use a global var. during interp.\n    # shutdown and thus raises an exception about trying to perform some\n    # operation on/with a NoneType\n    _exc_info = _sys.exc_info\n    # Keep sys.exc_clear too to clear the exception just before\n    # allowing .join() to return.\n    #XXX __exc_clear = _sys.exc_clear\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is the argument tuple for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        self._target = target\n        self._name = str(name or _newname())\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        self._tstate_lock = None\n        self._started = Event()\n        self._is_stopped = False\n        self._initialized = True\n        # sys.stderr is not stored in the class like\n        # sys.exc_info since it can be changed between instances\n        self._stderr = _sys.stderr\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _reset_internal_locks(self, is_alive):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        self._started._reset_internal_locks()\n        if is_alive:\n            self._set_tstate_lock()\n        else:\n            # The thread isn't alive after fork: it doesn't have a tstate\n            # anymore.\n            self._is_stopped = True\n            self._tstate_lock = None\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        self.is_alive() # easy way to get ._is_stopped set when appropriate\n        if self._is_stopped:\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self._bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    def _set_tstate_lock(self):\n        \"\"\"\n        Set a lock object which will be released by the interpreter when\n        the underlying thread state (see pystate.h) gets deleted.\n        \"\"\"\n        self._tstate_lock = _set_sentinel()\n        self._tstate_lock.acquire()\n\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _shutdown_locks.add(self._tstate_lock)\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self._set_tstate_lock()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except SystemExit:\n                pass\n            except:\n                # If sys.stderr is no more (most likely from interpreter\n                # shutdown) use self._stderr.  Otherwise still use sys (as in\n                # _sys) in case sys.stderr was redefined since the creation of\n                # self.\n                if _sys and _sys.stderr is not None:\n                    print(\"Exception in thread %s:\\n%s\" %\n                          (self.name, _format_exc()), file=_sys.stderr)\n                elif self._stderr is not None:\n                    # Do the best job possible w/o a huge amt. of code to\n                    # approximate a traceback (code ideas from\n                    # Lib/traceback.py)\n                    exc_type, exc_value, exc_tb = self._exc_info()\n                    try:\n                        print((\n                            \"Exception in thread \" + self.name +\n                            \" (most likely raised during interpreter shutdown):\"), file=self._stderr)\n                        print((\n                            \"Traceback (most recent call last):\"), file=self._stderr)\n                        while exc_tb:\n                            print((\n                                '  File \"%s\", line %s, in %s' %\n                                (exc_tb.tb_frame.f_code.co_filename,\n                                    exc_tb.tb_lineno,\n                                    exc_tb.tb_frame.f_code.co_name)), file=self._stderr)\n                            exc_tb = exc_tb.tb_next\n                        print((\"%s: %s\" % (exc_type, exc_value)), file=self._stderr)\n                        self._stderr.flush()\n                    # Make sure that exc_tb gets deleted since it is a memory\n                    # hog; deleting everything else is just for thoroughness\n                    finally:\n                        del exc_type, exc_value, exc_tb\n            finally:\n                # Prevent a race in\n                # test_threading.test_no_refcycle_through_target when\n                # the exception keeps the target alive past when we\n                # assert that it's dead.\n                #XXX self._exc_clear()\n                pass\n        finally:\n            with _active_limbo_lock:\n                try:\n                    # We don't call self._delete() because it also\n                    # grabs _active_limbo_lock.\n                    del _active[get_ident()]\n                except:\n                    pass\n\n    def _stop(self):\n        # After calling ._stop(), .is_alive() returns False and .join() returns\n        # immediately.  ._tstate_lock must be released before calling ._stop().\n        #\n        # Normal case:  C code at the end of the thread's life\n        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and\n        # that's detected by our ._wait_for_tstate_lock(), called by .join()\n        # and .is_alive().  Any number of threads _may_ call ._stop()\n        # simultaneously (for example, if multiple threads are blocked in\n        # .join() calls), and they're not serialized.  That's harmless -\n        # they'll just make redundant rebindings of ._is_stopped and\n        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the\n        # \"assert self._is_stopped\" in ._wait_for_tstate_lock() always works\n        # (the assert is executed only if ._tstate_lock is None).\n        #\n        # Special case:  _main_thread releases ._tstate_lock via this\n        # module's _shutdown() function.\n        lock = self._tstate_lock\n        if lock is not None:\n            assert not lock.locked()\n        self._is_stopped = True\n        self._tstate_lock = None\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _shutdown_locks.discard(lock)\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if timeout is None:\n            self._wait_for_tstate_lock()\n        else:\n            # the behavior of a negative timeout isn't documented, but\n            # historically .join(timeout=x) for x<0 has acted as if timeout=0\n            self._wait_for_tstate_lock(timeout=max(timeout, 0))\n\n    def _wait_for_tstate_lock(self, block=True, timeout=-1):\n        # Issue #18808: wait for the thread state to be gone.\n        # At the end of the thread's life, after all knowledge of the thread\n        # is removed from C data structures, C code releases our _tstate_lock.\n        # This method passes its arguments to _tstate_lock.acquire().\n        # If the lock is acquired, the C code is done, and self._stop() is\n        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.\n        lock = self._tstate_lock\n        if lock is None:  # already determined that the C code is done\n            assert self._is_stopped\n        elif lock.acquire(block, timeout):\n            lock.release()\n            self._stop()\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. The module function enumerate()\n        returns a list of all alive threads.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        if self._is_stopped or not self._started.is_set():\n            return False\n        self._wait_for_tstate_lock(False)\n        return not self._is_stopped\n\n    def isAlive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method is deprecated, use is_alive() instead.\n        \"\"\"\n        import warnings\n        warnings.warn('isAlive() is deprecated, use is_alive() instead',\n                      PendingDeprecationWarning, stacklevel=2)\n        return self.is_alive()\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        self.daemon = daemonic\n\n    def getName(self):\n        return self.name\n\n    def setName(self, name):\n        self.name = name\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._set_tstate_lock()\n        self._started.set()\n        self._set_ident()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"), daemon=True)\n\n        self._started.set()\n        self._set_ident()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n    def _stop(self):\n        pass\n\n    def is_alive(self):\n        assert not self._is_stopped and self._started.is_set()\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ncurrentThread = current_thread\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\nactiveCount = active_count\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure:  other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it.  We can't wait for C code to release\n    # the main thread's tstate_lock - that won't happen until the interpreter\n    # is nearly dead.  So we release it here.  Note that just calling _stop()\n    # isn't enough:  other threads may already be waiting on _tstate_lock.\n    if _main_thread._is_stopped:\n        # _shutdown() was already called\n        return\n\n    # Main thread\n    tlock = _main_thread._tstate_lock\n    # The main thread isn't finished yet, so its thread state lock can't have\n    # been released.\n    assert tlock is not None\n    assert tlock.locked()\n    tlock.release()\n    _main_thread._stop()\n\n    # Join all non-deamon threads\n    while True:\n        with _shutdown_locks_lock:\n            locks = list(_shutdown_locks)\n            _shutdown_locks.clear()\n\n        if not locks:\n            break\n\n        for lock in locks:\n            # mimick Thread.join()\n            lock.acquire()\n            lock.release()\n\n        # new threads can be spawned while we were waiting for the other\n        # threads to complete\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    return _main_thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    global _shutdown_locks_lock, _shutdown_locks\n    _active_limbo_lock = _allocate_lock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    # reset _shutdown() locks: threads re-register their _tstate_lock below\n    _shutdown_locks_lock = _allocate_lock()\n    _shutdown_locks = set()\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                thread._reset_internal_locks(True)\n                ident = get_ident()\n                thread._ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._reset_internal_locks(False)\n                thread._stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n", 1385], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py": ["'''A multi-producer, multi-consumer queue.'''\n\nimport threading\nfrom collections import deque\nfrom heapq import heappush, heappop\nfrom time import monotonic as time\ntry:\n    from _queue import SimpleQueue\nexcept ImportError:\n    SimpleQueue = None\n\n__all__ = ['Empty', 'Full', 'Queue', 'PriorityQueue', 'LifoQueue', 'SimpleQueue']\n\n\ntry:\n    from _queue import Empty\nexcept AttributeError:\n    class Empty(Exception):\n        'Exception raised by Queue.get(block=0)/get_nowait().'\n        pass\n\nclass Full(Exception):\n    'Exception raised by Queue.put(block=0)/put_nowait().'\n    pass\n\n\nclass Queue:\n    '''Create a queue object with a given maximum size.\n\n    If maxsize is <= 0, the queue size is infinite.\n    '''\n\n    def __init__(self, maxsize=0):\n        self.maxsize = maxsize\n        self._init(maxsize)\n\n        # mutex must be held whenever the queue is mutating.  All methods\n        # that acquire mutex must release it before returning.  mutex\n        # is shared between the three conditions, so acquiring and\n        # releasing the conditions also acquires and releases mutex.\n        self.mutex = threading.Lock()\n\n        # Notify not_empty whenever an item is added to the queue; a\n        # thread waiting to get is notified then.\n        self.not_empty = threading.Condition(self.mutex)\n\n        # Notify not_full whenever an item is removed from the queue;\n        # a thread waiting to put is notified then.\n        self.not_full = threading.Condition(self.mutex)\n\n        # Notify all_tasks_done whenever the number of unfinished tasks\n        # drops to zero; thread waiting to join() is notified to resume\n        self.all_tasks_done = threading.Condition(self.mutex)\n        self.unfinished_tasks = 0\n\n    def task_done(self):\n        '''Indicate that a formerly enqueued task is complete.\n\n        Used by Queue consumer threads.  For each get() used to fetch a task,\n        a subsequent call to task_done() tells the queue that the processing\n        on the task is complete.\n\n        If a join() is currently blocking, it will resume when all items\n        have been processed (meaning that a task_done() call was received\n        for every item that had been put() into the queue).\n\n        Raises a ValueError if called more times than there were items\n        placed in the queue.\n        '''\n        with self.all_tasks_done:\n            unfinished = self.unfinished_tasks - 1\n            if unfinished <= 0:\n                if unfinished < 0:\n                    raise ValueError('task_done() called too many times')\n                self.all_tasks_done.notify_all()\n            self.unfinished_tasks = unfinished\n\n    def join(self):\n        '''Blocks until all items in the Queue have been gotten and processed.\n\n        The count of unfinished tasks goes up whenever an item is added to the\n        queue. The count goes down whenever a consumer thread calls task_done()\n        to indicate the item was retrieved and all work on it is complete.\n\n        When the count of unfinished tasks drops to zero, join() unblocks.\n        '''\n        with self.all_tasks_done:\n            while self.unfinished_tasks:\n                self.all_tasks_done.wait()\n\n    def qsize(self):\n        '''Return the approximate size of the queue (not reliable!).'''\n        with self.mutex:\n            return self._qsize()\n\n    def empty(self):\n        '''Return True if the queue is empty, False otherwise (not reliable!).\n\n        This method is likely to be removed at some point.  Use qsize() == 0\n        as a direct substitute, but be aware that either approach risks a race\n        condition where a queue can grow before the result of empty() or\n        qsize() can be used.\n\n        To create code that needs to wait for all queued tasks to be\n        completed, the preferred technique is to use the join() method.\n        '''\n        with self.mutex:\n            return not self._qsize()\n\n    def full(self):\n        '''Return True if the queue is full, False otherwise (not reliable!).\n\n        This method is likely to be removed at some point.  Use qsize() >= n\n        as a direct substitute, but be aware that either approach risks a race\n        condition where a queue can shrink before the result of full() or\n        qsize() can be used.\n        '''\n        with self.mutex:\n            return 0 < self.maxsize <= self._qsize()\n\n    def put(self, item, block=True, timeout=None):\n        '''Put an item into the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until a free slot is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Full exception if no free slot was available within that time.\n        Otherwise ('block' is false), put an item on the queue if a free slot\n        is immediately available, else raise the Full exception ('timeout'\n        is ignored in that case).\n        '''\n        with self.not_full:\n            if self.maxsize > 0:\n                if not block:\n                    if self._qsize() >= self.maxsize:\n                        raise Full\n                elif timeout is None:\n                    while self._qsize() >= self.maxsize:\n                        self.not_full.wait()\n                elif timeout < 0:\n                    raise ValueError(\"'timeout' must be a non-negative number\")\n                else:\n                    endtime = time() + timeout\n                    while self._qsize() >= self.maxsize:\n                        remaining = endtime - time()\n                        if remaining <= 0.0:\n                            raise Full\n                        self.not_full.wait(remaining)\n            self._put(item)\n            self.unfinished_tasks += 1\n            self.not_empty.notify()\n\n    def get(self, block=True, timeout=None):\n        '''Remove and return an item from the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until an item is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Empty exception if no item was available within that time.\n        Otherwise ('block' is false), return an item if one is immediately\n        available, else raise the Empty exception ('timeout' is ignored\n        in that case).\n        '''\n        with self.not_empty:\n            if not block:\n                if not self._qsize():\n                    raise Empty\n            elif timeout is None:\n                while not self._qsize():\n                    self.not_empty.wait()\n            elif timeout < 0:\n                raise ValueError(\"'timeout' must be a non-negative number\")\n            else:\n                endtime = time() + timeout\n                while not self._qsize():\n                    remaining = endtime - time()\n                    if remaining <= 0.0:\n                        raise Empty\n                    self.not_empty.wait(remaining)\n            item = self._get()\n            self.not_full.notify()\n            return item\n\n    def put_nowait(self, item):\n        '''Put an item into the queue without blocking.\n\n        Only enqueue the item if a free slot is immediately available.\n        Otherwise raise the Full exception.\n        '''\n        return self.put(item, block=False)\n\n    def get_nowait(self):\n        '''Remove and return an item from the queue without blocking.\n\n        Only get an item if one is immediately available. Otherwise\n        raise the Empty exception.\n        '''\n        return self.get(block=False)\n\n    # Override these methods to implement other queue organizations\n    # (e.g. stack or priority queue).\n    # These will only be called with appropriate locks held\n\n    # Initialize the queue representation\n    def _init(self, maxsize):\n        self.queue = deque()\n\n    def _qsize(self):\n        return len(self.queue)\n\n    # Put a new item in the queue\n    def _put(self, item):\n        self.queue.append(item)\n\n    # Get an item from the queue\n    def _get(self):\n        return self.queue.popleft()\n\n\nclass PriorityQueue(Queue):\n    '''Variant of Queue that retrieves open entries in priority order (lowest first).\n\n    Entries are typically tuples of the form:  (priority number, data).\n    '''\n\n    def _init(self, maxsize):\n        self.queue = []\n\n    def _qsize(self):\n        return len(self.queue)\n\n    def _put(self, item):\n        heappush(self.queue, item)\n\n    def _get(self):\n        return heappop(self.queue)\n\n\nclass LifoQueue(Queue):\n    '''Variant of Queue that retrieves most recently added entries first.'''\n\n    def _init(self, maxsize):\n        self.queue = []\n\n    def _qsize(self):\n        return len(self.queue)\n\n    def _put(self, item):\n        self.queue.append(item)\n\n    def _get(self):\n        return self.queue.pop()\n\n\nclass _PySimpleQueue:\n    '''Simple, unbounded FIFO queue.\n\n    This pure Python implementation is not reentrant.\n    '''\n    # Note: while this pure Python version provides fairness\n    # (by using a threading.Semaphore which is itself fair, being based\n    #  on threading.Condition), fairness is not part of the API contract.\n    # This allows the C version to use a different implementation.\n\n    def __init__(self):\n        self._queue = deque()\n        self._count = threading.Semaphore(0)\n\n    def put(self, item, block=True, timeout=None):\n        '''Put the item on the queue.\n\n        The optional 'block' and 'timeout' arguments are ignored, as this method\n        never blocks.  They are provided for compatibility with the Queue class.\n        '''\n        self._queue.append(item)\n        self._count.release()\n\n    def get(self, block=True, timeout=None):\n        '''Remove and return an item from the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until an item is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Empty exception if no item was available within that time.\n        Otherwise ('block' is false), return an item if one is immediately\n        available, else raise the Empty exception ('timeout' is ignored\n        in that case).\n        '''\n        if timeout is not None and timeout < 0:\n            raise ValueError(\"'timeout' must be a non-negative number\")\n        if not self._count.acquire(block, timeout):\n            raise Empty\n        return self._queue.popleft()\n\n    def put_nowait(self, item):\n        '''Put an item into the queue without blocking.\n\n        This is exactly equivalent to `put(item)` and is only provided\n        for compatibility with the Queue class.\n        '''\n        return self.put(item, block=False)\n\n    def get_nowait(self):\n        '''Remove and return an item from the queue without blocking.\n\n        Only get an item if one is immediately available. Otherwise\n        raise the Empty exception.\n        '''\n        return self.get(block=False)\n\n    def empty(self):\n        '''Return True if the queue is empty, False otherwise (not reliable!).'''\n        return len(self._queue) == 0\n\n    def qsize(self):\n        '''Return the approximate size of the queue (not reliable!).'''\n        return len(self._queue)\n\n\nif SimpleQueue is None:\n    SimpleQueue = _PySimpleQueue\n", 321], "/Users/shopbox/.vscode/extensions/ms-python.python-2023.4.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py": ["# License: EPL\nimport os\nimport re\nimport sys\nfrom _pydev_bundle._pydev_saved_modules import threading\nfrom _pydevd_bundle.pydevd_constants import get_global_debugger, IS_WINDOWS, IS_JYTHON, get_current_thread_id, \\\n    sorted_dict_repr, set_global_debugger, DebugInfoHolder\nfrom _pydev_bundle import pydev_log\nfrom contextlib import contextmanager\nfrom _pydevd_bundle import pydevd_constants, pydevd_defaults\nfrom _pydevd_bundle.pydevd_defaults import PydevdCustomization\nimport ast\n\ntry:\n    from pathlib import Path\nexcept ImportError:\n    Path = None\n\n#===============================================================================\n# Things that are dependent on having the pydevd debugger\n#===============================================================================\n\npydev_src_dir = os.path.dirname(os.path.dirname(__file__))\n\n_arg_patch = threading.local()\n\n\n@contextmanager\ndef skip_subprocess_arg_patch():\n    _arg_patch.apply_arg_patching = False\n    try:\n        yield\n    finally:\n        _arg_patch.apply_arg_patching = True\n\n\ndef _get_apply_arg_patching():\n    return getattr(_arg_patch, 'apply_arg_patching', True)\n\n\ndef _get_setup_updated_with_protocol_and_ppid(setup, is_exec=False):\n    if setup is None:\n        setup = {}\n    setup = setup.copy()\n    # Discard anything related to the protocol (we'll set the the protocol based on the one\n    # currently set).\n    setup.pop(pydevd_constants.ARGUMENT_HTTP_JSON_PROTOCOL, None)\n    setup.pop(pydevd_constants.ARGUMENT_JSON_PROTOCOL, None)\n    setup.pop(pydevd_constants.ARGUMENT_QUOTED_LINE_PROTOCOL, None)\n\n    if not is_exec:\n        # i.e.: The ppid for the subprocess is the current pid.\n        # If it's an exec, keep it what it was.\n        setup[pydevd_constants.ARGUMENT_PPID] = os.getpid()\n\n    protocol = pydevd_constants.get_protocol()\n    if protocol == pydevd_constants.HTTP_JSON_PROTOCOL:\n        setup[pydevd_constants.ARGUMENT_HTTP_JSON_PROTOCOL] = True\n\n    elif protocol == pydevd_constants.JSON_PROTOCOL:\n        setup[pydevd_constants.ARGUMENT_JSON_PROTOCOL] = True\n\n    elif protocol == pydevd_constants.QUOTED_LINE_PROTOCOL:\n        setup[pydevd_constants.ARGUMENT_QUOTED_LINE_PROTOCOL] = True\n\n    elif protocol == pydevd_constants.HTTP_PROTOCOL:\n        setup[pydevd_constants.ARGUMENT_HTTP_PROTOCOL] = True\n\n    else:\n        pydev_log.debug('Unexpected protocol: %s', protocol)\n\n    mode = pydevd_defaults.PydevdCustomization.DEBUG_MODE\n    if mode:\n        setup['debug-mode'] = mode\n\n    preimport = pydevd_defaults.PydevdCustomization.PREIMPORT\n    if preimport:\n        setup['preimport'] = preimport\n\n    if DebugInfoHolder.PYDEVD_DEBUG_FILE:\n        setup['log-file'] = DebugInfoHolder.PYDEVD_DEBUG_FILE\n\n    if DebugInfoHolder.DEBUG_TRACE_LEVEL:\n        setup['log-level'] = DebugInfoHolder.DEBUG_TRACE_LEVEL\n\n    return setup\n\n\nclass _LastFutureImportFinder(ast.NodeVisitor):\n\n    def __init__(self):\n        self.last_future_import_found = None\n\n    def visit_ImportFrom(self, node):\n        if node.module == '__future__':\n            self.last_future_import_found = node\n\n\ndef _get_offset_from_line_col(code, line, col):\n    offset = 0\n    for i, line_contents in enumerate(code.splitlines(True)):\n        if i == line:\n            offset += col\n            return offset\n        else:\n            offset += len(line_contents)\n\n    return -1\n\n\ndef _separate_future_imports(code):\n    '''\n    :param code:\n        The code from where we want to get the __future__ imports (note that it's possible that\n        there's no such entry).\n\n    :return tuple(str, str):\n        The return is a tuple(future_import, code).\n\n        If the future import is not available a return such as ('', code) is given, otherwise, the\n        future import will end with a ';' (so that it can be put right before the pydevd attach\n        code).\n    '''\n    try:\n        node = ast.parse(code, '<string>', 'exec')\n        visitor = _LastFutureImportFinder()\n        visitor.visit(node)\n\n        if visitor.last_future_import_found is None:\n            return '', code\n\n        node = visitor.last_future_import_found\n        offset = -1\n        if hasattr(node, 'end_lineno') and hasattr(node, 'end_col_offset'):\n            # Python 3.8 onwards has these (so, use when possible).\n            line, col = node.end_lineno, node.end_col_offset\n            offset = _get_offset_from_line_col(code, line - 1, col)  # ast lines are 1-based, make it 0-based.\n\n        else:\n            # end line/col not available, let's just find the offset and then search\n            # for the alias from there.\n            line, col = node.lineno, node.col_offset\n            offset = _get_offset_from_line_col(code, line - 1, col)  # ast lines are 1-based, make it 0-based.\n            if offset >= 0 and node.names:\n                from_future_import_name = node.names[-1].name\n                i = code.find(from_future_import_name, offset)\n                if i < 0:\n                    offset = -1\n                else:\n                    offset = i + len(from_future_import_name)\n\n        if offset >= 0:\n            for i in range(offset, len(code)):\n                if code[i] in (' ', '\\t', ';', ')', '\\n'):\n                    offset += 1\n                else:\n                    break\n\n            future_import = code[:offset]\n            code_remainder = code[offset:]\n\n            # Now, put '\\n' lines back into the code remainder (we had to search for\n            # `\\n)`, but in case we just got the `\\n`, it should be at the remainder,\n            # not at the future import.\n            while future_import.endswith('\\n'):\n                future_import = future_import[:-1]\n                code_remainder = '\\n' + code_remainder\n\n            if not future_import.endswith(';'):\n                future_import += ';'\n            return future_import, code_remainder\n\n        # This shouldn't happen...\n        pydev_log.info('Unable to find line %s in code:\\n%r', line, code)\n        return '', code\n\n    except:\n        pydev_log.exception('Error getting from __future__ imports from: %r', code)\n        return '', code\n\n\ndef _get_python_c_args(host, port, code, args, setup):\n    setup = _get_setup_updated_with_protocol_and_ppid(setup)\n\n    # i.e.: We want to make the repr sorted so that it works in tests.\n    setup_repr = setup if setup is None else (sorted_dict_repr(setup))\n\n    future_imports = ''\n    if '__future__' in code:\n        # If the code has a __future__ import, we need to be able to strip the __future__\n        # imports from the code and add them to the start of our code snippet.\n        future_imports, code = _separate_future_imports(code)\n\n    return (\"%simport sys; sys.path.insert(0, r'%s'); import pydevd; pydevd.config(%r, %r); \"\n            \"pydevd.settrace(host=%r, port=%s, suspend=False, trace_only_current_thread=False, patch_multiprocessing=True, access_token=%r, client_access_token=%r, __setup_holder__=%s); \"\n            \"%s\"\n            ) % (\n               future_imports,\n               pydev_src_dir,\n               pydevd_constants.get_protocol(),\n               PydevdCustomization.DEBUG_MODE,\n               host,\n               port,\n               setup.get('access-token'),\n               setup.get('client-access-token'),\n               setup_repr,\n               code)\n\n\ndef _get_host_port():\n    import pydevd\n    host, port = pydevd.dispatch()\n    return host, port\n\n\ndef _is_managed_arg(arg):\n    pydevd_py = _get_str_type_compatible(arg, 'pydevd.py')\n    if arg.endswith(pydevd_py):\n        return True\n    return False\n\n\ndef _on_forked_process(setup_tracing=True):\n    pydevd_constants.after_fork()\n    pydev_log.initialize_debug_stream(reinitialize=True)\n\n    if setup_tracing:\n        pydev_log.debug('pydevd on forked process: %s', os.getpid())\n\n    import pydevd\n    pydevd.threadingCurrentThread().__pydevd_main_thread = True\n    pydevd.settrace_forked(setup_tracing=setup_tracing)\n\n\ndef _on_set_trace_for_new_thread(global_debugger):\n    if global_debugger is not None:\n        global_debugger.enable_tracing()\n\n\ndef _get_str_type_compatible(s, args):\n    '''\n    This method converts `args` to byte/unicode based on the `s' type.\n    '''\n    if isinstance(args, (list, tuple)):\n        ret = []\n        for arg in args:\n            if type(s) == type(arg):\n                ret.append(arg)\n            else:\n                if isinstance(s, bytes):\n                    ret.append(arg.encode('utf-8'))\n                else:\n                    ret.append(arg.decode('utf-8'))\n        return ret\n    else:\n        if type(s) == type(args):\n            return args\n        else:\n            if isinstance(s, bytes):\n                return args.encode('utf-8')\n            else:\n                return args.decode('utf-8')\n\n\n#===============================================================================\n# Things related to monkey-patching\n#===============================================================================\ndef is_python(path):\n    single_quote, double_quote = _get_str_type_compatible(path, [\"'\", '\"'])\n\n    if path.endswith(single_quote) or path.endswith(double_quote):\n        path = path[1:len(path) - 1]\n    filename = os.path.basename(path).lower()\n    for name in _get_str_type_compatible(filename, ['python', 'jython', 'pypy']):\n        if filename.find(name) != -1:\n            return True\n\n    return False\n\n\nclass InvalidTypeInArgsException(Exception):\n    pass\n\n\ndef remove_quotes_from_args(args):\n    if sys.platform == \"win32\":\n        new_args = []\n\n        for x in args:\n            if Path is not None and isinstance(x, Path):\n                x = str(x)\n            else:\n                if not isinstance(x, (bytes, str)):\n                    raise InvalidTypeInArgsException(str(type(x)))\n\n            double_quote, two_double_quotes = _get_str_type_compatible(x, ['\"', '\"\"'])\n\n            if x != two_double_quotes:\n                if len(x) > 1 and x.startswith(double_quote) and x.endswith(double_quote):\n                    x = x[1:-1]\n\n            new_args.append(x)\n        return new_args\n    else:\n        new_args = []\n        for x in args:\n            if Path is not None and isinstance(x, Path):\n                x = x.as_posix()\n            else:\n                if not isinstance(x, (bytes, str)):\n                    raise InvalidTypeInArgsException(str(type(x)))\n            new_args.append(x)\n\n        return new_args\n\n\ndef quote_arg_win32(arg):\n    fix_type = lambda x: _get_str_type_compatible(arg, x)\n\n    # See if we need to quote at all - empty strings need quoting, as do strings\n    # with whitespace or quotes in them. Backslashes do not need quoting.\n    if arg and not set(arg).intersection(fix_type(' \"\\t\\n\\v')):\n        return arg\n\n    # Per https://docs.microsoft.com/en-us/windows/desktop/api/shellapi/nf-shellapi-commandlinetoargvw,\n    # the standard way to interpret arguments in double quotes is as follows:\n    #\n    #       2N backslashes followed by a quotation mark produce N backslashes followed by\n    #       begin/end quote. This does not become part of the parsed argument, but toggles\n    #       the \"in quotes\" mode.\n    #\n    #       2N+1 backslashes followed by a quotation mark again produce N backslashes followed\n    #       by a quotation mark literal (\"). This does not toggle the \"in quotes\" mode.\n    #\n    #       N backslashes not followed by a quotation mark simply produce N backslashes.\n    #\n    # This code needs to do the reverse transformation, thus:\n    #\n    #       N backslashes followed by \" produce 2N+1 backslashes followed by \"\n    #\n    #       N backslashes at the end (i.e. where the closing \" goes) produce 2N backslashes.\n    #\n    #       N backslashes in any other position remain as is.\n\n    arg = re.sub(fix_type(r'(\\\\*)\\\"'), fix_type(r'\\1\\1\\\\\"'), arg)\n    arg = re.sub(fix_type(r'(\\\\*)$'), fix_type(r'\\1\\1'), arg)\n    return fix_type('\"') + arg + fix_type('\"')\n\n\ndef quote_args(args):\n    if sys.platform == \"win32\":\n        return list(map(quote_arg_win32, args))\n    else:\n        return args\n\n\ndef patch_args(args, is_exec=False):\n    '''\n    :param list args:\n        Arguments to patch.\n\n    :param bool is_exec:\n        If it's an exec, the current process will be replaced (this means we have\n        to keep the same ppid).\n    '''\n    try:\n        pydev_log.debug(\"Patching args: %s\", args)\n        original_args = args\n        try:\n            unquoted_args = remove_quotes_from_args(args)\n        except InvalidTypeInArgsException as e:\n            pydev_log.info('Unable to monkey-patch subprocess arguments because a type found in the args is invalid: %s', e)\n            return original_args\n\n        # Internally we should reference original_args (if we want to return them) or unquoted_args\n        # to add to the list which will be then quoted in the end.\n        del args\n\n        from pydevd import SetupHolder\n        if not unquoted_args:\n            return original_args\n\n        if not is_python(unquoted_args[0]):\n            pydev_log.debug(\"Process is not python, returning.\")\n            return original_args\n\n        # Note: we create a copy as string to help with analyzing the arguments, but\n        # the final list should have items from the unquoted_args as they were initially.\n        args_as_str = _get_str_type_compatible('', unquoted_args)\n\n        params_with_value_in_separate_arg = (\n            '--check-hash-based-pycs',\n            '--jit'  # pypy option\n        )\n\n        # All short switches may be combined together. The ones below require a value and the\n        # value itself may be embedded in the arg.\n        #\n        # i.e.: Python accepts things as:\n        #\n        # python -OQold -qmtest\n        #\n        # Which is the same as:\n        #\n        # python -O -Q old -q -m test\n        #\n        # or even:\n        #\n        # python -OQold \"-vcimport sys;print(sys)\"\n        #\n        # Which is the same as:\n        #\n        # python -O -Q old -v -c \"import sys;print(sys)\"\n\n        params_with_combinable_arg = set(('W', 'X', 'Q', 'c', 'm'))\n\n        module_name = None\n        before_module_flag = ''\n        module_name_i_start = -1\n        module_name_i_end = -1\n\n        code = None\n        code_i = -1\n        code_i_end = -1\n        code_flag = ''\n\n        filename = None\n        filename_i = -1\n\n        ignore_next = True  # start ignoring the first (the first entry is the python executable)\n        for i, arg_as_str in enumerate(args_as_str):\n            if ignore_next:\n                ignore_next = False\n                continue\n\n            if arg_as_str.startswith('-'):\n                if arg_as_str == '-':\n                    # Contents will be read from the stdin. This is not currently handled.\n                    pydev_log.debug('Unable to fix arguments to attach debugger on subprocess when reading from stdin (\"python ... -\").')\n                    return original_args\n\n                if arg_as_str.startswith(params_with_value_in_separate_arg):\n                    if arg_as_str in params_with_value_in_separate_arg:\n                        ignore_next = True\n                    continue\n\n                break_out = False\n                for j, c in enumerate(arg_as_str):\n\n                    # i.e.: Python supports -X faulthandler as well as -Xfaulthandler\n                    # (in one case we have to ignore the next and in the other we don't\n                    # have to ignore it).\n                    if c in params_with_combinable_arg:\n                        remainder = arg_as_str[j + 1:]\n                        if not remainder:\n                            ignore_next = True\n\n                        if c == 'm':\n                            # i.e.: Something as\n                            # python -qm test\n                            # python -m test\n                            # python -qmtest\n                            before_module_flag = arg_as_str[:j]  # before_module_flag would then be \"-q\"\n                            if before_module_flag == '-':\n                                before_module_flag = ''\n                            module_name_i_start = i\n                            if not remainder:\n                                module_name = unquoted_args[i + 1]\n                                module_name_i_end = i + 1\n                            else:\n                                # i.e.: python -qmtest should provide 'test' as the module_name\n                                module_name = unquoted_args[i][j + 1:]\n                                module_name_i_end = module_name_i_start\n                            break_out = True\n                            break\n\n                        elif c == 'c':\n                            # i.e.: Something as\n                            # python -qc \"import sys\"\n                            # python -c \"import sys\"\n                            # python \"-qcimport sys\"\n                            code_flag = arg_as_str[:j + 1]  # code_flag would then be \"-qc\"\n\n                            if not remainder:\n                                # arg_as_str is something as \"-qc\", \"import sys\"\n                                code = unquoted_args[i + 1]\n                                code_i_end = i + 2\n                            else:\n                                # if arg_as_str is something as \"-qcimport sys\"\n                                code = remainder  # code would be \"import sys\"\n                                code_i_end = i + 1\n                            code_i = i\n                            break_out = True\n                            break\n\n                        else:\n                            break\n\n                if break_out:\n                    break\n\n            else:\n                # It doesn't start with '-' and we didn't ignore this entry:\n                # this means that this is the file to be executed.\n                filename = unquoted_args[i]\n\n                # Note that the filename is not validated here.\n                # There are cases where even a .exe is valid (xonsh.exe):\n                # https://github.com/microsoft/debugpy/issues/945\n                # So, we should support whatever runpy.run_path\n                # supports in this case.\n\n                filename_i = i\n\n                if _is_managed_arg(filename):  # no need to add pydevd twice\n                    pydev_log.debug('Skipped monkey-patching as pydevd.py is in args already.')\n                    return original_args\n\n                break\n        else:\n            # We didn't find the filename (something is unexpected).\n            pydev_log.debug('Unable to fix arguments to attach debugger on subprocess (filename not found).')\n            return original_args\n\n        if code_i != -1:\n            host, port = _get_host_port()\n\n            if port is not None:\n                new_args = []\n                new_args.extend(unquoted_args[:code_i])\n                new_args.append(code_flag)\n                new_args.append(_get_python_c_args(host, port, code, unquoted_args, SetupHolder.setup))\n                new_args.extend(unquoted_args[code_i_end:])\n\n                return quote_args(new_args)\n\n        first_non_vm_index = max(filename_i, module_name_i_start)\n        if first_non_vm_index == -1:\n            pydev_log.debug('Unable to fix arguments to attach debugger on subprocess (could not resolve filename nor module name).')\n            return original_args\n\n        # Original args should be something as:\n        # ['X:\\\\pysrc\\\\pydevd.py', '--multiprocess', '--print-in-debugger-startup',\n        #  '--vm_type', 'python', '--client', '127.0.0.1', '--port', '56352', '--file', 'x:\\\\snippet1.py']\n        from _pydevd_bundle.pydevd_command_line_handling import setup_to_argv\n        new_args = []\n        new_args.extend(unquoted_args[:first_non_vm_index])\n        if before_module_flag:\n            new_args.append(before_module_flag)\n\n        add_module_at = len(new_args) + 1\n\n        new_args.extend(setup_to_argv(\n            _get_setup_updated_with_protocol_and_ppid(SetupHolder.setup, is_exec=is_exec),\n            skip_names=set(('module', 'cmd-line'))\n        ))\n        new_args.append('--file')\n\n        if module_name is not None:\n            assert module_name_i_start != -1\n            assert module_name_i_end != -1\n            # Always after 'pydevd' (i.e.: pydevd \"--module\" --multiprocess ...)\n            new_args.insert(add_module_at, '--module')\n            new_args.append(module_name)\n            new_args.extend(unquoted_args[module_name_i_end + 1:])\n\n        elif filename is not None:\n            assert filename_i != -1\n            new_args.append(filename)\n            new_args.extend(unquoted_args[filename_i + 1:])\n\n        else:\n            raise AssertionError('Internal error (unexpected condition)')\n\n        return quote_args(new_args)\n    except:\n        pydev_log.exception('Error patching args (debugger not attached to subprocess).')\n        return original_args\n\n\ndef str_to_args_windows(args):\n    # See https://docs.microsoft.com/en-us/cpp/c-language/parsing-c-command-line-arguments.\n    #\n    # Implemetation ported from DebugPlugin.parseArgumentsWindows:\n    # https://github.com/eclipse/eclipse.platform.debug/blob/master/org.eclipse.debug.core/core/org/eclipse/debug/core/DebugPlugin.java\n\n    result = []\n\n    DEFAULT = 0\n    ARG = 1\n    IN_DOUBLE_QUOTE = 2\n\n    state = DEFAULT\n    backslashes = 0\n    buf = ''\n\n    args_len = len(args)\n    for i in range(args_len):\n        ch = args[i]\n        if (ch == '\\\\'):\n            backslashes += 1\n            continue\n        elif (backslashes != 0):\n            if ch == '\"':\n                while backslashes >= 2:\n                    backslashes -= 2\n                    buf += '\\\\'\n                if (backslashes == 1):\n                    if (state == DEFAULT):\n                        state = ARG\n\n                    buf += '\"'\n                    backslashes = 0\n                    continue\n                # else fall through to switch\n            else:\n                # false alarm, treat passed backslashes literally...\n                if (state == DEFAULT):\n                    state = ARG\n\n                while backslashes > 0:\n                    backslashes -= 1\n                    buf += '\\\\'\n                # fall through to switch\n        if ch in (' ', '\\t'):\n            if (state == DEFAULT):\n                # skip\n                continue\n            elif (state == ARG):\n                state = DEFAULT\n                result.append(buf)\n                buf = ''\n                continue\n\n        if state in (DEFAULT, ARG):\n            if ch == '\"':\n                state = IN_DOUBLE_QUOTE\n            else:\n                state = ARG\n                buf += ch\n\n        elif state == IN_DOUBLE_QUOTE:\n            if ch == '\"':\n                if (i + 1 < args_len and args[i + 1] == '\"'):\n                    # Undocumented feature in Windows:\n                    # Two consecutive double quotes inside a double-quoted argument are interpreted as\n                    # a single double quote.\n                    buf += '\"'\n                    i += 1\n                else:\n                    state = ARG\n            else:\n                buf += ch\n\n        else:\n            raise RuntimeError('Illegal condition')\n\n    if len(buf) > 0 or state != DEFAULT:\n        result.append(buf)\n\n    return result\n\n\ndef patch_arg_str_win(arg_str):\n    args = str_to_args_windows(arg_str)\n    # Fix https://youtrack.jetbrains.com/issue/PY-9767 (args may be empty)\n    if not args or not is_python(args[0]):\n        return arg_str\n    arg_str = ' '.join(patch_args(args))\n    pydev_log.debug(\"New args: %s\", arg_str)\n    return arg_str\n\n\ndef monkey_patch_module(module, funcname, create_func):\n    if hasattr(module, funcname):\n        original_name = 'original_' + funcname\n        if not hasattr(module, original_name):\n            setattr(module, original_name, getattr(module, funcname))\n            setattr(module, funcname, create_func(original_name))\n\n\ndef monkey_patch_os(funcname, create_func):\n    monkey_patch_module(os, funcname, create_func)\n\n\ndef warn_multiproc():\n    pass  # TODO: Provide logging as messages to the IDE.\n    # pydev_log.error_once(\n    #     \"pydev debugger: New process is launching (breakpoints won't work in the new process).\\n\"\n    #     \"pydev debugger: To debug that process please enable 'Attach to subprocess automatically while debugging?' option in the debugger settings.\\n\")\n    #\n\n\ndef create_warn_multiproc(original_name):\n\n    def new_warn_multiproc(*args, **kwargs):\n        import os\n\n        warn_multiproc()\n\n        return getattr(os, original_name)(*args, **kwargs)\n\n    return new_warn_multiproc\n\n\ndef create_execl(original_name):\n\n    def new_execl(path, *args):\n        \"\"\"\n        os.execl(path, arg0, arg1, ...)\n        os.execle(path, arg0, arg1, ..., env)\n        os.execlp(file, arg0, arg1, ...)\n        os.execlpe(file, arg0, arg1, ..., env)\n        \"\"\"\n        if _get_apply_arg_patching():\n            args = patch_args(args, is_exec=True)\n            send_process_created_message()\n            send_process_about_to_be_replaced()\n\n        return getattr(os, original_name)(path, *args)\n\n    return new_execl\n\n\ndef create_execv(original_name):\n\n    def new_execv(path, args):\n        \"\"\"\n        os.execv(path, args)\n        os.execvp(file, args)\n        \"\"\"\n        if _get_apply_arg_patching():\n            args = patch_args(args, is_exec=True)\n            send_process_created_message()\n            send_process_about_to_be_replaced()\n\n        return getattr(os, original_name)(path, args)\n\n    return new_execv\n\n\ndef create_execve(original_name):\n    \"\"\"\n    os.execve(path, args, env)\n    os.execvpe(file, args, env)\n    \"\"\"\n\n    def new_execve(path, args, env):\n        if _get_apply_arg_patching():\n            args = patch_args(args, is_exec=True)\n            send_process_created_message()\n            send_process_about_to_be_replaced()\n\n        return getattr(os, original_name)(path, args, env)\n\n    return new_execve\n\n\ndef create_spawnl(original_name):\n\n    def new_spawnl(mode, path, *args):\n        \"\"\"\n        os.spawnl(mode, path, arg0, arg1, ...)\n        os.spawnlp(mode, file, arg0, arg1, ...)\n        \"\"\"\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(os, original_name)(mode, path, *args)\n\n    return new_spawnl\n\n\ndef create_spawnv(original_name):\n\n    def new_spawnv(mode, path, args):\n        \"\"\"\n        os.spawnv(mode, path, args)\n        os.spawnvp(mode, file, args)\n        \"\"\"\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(os, original_name)(mode, path, args)\n\n    return new_spawnv\n\n\ndef create_spawnve(original_name):\n    \"\"\"\n    os.spawnve(mode, path, args, env)\n    os.spawnvpe(mode, file, args, env)\n    \"\"\"\n\n    def new_spawnve(mode, path, args, env):\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(os, original_name)(mode, path, args, env)\n\n    return new_spawnve\n\n\ndef create_posix_spawn(original_name):\n    \"\"\"\n    os.posix_spawn(executable, args, env, **kwargs)\n    \"\"\"\n\n    def new_posix_spawn(executable, args, env, **kwargs):\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(os, original_name)(executable, args, env, **kwargs)\n\n    return new_posix_spawn\n\n\ndef create_fork_exec(original_name):\n    \"\"\"\n    _posixsubprocess.fork_exec(args, executable_list, close_fds, ... (13 more))\n    \"\"\"\n\n    def new_fork_exec(args, *other_args):\n        import _posixsubprocess  # @UnresolvedImport\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(_posixsubprocess, original_name)(args, *other_args)\n\n    return new_fork_exec\n\n\ndef create_warn_fork_exec(original_name):\n    \"\"\"\n    _posixsubprocess.fork_exec(args, executable_list, close_fds, ... (13 more))\n    \"\"\"\n\n    def new_warn_fork_exec(*args):\n        try:\n            import _posixsubprocess\n            warn_multiproc()\n            return getattr(_posixsubprocess, original_name)(*args)\n        except:\n            pass\n\n    return new_warn_fork_exec\n\n\ndef create_subprocess_fork_exec(original_name):\n    \"\"\"\n    subprocess._fork_exec(args, executable_list, close_fds, ... (13 more))\n    \"\"\"\n\n    def new_fork_exec(args, *other_args):\n        import subprocess\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(subprocess, original_name)(args, *other_args)\n\n    return new_fork_exec\n\n\ndef create_subprocess_warn_fork_exec(original_name):\n    \"\"\"\n    subprocess._fork_exec(args, executable_list, close_fds, ... (13 more))\n    \"\"\"\n\n    def new_warn_fork_exec(*args):\n        try:\n            import subprocess\n            warn_multiproc()\n            return getattr(subprocess, original_name)(*args)\n        except:\n            pass\n\n    return new_warn_fork_exec\n\n\ndef create_CreateProcess(original_name):\n    \"\"\"\n    CreateProcess(*args, **kwargs)\n    \"\"\"\n\n    def new_CreateProcess(app_name, cmd_line, *args):\n        try:\n            import _subprocess\n        except ImportError:\n            import _winapi as _subprocess\n\n        if _get_apply_arg_patching():\n            cmd_line = patch_arg_str_win(cmd_line)\n            send_process_created_message()\n\n        return getattr(_subprocess, original_name)(app_name, cmd_line, *args)\n\n    return new_CreateProcess\n\n\ndef create_CreateProcessWarnMultiproc(original_name):\n    \"\"\"\n    CreateProcess(*args, **kwargs)\n    \"\"\"\n\n    def new_CreateProcess(*args):\n        try:\n            import _subprocess\n        except ImportError:\n            import _winapi as _subprocess\n        warn_multiproc()\n        return getattr(_subprocess, original_name)(*args)\n\n    return new_CreateProcess\n\n\ndef create_fork(original_name):\n\n    def new_fork():\n        # A simple fork will result in a new python process\n        is_new_python_process = True\n        frame = sys._getframe()\n\n        apply_arg_patch = _get_apply_arg_patching()\n\n        is_subprocess_fork = False\n        while frame is not None:\n            if frame.f_code.co_name == '_execute_child' and 'subprocess' in frame.f_code.co_filename:\n                is_subprocess_fork = True\n                # If we're actually in subprocess.Popen creating a child, it may\n                # result in something which is not a Python process, (so, we\n                # don't want to connect with it in the forked version).\n                executable = frame.f_locals.get('executable')\n                if executable is not None:\n                    is_new_python_process = False\n                    if is_python(executable):\n                        is_new_python_process = True\n                break\n\n            frame = frame.f_back\n        frame = None  # Just make sure we don't hold on to it.\n\n        protocol = pydevd_constants.get_protocol()\n        debug_mode = PydevdCustomization.DEBUG_MODE\n\n        child_process = getattr(os, original_name)()  # fork\n        if not child_process:\n            if is_new_python_process:\n                PydevdCustomization.DEFAULT_PROTOCOL = protocol\n                PydevdCustomization.DEBUG_MODE = debug_mode\n                _on_forked_process(setup_tracing=apply_arg_patch and not is_subprocess_fork)\n            else:\n                set_global_debugger(None)\n        else:\n            if is_new_python_process:\n                send_process_created_message()\n        return child_process\n\n    return new_fork\n\n\ndef send_process_created_message():\n    py_db = get_global_debugger()\n    if py_db is not None:\n        py_db.send_process_created_message()\n\n\ndef send_process_about_to_be_replaced():\n    py_db = get_global_debugger()\n    if py_db is not None:\n        py_db.send_process_about_to_be_replaced()\n\n\ndef patch_new_process_functions():\n    # os.execl(path, arg0, arg1, ...)\n    # os.execle(path, arg0, arg1, ..., env)\n    # os.execlp(file, arg0, arg1, ...)\n    # os.execlpe(file, arg0, arg1, ..., env)\n    # os.execv(path, args)\n    # os.execve(path, args, env)\n    # os.execvp(file, args)\n    # os.execvpe(file, args, env)\n    monkey_patch_os('execl', create_execl)\n    monkey_patch_os('execle', create_execl)\n    monkey_patch_os('execlp', create_execl)\n    monkey_patch_os('execlpe', create_execl)\n    monkey_patch_os('execv', create_execv)\n    monkey_patch_os('execve', create_execve)\n    monkey_patch_os('execvp', create_execv)\n    monkey_patch_os('execvpe', create_execve)\n\n    # os.spawnl(mode, path, ...)\n    # os.spawnle(mode, path, ..., env)\n    # os.spawnlp(mode, file, ...)\n    # os.spawnlpe(mode, file, ..., env)\n    # os.spawnv(mode, path, args)\n    # os.spawnve(mode, path, args, env)\n    # os.spawnvp(mode, file, args)\n    # os.spawnvpe(mode, file, args, env)\n\n    monkey_patch_os('spawnl', create_spawnl)\n    monkey_patch_os('spawnle', create_spawnl)\n    monkey_patch_os('spawnlp', create_spawnl)\n    monkey_patch_os('spawnlpe', create_spawnl)\n    monkey_patch_os('spawnv', create_spawnv)\n    monkey_patch_os('spawnve', create_spawnve)\n    monkey_patch_os('spawnvp', create_spawnv)\n    monkey_patch_os('spawnvpe', create_spawnve)\n    monkey_patch_os('posix_spawn', create_posix_spawn)\n\n    if not IS_JYTHON:\n        if not IS_WINDOWS:\n            monkey_patch_os('fork', create_fork)\n            try:\n                import _posixsubprocess\n                monkey_patch_module(_posixsubprocess, 'fork_exec', create_fork_exec)\n            except ImportError:\n                pass\n\n            try:\n                import subprocess\n                monkey_patch_module(subprocess, '_fork_exec', create_subprocess_fork_exec)\n            except AttributeError:\n                pass\n        else:\n            # Windows\n            try:\n                import _subprocess\n            except ImportError:\n                import _winapi as _subprocess\n            monkey_patch_module(_subprocess, 'CreateProcess', create_CreateProcess)\n\n\ndef patch_new_process_functions_with_warning():\n    monkey_patch_os('execl', create_warn_multiproc)\n    monkey_patch_os('execle', create_warn_multiproc)\n    monkey_patch_os('execlp', create_warn_multiproc)\n    monkey_patch_os('execlpe', create_warn_multiproc)\n    monkey_patch_os('execv', create_warn_multiproc)\n    monkey_patch_os('execve', create_warn_multiproc)\n    monkey_patch_os('execvp', create_warn_multiproc)\n    monkey_patch_os('execvpe', create_warn_multiproc)\n    monkey_patch_os('spawnl', create_warn_multiproc)\n    monkey_patch_os('spawnle', create_warn_multiproc)\n    monkey_patch_os('spawnlp', create_warn_multiproc)\n    monkey_patch_os('spawnlpe', create_warn_multiproc)\n    monkey_patch_os('spawnv', create_warn_multiproc)\n    monkey_patch_os('spawnve', create_warn_multiproc)\n    monkey_patch_os('spawnvp', create_warn_multiproc)\n    monkey_patch_os('spawnvpe', create_warn_multiproc)\n    monkey_patch_os('posix_spawn', create_warn_multiproc)\n\n    if not IS_JYTHON:\n        if not IS_WINDOWS:\n            monkey_patch_os('fork', create_warn_multiproc)\n            try:\n                import _posixsubprocess\n                monkey_patch_module(_posixsubprocess, 'fork_exec', create_warn_fork_exec)\n            except ImportError:\n                pass\n\n            try:\n                import subprocess\n                monkey_patch_module(subprocess, '_fork_exec', create_subprocess_warn_fork_exec)\n            except AttributeError:\n                pass\n\n        else:\n            # Windows\n            try:\n                import _subprocess\n            except ImportError:\n                import _winapi as _subprocess\n            monkey_patch_module(_subprocess, 'CreateProcess', create_CreateProcessWarnMultiproc)\n\n\nclass _NewThreadStartupWithTrace:\n\n    def __init__(self, original_func, args, kwargs):\n        self.original_func = original_func\n        self.args = args\n        self.kwargs = kwargs\n\n    def __call__(self):\n        # We monkey-patch the thread creation so that this function is called in the new thread. At this point\n        # we notify of its creation and start tracing it.\n        py_db = get_global_debugger()\n\n        thread_id = None\n        if py_db is not None:\n            # Note: if this is a thread from threading.py, we're too early in the boostrap process (because we mocked\n            # the start_new_thread internal machinery and thread._bootstrap has not finished), so, the code below needs\n            # to make sure that we use the current thread bound to the original function and not use\n            # threading.current_thread() unless we're sure it's a dummy thread.\n            t = getattr(self.original_func, '__self__', getattr(self.original_func, 'im_self', None))\n            if not isinstance(t, threading.Thread):\n                # This is not a threading.Thread but a Dummy thread (so, get it as a dummy thread using\n                # currentThread).\n                t = threading.current_thread()\n\n            if not getattr(t, 'is_pydev_daemon_thread', False):\n                thread_id = get_current_thread_id(t)\n                py_db.notify_thread_created(thread_id, t)\n                _on_set_trace_for_new_thread(py_db)\n\n            if getattr(py_db, 'thread_analyser', None) is not None:\n                try:\n                    from _pydevd_bundle.pydevd_concurrency_analyser.pydevd_concurrency_logger import log_new_thread\n                    log_new_thread(py_db, t)\n                except:\n                    sys.stderr.write(\"Failed to detect new thread for visualization\")\n        try:\n            ret = self.original_func(*self.args, **self.kwargs)\n        finally:\n            if thread_id is not None:\n                if py_db is not None:\n                    # At thread shutdown we only have pydevd-related code running (which shouldn't\n                    # be tracked).\n                    py_db.disable_tracing()\n                    py_db.notify_thread_not_alive(thread_id)\n\n        return ret\n\n\nclass _NewThreadStartupWithoutTrace:\n\n    def __init__(self, original_func, args, kwargs):\n        self.original_func = original_func\n        self.args = args\n        self.kwargs = kwargs\n\n    def __call__(self):\n        return self.original_func(*self.args, **self.kwargs)\n\n\n_UseNewThreadStartup = _NewThreadStartupWithTrace\n\n\ndef _get_threading_modules_to_patch():\n    threading_modules_to_patch = []\n\n    try:\n        import thread as _thread\n    except:\n        import _thread\n    threading_modules_to_patch.append(_thread)\n    threading_modules_to_patch.append(threading)\n\n    return threading_modules_to_patch\n\n\nthreading_modules_to_patch = _get_threading_modules_to_patch()\n\n\ndef patch_thread_module(thread_module):\n\n    if getattr(thread_module, '_original_start_new_thread', None) is None:\n        if thread_module is threading:\n            if not hasattr(thread_module, '_start_new_thread'):\n                return  # Jython doesn't have it.\n            _original_start_new_thread = thread_module._original_start_new_thread = thread_module._start_new_thread\n        else:\n            _original_start_new_thread = thread_module._original_start_new_thread = thread_module.start_new_thread\n    else:\n        _original_start_new_thread = thread_module._original_start_new_thread\n\n    class ClassWithPydevStartNewThread:\n\n        def pydev_start_new_thread(self, function, args=(), kwargs={}):\n            '''\n            We need to replace the original thread_module.start_new_thread with this function so that threads started\n            through it and not through the threading module are properly traced.\n            '''\n            return _original_start_new_thread(_UseNewThreadStartup(function, args, kwargs), ())\n\n    # This is a hack for the situation where the thread_module.start_new_thread is declared inside a class, such as the one below\n    # class F(object):\n    #    start_new_thread = thread_module.start_new_thread\n    #\n    #    def start_it(self):\n    #        self.start_new_thread(self.function, args, kwargs)\n    # So, if it's an already bound method, calling self.start_new_thread won't really receive a different 'self' -- it\n    # does work in the default case because in builtins self isn't passed either.\n    pydev_start_new_thread = ClassWithPydevStartNewThread().pydev_start_new_thread\n\n    try:\n        # We need to replace the original thread_module.start_new_thread with this function so that threads started through\n        # it and not through the threading module are properly traced.\n        if thread_module is threading:\n            thread_module._start_new_thread = pydev_start_new_thread\n        else:\n            thread_module.start_new_thread = pydev_start_new_thread\n            thread_module.start_new = pydev_start_new_thread\n    except:\n        pass\n\n\ndef patch_thread_modules():\n    for t in threading_modules_to_patch:\n        patch_thread_module(t)\n\n\ndef undo_patch_thread_modules():\n    for t in threading_modules_to_patch:\n        try:\n            t.start_new_thread = t._original_start_new_thread\n        except:\n            pass\n\n        try:\n            t.start_new = t._original_start_new_thread\n        except:\n            pass\n\n        try:\n            t._start_new_thread = t._original_start_new_thread\n        except:\n            pass\n\n\ndef disable_trace_thread_modules():\n    '''\n    Can be used to temporarily stop tracing threads created with thread.start_new_thread.\n    '''\n    global _UseNewThreadStartup\n    _UseNewThreadStartup = _NewThreadStartupWithoutTrace\n\n\ndef enable_trace_thread_modules():\n    '''\n    Can be used to start tracing threads created with thread.start_new_thread again.\n    '''\n    global _UseNewThreadStartup\n    _UseNewThreadStartup = _NewThreadStartupWithTrace\n\n\ndef get_original_start_new_thread(threading_module):\n    try:\n        return threading_module._original_start_new_thread\n    except:\n        return threading_module.start_new_thread\n", 1246], "/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py": ["from time import sleep\nfrom fastapi import APIRouter, FastAPI\nfrom fastapi.testclient import TestClient\nfrom profyle.middleware.fastapi import ProfileMiddleware\n\napp = FastAPI()\napp.add_middleware(ProfileMiddleware)\n\nrouter = APIRouter()\n\ndef tarda():\n    numbers = []\n    for num in range(10):\n        sleep(.5)\n        numbers.append(num*num)\n\n\n@router.post('/test')\ndef run_middleware():\n    return {'message': 'OK'}\n\n@router.post('/test1')\ndef run_middleware_1():\n    tarda()\n    tarda()\n    tarda()\n    return {'message': 'OK'}\n\napp.include_router(router)\n\nclient = TestClient(app)\n\n\ndef test_fastapi_middleware():\n    res = client.post('test')\n    res1 = client.post('test1')\n    print(res)\n    print(res1)\n", 38], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/unix_events.py": ["\"\"\"Selector event loop for Unix with signal handling.\"\"\"\n\nimport errno\nimport io\nimport os\nimport selectors\nimport signal\nimport socket\nimport stat\nimport subprocess\nimport sys\nimport threading\nimport warnings\n\n\nfrom . import base_events\nfrom . import base_subprocess\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import futures\nfrom . import selector_events\nfrom . import tasks\nfrom . import transports\nfrom .log import logger\n\n\n__all__ = (\n    'SelectorEventLoop',\n    'AbstractChildWatcher', 'SafeChildWatcher',\n    'FastChildWatcher', 'DefaultEventLoopPolicy',\n)\n\n\nif sys.platform == 'win32':  # pragma: no cover\n    raise ImportError('Signals are not really supported on Windows')\n\n\ndef _sighandler_noop(signum, frame):\n    \"\"\"Dummy signal handler.\"\"\"\n    pass\n\n\nclass _UnixSelectorEventLoop(selector_events.BaseSelectorEventLoop):\n    \"\"\"Unix event loop.\n\n    Adds signal handling and UNIX Domain Socket support to SelectorEventLoop.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__(selector)\n        self._signal_handlers = {}\n\n    def close(self):\n        super().close()\n        if not sys.is_finalizing():\n            for sig in list(self._signal_handlers):\n                self.remove_signal_handler(sig)\n        else:\n            if self._signal_handlers:\n                warnings.warn(f\"Closing the loop {self!r} \"\n                              f\"on interpreter shutdown \"\n                              f\"stage, skipping signal handlers removal\",\n                              ResourceWarning,\n                              source=self)\n                self._signal_handlers.clear()\n\n    def _process_self_data(self, data):\n        for signum in data:\n            if not signum:\n                # ignore null bytes written by _write_to_self()\n                continue\n            self._handle_signal(signum)\n\n    def add_signal_handler(self, sig, callback, *args):\n        \"\"\"Add a handler for a signal.  UNIX only.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\"coroutines cannot be used \"\n                            \"with add_signal_handler()\")\n        self._check_signal(sig)\n        self._check_closed()\n        try:\n            # set_wakeup_fd() raises ValueError if this is not the\n            # main thread.  By calling it early we ensure that an\n            # event loop running in another thread cannot add a signal\n            # handler.\n            signal.set_wakeup_fd(self._csock.fileno())\n        except (ValueError, OSError) as exc:\n            raise RuntimeError(str(exc))\n\n        handle = events.Handle(callback, args, self, None)\n        self._signal_handlers[sig] = handle\n\n        try:\n            # Register a dummy signal handler to ask Python to write the signal\n            # number in the wakeup file descriptor. _process_self_data() will\n            # read signal numbers from this file descriptor to handle signals.\n            signal.signal(sig, _sighandler_noop)\n\n            # Set SA_RESTART to limit EINTR occurrences.\n            signal.siginterrupt(sig, False)\n        except OSError as exc:\n            del self._signal_handlers[sig]\n            if not self._signal_handlers:\n                try:\n                    signal.set_wakeup_fd(-1)\n                except (ValueError, OSError) as nexc:\n                    logger.info('set_wakeup_fd(-1) failed: %s', nexc)\n\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n    def _handle_signal(self, sig):\n        \"\"\"Internal helper that is the actual signal handler.\"\"\"\n        handle = self._signal_handlers.get(sig)\n        if handle is None:\n            return  # Assume it's some race condition.\n        if handle._cancelled:\n            self.remove_signal_handler(sig)  # Remove it properly.\n        else:\n            self._add_callback_signalsafe(handle)\n\n    def remove_signal_handler(self, sig):\n        \"\"\"Remove a handler for a signal.  UNIX only.\n\n        Return True if a signal handler was removed, False if not.\n        \"\"\"\n        self._check_signal(sig)\n        try:\n            del self._signal_handlers[sig]\n        except KeyError:\n            return False\n\n        if sig == signal.SIGINT:\n            handler = signal.default_int_handler\n        else:\n            handler = signal.SIG_DFL\n\n        try:\n            signal.signal(sig, handler)\n        except OSError as exc:\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n        if not self._signal_handlers:\n            try:\n                signal.set_wakeup_fd(-1)\n            except (ValueError, OSError) as exc:\n                logger.info('set_wakeup_fd(-1) failed: %s', exc)\n\n        return True\n\n    def _check_signal(self, sig):\n        \"\"\"Internal helper to validate a signal.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if not isinstance(sig, int):\n            raise TypeError(f'sig must be an int, not {sig!r}')\n\n        if not (1 <= sig < signal.NSIG):\n            raise ValueError(f'sig {sig} out of range(1, {signal.NSIG})')\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        return _UnixReadPipeTransport(self, pipe, protocol, waiter, extra)\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        return _UnixWritePipeTransport(self, pipe, protocol, waiter, extra)\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        with events.get_child_watcher() as watcher:\n            waiter = self.create_future()\n            transp = _UnixSubprocessTransport(self, protocol, args, shell,\n                                              stdin, stdout, stderr, bufsize,\n                                              waiter=waiter, extra=extra,\n                                              **kwargs)\n\n            watcher.add_child_handler(transp.get_pid(),\n                                      self._child_watcher_callback, transp)\n            try:\n                await waiter\n            except Exception:\n                transp.close()\n                await transp._wait()\n                raise\n\n        return transp\n\n    def _child_watcher_callback(self, pid, returncode, transp):\n        self.call_soon_threadsafe(transp._process_exited, returncode)\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None):\n        assert server_hostname is None or isinstance(server_hostname, str)\n        if ssl:\n            if server_hostname is None:\n                raise ValueError(\n                    'you have to pass server_hostname when using ssl')\n        else:\n            if server_hostname is not None:\n                raise ValueError('server_hostname is only meaningful with ssl')\n            if ssl_handshake_timeout is not None:\n                raise ValueError(\n                    'ssl_handshake_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM, 0)\n            try:\n                sock.setblocking(False)\n                await self.sock_connect(sock, path)\n            except:\n                sock.close()\n                raise\n\n        else:\n            if sock is None:\n                raise ValueError('no path and sock were specified')\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n            sock.setblocking(False)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        return transport, protocol\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n\n            # Check for abstract socket. `str` and `bytes` paths are supported.\n            if path[0] not in (0, '\\x00'):\n                try:\n                    if stat.S_ISSOCK(os.stat(path).st_mode):\n                        os.remove(path)\n                except FileNotFoundError:\n                    pass\n                except OSError as err:\n                    # Directory may have permissions only to create socket.\n                    logger.error('Unable to check or remove stale UNIX socket '\n                                 '%r: %r', path, err)\n\n            try:\n                sock.bind(path)\n            except OSError as exc:\n                sock.close()\n                if exc.errno == errno.EADDRINUSE:\n                    # Let's improve the error message by adding\n                    # with what exact address it occurs.\n                    msg = f'Address {path!r} is already in use'\n                    raise OSError(errno.EADDRINUSE, msg) from None\n                else:\n                    raise\n            except:\n                sock.close()\n                raise\n        else:\n            if sock is None:\n                raise ValueError(\n                    'path was not specified, and no sock specified')\n\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n\n        sock.setblocking(False)\n        server = base_events.Server(self, [sock], protocol_factory,\n                                    ssl, backlog, ssl_handshake_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0, loop=self)\n\n        return server\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        try:\n            os.sendfile\n        except AttributeError as exc:\n            raise events.SendfileNotAvailableError(\n                \"os.sendfile() is not available\")\n        try:\n            fileno = file.fileno()\n        except (AttributeError, io.UnsupportedOperation) as err:\n            raise events.SendfileNotAvailableError(\"not a regular file\")\n        try:\n            fsize = os.fstat(fileno).st_size\n        except OSError as err:\n            raise events.SendfileNotAvailableError(\"not a regular file\")\n        blocksize = count if count else fsize\n        if not blocksize:\n            return 0  # empty file\n\n        fut = self.create_future()\n        self._sock_sendfile_native_impl(fut, None, sock, fileno,\n                                        offset, count, blocksize, 0)\n        return await fut\n\n    def _sock_sendfile_native_impl(self, fut, registered_fd, sock, fileno,\n                                   offset, count, blocksize, total_sent):\n        fd = sock.fileno()\n        if registered_fd is not None:\n            # Remove the callback early.  It should be rare that the\n            # selector says the fd is ready but the call still returns\n            # EAGAIN, and I am willing to take a hit in that case in\n            # order to simplify the common case.\n            self.remove_writer(registered_fd)\n        if fut.cancelled():\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            return\n        if count:\n            blocksize = count - total_sent\n            if blocksize <= 0:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n                return\n\n        try:\n            sent = os.sendfile(fd, fileno, offset, blocksize)\n        except (BlockingIOError, InterruptedError):\n            if registered_fd is None:\n                self._sock_add_cancellation_callback(fut, sock)\n            self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                            fd, sock, fileno,\n                            offset, count, blocksize, total_sent)\n        except OSError as exc:\n            if (registered_fd is not None and\n                    exc.errno == errno.ENOTCONN and\n                    type(exc) is not ConnectionError):\n                # If we have an ENOTCONN and this isn't a first call to\n                # sendfile(), i.e. the connection was closed in the middle\n                # of the operation, normalize the error to ConnectionError\n                # to make it consistent across all Posix systems.\n                new_exc = ConnectionError(\n                    \"socket is not connected\", errno.ENOTCONN)\n                new_exc.__cause__ = exc\n                exc = new_exc\n            if total_sent == 0:\n                # We can get here for different reasons, the main\n                # one being 'file' is not a regular mmap(2)-like\n                # file, in which case we'll fall back on using\n                # plain send().\n                err = events.SendfileNotAvailableError(\n                    \"os.sendfile call failed\")\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(err)\n            else:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(exc)\n        except Exception as exc:\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            fut.set_exception(exc)\n        else:\n            if sent == 0:\n                # EOF\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n            else:\n                offset += sent\n                total_sent += sent\n                if registered_fd is None:\n                    self._sock_add_cancellation_callback(fut, sock)\n                self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                                fd, sock, fileno,\n                                offset, count, blocksize, total_sent)\n\n    def _sock_sendfile_update_filepos(self, fileno, offset, total_sent):\n        if total_sent > 0:\n            os.lseek(fileno, offset, os.SEEK_SET)\n\n    def _sock_add_cancellation_callback(self, fut, sock):\n        def cb(fut):\n            if fut.cancelled():\n                fd = sock.fileno()\n                if fd != -1:\n                    self.remove_writer(fd)\n        fut.add_done_callback(cb)\n\n\nclass _UnixReadPipeTransport(transports.ReadTransport):\n\n    max_size = 256 * 1024  # max bytes we read in one event loop iteration\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra)\n        self._extra['pipe'] = pipe\n        self._loop = loop\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._closing = False\n        self._paused = False\n\n        mode = os.fstat(self._fileno).st_mode\n        if not (stat.S_ISFIFO(mode) or\n                stat.S_ISSOCK(mode) or\n                stat.S_ISCHR(mode)):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is for pipes/sockets only.\")\n\n        os.set_blocking(self._fileno, False)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._loop._add_reader,\n                             self._fileno, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_READ)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def _read_ready(self):\n        try:\n            data = os.read(self._fileno, self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._fatal_error(exc, 'Fatal read error on pipe transport')\n        else:\n            if data:\n                self._protocol.data_received(data)\n            else:\n                if self._loop.get_debug():\n                    logger.info(\"%r was closed by peer\", self)\n                self._closing = True\n                self._loop._remove_reader(self._fileno)\n                self._loop.call_soon(self._protocol.eof_received)\n                self._loop.call_soon(self._call_connection_lost, None)\n\n    def pause_reading(self):\n        if self._closing or self._paused:\n            return\n        self._paused = True\n        self._loop._remove_reader(self._fileno)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._loop._add_reader(self._fileno, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if not self._closing:\n            self._close(None)\n\n    def __del__(self):\n        if self._pipe is not None:\n            warnings.warn(f\"unclosed transport {self!r}\", ResourceWarning,\n                          source=self)\n            self._pipe.close()\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if (isinstance(exc, OSError) and exc.errno == errno.EIO):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc):\n        self._closing = True\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixWritePipeTransport(transports._FlowControlMixin,\n                              transports.WriteTransport):\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra, loop)\n        self._extra['pipe'] = pipe\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._buffer = bytearray()\n        self._conn_lost = 0\n        self._closing = False  # Set when close() or write_eof() called.\n\n        mode = os.fstat(self._fileno).st_mode\n        is_char = stat.S_ISCHR(mode)\n        is_fifo = stat.S_ISFIFO(mode)\n        is_socket = stat.S_ISSOCK(mode)\n        if not (is_char or is_fifo or is_socket):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is only for \"\n                             \"pipes, sockets and character devices\")\n\n        os.set_blocking(self._fileno, False)\n        self._loop.call_soon(self._protocol.connection_made, self)\n\n        # On AIX, the reader trick (to be notified when the read end of the\n        # socket is closed) only works for sockets. On other platforms it\n        # works for pipes and sockets. (Exception: OS X 10.4?  Issue #19294.)\n        if is_socket or (is_fifo and not sys.platform.startswith(\"aix\")):\n            # only start reading when connection_made() has been called\n            self._loop.call_soon(self._loop._add_reader,\n                                 self._fileno, self._read_ready)\n\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_WRITE)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'bufsize={bufsize}')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def get_write_buffer_size(self):\n        return len(self._buffer)\n\n    def _read_ready(self):\n        # Pipe was closed by peer.\n        if self._loop.get_debug():\n            logger.info(\"%r was closed by peer\", self)\n        if self._buffer:\n            self._close(BrokenPipeError())\n        else:\n            self._close()\n\n    def write(self, data):\n        assert isinstance(data, (bytes, bytearray, memoryview)), repr(data)\n        if isinstance(data, bytearray):\n            data = memoryview(data)\n        if not data:\n            return\n\n        if self._conn_lost or self._closing:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('pipe closed by peer or '\n                               'os.write(pipe, data) raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                n = os.write(self._fileno, data)\n            except (BlockingIOError, InterruptedError):\n                n = 0\n            except Exception as exc:\n                self._conn_lost += 1\n                self._fatal_error(exc, 'Fatal write error on pipe transport')\n                return\n            if n == len(data):\n                return\n            elif n > 0:\n                data = memoryview(data)[n:]\n            self._loop._add_writer(self._fileno, self._write_ready)\n\n        self._buffer += data\n        self._maybe_pause_protocol()\n\n    def _write_ready(self):\n        assert self._buffer, 'Data should not be empty'\n\n        try:\n            n = os.write(self._fileno, self._buffer)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except Exception as exc:\n            self._buffer.clear()\n            self._conn_lost += 1\n            # Remove writer here, _fatal_error() doesn't it\n            # because _buffer is empty.\n            self._loop._remove_writer(self._fileno)\n            self._fatal_error(exc, 'Fatal write error on pipe transport')\n        else:\n            if n == len(self._buffer):\n                self._buffer.clear()\n                self._loop._remove_writer(self._fileno)\n                self._maybe_resume_protocol()  # May append to buffer.\n                if self._closing:\n                    self._loop._remove_reader(self._fileno)\n                    self._call_connection_lost(None)\n                return\n            elif n > 0:\n                del self._buffer[:n]\n\n    def can_write_eof(self):\n        return True\n\n    def write_eof(self):\n        if self._closing:\n            return\n        assert self._pipe\n        self._closing = True\n        if not self._buffer:\n            self._loop._remove_reader(self._fileno)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if self._pipe is not None and not self._closing:\n            # write_eof is all what we needed to close the write pipe\n            self.write_eof()\n\n    def __del__(self):\n        if self._pipe is not None:\n            warnings.warn(f\"unclosed transport {self!r}\", ResourceWarning,\n                          source=self)\n            self._pipe.close()\n\n    def abort(self):\n        self._close(None)\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc=None):\n        self._closing = True\n        if self._buffer:\n            self._loop._remove_writer(self._fileno)\n        self._buffer.clear()\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixSubprocessTransport(base_subprocess.BaseSubprocessTransport):\n\n    def _start(self, args, shell, stdin, stdout, stderr, bufsize, **kwargs):\n        stdin_w = None\n        if stdin == subprocess.PIPE:\n            # Use a socket pair for stdin, since not all platforms\n            # support selecting read events on the write end of a\n            # socket (which we use in order to detect closing of the\n            # other end).  Notably this is needed on AIX, and works\n            # just fine on other platforms.\n            stdin, stdin_w = socket.socketpair()\n        try:\n            self._proc = subprocess.Popen(\n                args, shell=shell, stdin=stdin, stdout=stdout, stderr=stderr,\n                universal_newlines=False, bufsize=bufsize, **kwargs)\n            if stdin_w is not None:\n                stdin.close()\n                self._proc.stdin = open(stdin_w.detach(), 'wb', buffering=bufsize)\n                stdin_w = None\n        finally:\n            if stdin_w is not None:\n                stdin.close()\n                stdin_w.close()\n\n\nclass AbstractChildWatcher:\n    \"\"\"Abstract base class for monitoring child processes.\n\n    Objects derived from this class monitor a collection of subprocesses and\n    report their termination or interruption by a signal.\n\n    New callbacks are registered with .add_child_handler(). Starting a new\n    process must be done within a 'with' block to allow the watcher to suspend\n    its activity until the new process if fully registered (this is needed to\n    prevent a race condition in some implementations).\n\n    Example:\n        with watcher:\n            proc = subprocess.Popen(\"sleep 1\")\n            watcher.add_child_handler(proc.pid, callback)\n\n    Notes:\n        Implementations of this class must be thread-safe.\n\n        Since child watcher objects may catch the SIGCHLD signal and call\n        waitpid(-1), there should be only one active object per process.\n    \"\"\"\n\n    def add_child_handler(self, pid, callback, *args):\n        \"\"\"Register a new child handler.\n\n        Arrange for callback(pid, returncode, *args) to be called when\n        process 'pid' terminates. Specifying another callback for the same\n        process replaces the previous handler.\n\n        Note: callback() must be thread-safe.\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_child_handler(self, pid):\n        \"\"\"Removes the handler for process 'pid'.\n\n        The function returns True if the handler was successfully removed,\n        False if there was nothing to remove.\"\"\"\n\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        \"\"\"Attach the watcher to an event loop.\n\n        If the watcher was previously attached to an event loop, then it is\n        first detached before attaching to the new loop.\n\n        Note: loop may be None.\n        \"\"\"\n        raise NotImplementedError()\n\n    def close(self):\n        \"\"\"Close the watcher.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        raise NotImplementedError()\n\n    def __enter__(self):\n        \"\"\"Enter the watcher's context and allow starting new processes\n\n        This function must return self\"\"\"\n        raise NotImplementedError()\n\n    def __exit__(self, a, b, c):\n        \"\"\"Exit the watcher's context\"\"\"\n        raise NotImplementedError()\n\n\nclass BaseChildWatcher(AbstractChildWatcher):\n\n    def __init__(self):\n        self._loop = None\n        self._callbacks = {}\n\n    def close(self):\n        self.attach_loop(None)\n\n    def _do_waitpid(self, expected_pid):\n        raise NotImplementedError()\n\n    def _do_waitpid_all(self):\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        assert loop is None or isinstance(loop, events.AbstractEventLoop)\n\n        if self._loop is not None and loop is None and self._callbacks:\n            warnings.warn(\n                'A loop is being detached '\n                'from a child watcher with pending handlers',\n                RuntimeWarning)\n\n        if self._loop is not None:\n            self._loop.remove_signal_handler(signal.SIGCHLD)\n\n        self._loop = loop\n        if loop is not None:\n            loop.add_signal_handler(signal.SIGCHLD, self._sig_chld)\n\n            # Prevent a race condition in case a child terminated\n            # during the switch.\n            self._do_waitpid_all()\n\n    def _sig_chld(self):\n        try:\n            self._do_waitpid_all()\n        except Exception as exc:\n            # self._loop should always be available here\n            # as '_sig_chld' is added as a signal handler\n            # in 'attach_loop'\n            self._loop.call_exception_handler({\n                'message': 'Unknown exception in SIGCHLD handler',\n                'exception': exc,\n            })\n\n    def _compute_returncode(self, status):\n        if os.WIFSIGNALED(status):\n            # The child process died because of a signal.\n            return -os.WTERMSIG(status)\n        elif os.WIFEXITED(status):\n            # The child process exited (e.g sys.exit()).\n            return os.WEXITSTATUS(status)\n        else:\n            # The child exited, but we don't understand its status.\n            # This shouldn't happen, but if it does, let's just\n            # return that status; perhaps that helps debug it.\n            return status\n\n\nclass SafeChildWatcher(BaseChildWatcher):\n    \"\"\"'Safe' child watcher implementation.\n\n    This implementation avoids disrupting other code spawning processes by\n    polling explicitly each process in the SIGCHLD handler instead of calling\n    os.waitpid(-1).\n\n    This is a safe solution but it has a significant overhead when handling a\n    big number of children (O(n) each time SIGCHLD is raised)\n    \"\"\"\n\n    def close(self):\n        self._callbacks.clear()\n        super().close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, a, b, c):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        if self._loop is None:\n            raise RuntimeError(\n                \"Cannot add child handler, \"\n                \"the child watcher does not have a loop attached\")\n\n        self._callbacks[pid] = (callback, args)\n\n        # Prevent a race condition in case the child is already terminated.\n        self._do_waitpid(pid)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n\n        for pid in list(self._callbacks):\n            self._do_waitpid(pid)\n\n    def _do_waitpid(self, expected_pid):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, os.WNOHANG)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n        else:\n            if pid == 0:\n                # The child process is still alive.\n                return\n\n            returncode = self._compute_returncode(status)\n            if self._loop.get_debug():\n                logger.debug('process %s exited with returncode %s',\n                             expected_pid, returncode)\n\n        try:\n            callback, args = self._callbacks.pop(pid)\n        except KeyError:  # pragma: no cover\n            # May happen if .remove_child_handler() is called\n            # after os.waitpid() returns.\n            if self._loop.get_debug():\n                logger.warning(\"Child watcher got an unexpected pid: %r\",\n                               pid, exc_info=True)\n        else:\n            callback(pid, returncode, *args)\n\n\nclass FastChildWatcher(BaseChildWatcher):\n    \"\"\"'Fast' child watcher implementation.\n\n    This implementation reaps every terminated processes by calling\n    os.waitpid(-1) directly, possibly breaking other code spawning processes\n    and waiting for their termination.\n\n    There is no noticeable overhead when handling a big number of children\n    (O(1) each time a child terminates).\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._lock = threading.Lock()\n        self._zombies = {}\n        self._forks = 0\n\n    def close(self):\n        self._callbacks.clear()\n        self._zombies.clear()\n        super().close()\n\n    def __enter__(self):\n        with self._lock:\n            self._forks += 1\n\n            return self\n\n    def __exit__(self, a, b, c):\n        with self._lock:\n            self._forks -= 1\n\n            if self._forks or not self._zombies:\n                return\n\n            collateral_victims = str(self._zombies)\n            self._zombies.clear()\n\n        logger.warning(\n            \"Caught subprocesses termination from unknown pids: %s\",\n            collateral_victims)\n\n    def add_child_handler(self, pid, callback, *args):\n        assert self._forks, \"Must use the context manager\"\n\n        if self._loop is None:\n            raise RuntimeError(\n                \"Cannot add child handler, \"\n                \"the child watcher does not have a loop attached\")\n\n        with self._lock:\n            try:\n                returncode = self._zombies.pop(pid)\n            except KeyError:\n                # The child is running.\n                self._callbacks[pid] = callback, args\n                return\n\n        # The child is dead already. We can fire the callback.\n        callback(pid, returncode, *args)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n        # Because of signal coalescing, we must keep calling waitpid() as\n        # long as we're able to reap a child.\n        while True:\n            try:\n                pid, status = os.waitpid(-1, os.WNOHANG)\n            except ChildProcessError:\n                # No more child processes exist.\n                return\n            else:\n                if pid == 0:\n                    # A child process is still alive.\n                    return\n\n                returncode = self._compute_returncode(status)\n\n            with self._lock:\n                try:\n                    callback, args = self._callbacks.pop(pid)\n                except KeyError:\n                    # unknown child\n                    if self._forks:\n                        # It may not be registered yet.\n                        self._zombies[pid] = returncode\n                        if self._loop.get_debug():\n                            logger.debug('unknown process %s exited '\n                                         'with returncode %s',\n                                         pid, returncode)\n                        continue\n                    callback = None\n                else:\n                    if self._loop.get_debug():\n                        logger.debug('process %s exited with returncode %s',\n                                     pid, returncode)\n\n            if callback is None:\n                logger.warning(\n                    \"Caught subprocess termination from unknown pid: \"\n                    \"%d -> %d\", pid, returncode)\n            else:\n                callback(pid, returncode, *args)\n\n\nclass _UnixDefaultEventLoopPolicy(events.BaseDefaultEventLoopPolicy):\n    \"\"\"UNIX event loop policy with a watcher for child processes.\"\"\"\n    _loop_factory = _UnixSelectorEventLoop\n\n    def __init__(self):\n        super().__init__()\n        self._watcher = None\n\n    def _init_watcher(self):\n        with events._lock:\n            if self._watcher is None:  # pragma: no branch\n                self._watcher = SafeChildWatcher()\n                if isinstance(threading.current_thread(),\n                              threading._MainThread):\n                    self._watcher.attach_loop(self._local._loop)\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\n\n        As a side effect, if a child watcher was set before, then calling\n        .set_event_loop() from the main thread will call .attach_loop(loop) on\n        the child watcher.\n        \"\"\"\n\n        super().set_event_loop(loop)\n\n        if (self._watcher is not None and\n                isinstance(threading.current_thread(), threading._MainThread)):\n            self._watcher.attach_loop(loop)\n\n    def get_child_watcher(self):\n        \"\"\"Get the watcher for child processes.\n\n        If not yet set, a SafeChildWatcher object is automatically created.\n        \"\"\"\n        if self._watcher is None:\n            self._init_watcher()\n\n        return self._watcher\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n\n        assert watcher is None or isinstance(watcher, AbstractChildWatcher)\n\n        if self._watcher is not None:\n            self._watcher.close()\n\n        self._watcher = watcher\n\n\nSelectorEventLoop = _UnixSelectorEventLoop\nDefaultEventLoopPolicy = _UnixDefaultEventLoopPolicy\n", 1158], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/dataclasses.py": ["import re\nimport sys\nimport copy\nimport types\nimport inspect\nimport keyword\nimport builtins\nimport functools\nimport _thread\n\n\n__all__ = ['dataclass',\n           'field',\n           'Field',\n           'FrozenInstanceError',\n           'InitVar',\n           'MISSING',\n\n           # Helper functions.\n           'fields',\n           'asdict',\n           'astuple',\n           'make_dataclass',\n           'replace',\n           'is_dataclass',\n           ]\n\n# Conditions for adding methods.  The boxes indicate what action the\n# dataclass decorator takes.  For all of these tables, when I talk\n# about init=, repr=, eq=, order=, unsafe_hash=, or frozen=, I'm\n# referring to the arguments to the @dataclass decorator.  When\n# checking if a dunder method already exists, I mean check for an\n# entry in the class's __dict__.  I never check to see if an attribute\n# is defined in a base class.\n\n# Key:\n# +=========+=========================================+\n# + Value   | Meaning                                 |\n# +=========+=========================================+\n# | <blank> | No action: no method is added.          |\n# +---------+-----------------------------------------+\n# | add     | Generated method is added.              |\n# +---------+-----------------------------------------+\n# | raise   | TypeError is raised.                    |\n# +---------+-----------------------------------------+\n# | None    | Attribute is set to None.               |\n# +=========+=========================================+\n\n# __init__\n#\n#   +--- init= parameter\n#   |\n#   v     |       |       |\n#         |  no   |  yes  |  <--- class has __init__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __repr__\n#\n#    +--- repr= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __repr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n\n# __setattr__\n# __delattr__\n#\n#    +--- frozen= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __setattr__ or __delattr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because not adding these methods would break the \"frozen-ness\"\n# of the class.\n\n# __eq__\n#\n#    +--- eq= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __eq__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __lt__\n# __le__\n# __gt__\n# __ge__\n#\n#    +--- order= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has any comparison method in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because to allow this case would interfere with using\n# functools.total_ordering.\n\n# __hash__\n\n#    +------------------- unsafe_hash= parameter\n#    |       +----------- eq= parameter\n#    |       |       +--- frozen= parameter\n#    |       |       |\n#    v       v       v    |        |        |\n#                         |   no   |  yes   |  <--- class has explicitly defined __hash__\n# +=======+=======+=======+========+========+\n# | False | False | False |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | False | True  |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | True  | False | None   |        | <-- the default, not hashable\n# +-------+-------+-------+--------+--------+\n# | False | True  | True  | add    |        | Frozen, so hashable, allows override\n# +-------+-------+-------+--------+--------+\n# | True  | False | False | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | False | True  | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | False | add    | raise  | Not frozen, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | True  | add    | raise  | Frozen, so hashable\n# +=======+=======+=======+========+========+\n# For boxes that are blank, __hash__ is untouched and therefore\n# inherited from the base class.  If the base is object, then\n# id-based hashing is used.\n#\n# Note that a class may already have __hash__=None if it specified an\n# __eq__ method in the class body (not one that was created by\n# @dataclass).\n#\n# See _hash_action (below) for a coded version of this table.\n\n\n# Raised when an attempt is made to modify a frozen class.\nclass FrozenInstanceError(AttributeError): pass\n\n# A sentinel object for default values to signal that a default\n# factory will be used.  This is given a nice repr() which will appear\n# in the function signature of dataclasses' constructors.\nclass _HAS_DEFAULT_FACTORY_CLASS:\n    def __repr__(self):\n        return '<factory>'\n_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\n\n# A sentinel object to detect if a parameter is supplied or not.  Use\n# a class to give it a better repr.\nclass _MISSING_TYPE:\n    pass\nMISSING = _MISSING_TYPE()\n\n# Since most per-field metadata will be unused, create an empty\n# read-only proxy that can be shared among all fields.\n_EMPTY_METADATA = types.MappingProxyType({})\n\n# Markers for the various kinds of fields and pseudo-fields.\nclass _FIELD_BASE:\n    def __init__(self, name):\n        self.name = name\n    def __repr__(self):\n        return self.name\n_FIELD = _FIELD_BASE('_FIELD')\n_FIELD_CLASSVAR = _FIELD_BASE('_FIELD_CLASSVAR')\n_FIELD_INITVAR = _FIELD_BASE('_FIELD_INITVAR')\n\n# The name of an attribute on the class where we store the Field\n# objects.  Also used to check if a class is a Data Class.\n_FIELDS = '__dataclass_fields__'\n\n# The name of an attribute on the class that stores the parameters to\n# @dataclass.\n_PARAMS = '__dataclass_params__'\n\n# The name of the function, that if it exists, is called at the end of\n# __init__.\n_POST_INIT_NAME = '__post_init__'\n\n# String regex that string annotations for ClassVar or InitVar must match.\n# Allows \"identifier.identifier[\" or \"identifier[\".\n# https://bugs.python.org/issue33453 for details.\n_MODULE_IDENTIFIER_RE = re.compile(r'^(?:\\s*(\\w+)\\s*\\.)?\\s*(\\w+)')\n\nclass _InitVarMeta(type):\n    def __getitem__(self, params):\n        return self\n\nclass InitVar(metaclass=_InitVarMeta):\n    pass\n\n\n# Instances of Field are only ever created from within this module,\n# and only from the field() function, although Field instances are\n# exposed externally as (conceptually) read-only objects.\n#\n# name and type are filled in after the fact, not in __init__.\n# They're not known at the time this class is instantiated, but it's\n# convenient if they're available later.\n#\n# When cls._FIELDS is filled in with a list of Field objects, the name\n# and type fields will have been populated.\nclass Field:\n    __slots__ = ('name',\n                 'type',\n                 'default',\n                 'default_factory',\n                 'repr',\n                 'hash',\n                 'init',\n                 'compare',\n                 'metadata',\n                 '_field_type',  # Private: not to be used by user code.\n                 )\n\n    def __init__(self, default, default_factory, init, repr, hash, compare,\n                 metadata):\n        self.name = None\n        self.type = None\n        self.default = default\n        self.default_factory = default_factory\n        self.init = init\n        self.repr = repr\n        self.hash = hash\n        self.compare = compare\n        self.metadata = (_EMPTY_METADATA\n                         if metadata is None else\n                         types.MappingProxyType(metadata))\n        self._field_type = None\n\n    def __repr__(self):\n        return ('Field('\n                f'name={self.name!r},'\n                f'type={self.type!r},'\n                f'default={self.default!r},'\n                f'default_factory={self.default_factory!r},'\n                f'init={self.init!r},'\n                f'repr={self.repr!r},'\n                f'hash={self.hash!r},'\n                f'compare={self.compare!r},'\n                f'metadata={self.metadata!r},'\n                f'_field_type={self._field_type}'\n                ')')\n\n    # This is used to support the PEP 487 __set_name__ protocol in the\n    # case where we're using a field that contains a descriptor as a\n    # default value.  For details on __set_name__, see\n    # https://www.python.org/dev/peps/pep-0487/#implementation-details.\n    #\n    # Note that in _process_class, this Field object is overwritten\n    # with the default value, so the end result is a descriptor that\n    # had __set_name__ called on it at the right time.\n    def __set_name__(self, owner, name):\n        func = getattr(type(self.default), '__set_name__', None)\n        if func:\n            # There is a __set_name__ method on the descriptor, call\n            # it.\n            func(self.default, owner, name)\n\n\nclass _DataclassParams:\n    __slots__ = ('init',\n                 'repr',\n                 'eq',\n                 'order',\n                 'unsafe_hash',\n                 'frozen',\n                 )\n\n    def __init__(self, init, repr, eq, order, unsafe_hash, frozen):\n        self.init = init\n        self.repr = repr\n        self.eq = eq\n        self.order = order\n        self.unsafe_hash = unsafe_hash\n        self.frozen = frozen\n\n    def __repr__(self):\n        return ('_DataclassParams('\n                f'init={self.init!r},'\n                f'repr={self.repr!r},'\n                f'eq={self.eq!r},'\n                f'order={self.order!r},'\n                f'unsafe_hash={self.unsafe_hash!r},'\n                f'frozen={self.frozen!r}'\n                ')')\n\n\n# This function is used instead of exposing Field creation directly,\n# so that a type checker can be told (via overloads) that this is a\n# function whose type depends on its parameters.\ndef field(*, default=MISSING, default_factory=MISSING, init=True, repr=True,\n          hash=None, compare=True, metadata=None):\n    \"\"\"Return an object to identify dataclass fields.\n\n    default is the default value of the field.  default_factory is a\n    0-argument function called to initialize a field's value.  If init\n    is True, the field will be a parameter to the class's __init__()\n    function.  If repr is True, the field will be included in the\n    object's repr().  If hash is True, the field will be included in\n    the object's hash().  If compare is True, the field will be used\n    in comparison functions.  metadata, if specified, must be a\n    mapping which is stored but not otherwise examined by dataclass.\n\n    It is an error to specify both default and default_factory.\n    \"\"\"\n\n    if default is not MISSING and default_factory is not MISSING:\n        raise ValueError('cannot specify both default and default_factory')\n    return Field(default, default_factory, init, repr, hash, compare,\n                 metadata)\n\n\ndef _tuple_str(obj_name, fields):\n    # Return a string representing each field of obj_name as a tuple\n    # member.  So, if fields is ['x', 'y'] and obj_name is \"self\",\n    # return \"(self.x,self.y)\".\n\n    # Special case for the 0-tuple.\n    if not fields:\n        return '()'\n    # Note the trailing comma, needed if this turns out to be a 1-tuple.\n    return f'({\",\".join([f\"{obj_name}.{f.name}\" for f in fields])},)'\n\n\n# This function's logic is copied from \"recursive_repr\" function in\n# reprlib module to avoid dependency.\ndef _recursive_repr(user_function):\n    # Decorator to make a repr function return \"...\" for a recursive\n    # call.\n    repr_running = set()\n\n    @functools.wraps(user_function)\n    def wrapper(self):\n        key = id(self), _thread.get_ident()\n        if key in repr_running:\n            return '...'\n        repr_running.add(key)\n        try:\n            result = user_function(self)\n        finally:\n            repr_running.discard(key)\n        return result\n    return wrapper\n\n\ndef _create_fn(name, args, body, *, globals=None, locals=None,\n               return_type=MISSING):\n    # Note that we mutate locals when exec() is called.  Caller\n    # beware!  The only callers are internal to this module, so no\n    # worries about external callers.\n    if locals is None:\n        locals = {}\n    if 'BUILTINS' not in locals:\n        locals['BUILTINS'] = builtins\n    return_annotation = ''\n    if return_type is not MISSING:\n        locals['_return_type'] = return_type\n        return_annotation = '->_return_type'\n    args = ','.join(args)\n    body = '\\n'.join(f'  {b}' for b in body)\n\n    # Compute the text of the entire function.\n    txt = f' def {name}({args}){return_annotation}:\\n{body}'\n\n    local_vars = ', '.join(locals.keys())\n    txt = f\"def __create_fn__({local_vars}):\\n{txt}\\n return {name}\"\n\n    ns = {}\n    exec(txt, globals, ns)\n    return ns['__create_fn__'](**locals)\n\n\ndef _field_assign(frozen, name, value, self_name):\n    # If we're a frozen class, then assign to our fields in __init__\n    # via object.__setattr__.  Otherwise, just use a simple\n    # assignment.\n    #\n    # self_name is what \"self\" is called in this function: don't\n    # hard-code \"self\", since that might be a field name.\n    if frozen:\n        return f'BUILTINS.object.__setattr__({self_name},{name!r},{value})'\n    return f'{self_name}.{name}={value}'\n\n\ndef _field_init(f, frozen, globals, self_name):\n    # Return the text of the line in the body of __init__ that will\n    # initialize this field.\n\n    default_name = f'_dflt_{f.name}'\n    if f.default_factory is not MISSING:\n        if f.init:\n            # This field has a default factory.  If a parameter is\n            # given, use it.  If not, call the factory.\n            globals[default_name] = f.default_factory\n            value = (f'{default_name}() '\n                     f'if {f.name} is _HAS_DEFAULT_FACTORY '\n                     f'else {f.name}')\n        else:\n            # This is a field that's not in the __init__ params, but\n            # has a default factory function.  It needs to be\n            # initialized here by calling the factory function,\n            # because there's no other way to initialize it.\n\n            # For a field initialized with a default=defaultvalue, the\n            # class dict just has the default value\n            # (cls.fieldname=defaultvalue).  But that won't work for a\n            # default factory, the factory must be called in __init__\n            # and we must assign that to self.fieldname.  We can't\n            # fall back to the class dict's value, both because it's\n            # not set, and because it might be different per-class\n            # (which, after all, is why we have a factory function!).\n\n            globals[default_name] = f.default_factory\n            value = f'{default_name}()'\n    else:\n        # No default factory.\n        if f.init:\n            if f.default is MISSING:\n                # There's no default, just do an assignment.\n                value = f.name\n            elif f.default is not MISSING:\n                globals[default_name] = f.default\n                value = f.name\n        else:\n            # This field does not need initialization.  Signify that\n            # to the caller by returning None.\n            return None\n\n    # Only test this now, so that we can create variables for the\n    # default.  However, return None to signify that we're not going\n    # to actually do the assignment statement for InitVars.\n    if f._field_type is _FIELD_INITVAR:\n        return None\n\n    # Now, actually generate the field assignment.\n    return _field_assign(frozen, f.name, value, self_name)\n\n\ndef _init_param(f):\n    # Return the __init__ parameter string for this field.  For\n    # example, the equivalent of 'x:int=3' (except instead of 'int',\n    # reference a variable set to int, and instead of '3', reference a\n    # variable set to 3).\n    if f.default is MISSING and f.default_factory is MISSING:\n        # There's no default, and no default_factory, just output the\n        # variable name and type.\n        default = ''\n    elif f.default is not MISSING:\n        # There's a default, this will be the name that's used to look\n        # it up.\n        default = f'=_dflt_{f.name}'\n    elif f.default_factory is not MISSING:\n        # There's a factory function.  Set a marker.\n        default = '=_HAS_DEFAULT_FACTORY'\n    return f'{f.name}:_type_{f.name}{default}'\n\n\ndef _init_fn(fields, frozen, has_post_init, self_name, globals):\n    # fields contains both real fields and InitVar pseudo-fields.\n\n    # Make sure we don't have fields without defaults following fields\n    # with defaults.  This actually would be caught when exec-ing the\n    # function source code, but catching it here gives a better error\n    # message, and future-proofs us in case we build up the function\n    # using ast.\n    seen_default = False\n    for f in fields:\n        # Only consider fields in the __init__ call.\n        if f.init:\n            if not (f.default is MISSING and f.default_factory is MISSING):\n                seen_default = True\n            elif seen_default:\n                raise TypeError(f'non-default argument {f.name!r} '\n                                'follows default argument')\n\n    locals = {f'_type_{f.name}': f.type for f in fields}\n    locals.update({\n        'MISSING': MISSING,\n        '_HAS_DEFAULT_FACTORY': _HAS_DEFAULT_FACTORY,\n    })\n\n    body_lines = []\n    for f in fields:\n        line = _field_init(f, frozen, locals, self_name)\n        # line is None means that this field doesn't require\n        # initialization (it's a pseudo-field).  Just skip it.\n        if line:\n            body_lines.append(line)\n\n    # Does this class have a post-init function?\n    if has_post_init:\n        params_str = ','.join(f.name for f in fields\n                              if f._field_type is _FIELD_INITVAR)\n        body_lines.append(f'{self_name}.{_POST_INIT_NAME}({params_str})')\n\n    # If no body lines, use 'pass'.\n    if not body_lines:\n        body_lines = ['pass']\n\n    return _create_fn('__init__',\n                      [self_name] + [_init_param(f) for f in fields if f.init],\n                      body_lines,\n                      locals=locals,\n                      globals=globals,\n                      return_type=None)\n\n\ndef _repr_fn(fields, globals):\n    fn = _create_fn('__repr__',\n                    ('self',),\n                    ['return self.__class__.__qualname__ + f\"(' +\n                     ', '.join([f\"{f.name}={{self.{f.name}!r}}\"\n                                for f in fields]) +\n                     ')\"'],\n                     globals=globals)\n    return _recursive_repr(fn)\n\n\ndef _frozen_get_del_attr(cls, fields, globals):\n    locals = {'cls': cls,\n              'FrozenInstanceError': FrozenInstanceError}\n    if fields:\n        fields_str = '(' + ','.join(repr(f.name) for f in fields) + ',)'\n    else:\n        # Special case for the zero-length tuple.\n        fields_str = '()'\n    return (_create_fn('__setattr__',\n                      ('self', 'name', 'value'),\n                      (f'if type(self) is cls or name in {fields_str}:',\n                        ' raise FrozenInstanceError(f\"cannot assign to field {name!r}\")',\n                       f'super(cls, self).__setattr__(name, value)'),\n                       locals=locals,\n                       globals=globals),\n            _create_fn('__delattr__',\n                      ('self', 'name'),\n                      (f'if type(self) is cls or name in {fields_str}:',\n                        ' raise FrozenInstanceError(f\"cannot delete field {name!r}\")',\n                       f'super(cls, self).__delattr__(name)'),\n                       locals=locals,\n                       globals=globals),\n            )\n\n\ndef _cmp_fn(name, op, self_tuple, other_tuple, globals):\n    # Create a comparison function.  If the fields in the object are\n    # named 'x' and 'y', then self_tuple is the string\n    # '(self.x,self.y)' and other_tuple is the string\n    # '(other.x,other.y)'.\n\n    return _create_fn(name,\n                      ('self', 'other'),\n                      [ 'if other.__class__ is self.__class__:',\n                       f' return {self_tuple}{op}{other_tuple}',\n                        'return NotImplemented'],\n                      globals=globals)\n\n\ndef _hash_fn(fields, globals):\n    self_tuple = _tuple_str('self', fields)\n    return _create_fn('__hash__',\n                      ('self',),\n                      [f'return hash({self_tuple})'],\n                      globals=globals)\n\n\ndef _is_classvar(a_type, typing):\n    # This test uses a typing internal class, but it's the best way to\n    # test if this is a ClassVar.\n    return (a_type is typing.ClassVar\n            or (type(a_type) is typing._GenericAlias\n                and a_type.__origin__ is typing.ClassVar))\n\n\ndef _is_initvar(a_type, dataclasses):\n    # The module we're checking against is the module we're\n    # currently in (dataclasses.py).\n    return a_type is dataclasses.InitVar\n\n\ndef _is_type(annotation, cls, a_module, a_type, is_type_predicate):\n    # Given a type annotation string, does it refer to a_type in\n    # a_module?  For example, when checking that annotation denotes a\n    # ClassVar, then a_module is typing, and a_type is\n    # typing.ClassVar.\n\n    # It's possible to look up a_module given a_type, but it involves\n    # looking in sys.modules (again!), and seems like a waste since\n    # the caller already knows a_module.\n\n    # - annotation is a string type annotation\n    # - cls is the class that this annotation was found in\n    # - a_module is the module we want to match\n    # - a_type is the type in that module we want to match\n    # - is_type_predicate is a function called with (obj, a_module)\n    #   that determines if obj is of the desired type.\n\n    # Since this test does not do a local namespace lookup (and\n    # instead only a module (global) lookup), there are some things it\n    # gets wrong.\n\n    # With string annotations, cv0 will be detected as a ClassVar:\n    #   CV = ClassVar\n    #   @dataclass\n    #   class C0:\n    #     cv0: CV\n\n    # But in this example cv1 will not be detected as a ClassVar:\n    #   @dataclass\n    #   class C1:\n    #     CV = ClassVar\n    #     cv1: CV\n\n    # In C1, the code in this function (_is_type) will look up \"CV\" in\n    # the module and not find it, so it will not consider cv1 as a\n    # ClassVar.  This is a fairly obscure corner case, and the best\n    # way to fix it would be to eval() the string \"CV\" with the\n    # correct global and local namespaces.  However that would involve\n    # a eval() penalty for every single field of every dataclass\n    # that's defined.  It was judged not worth it.\n\n    match = _MODULE_IDENTIFIER_RE.match(annotation)\n    if match:\n        ns = None\n        module_name = match.group(1)\n        if not module_name:\n            # No module name, assume the class's module did\n            # \"from dataclasses import InitVar\".\n            ns = sys.modules.get(cls.__module__).__dict__\n        else:\n            # Look up module_name in the class's module.\n            module = sys.modules.get(cls.__module__)\n            if module and module.__dict__.get(module_name) is a_module:\n                ns = sys.modules.get(a_type.__module__).__dict__\n        if ns and is_type_predicate(ns.get(match.group(2)), a_module):\n            return True\n    return False\n\n\ndef _get_field(cls, a_name, a_type):\n    # Return a Field object for this field name and type.  ClassVars\n    # and InitVars are also returned, but marked as such (see\n    # f._field_type).\n\n    # If the default value isn't derived from Field, then it's only a\n    # normal default value.  Convert it to a Field().\n    default = getattr(cls, a_name, MISSING)\n    if isinstance(default, Field):\n        f = default\n    else:\n        if isinstance(default, types.MemberDescriptorType):\n            # This is a field in __slots__, so it has no default value.\n            default = MISSING\n        f = field(default=default)\n\n    # Only at this point do we know the name and the type.  Set them.\n    f.name = a_name\n    f.type = a_type\n\n    # Assume it's a normal field until proven otherwise.  We're next\n    # going to decide if it's a ClassVar or InitVar, everything else\n    # is just a normal field.\n    f._field_type = _FIELD\n\n    # In addition to checking for actual types here, also check for\n    # string annotations.  get_type_hints() won't always work for us\n    # (see https://github.com/python/typing/issues/508 for example),\n    # plus it's expensive and would require an eval for every stirng\n    # annotation.  So, make a best effort to see if this is a ClassVar\n    # or InitVar using regex's and checking that the thing referenced\n    # is actually of the correct type.\n\n    # For the complete discussion, see https://bugs.python.org/issue33453\n\n    # If typing has not been imported, then it's impossible for any\n    # annotation to be a ClassVar.  So, only look for ClassVar if\n    # typing has been imported by any module (not necessarily cls's\n    # module).\n    typing = sys.modules.get('typing')\n    if typing:\n        if (_is_classvar(a_type, typing)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, typing, typing.ClassVar,\n                             _is_classvar))):\n            f._field_type = _FIELD_CLASSVAR\n\n    # If the type is InitVar, or if it's a matching string annotation,\n    # then it's an InitVar.\n    if f._field_type is _FIELD:\n        # The module we're checking against is the module we're\n        # currently in (dataclasses.py).\n        dataclasses = sys.modules[__name__]\n        if (_is_initvar(a_type, dataclasses)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, dataclasses, dataclasses.InitVar,\n                             _is_initvar))):\n            f._field_type = _FIELD_INITVAR\n\n    # Validations for individual fields.  This is delayed until now,\n    # instead of in the Field() constructor, since only here do we\n    # know the field name, which allows for better error reporting.\n\n    # Special restrictions for ClassVar and InitVar.\n    if f._field_type in (_FIELD_CLASSVAR, _FIELD_INITVAR):\n        if f.default_factory is not MISSING:\n            raise TypeError(f'field {f.name} cannot have a '\n                            'default factory')\n        # Should I check for other field settings? default_factory\n        # seems the most serious to check for.  Maybe add others.  For\n        # example, how about init=False (or really,\n        # init=<not-the-default-init-value>)?  It makes no sense for\n        # ClassVar and InitVar to specify init=<anything>.\n\n    # For real fields, disallow mutable defaults for known types.\n    if f._field_type is _FIELD and isinstance(f.default, (list, dict, set)):\n        raise ValueError(f'mutable default {type(f.default)} for field '\n                         f'{f.name} is not allowed: use default_factory')\n\n    return f\n\n\ndef _set_new_attribute(cls, name, value):\n    # Never overwrites an existing attribute.  Returns True if the\n    # attribute already exists.\n    if name in cls.__dict__:\n        return True\n    setattr(cls, name, value)\n    return False\n\n\n# Decide if/how we're going to create a hash function.  Key is\n# (unsafe_hash, eq, frozen, does-hash-exist).  Value is the action to\n# take.  The common case is to do nothing, so instead of providing a\n# function that is a no-op, use None to signify that.\n\ndef _hash_set_none(cls, fields, globals):\n    return None\n\ndef _hash_add(cls, fields, globals):\n    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\n    return _hash_fn(flds, globals)\n\ndef _hash_exception(cls, fields, globals):\n    # Raise an exception.\n    raise TypeError(f'Cannot overwrite attribute __hash__ '\n                    f'in class {cls.__name__}')\n\n#\n#                +-------------------------------------- unsafe_hash?\n#                |      +------------------------------- eq?\n#                |      |      +------------------------ frozen?\n#                |      |      |      +----------------  has-explicit-hash?\n#                |      |      |      |\n#                |      |      |      |        +-------  action\n#                |      |      |      |        |\n#                v      v      v      v        v\n_hash_action = {(False, False, False, False): None,\n                (False, False, False, True ): None,\n                (False, False, True,  False): None,\n                (False, False, True,  True ): None,\n                (False, True,  False, False): _hash_set_none,\n                (False, True,  False, True ): None,\n                (False, True,  True,  False): _hash_add,\n                (False, True,  True,  True ): None,\n                (True,  False, False, False): _hash_add,\n                (True,  False, False, True ): _hash_exception,\n                (True,  False, True,  False): _hash_add,\n                (True,  False, True,  True ): _hash_exception,\n                (True,  True,  False, False): _hash_add,\n                (True,  True,  False, True ): _hash_exception,\n                (True,  True,  True,  False): _hash_add,\n                (True,  True,  True,  True ): _hash_exception,\n                }\n# See https://bugs.python.org/issue32929#msg312829 for an if-statement\n# version of this table.\n\n\ndef _process_class(cls, init, repr, eq, order, unsafe_hash, frozen):\n    # Now that dicts retain insertion order, there's no reason to use\n    # an ordered dict.  I am leveraging that ordering here, because\n    # derived class fields overwrite base class fields, but the order\n    # is defined by the base class, which is found first.\n    fields = {}\n\n    if cls.__module__ in sys.modules:\n        globals = sys.modules[cls.__module__].__dict__\n    else:\n        # Theoretically this can happen if someone writes\n        # a custom string to cls.__module__.  In which case\n        # such dataclass won't be fully introspectable\n        # (w.r.t. typing.get_type_hints) but will still function\n        # correctly.\n        globals = {}\n\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\n                                           unsafe_hash, frozen))\n\n    # Find our base classes in reverse MRO order, and exclude\n    # ourselves.  In reversed order so that more derived classes\n    # override earlier field definitions in base classes.  As long as\n    # we're iterating over them, see if any are frozen.\n    any_frozen_base = False\n    has_dataclass_bases = False\n    for b in cls.__mro__[-1:0:-1]:\n        # Only process classes that have been processed by our\n        # decorator.  That is, they have a _FIELDS attribute.\n        base_fields = getattr(b, _FIELDS, None)\n        if base_fields:\n            has_dataclass_bases = True\n            for f in base_fields.values():\n                fields[f.name] = f\n            if getattr(b, _PARAMS).frozen:\n                any_frozen_base = True\n\n    # Annotations that are defined in this class (not in base\n    # classes).  If __annotations__ isn't present, then this class\n    # adds no new annotations.  We use this to compute fields that are\n    # added by this class.\n    #\n    # Fields are found from cls_annotations, which is guaranteed to be\n    # ordered.  Default values are from class attributes, if a field\n    # has a default.  If the default value is a Field(), then it\n    # contains additional info beyond (and possibly including) the\n    # actual default value.  Pseudo-fields ClassVars and InitVars are\n    # included, despite the fact that they're not real fields.  That's\n    # dealt with later.\n    cls_annotations = cls.__dict__.get('__annotations__', {})\n\n    # Now find fields in our class.  While doing so, validate some\n    # things, and set the default values (as class attributes) where\n    # we can.\n    cls_fields = [_get_field(cls, name, type)\n                  for name, type in cls_annotations.items()]\n    for f in cls_fields:\n        fields[f.name] = f\n\n        # If the class attribute (which is the default value for this\n        # field) exists and is of type 'Field', replace it with the\n        # real default.  This is so that normal class introspection\n        # sees a real default value, not a Field.\n        if isinstance(getattr(cls, f.name, None), Field):\n            if f.default is MISSING:\n                # If there's no default, delete the class attribute.\n                # This happens if we specify field(repr=False), for\n                # example (that is, we specified a field object, but\n                # no default value).  Also if we're using a default\n                # factory.  The class attribute should not be set at\n                # all in the post-processed class.\n                delattr(cls, f.name)\n            else:\n                setattr(cls, f.name, f.default)\n\n    # Do we have any Field members that don't also have annotations?\n    for name, value in cls.__dict__.items():\n        if isinstance(value, Field) and not name in cls_annotations:\n            raise TypeError(f'{name!r} is a field but has no type annotation')\n\n    # Check rules that apply if we are derived from any dataclasses.\n    if has_dataclass_bases:\n        # Raise an exception if any of our bases are frozen, but we're not.\n        if any_frozen_base and not frozen:\n            raise TypeError('cannot inherit non-frozen dataclass from a '\n                            'frozen one')\n\n        # Raise an exception if we're frozen, but none of our bases are.\n        if not any_frozen_base and frozen:\n            raise TypeError('cannot inherit frozen dataclass from a '\n                            'non-frozen one')\n\n    # Remember all of the fields on our class (including bases).  This\n    # also marks this class as being a dataclass.\n    setattr(cls, _FIELDS, fields)\n\n    # Was this class defined with an explicit __hash__?  Note that if\n    # __eq__ is defined in this class, then python will automatically\n    # set __hash__ to None.  This is a heuristic, as it's possible\n    # that such a __hash__ == None was not auto-generated, but it\n    # close enough.\n    class_hash = cls.__dict__.get('__hash__', MISSING)\n    has_explicit_hash = not (class_hash is MISSING or\n                             (class_hash is None and '__eq__' in cls.__dict__))\n\n    # If we're generating ordering methods, we must be generating the\n    # eq methods.\n    if order and not eq:\n        raise ValueError('eq must be true if order is true')\n\n    if init:\n        # Does this class have a post-init function?\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\n\n        # Include InitVars and regular fields (so, not ClassVars).\n        flds = [f for f in fields.values()\n                if f._field_type in (_FIELD, _FIELD_INITVAR)]\n        _set_new_attribute(cls, '__init__',\n                           _init_fn(flds,\n                                    frozen,\n                                    has_post_init,\n                                    # The name to use for the \"self\"\n                                    # param in __init__.  Use \"self\"\n                                    # if possible.\n                                    '__dataclass_self__' if 'self' in fields\n                                            else 'self',\n                                    globals,\n                          ))\n\n    # Get the fields as a list, and include only real fields.  This is\n    # used in all of the following methods.\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\n\n    if repr:\n        flds = [f for f in field_list if f.repr]\n        _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\n\n    if eq:\n        # Create _eq__ method.  There's no need for a __ne__ method,\n        # since python will call __eq__ and negate it.\n        flds = [f for f in field_list if f.compare]\n        self_tuple = _tuple_str('self', flds)\n        other_tuple = _tuple_str('other', flds)\n        _set_new_attribute(cls, '__eq__',\n                           _cmp_fn('__eq__', '==',\n                                   self_tuple, other_tuple,\n                                   globals=globals))\n\n    if order:\n        # Create and set the ordering methods.\n        flds = [f for f in field_list if f.compare]\n        self_tuple = _tuple_str('self', flds)\n        other_tuple = _tuple_str('other', flds)\n        for name, op in [('__lt__', '<'),\n                         ('__le__', '<='),\n                         ('__gt__', '>'),\n                         ('__ge__', '>='),\n                         ]:\n            if _set_new_attribute(cls, name,\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\n                                          globals=globals)):\n                raise TypeError(f'Cannot overwrite attribute {name} '\n                                f'in class {cls.__name__}. Consider using '\n                                'functools.total_ordering')\n\n    if frozen:\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\n            if _set_new_attribute(cls, fn.__name__, fn):\n                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\n                                f'in class {cls.__name__}')\n\n    # Decide if/how we're going to create a hash function.\n    hash_action = _hash_action[bool(unsafe_hash),\n                               bool(eq),\n                               bool(frozen),\n                               has_explicit_hash]\n    if hash_action:\n        # No need to call _set_new_attribute here, since by the time\n        # we're here the overwriting is unconditional.\n        cls.__hash__ = hash_action(cls, field_list, globals)\n\n    if not getattr(cls, '__doc__'):\n        # Create a class doc-string.\n        cls.__doc__ = (cls.__name__ +\n                       str(inspect.signature(cls)).replace(' -> None', ''))\n\n    return cls\n\n\n# _cls should never be specified by keyword, so start it with an\n# underscore.  The presence of _cls is used to detect if this\n# decorator is being called with parameters or not.\ndef dataclass(_cls=None, *, init=True, repr=True, eq=True, order=False,\n              unsafe_hash=False, frozen=False):\n    \"\"\"Returns the same class as was passed in, with dunder methods\n    added based on the fields defined in the class.\n\n    Examines PEP 526 __annotations__ to determine fields.\n\n    If init is true, an __init__() method is added to the class. If\n    repr is true, a __repr__() method is added. If order is true, rich\n    comparison dunder methods are added. If unsafe_hash is true, a\n    __hash__() method function is added. If frozen is true, fields may\n    not be assigned to after instance creation.\n    \"\"\"\n\n    def wrap(cls):\n        return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen)\n\n    # See if we're being called as @dataclass or @dataclass().\n    if _cls is None:\n        # We're called with parens.\n        return wrap\n\n    # We're called as @dataclass without parens.\n    return wrap(_cls)\n\n\ndef fields(class_or_instance):\n    \"\"\"Return a tuple describing the fields of this dataclass.\n\n    Accepts a dataclass or an instance of one. Tuple elements are of\n    type Field.\n    \"\"\"\n\n    # Might it be worth caching this, per class?\n    try:\n        fields = getattr(class_or_instance, _FIELDS)\n    except AttributeError:\n        raise TypeError('must be called with a dataclass type or instance')\n\n    # Exclude pseudo-fields.  Note that fields is sorted by insertion\n    # order, so the order of the tuple is as the fields were defined.\n    return tuple(f for f in fields.values() if f._field_type is _FIELD)\n\n\ndef _is_dataclass_instance(obj):\n    \"\"\"Returns True if obj is an instance of a dataclass.\"\"\"\n    return hasattr(type(obj), _FIELDS)\n\n\ndef is_dataclass(obj):\n    \"\"\"Returns True if obj is a dataclass or an instance of a\n    dataclass.\"\"\"\n    cls = obj if isinstance(obj, type) else type(obj)\n    return hasattr(cls, _FIELDS)\n\n\ndef asdict(obj, *, dict_factory=dict):\n    \"\"\"Return the fields of a dataclass instance as a new dictionary mapping\n    field names to field values.\n\n    Example usage:\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      assert asdict(c) == {'x': 1, 'y': 2}\n\n    If given, 'dict_factory' will be used instead of built-in dict.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts.\n    \"\"\"\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"asdict() should be called on dataclass instances\")\n    return _asdict_inner(obj, dict_factory)\n\n\ndef _asdict_inner(obj, dict_factory):\n    if _is_dataclass_instance(obj):\n        result = []\n        for f in fields(obj):\n            value = _asdict_inner(getattr(obj, f.name), dict_factory)\n            result.append((f.name, value))\n        return dict_factory(result)\n    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):\n        # obj is a namedtuple.  Recurse into it, but the returned\n        # object is another namedtuple of the same type.  This is\n        # similar to how other list- or tuple-derived classes are\n        # treated (see below), but we just need to create them\n        # differently because a namedtuple's __init__ needs to be\n        # called differently (see bpo-34363).\n\n        # I'm not using namedtuple's _asdict()\n        # method, because:\n        # - it does not recurse in to the namedtuple fields and\n        #   convert them to dicts (using dict_factory).\n        # - I don't actually want to return a dict here.  The the main\n        #   use case here is json.dumps, and it handles converting\n        #   namedtuples to lists.  Admittedly we're losing some\n        #   information here when we produce a json list instead of a\n        #   dict.  Note that if we returned dicts here instead of\n        #   namedtuples, we could no longer call asdict() on a data\n        #   structure where a namedtuple was used as a dict key.\n\n        return type(obj)(*[_asdict_inner(v, dict_factory) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        # Assume we can create an object of this type by passing in a\n        # generator (which is not true for namedtuples, handled\n        # above).\n        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)\n    elif isinstance(obj, dict):\n        return type(obj)((_asdict_inner(k, dict_factory),\n                          _asdict_inner(v, dict_factory))\n                         for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n\ndef astuple(obj, *, tuple_factory=tuple):\n    \"\"\"Return the fields of a dataclass instance as a new tuple of field values.\n\n    Example usage::\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n    c = C(1, 2)\n    assert astuple(c) == (1, 2)\n\n    If given, 'tuple_factory' will be used instead of built-in tuple.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts.\n    \"\"\"\n\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"astuple() should be called on dataclass instances\")\n    return _astuple_inner(obj, tuple_factory)\n\n\ndef _astuple_inner(obj, tuple_factory):\n    if _is_dataclass_instance(obj):\n        result = []\n        for f in fields(obj):\n            value = _astuple_inner(getattr(obj, f.name), tuple_factory)\n            result.append(value)\n        return tuple_factory(result)\n    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):\n        # obj is a namedtuple.  Recurse into it, but the returned\n        # object is another namedtuple of the same type.  This is\n        # similar to how other list- or tuple-derived classes are\n        # treated (see below), but we just need to create them\n        # differently because a namedtuple's __init__ needs to be\n        # called differently (see bpo-34363).\n        return type(obj)(*[_astuple_inner(v, tuple_factory) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        # Assume we can create an object of this type by passing in a\n        # generator (which is not true for namedtuples, handled\n        # above).\n        return type(obj)(_astuple_inner(v, tuple_factory) for v in obj)\n    elif isinstance(obj, dict):\n        return type(obj)((_astuple_inner(k, tuple_factory), _astuple_inner(v, tuple_factory))\n                          for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n\ndef make_dataclass(cls_name, fields, *, bases=(), namespace=None, init=True,\n                   repr=True, eq=True, order=False, unsafe_hash=False,\n                   frozen=False):\n    \"\"\"Return a new dynamically created dataclass.\n\n    The dataclass name will be 'cls_name'.  'fields' is an iterable\n    of either (name), (name, type) or (name, type, Field) objects. If type is\n    omitted, use the string 'typing.Any'.  Field objects are created by\n    the equivalent of calling 'field(name, type [, Field-info])'.\n\n      C = make_dataclass('C', ['x', ('y', int), ('z', int, field(init=False))], bases=(Base,))\n\n    is equivalent to:\n\n      @dataclass\n      class C(Base):\n          x: 'typing.Any'\n          y: int\n          z: int = field(init=False)\n\n    For the bases and namespace parameters, see the builtin type() function.\n\n    The parameters init, repr, eq, order, unsafe_hash, and frozen are passed to\n    dataclass().\n    \"\"\"\n\n    if namespace is None:\n        namespace = {}\n    else:\n        # Copy namespace since we're going to mutate it.\n        namespace = namespace.copy()\n\n    # While we're looking through the field names, validate that they\n    # are identifiers, are not keywords, and not duplicates.\n    seen = set()\n    anns = {}\n    for item in fields:\n        if isinstance(item, str):\n            name = item\n            tp = 'typing.Any'\n        elif len(item) == 2:\n            name, tp, = item\n        elif len(item) == 3:\n            name, tp, spec = item\n            namespace[name] = spec\n        else:\n            raise TypeError(f'Invalid field: {item!r}')\n\n        if not isinstance(name, str) or not name.isidentifier():\n            raise TypeError(f'Field names must be valid identifiers: {name!r}')\n        if keyword.iskeyword(name):\n            raise TypeError(f'Field names must not be keywords: {name!r}')\n        if name in seen:\n            raise TypeError(f'Field name duplicated: {name!r}')\n\n        seen.add(name)\n        anns[name] = tp\n\n    namespace['__annotations__'] = anns\n    # We use `types.new_class()` instead of simply `type()` to allow dynamic creation\n    # of generic dataclassses.\n    cls = types.new_class(cls_name, bases, {}, lambda ns: ns.update(namespace))\n    return dataclass(cls, init=init, repr=repr, eq=eq, order=order,\n                     unsafe_hash=unsafe_hash, frozen=frozen)\n\n\ndef replace(*args, **changes):\n    \"\"\"Return a new object replacing specified fields with new values.\n\n    This is especially useful for frozen classes.  Example usage:\n\n      @dataclass(frozen=True)\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      c1 = replace(c, x=3)\n      assert c1.x == 3 and c1.y == 2\n      \"\"\"\n    if len(args) > 1:\n        raise TypeError(f'replace() takes 1 positional argument but {len(args)} were given')\n    if args:\n        obj, = args\n    elif 'obj' in changes:\n        obj = changes.pop('obj')\n    else:\n        raise TypeError(\"replace() missing 1 required positional argument: 'obj'\")\n\n    # We're going to mutate 'changes', but that's okay because it's a\n    # new dict, even if called with 'replace(obj, **my_changes)'.\n\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"replace() should be called on dataclass instances\")\n\n    # It's an error to have init=False fields in 'changes'.\n    # If a field is not in 'changes', read its value from the provided obj.\n\n    for f in getattr(obj, _FIELDS).values():\n        # Only consider normal fields or InitVars.\n        if f._field_type is _FIELD_CLASSVAR:\n            continue\n\n        if not f.init:\n            # Error if this field is specified in changes.\n            if f.name in changes:\n                raise ValueError(f'field {f.name} is declared with '\n                                 'init=False, it cannot be specified with '\n                                 'replace()')\n            continue\n\n        if f.name not in changes:\n            if f._field_type is _FIELD_INITVAR:\n                raise ValueError(f\"InitVar {f.name!r} \"\n                                 'must be specified with replace()')\n            changes[f.name] = getattr(obj, f.name)\n\n    # Create the new object, which calls __init__() and\n    # __post_init__() (if defined), using all of the init fields we've\n    # added and/or left in 'changes'.  If there are values supplied in\n    # changes that aren't fields, this will correctly raise a\n    # TypeError.\n    return obj.__class__(**changes)\n", 1281], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/encoders.py": ["import dataclasses\nfrom collections import defaultdict\nfrom enum import Enum\nfrom pathlib import PurePath\nfrom types import GeneratorType\nfrom typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n\nfrom pydantic import BaseModel\nfrom pydantic.json import ENCODERS_BY_TYPE\n\nSetIntStr = Set[Union[int, str]]\nDictIntStrAny = Dict[Union[int, str], Any]\n\n\ndef generate_encoders_by_class_tuples(\n    type_encoder_map: Dict[Any, Callable[[Any], Any]]\n) -> Dict[Callable[[Any], Any], Tuple[Any, ...]]:\n    encoders_by_class_tuples: Dict[Callable[[Any], Any], Tuple[Any, ...]] = defaultdict(\n        tuple\n    )\n    for type_, encoder in type_encoder_map.items():\n        encoders_by_class_tuples[encoder] += (type_,)\n    return encoders_by_class_tuples\n\n\nencoders_by_class_tuples = generate_encoders_by_class_tuples(ENCODERS_BY_TYPE)\n\n\ndef jsonable_encoder(\n    obj: Any,\n    include: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n    exclude: Optional[Union[SetIntStr, DictIntStrAny]] = None,\n    by_alias: bool = True,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    custom_encoder: Optional[Dict[Any, Callable[[Any], Any]]] = None,\n    sqlalchemy_safe: bool = True,\n) -> Any:\n    custom_encoder = custom_encoder or {}\n    if custom_encoder:\n        if type(obj) in custom_encoder:\n            return custom_encoder[type(obj)](obj)\n        else:\n            for encoder_type, encoder_instance in custom_encoder.items():\n                if isinstance(obj, encoder_type):\n                    return encoder_instance(obj)\n    if include is not None and not isinstance(include, (set, dict)):\n        include = set(include)\n    if exclude is not None and not isinstance(exclude, (set, dict)):\n        exclude = set(exclude)\n    if isinstance(obj, BaseModel):\n        encoder = getattr(obj.__config__, \"json_encoders\", {})\n        if custom_encoder:\n            encoder.update(custom_encoder)\n        obj_dict = obj.dict(\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_none=exclude_none,\n            exclude_defaults=exclude_defaults,\n        )\n        if \"__root__\" in obj_dict:\n            obj_dict = obj_dict[\"__root__\"]\n        return jsonable_encoder(\n            obj_dict,\n            exclude_none=exclude_none,\n            exclude_defaults=exclude_defaults,\n            custom_encoder=encoder,\n            sqlalchemy_safe=sqlalchemy_safe,\n        )\n    if dataclasses.is_dataclass(obj):\n        obj_dict = dataclasses.asdict(obj)\n        return jsonable_encoder(\n            obj_dict,\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            custom_encoder=custom_encoder,\n            sqlalchemy_safe=sqlalchemy_safe,\n        )\n    if isinstance(obj, Enum):\n        return obj.value\n    if isinstance(obj, PurePath):\n        return str(obj)\n    if isinstance(obj, (str, int, float, type(None))):\n        return obj\n    if isinstance(obj, dict):\n        encoded_dict = {}\n        allowed_keys = set(obj.keys())\n        if include is not None:\n            allowed_keys &= set(include)\n        if exclude is not None:\n            allowed_keys -= set(exclude)\n        for key, value in obj.items():\n            if (\n                (\n                    not sqlalchemy_safe\n                    or (not isinstance(key, str))\n                    or (not key.startswith(\"_sa\"))\n                )\n                and (value is not None or not exclude_none)\n                and key in allowed_keys\n            ):\n                encoded_key = jsonable_encoder(\n                    key,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_none=exclude_none,\n                    custom_encoder=custom_encoder,\n                    sqlalchemy_safe=sqlalchemy_safe,\n                )\n                encoded_value = jsonable_encoder(\n                    value,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_none=exclude_none,\n                    custom_encoder=custom_encoder,\n                    sqlalchemy_safe=sqlalchemy_safe,\n                )\n                encoded_dict[encoded_key] = encoded_value\n        return encoded_dict\n    if isinstance(obj, (list, set, frozenset, GeneratorType, tuple)):\n        encoded_list = []\n        for item in obj:\n            encoded_list.append(\n                jsonable_encoder(\n                    item,\n                    include=include,\n                    exclude=exclude,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    exclude_none=exclude_none,\n                    custom_encoder=custom_encoder,\n                    sqlalchemy_safe=sqlalchemy_safe,\n                )\n            )\n        return encoded_list\n\n    if type(obj) in ENCODERS_BY_TYPE:\n        return ENCODERS_BY_TYPE[type(obj)](obj)\n    for encoder, classes_tuple in encoders_by_class_tuples.items():\n        if isinstance(obj, classes_tuple):\n            return encoder(obj)\n\n    try:\n        data = dict(obj)\n    except Exception as e:\n        errors: List[Exception] = []\n        errors.append(e)\n        try:\n            data = vars(obj)\n        except Exception as e:\n            errors.append(e)\n            raise ValueError(errors) from e\n    return jsonable_encoder(\n        data,\n        include=include,\n        exclude=exclude,\n        by_alias=by_alias,\n        exclude_unset=exclude_unset,\n        exclude_defaults=exclude_defaults,\n        exclude_none=exclude_none,\n        custom_encoder=custom_encoder,\n        sqlalchemy_safe=sqlalchemy_safe,\n    )\n", 171], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py": ["\"\"\"Implementation of JSONEncoder\n\"\"\"\nimport re\n\ntry:\n    from _json import encode_basestring_ascii as c_encode_basestring_ascii\nexcept ImportError:\n    c_encode_basestring_ascii = None\ntry:\n    from _json import encode_basestring as c_encode_basestring\nexcept ImportError:\n    c_encode_basestring = None\ntry:\n    from _json import make_encoder as c_make_encoder\nexcept ImportError:\n    c_make_encoder = None\n\nESCAPE = re.compile(r'[\\x00-\\x1f\\\\\"\\b\\f\\n\\r\\t]')\nESCAPE_ASCII = re.compile(r'([\\\\\"]|[^\\ -~])')\nHAS_UTF8 = re.compile(b'[\\x80-\\xff]')\nESCAPE_DCT = {\n    '\\\\': '\\\\\\\\',\n    '\"': '\\\\\"',\n    '\\b': '\\\\b',\n    '\\f': '\\\\f',\n    '\\n': '\\\\n',\n    '\\r': '\\\\r',\n    '\\t': '\\\\t',\n}\nfor i in range(0x20):\n    ESCAPE_DCT.setdefault(chr(i), '\\\\u{0:04x}'.format(i))\n    #ESCAPE_DCT.setdefault(chr(i), '\\\\u%04x' % (i,))\n\nINFINITY = float('inf')\n\ndef py_encode_basestring(s):\n    \"\"\"Return a JSON representation of a Python string\n\n    \"\"\"\n    def replace(match):\n        return ESCAPE_DCT[match.group(0)]\n    return '\"' + ESCAPE.sub(replace, s) + '\"'\n\n\nencode_basestring = (c_encode_basestring or py_encode_basestring)\n\n\ndef py_encode_basestring_ascii(s):\n    \"\"\"Return an ASCII-only JSON representation of a Python string\n\n    \"\"\"\n    def replace(match):\n        s = match.group(0)\n        try:\n            return ESCAPE_DCT[s]\n        except KeyError:\n            n = ord(s)\n            if n < 0x10000:\n                return '\\\\u{0:04x}'.format(n)\n                #return '\\\\u%04x' % (n,)\n            else:\n                # surrogate pair\n                n -= 0x10000\n                s1 = 0xd800 | ((n >> 10) & 0x3ff)\n                s2 = 0xdc00 | (n & 0x3ff)\n                return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\n    return '\"' + ESCAPE_ASCII.sub(replace, s) + '\"'\n\n\nencode_basestring_ascii = (\n    c_encode_basestring_ascii or py_encode_basestring_ascii)\n\nclass JSONEncoder(object):\n    \"\"\"Extensible JSON <http://json.org> encoder for Python data structures.\n\n    Supports the following objects and types by default:\n\n    +-------------------+---------------+\n    | Python            | JSON          |\n    +===================+===============+\n    | dict              | object        |\n    +-------------------+---------------+\n    | list, tuple       | array         |\n    +-------------------+---------------+\n    | str               | string        |\n    +-------------------+---------------+\n    | int, float        | number        |\n    +-------------------+---------------+\n    | True              | true          |\n    +-------------------+---------------+\n    | False             | false         |\n    +-------------------+---------------+\n    | None              | null          |\n    +-------------------+---------------+\n\n    To extend this to recognize other objects, subclass and implement a\n    ``.default()`` method with another method that returns a serializable\n    object for ``o`` if possible, otherwise it should call the superclass\n    implementation (to raise ``TypeError``).\n\n    \"\"\"\n    item_separator = ', '\n    key_separator = ': '\n    def __init__(self, *, skipkeys=False, ensure_ascii=True,\n            check_circular=True, allow_nan=True, sort_keys=False,\n            indent=None, separators=None, default=None):\n        \"\"\"Constructor for JSONEncoder, with sensible defaults.\n\n        If skipkeys is false, then it is a TypeError to attempt\n        encoding of keys that are not str, int, float or None.  If\n        skipkeys is True, such items are simply skipped.\n\n        If ensure_ascii is true, the output is guaranteed to be str\n        objects with all incoming non-ASCII characters escaped.  If\n        ensure_ascii is false, the output can contain non-ASCII characters.\n\n        If check_circular is true, then lists, dicts, and custom encoded\n        objects will be checked for circular references during encoding to\n        prevent an infinite recursion (which would cause an OverflowError).\n        Otherwise, no such check takes place.\n\n        If allow_nan is true, then NaN, Infinity, and -Infinity will be\n        encoded as such.  This behavior is not JSON specification compliant,\n        but is consistent with most JavaScript based encoders and decoders.\n        Otherwise, it will be a ValueError to encode such floats.\n\n        If sort_keys is true, then the output of dictionaries will be\n        sorted by key; this is useful for regression tests to ensure\n        that JSON serializations can be compared on a day-to-day basis.\n\n        If indent is a non-negative integer, then JSON array\n        elements and object members will be pretty-printed with that\n        indent level.  An indent level of 0 will only insert newlines.\n        None is the most compact representation.\n\n        If specified, separators should be an (item_separator, key_separator)\n        tuple.  The default is (', ', ': ') if *indent* is ``None`` and\n        (',', ': ') otherwise.  To get the most compact JSON representation,\n        you should specify (',', ':') to eliminate whitespace.\n\n        If specified, default is a function that gets called for objects\n        that can't otherwise be serialized.  It should return a JSON encodable\n        version of the object or raise a ``TypeError``.\n\n        \"\"\"\n\n        self.skipkeys = skipkeys\n        self.ensure_ascii = ensure_ascii\n        self.check_circular = check_circular\n        self.allow_nan = allow_nan\n        self.sort_keys = sort_keys\n        self.indent = indent\n        if separators is not None:\n            self.item_separator, self.key_separator = separators\n        elif indent is not None:\n            self.item_separator = ','\n        if default is not None:\n            self.default = default\n\n    def default(self, o):\n        \"\"\"Implement this method in a subclass such that it returns\n        a serializable object for ``o``, or calls the base implementation\n        (to raise a ``TypeError``).\n\n        For example, to support arbitrary iterators, you could\n        implement default like this::\n\n            def default(self, o):\n                try:\n                    iterable = iter(o)\n                except TypeError:\n                    pass\n                else:\n                    return list(iterable)\n                # Let the base class default method raise the TypeError\n                return JSONEncoder.default(self, o)\n\n        \"\"\"\n        raise TypeError(f'Object of type {o.__class__.__name__} '\n                        f'is not JSON serializable')\n\n    def encode(self, o):\n        \"\"\"Return a JSON string representation of a Python data structure.\n\n        >>> from json.encoder import JSONEncoder\n        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n        '{\"foo\": [\"bar\", \"baz\"]}'\n\n        \"\"\"\n        # This is for extremely simple cases and benchmarks.\n        if isinstance(o, str):\n            if self.ensure_ascii:\n                return encode_basestring_ascii(o)\n            else:\n                return encode_basestring(o)\n        # This doesn't pass the iterator directly to ''.join() because the\n        # exceptions aren't as detailed.  The list call should be roughly\n        # equivalent to the PySequence_Fast that ''.join() would do.\n        chunks = self.iterencode(o, _one_shot=True)\n        if not isinstance(chunks, (list, tuple)):\n            chunks = list(chunks)\n        return ''.join(chunks)\n\n    def iterencode(self, o, _one_shot=False):\n        \"\"\"Encode the given object and yield each string\n        representation as available.\n\n        For example::\n\n            for chunk in JSONEncoder().iterencode(bigobject):\n                mysocket.write(chunk)\n\n        \"\"\"\n        if self.check_circular:\n            markers = {}\n        else:\n            markers = None\n        if self.ensure_ascii:\n            _encoder = encode_basestring_ascii\n        else:\n            _encoder = encode_basestring\n\n        def floatstr(o, allow_nan=self.allow_nan,\n                _repr=float.__repr__, _inf=INFINITY, _neginf=-INFINITY):\n            # Check for specials.  Note that this type of test is processor\n            # and/or platform-specific, so do tests which don't depend on the\n            # internals.\n\n            if o != o:\n                text = 'NaN'\n            elif o == _inf:\n                text = 'Infinity'\n            elif o == _neginf:\n                text = '-Infinity'\n            else:\n                return _repr(o)\n\n            if not allow_nan:\n                raise ValueError(\n                    \"Out of range float values are not JSON compliant: \" +\n                    repr(o))\n\n            return text\n\n\n        if (_one_shot and c_make_encoder is not None\n                and self.indent is None):\n            _iterencode = c_make_encoder(\n                markers, self.default, _encoder, self.indent,\n                self.key_separator, self.item_separator, self.sort_keys,\n                self.skipkeys, self.allow_nan)\n        else:\n            _iterencode = _make_iterencode(\n                markers, self.default, _encoder, self.indent, floatstr,\n                self.key_separator, self.item_separator, self.sort_keys,\n                self.skipkeys, _one_shot)\n        return _iterencode(o, 0)\n\ndef _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n        _key_separator, _item_separator, _sort_keys, _skipkeys, _one_shot,\n        ## HACK: hand-optimized bytecode; turn globals into locals\n        ValueError=ValueError,\n        dict=dict,\n        float=float,\n        id=id,\n        int=int,\n        isinstance=isinstance,\n        list=list,\n        str=str,\n        tuple=tuple,\n        _intstr=int.__str__,\n    ):\n\n    if _indent is not None and not isinstance(_indent, str):\n        _indent = ' ' * _indent\n\n    def _iterencode_list(lst, _current_indent_level):\n        if not lst:\n            yield '[]'\n            return\n        if markers is not None:\n            markerid = id(lst)\n            if markerid in markers:\n                raise ValueError(\"Circular reference detected\")\n            markers[markerid] = lst\n        buf = '['\n        if _indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + _indent * _current_indent_level\n            separator = _item_separator + newline_indent\n            buf += newline_indent\n        else:\n            newline_indent = None\n            separator = _item_separator\n        first = True\n        for value in lst:\n            if first:\n                first = False\n            else:\n                buf = separator\n            if isinstance(value, str):\n                yield buf + _encoder(value)\n            elif value is None:\n                yield buf + 'null'\n            elif value is True:\n                yield buf + 'true'\n            elif value is False:\n                yield buf + 'false'\n            elif isinstance(value, int):\n                # Subclasses of int/float may override __str__, but we still\n                # want to encode them as integers/floats in JSON. One example\n                # within the standard library is IntEnum.\n                yield buf + _intstr(value)\n            elif isinstance(value, float):\n                # see comment above for int\n                yield buf + _floatstr(value)\n            else:\n                yield buf\n                if isinstance(value, (list, tuple)):\n                    chunks = _iterencode_list(value, _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = _iterencode_dict(value, _current_indent_level)\n                else:\n                    chunks = _iterencode(value, _current_indent_level)\n                yield from chunks\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + _indent * _current_indent_level\n        yield ']'\n        if markers is not None:\n            del markers[markerid]\n\n    def _iterencode_dict(dct, _current_indent_level):\n        if not dct:\n            yield '{}'\n            return\n        if markers is not None:\n            markerid = id(dct)\n            if markerid in markers:\n                raise ValueError(\"Circular reference detected\")\n            markers[markerid] = dct\n        yield '{'\n        if _indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + _indent * _current_indent_level\n            item_separator = _item_separator + newline_indent\n            yield newline_indent\n        else:\n            newline_indent = None\n            item_separator = _item_separator\n        first = True\n        if _sort_keys:\n            items = sorted(dct.items(), key=lambda kv: kv[0])\n        else:\n            items = dct.items()\n        for key, value in items:\n            if isinstance(key, str):\n                pass\n            # JavaScript is weakly typed for these, so it makes sense to\n            # also allow them.  Many encoders seem to do something like this.\n            elif isinstance(key, float):\n                # see comment for int/float in _make_iterencode\n                key = _floatstr(key)\n            elif key is True:\n                key = 'true'\n            elif key is False:\n                key = 'false'\n            elif key is None:\n                key = 'null'\n            elif isinstance(key, int):\n                # see comment for int/float in _make_iterencode\n                key = _intstr(key)\n            elif _skipkeys:\n                continue\n            else:\n                raise TypeError(f'keys must be str, int, float, bool or None, '\n                                f'not {key.__class__.__name__}')\n            if first:\n                first = False\n            else:\n                yield item_separator\n            yield _encoder(key)\n            yield _key_separator\n            if isinstance(value, str):\n                yield _encoder(value)\n            elif value is None:\n                yield 'null'\n            elif value is True:\n                yield 'true'\n            elif value is False:\n                yield 'false'\n            elif isinstance(value, int):\n                # see comment for int/float in _make_iterencode\n                yield _intstr(value)\n            elif isinstance(value, float):\n                # see comment for int/float in _make_iterencode\n                yield _floatstr(value)\n            else:\n                if isinstance(value, (list, tuple)):\n                    chunks = _iterencode_list(value, _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = _iterencode_dict(value, _current_indent_level)\n                else:\n                    chunks = _iterencode(value, _current_indent_level)\n                yield from chunks\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + _indent * _current_indent_level\n        yield '}'\n        if markers is not None:\n            del markers[markerid]\n\n    def _iterencode(o, _current_indent_level):\n        if isinstance(o, str):\n            yield _encoder(o)\n        elif o is None:\n            yield 'null'\n        elif o is True:\n            yield 'true'\n        elif o is False:\n            yield 'false'\n        elif isinstance(o, int):\n            # see comment for int/float in _make_iterencode\n            yield _intstr(o)\n        elif isinstance(o, float):\n            # see comment for int/float in _make_iterencode\n            yield _floatstr(o)\n        elif isinstance(o, (list, tuple)):\n            yield from _iterencode_list(o, _current_indent_level)\n        elif isinstance(o, dict):\n            yield from _iterencode_dict(o, _current_indent_level)\n        else:\n            if markers is not None:\n                markerid = id(o)\n                if markerid in markers:\n                    raise ValueError(\"Circular reference detected\")\n                markers[markerid] = o\n            o = _default(o)\n            yield from _iterencode(o, _current_indent_level)\n            if markers is not None:\n                del markers[markerid]\n    return _iterencode\n", 442], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py": ["r\"\"\"JSON (JavaScript Object Notation) <http://json.org> is a subset of\nJavaScript syntax (ECMA-262 3rd edition) used as a lightweight data\ninterchange format.\n\n:mod:`json` exposes an API familiar to users of the standard library\n:mod:`marshal` and :mod:`pickle` modules.  It is derived from a\nversion of the externally maintained simplejson library.\n\nEncoding basic Python object hierarchies::\n\n    >>> import json\n    >>> json.dumps(['foo', {'bar': ('baz', None, 1.0, 2)}])\n    '[\"foo\", {\"bar\": [\"baz\", null, 1.0, 2]}]'\n    >>> print(json.dumps(\"\\\"foo\\bar\"))\n    \"\\\"foo\\bar\"\n    >>> print(json.dumps('\\u1234'))\n    \"\\u1234\"\n    >>> print(json.dumps('\\\\'))\n    \"\\\\\"\n    >>> print(json.dumps({\"c\": 0, \"b\": 0, \"a\": 0}, sort_keys=True))\n    {\"a\": 0, \"b\": 0, \"c\": 0}\n    >>> from io import StringIO\n    >>> io = StringIO()\n    >>> json.dump(['streaming API'], io)\n    >>> io.getvalue()\n    '[\"streaming API\"]'\n\nCompact encoding::\n\n    >>> import json\n    >>> mydict = {'4': 5, '6': 7}\n    >>> json.dumps([1,2,3,mydict], separators=(',', ':'))\n    '[1,2,3,{\"4\":5,\"6\":7}]'\n\nPretty printing::\n\n    >>> import json\n    >>> print(json.dumps({'4': 5, '6': 7}, sort_keys=True, indent=4))\n    {\n        \"4\": 5,\n        \"6\": 7\n    }\n\nDecoding JSON::\n\n    >>> import json\n    >>> obj = ['foo', {'bar': ['baz', None, 1.0, 2]}]\n    >>> json.loads('[\"foo\", {\"bar\":[\"baz\", null, 1.0, 2]}]') == obj\n    True\n    >>> json.loads('\"\\\\\"foo\\\\bar\"') == '\"foo\\x08ar'\n    True\n    >>> from io import StringIO\n    >>> io = StringIO('[\"streaming API\"]')\n    >>> json.load(io)[0] == 'streaming API'\n    True\n\nSpecializing JSON object decoding::\n\n    >>> import json\n    >>> def as_complex(dct):\n    ...     if '__complex__' in dct:\n    ...         return complex(dct['real'], dct['imag'])\n    ...     return dct\n    ...\n    >>> json.loads('{\"__complex__\": true, \"real\": 1, \"imag\": 2}',\n    ...     object_hook=as_complex)\n    (1+2j)\n    >>> from decimal import Decimal\n    >>> json.loads('1.1', parse_float=Decimal) == Decimal('1.1')\n    True\n\nSpecializing JSON object encoding::\n\n    >>> import json\n    >>> def encode_complex(obj):\n    ...     if isinstance(obj, complex):\n    ...         return [obj.real, obj.imag]\n    ...     raise TypeError(f'Object of type {obj.__class__.__name__} '\n    ...                     f'is not JSON serializable')\n    ...\n    >>> json.dumps(2 + 1j, default=encode_complex)\n    '[2.0, 1.0]'\n    >>> json.JSONEncoder(default=encode_complex).encode(2 + 1j)\n    '[2.0, 1.0]'\n    >>> ''.join(json.JSONEncoder(default=encode_complex).iterencode(2 + 1j))\n    '[2.0, 1.0]'\n\n\nUsing json.tool from the shell to validate and pretty-print::\n\n    $ echo '{\"json\":\"obj\"}' | python -m json.tool\n    {\n        \"json\": \"obj\"\n    }\n    $ echo '{ 1.2:3.4}' | python -m json.tool\n    Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n\"\"\"\n__version__ = '2.0.9'\n__all__ = [\n    'dump', 'dumps', 'load', 'loads',\n    'JSONDecoder', 'JSONDecodeError', 'JSONEncoder',\n]\n\n__author__ = 'Bob Ippolito <bob@redivi.com>'\n\nfrom .decoder import JSONDecoder, JSONDecodeError\nfrom .encoder import JSONEncoder\nimport codecs\n\n_default_encoder = JSONEncoder(\n    skipkeys=False,\n    ensure_ascii=True,\n    check_circular=True,\n    allow_nan=True,\n    indent=None,\n    separators=None,\n    default=None,\n)\n\ndef dump(obj, fp, *, skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, cls=None, indent=None, separators=None,\n        default=None, sort_keys=False, **kw):\n    \"\"\"Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n    ``.write()``-supporting file-like object).\n\n    If ``skipkeys`` is true then ``dict`` keys that are not basic types\n    (``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped\n    instead of raising a ``TypeError``.\n\n    If ``ensure_ascii`` is false, then the strings written to ``fp`` can\n    contain non-ASCII characters if they appear in strings contained in\n    ``obj``. Otherwise, all such characters are escaped in JSON strings.\n\n    If ``check_circular`` is false, then the circular reference check\n    for container types will be skipped and a circular reference will\n    result in an ``OverflowError`` (or worse).\n\n    If ``allow_nan`` is false, then it will be a ``ValueError`` to\n    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``)\n    in strict compliance of the JSON specification, instead of using the\n    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\n    If ``indent`` is a non-negative integer, then JSON array elements and\n    object members will be pretty-printed with that indent level. An indent\n    level of 0 will only insert newlines. ``None`` is the most compact\n    representation.\n\n    If specified, ``separators`` should be an ``(item_separator, key_separator)``\n    tuple.  The default is ``(', ', ': ')`` if *indent* is ``None`` and\n    ``(',', ': ')`` otherwise.  To get the most compact JSON representation,\n    you should specify ``(',', ':')`` to eliminate whitespace.\n\n    ``default(obj)`` is a function that should return a serializable version\n    of obj or raise TypeError. The default simply raises TypeError.\n\n    If *sort_keys* is true (default: ``False``), then the output of\n    dictionaries will be sorted by key.\n\n    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n    ``.default()`` method to serialize additional types), specify it with\n    the ``cls`` kwarg; otherwise ``JSONEncoder`` is used.\n\n    \"\"\"\n    # cached encoder\n    if (not skipkeys and ensure_ascii and\n        check_circular and allow_nan and\n        cls is None and indent is None and separators is None and\n        default is None and not sort_keys and not kw):\n        iterable = _default_encoder.iterencode(obj)\n    else:\n        if cls is None:\n            cls = JSONEncoder\n        iterable = cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n            check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n            separators=separators,\n            default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n    # could accelerate with writelines in some versions of Python, at\n    # a debuggability cost\n    for chunk in iterable:\n        fp.write(chunk)\n\n\ndef dumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, cls=None, indent=None, separators=None,\n        default=None, sort_keys=False, **kw):\n    \"\"\"Serialize ``obj`` to a JSON formatted ``str``.\n\n    If ``skipkeys`` is true then ``dict`` keys that are not basic types\n    (``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped\n    instead of raising a ``TypeError``.\n\n    If ``ensure_ascii`` is false, then the return value can contain non-ASCII\n    characters if they appear in strings contained in ``obj``. Otherwise, all\n    such characters are escaped in JSON strings.\n\n    If ``check_circular`` is false, then the circular reference check\n    for container types will be skipped and a circular reference will\n    result in an ``OverflowError`` (or worse).\n\n    If ``allow_nan`` is false, then it will be a ``ValueError`` to\n    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in\n    strict compliance of the JSON specification, instead of using the\n    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\n    If ``indent`` is a non-negative integer, then JSON array elements and\n    object members will be pretty-printed with that indent level. An indent\n    level of 0 will only insert newlines. ``None`` is the most compact\n    representation.\n\n    If specified, ``separators`` should be an ``(item_separator, key_separator)``\n    tuple.  The default is ``(', ', ': ')`` if *indent* is ``None`` and\n    ``(',', ': ')`` otherwise.  To get the most compact JSON representation,\n    you should specify ``(',', ':')`` to eliminate whitespace.\n\n    ``default(obj)`` is a function that should return a serializable version\n    of obj or raise TypeError. The default simply raises TypeError.\n\n    If *sort_keys* is true (default: ``False``), then the output of\n    dictionaries will be sorted by key.\n\n    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n    ``.default()`` method to serialize additional types), specify it with\n    the ``cls`` kwarg; otherwise ``JSONEncoder`` is used.\n\n    \"\"\"\n    # cached encoder\n    if (not skipkeys and ensure_ascii and\n        check_circular and allow_nan and\n        cls is None and indent is None and separators is None and\n        default is None and not sort_keys and not kw):\n        return _default_encoder.encode(obj)\n    if cls is None:\n        cls = JSONEncoder\n    return cls(\n        skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n        check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n        separators=separators, default=default, sort_keys=sort_keys,\n        **kw).encode(obj)\n\n\n_default_decoder = JSONDecoder(object_hook=None, object_pairs_hook=None)\n\n\ndef detect_encoding(b):\n    bstartswith = b.startswith\n    if bstartswith((codecs.BOM_UTF32_BE, codecs.BOM_UTF32_LE)):\n        return 'utf-32'\n    if bstartswith((codecs.BOM_UTF16_BE, codecs.BOM_UTF16_LE)):\n        return 'utf-16'\n    if bstartswith(codecs.BOM_UTF8):\n        return 'utf-8-sig'\n\n    if len(b) >= 4:\n        if not b[0]:\n            # 00 00 -- -- - utf-32-be\n            # 00 XX -- -- - utf-16-be\n            return 'utf-16-be' if b[1] else 'utf-32-be'\n        if not b[1]:\n            # XX 00 00 00 - utf-32-le\n            # XX 00 00 XX - utf-16-le\n            # XX 00 XX -- - utf-16-le\n            return 'utf-16-le' if b[2] or b[3] else 'utf-32-le'\n    elif len(b) == 2:\n        if not b[0]:\n            # 00 XX - utf-16-be\n            return 'utf-16-be'\n        if not b[1]:\n            # XX 00 - utf-16-le\n            return 'utf-16-le'\n    # default\n    return 'utf-8'\n\n\ndef load(fp, *, cls=None, object_hook=None, parse_float=None,\n        parse_int=None, parse_constant=None, object_pairs_hook=None, **kw):\n    \"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\n    a JSON document) to a Python object.\n\n    ``object_hook`` is an optional function that will be called with the\n    result of any object literal decode (a ``dict``). The return value of\n    ``object_hook`` will be used instead of the ``dict``. This feature\n    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n    ``object_pairs_hook`` is an optional function that will be called with the\n    result of any object literal decoded with an ordered list of pairs.  The\n    return value of ``object_pairs_hook`` will be used instead of the ``dict``.\n    This feature can be used to implement custom decoders.  If ``object_hook``\n    is also defined, the ``object_pairs_hook`` takes priority.\n\n    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n    kwarg; otherwise ``JSONDecoder`` is used.\n    \"\"\"\n    return loads(fp.read(),\n        cls=cls, object_hook=object_hook,\n        parse_float=parse_float, parse_int=parse_int,\n        parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\n\ndef loads(s, *, encoding=None, cls=None, object_hook=None, parse_float=None,\n        parse_int=None, parse_constant=None, object_pairs_hook=None, **kw):\n    \"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\n    containing a JSON document) to a Python object.\n\n    ``object_hook`` is an optional function that will be called with the\n    result of any object literal decode (a ``dict``). The return value of\n    ``object_hook`` will be used instead of the ``dict``. This feature\n    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n    ``object_pairs_hook`` is an optional function that will be called with the\n    result of any object literal decoded with an ordered list of pairs.  The\n    return value of ``object_pairs_hook`` will be used instead of the ``dict``.\n    This feature can be used to implement custom decoders.  If ``object_hook``\n    is also defined, the ``object_pairs_hook`` takes priority.\n\n    ``parse_float``, if specified, will be called with the string\n    of every JSON float to be decoded. By default this is equivalent to\n    float(num_str). This can be used to use another datatype or parser\n    for JSON floats (e.g. decimal.Decimal).\n\n    ``parse_int``, if specified, will be called with the string\n    of every JSON int to be decoded. By default this is equivalent to\n    int(num_str). This can be used to use another datatype or parser\n    for JSON integers (e.g. float).\n\n    ``parse_constant``, if specified, will be called with one of the\n    following strings: -Infinity, Infinity, NaN.\n    This can be used to raise an exception if invalid JSON numbers\n    are encountered.\n\n    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n    kwarg; otherwise ``JSONDecoder`` is used.\n\n    The ``encoding`` argument is ignored and deprecated.\n    \"\"\"\n    if isinstance(s, str):\n        if s.startswith('\\ufeff'):\n            raise JSONDecodeError(\"Unexpected UTF-8 BOM (decode using utf-8-sig)\",\n                                  s, 0)\n    else:\n        if not isinstance(s, (bytes, bytearray)):\n            raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n                            f'not {s.__class__.__name__}')\n        s = s.decode(detect_encoding(s), 'surrogatepass')\n\n    if (cls is None and object_hook is None and\n            parse_int is None and parse_float is None and\n            parse_constant is None and object_pairs_hook is None and not kw):\n        return _default_decoder.decode(s)\n    if cls is None:\n        cls = JSONDecoder\n    if object_hook is not None:\n        kw['object_hook'] = object_hook\n    if object_pairs_hook is not None:\n        kw['object_pairs_hook'] = object_pairs_hook\n    if parse_float is not None:\n        kw['parse_float'] = parse_float\n    if parse_int is not None:\n        kw['parse_int'] = parse_int\n    if parse_constant is not None:\n        kw['parse_constant'] = parse_constant\n    return cls(**kw).decode(s)\n", 361], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/utils.py": ["import functools\nimport re\nimport warnings\nfrom dataclasses import is_dataclass\nfrom enum import Enum\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Set, Type, Union, cast\n\nimport fastapi\nfrom fastapi.datastructures import DefaultPlaceholder, DefaultType\nfrom fastapi.openapi.constants import REF_PREFIX\nfrom pydantic import BaseConfig, BaseModel, create_model\nfrom pydantic.class_validators import Validator\nfrom pydantic.fields import FieldInfo, ModelField, UndefinedType\nfrom pydantic.schema import model_process_schema\nfrom pydantic.utils import lenient_issubclass\n\nif TYPE_CHECKING:  # pragma: nocover\n    from .routing import APIRoute\n\n\ndef is_body_allowed_for_status_code(status_code: Union[int, str, None]) -> bool:\n    if status_code is None:\n        return True\n    # Ref: https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.1.0.md#patterned-fields-1\n    if status_code in {\n        \"default\",\n        \"1XX\",\n        \"2XX\",\n        \"3XX\",\n        \"4XX\",\n        \"5XX\",\n    }:\n        return True\n    current_status_code = int(status_code)\n    return not (current_status_code < 200 or current_status_code in {204, 304})\n\n\ndef get_model_definitions(\n    *,\n    flat_models: Set[Union[Type[BaseModel], Type[Enum]]],\n    model_name_map: Dict[Union[Type[BaseModel], Type[Enum]], str],\n) -> Dict[str, Any]:\n    definitions: Dict[str, Dict[str, Any]] = {}\n    for model in flat_models:\n        m_schema, m_definitions, m_nested_models = model_process_schema(\n            model, model_name_map=model_name_map, ref_prefix=REF_PREFIX\n        )\n        definitions.update(m_definitions)\n        model_name = model_name_map[model]\n        if \"description\" in m_schema:\n            m_schema[\"description\"] = m_schema[\"description\"].split(\"\\f\")[0]\n        definitions[model_name] = m_schema\n    return definitions\n\n\ndef get_path_param_names(path: str) -> Set[str]:\n    return set(re.findall(\"{(.*?)}\", path))\n\n\ndef create_response_field(\n    name: str,\n    type_: Type[Any],\n    class_validators: Optional[Dict[str, Validator]] = None,\n    default: Optional[Any] = None,\n    required: Union[bool, UndefinedType] = True,\n    model_config: Type[BaseConfig] = BaseConfig,\n    field_info: Optional[FieldInfo] = None,\n    alias: Optional[str] = None,\n) -> ModelField:\n    \"\"\"\n    Create a new response field. Raises if type_ is invalid.\n    \"\"\"\n    class_validators = class_validators or {}\n    field_info = field_info or FieldInfo()\n\n    response_field = functools.partial(\n        ModelField,\n        name=name,\n        type_=type_,\n        class_validators=class_validators,\n        default=default,\n        required=required,\n        model_config=model_config,\n        alias=alias,\n    )\n\n    try:\n        return response_field(field_info=field_info)\n    except RuntimeError:\n        raise fastapi.exceptions.FastAPIError(\n            \"Invalid args for response field! Hint: \"\n            f\"check that {type_} is a valid Pydantic field type. \"\n            \"If you are using a return type annotation that is not a valid Pydantic \"\n            \"field (e.g. Union[Response, dict, None]) you can disable generating the \"\n            \"response model from the type annotation with the path operation decorator \"\n            \"parameter response_model=None. Read more: \"\n            \"https://fastapi.tiangolo.com/tutorial/response-model/\"\n        ) from None\n\n\ndef create_cloned_field(\n    field: ModelField,\n    *,\n    cloned_types: Optional[Dict[Type[BaseModel], Type[BaseModel]]] = None,\n) -> ModelField:\n    # _cloned_types has already cloned types, to support recursive models\n    if cloned_types is None:\n        cloned_types = {}\n    original_type = field.type_\n    if is_dataclass(original_type) and hasattr(original_type, \"__pydantic_model__\"):\n        original_type = original_type.__pydantic_model__\n    use_type = original_type\n    if lenient_issubclass(original_type, BaseModel):\n        original_type = cast(Type[BaseModel], original_type)\n        use_type = cloned_types.get(original_type)\n        if use_type is None:\n            use_type = create_model(original_type.__name__, __base__=original_type)\n            cloned_types[original_type] = use_type\n            for f in original_type.__fields__.values():\n                use_type.__fields__[f.name] = create_cloned_field(\n                    f, cloned_types=cloned_types\n                )\n    new_field = create_response_field(name=field.name, type_=use_type)\n    new_field.has_alias = field.has_alias\n    new_field.alias = field.alias\n    new_field.class_validators = field.class_validators\n    new_field.default = field.default\n    new_field.required = field.required\n    new_field.model_config = field.model_config\n    new_field.field_info = field.field_info\n    new_field.allow_none = field.allow_none\n    new_field.validate_always = field.validate_always\n    if field.sub_fields:\n        new_field.sub_fields = [\n            create_cloned_field(sub_field, cloned_types=cloned_types)\n            for sub_field in field.sub_fields\n        ]\n    if field.key_field:\n        new_field.key_field = create_cloned_field(\n            field.key_field, cloned_types=cloned_types\n        )\n    new_field.validators = field.validators\n    new_field.pre_validators = field.pre_validators\n    new_field.post_validators = field.post_validators\n    new_field.parse_json = field.parse_json\n    new_field.shape = field.shape\n    new_field.populate_validators()\n    return new_field\n\n\ndef generate_operation_id_for_path(\n    *, name: str, path: str, method: str\n) -> str:  # pragma: nocover\n    warnings.warn(\n        \"fastapi.utils.generate_operation_id_for_path() was deprecated, \"\n        \"it is not used internally, and will be removed soon\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    operation_id = name + path\n    operation_id = re.sub(r\"\\W\", \"_\", operation_id)\n    operation_id = operation_id + \"_\" + method.lower()\n    return operation_id\n\n\ndef generate_unique_id(route: \"APIRoute\") -> str:\n    operation_id = route.name + route.path_format\n    operation_id = re.sub(r\"\\W\", \"_\", operation_id)\n    assert route.methods\n    operation_id = operation_id + \"_\" + list(route.methods)[0].lower()\n    return operation_id\n\n\ndef deep_dict_update(main_dict: Dict[Any, Any], update_dict: Dict[Any, Any]) -> None:\n    for key, value in update_dict.items():\n        if (\n            key in main_dict\n            and isinstance(main_dict[key], dict)\n            and isinstance(value, dict)\n        ):\n            deep_dict_update(main_dict[key], value)\n        elif (\n            key in main_dict\n            and isinstance(main_dict[key], list)\n            and isinstance(update_dict[key], list)\n        ):\n            main_dict[key] = main_dict[key] + update_dict[key]\n        else:\n            main_dict[key] = value\n\n\ndef get_value_or_default(\n    first_item: Union[DefaultPlaceholder, DefaultType],\n    *extra_items: Union[DefaultPlaceholder, DefaultType],\n) -> Union[DefaultPlaceholder, DefaultType]:\n    \"\"\"\n    Pass items or `DefaultPlaceholder`s by descending priority.\n\n    The first one to _not_ be a `DefaultPlaceholder` will be returned.\n\n    Otherwise, the first item (a `DefaultPlaceholder`) will be returned.\n    \"\"\"\n    items = (first_item,) + extra_items\n    for item in items:\n        if not isinstance(item, DefaultPlaceholder):\n            return item\n    return first_item\n", 207], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/testclient.py": ["import contextlib\nimport inspect\nimport io\nimport json\nimport math\nimport queue\nimport sys\nimport typing\nimport warnings\nfrom concurrent.futures import Future\nfrom types import GeneratorType\nfrom urllib.parse import unquote, urljoin\n\nimport anyio\nimport anyio.from_thread\nimport httpx\nfrom anyio.streams.stapled import StapledObjectStream\n\nfrom starlette._utils import is_async_callable\nfrom starlette.types import ASGIApp, Message, Receive, Scope, Send\nfrom starlette.websockets import WebSocketDisconnect\n\nif sys.version_info >= (3, 8):  # pragma: no cover\n    from typing import TypedDict\nelse:  # pragma: no cover\n    from typing_extensions import TypedDict\n\n_PortalFactoryType = typing.Callable[\n    [], typing.ContextManager[anyio.abc.BlockingPortal]\n]\n\nASGIInstance = typing.Callable[[Receive, Send], typing.Awaitable[None]]\nASGI2App = typing.Callable[[Scope], ASGIInstance]\nASGI3App = typing.Callable[[Scope, Receive, Send], typing.Awaitable[None]]\n\n\n_RequestData = typing.Mapping[str, typing.Union[str, typing.Iterable[str]]]\n\n\ndef _is_asgi3(app: typing.Union[ASGI2App, ASGI3App]) -> bool:\n    if inspect.isclass(app):\n        return hasattr(app, \"__await__\")\n    return is_async_callable(app)\n\n\nclass _WrapASGI2:\n    \"\"\"\n    Provide an ASGI3 interface onto an ASGI2 app.\n    \"\"\"\n\n    def __init__(self, app: ASGI2App) -> None:\n        self.app = app\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        instance = self.app(scope)\n        await instance(receive, send)\n\n\nclass _AsyncBackend(TypedDict):\n    backend: str\n    backend_options: typing.Dict[str, typing.Any]\n\n\nclass _Upgrade(Exception):\n    def __init__(self, session: \"WebSocketTestSession\") -> None:\n        self.session = session\n\n\nclass WebSocketTestSession:\n    def __init__(\n        self,\n        app: ASGI3App,\n        scope: Scope,\n        portal_factory: _PortalFactoryType,\n    ) -> None:\n        self.app = app\n        self.scope = scope\n        self.accepted_subprotocol = None\n        self.portal_factory = portal_factory\n        self._receive_queue: \"queue.Queue[typing.Any]\" = queue.Queue()\n        self._send_queue: \"queue.Queue[typing.Any]\" = queue.Queue()\n        self.extra_headers = None\n\n    def __enter__(self) -> \"WebSocketTestSession\":\n        self.exit_stack = contextlib.ExitStack()\n        self.portal = self.exit_stack.enter_context(self.portal_factory())\n\n        try:\n            _: \"Future[None]\" = self.portal.start_task_soon(self._run)\n            self.send({\"type\": \"websocket.connect\"})\n            message = self.receive()\n            self._raise_on_close(message)\n        except Exception:\n            self.exit_stack.close()\n            raise\n        self.accepted_subprotocol = message.get(\"subprotocol\", None)\n        self.extra_headers = message.get(\"headers\", None)\n        return self\n\n    def __exit__(self, *args: typing.Any) -> None:\n        try:\n            self.close(1000)\n        finally:\n            self.exit_stack.close()\n        while not self._send_queue.empty():\n            message = self._send_queue.get()\n            if isinstance(message, BaseException):\n                raise message\n\n    async def _run(self) -> None:\n        \"\"\"\n        The sub-thread in which the websocket session runs.\n        \"\"\"\n        scope = self.scope\n        receive = self._asgi_receive\n        send = self._asgi_send\n        try:\n            await self.app(scope, receive, send)\n        except BaseException as exc:\n            self._send_queue.put(exc)\n            raise\n\n    async def _asgi_receive(self) -> Message:\n        while self._receive_queue.empty():\n            await anyio.sleep(0)\n        return self._receive_queue.get()\n\n    async def _asgi_send(self, message: Message) -> None:\n        self._send_queue.put(message)\n\n    def _raise_on_close(self, message: Message) -> None:\n        if message[\"type\"] == \"websocket.close\":\n            raise WebSocketDisconnect(\n                message.get(\"code\", 1000), message.get(\"reason\", \"\")\n            )\n\n    def send(self, message: Message) -> None:\n        self._receive_queue.put(message)\n\n    def send_text(self, data: str) -> None:\n        self.send({\"type\": \"websocket.receive\", \"text\": data})\n\n    def send_bytes(self, data: bytes) -> None:\n        self.send({\"type\": \"websocket.receive\", \"bytes\": data})\n\n    def send_json(self, data: typing.Any, mode: str = \"text\") -> None:\n        assert mode in [\"text\", \"binary\"]\n        text = json.dumps(data)\n        if mode == \"text\":\n            self.send({\"type\": \"websocket.receive\", \"text\": text})\n        else:\n            self.send({\"type\": \"websocket.receive\", \"bytes\": text.encode(\"utf-8\")})\n\n    def close(self, code: int = 1000) -> None:\n        self.send({\"type\": \"websocket.disconnect\", \"code\": code})\n\n    def receive(self) -> Message:\n        message = self._send_queue.get()\n        if isinstance(message, BaseException):\n            raise message\n        return message\n\n    def receive_text(self) -> str:\n        message = self.receive()\n        self._raise_on_close(message)\n        return message[\"text\"]\n\n    def receive_bytes(self) -> bytes:\n        message = self.receive()\n        self._raise_on_close(message)\n        return message[\"bytes\"]\n\n    def receive_json(self, mode: str = \"text\") -> typing.Any:\n        assert mode in [\"text\", \"binary\"]\n        message = self.receive()\n        self._raise_on_close(message)\n        if mode == \"text\":\n            text = message[\"text\"]\n        else:\n            text = message[\"bytes\"].decode(\"utf-8\")\n        return json.loads(text)\n\n\nclass _TestClientTransport(httpx.BaseTransport):\n    def __init__(\n        self,\n        app: ASGI3App,\n        portal_factory: _PortalFactoryType,\n        raise_server_exceptions: bool = True,\n        root_path: str = \"\",\n    ) -> None:\n        self.app = app\n        self.raise_server_exceptions = raise_server_exceptions\n        self.root_path = root_path\n        self.portal_factory = portal_factory\n\n    def handle_request(self, request: httpx.Request) -> httpx.Response:\n        scheme = request.url.scheme\n        netloc = request.url.netloc.decode(encoding=\"ascii\")\n        path = request.url.path\n        raw_path = request.url.raw_path\n        query = request.url.query.decode(encoding=\"ascii\")\n\n        default_port = {\"http\": 80, \"ws\": 80, \"https\": 443, \"wss\": 443}[scheme]\n\n        if \":\" in netloc:\n            host, port_string = netloc.split(\":\", 1)\n            port = int(port_string)\n        else:\n            host = netloc\n            port = default_port\n\n        # Include the 'host' header.\n        if \"host\" in request.headers:\n            headers: typing.List[typing.Tuple[bytes, bytes]] = []\n        elif port == default_port:  # pragma: no cover\n            headers = [(b\"host\", host.encode())]\n        else:  # pragma: no cover\n            headers = [(b\"host\", (f\"{host}:{port}\").encode())]\n\n        # Include other request headers.\n        headers += [\n            (key.lower().encode(), value.encode())\n            for key, value in request.headers.items()\n        ]\n\n        scope: typing.Dict[str, typing.Any]\n\n        if scheme in {\"ws\", \"wss\"}:\n            subprotocol = request.headers.get(\"sec-websocket-protocol\", None)\n            if subprotocol is None:\n                subprotocols: typing.Sequence[str] = []\n            else:\n                subprotocols = [value.strip() for value in subprotocol.split(\",\")]\n            scope = {\n                \"type\": \"websocket\",\n                \"path\": unquote(path),\n                \"raw_path\": raw_path,\n                \"root_path\": self.root_path,\n                \"scheme\": scheme,\n                \"query_string\": query.encode(),\n                \"headers\": headers,\n                \"client\": [\"testclient\", 50000],\n                \"server\": [host, port],\n                \"subprotocols\": subprotocols,\n            }\n            session = WebSocketTestSession(self.app, scope, self.portal_factory)\n            raise _Upgrade(session)\n\n        scope = {\n            \"type\": \"http\",\n            \"http_version\": \"1.1\",\n            \"method\": request.method,\n            \"path\": unquote(path),\n            \"raw_path\": raw_path,\n            \"root_path\": self.root_path,\n            \"scheme\": scheme,\n            \"query_string\": query.encode(),\n            \"headers\": headers,\n            \"client\": [\"testclient\", 50000],\n            \"server\": [host, port],\n            \"extensions\": {\"http.response.debug\": {}},\n        }\n\n        request_complete = False\n        response_started = False\n        response_complete: anyio.Event\n        raw_kwargs: typing.Dict[str, typing.Any] = {\"stream\": io.BytesIO()}\n        template = None\n        context = None\n\n        async def receive() -> Message:\n            nonlocal request_complete\n\n            if request_complete:\n                if not response_complete.is_set():\n                    await response_complete.wait()\n                return {\"type\": \"http.disconnect\"}\n\n            body = request.read()\n            if isinstance(body, str):\n                body_bytes: bytes = body.encode(\"utf-8\")  # pragma: no cover\n            elif body is None:\n                body_bytes = b\"\"  # pragma: no cover\n            elif isinstance(body, GeneratorType):\n                try:  # pragma: no cover\n                    chunk = body.send(None)\n                    if isinstance(chunk, str):\n                        chunk = chunk.encode(\"utf-8\")\n                    return {\"type\": \"http.request\", \"body\": chunk, \"more_body\": True}\n                except StopIteration:  # pragma: no cover\n                    request_complete = True\n                    return {\"type\": \"http.request\", \"body\": b\"\"}\n            else:\n                body_bytes = body\n\n            request_complete = True\n            return {\"type\": \"http.request\", \"body\": body_bytes}\n\n        async def send(message: Message) -> None:\n            nonlocal raw_kwargs, response_started, template, context\n\n            if message[\"type\"] == \"http.response.start\":\n                assert (\n                    not response_started\n                ), 'Received multiple \"http.response.start\" messages.'\n                raw_kwargs[\"status_code\"] = message[\"status\"]\n                raw_kwargs[\"headers\"] = [\n                    (key.decode(), value.decode())\n                    for key, value in message.get(\"headers\", [])\n                ]\n                response_started = True\n            elif message[\"type\"] == \"http.response.body\":\n                assert (\n                    response_started\n                ), 'Received \"http.response.body\" without \"http.response.start\".'\n                assert (\n                    not response_complete.is_set()\n                ), 'Received \"http.response.body\" after response completed.'\n                body = message.get(\"body\", b\"\")\n                more_body = message.get(\"more_body\", False)\n                if request.method != \"HEAD\":\n                    raw_kwargs[\"stream\"].write(body)\n                if not more_body:\n                    raw_kwargs[\"stream\"].seek(0)\n                    response_complete.set()\n            elif message[\"type\"] == \"http.response.debug\":\n                template = message[\"info\"][\"template\"]\n                context = message[\"info\"][\"context\"]\n\n        try:\n            with self.portal_factory() as portal:\n                response_complete = portal.call(anyio.Event)\n                portal.call(self.app, scope, receive, send)\n        except BaseException as exc:\n            if self.raise_server_exceptions:\n                raise exc\n\n        if self.raise_server_exceptions:\n            assert response_started, \"TestClient did not receive any response.\"\n        elif not response_started:\n            raw_kwargs = {\n                \"status_code\": 500,\n                \"headers\": [],\n                \"stream\": io.BytesIO(),\n            }\n\n        raw_kwargs[\"stream\"] = httpx.ByteStream(raw_kwargs[\"stream\"].read())\n\n        response = httpx.Response(**raw_kwargs, request=request)\n        if template is not None:\n            response.template = template  # type: ignore[attr-defined]\n            response.context = context  # type: ignore[attr-defined]\n        return response\n\n\nclass TestClient(httpx.Client):\n    __test__ = False\n    task: \"Future[None]\"\n    portal: typing.Optional[anyio.abc.BlockingPortal] = None\n\n    def __init__(\n        self,\n        app: ASGIApp,\n        base_url: str = \"http://testserver\",\n        raise_server_exceptions: bool = True,\n        root_path: str = \"\",\n        backend: str = \"asyncio\",\n        backend_options: typing.Optional[typing.Dict[str, typing.Any]] = None,\n        cookies: httpx._client.CookieTypes = None,\n        headers: typing.Dict[str, str] = None,\n    ) -> None:\n        self.async_backend = _AsyncBackend(\n            backend=backend, backend_options=backend_options or {}\n        )\n        if _is_asgi3(app):\n            app = typing.cast(ASGI3App, app)\n            asgi_app = app\n        else:\n            app = typing.cast(ASGI2App, app)  # type: ignore[assignment]\n            asgi_app = _WrapASGI2(app)  # type: ignore[arg-type]\n        self.app = asgi_app\n        transport = _TestClientTransport(\n            self.app,\n            portal_factory=self._portal_factory,\n            raise_server_exceptions=raise_server_exceptions,\n            root_path=root_path,\n        )\n        if headers is None:\n            headers = {}\n        headers.setdefault(\"user-agent\", \"testclient\")\n        super().__init__(\n            app=self.app,\n            base_url=base_url,\n            headers=headers,\n            transport=transport,\n            follow_redirects=True,\n            cookies=cookies,\n        )\n\n    @contextlib.contextmanager\n    def _portal_factory(self) -> typing.Generator[anyio.abc.BlockingPortal, None, None]:\n        if self.portal is not None:\n            yield self.portal\n        else:\n            with anyio.from_thread.start_blocking_portal(\n                **self.async_backend\n            ) as portal:\n                yield portal\n\n    def _choose_redirect_arg(\n        self,\n        follow_redirects: typing.Optional[bool],\n        allow_redirects: typing.Optional[bool],\n    ) -> typing.Union[bool, httpx._client.UseClientDefault]:\n        redirect: typing.Union[\n            bool, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT\n        if allow_redirects is not None:\n            message = (\n                \"The `allow_redirects` argument is deprecated. \"\n                \"Use `follow_redirects` instead.\"\n            )\n            warnings.warn(message, DeprecationWarning)\n            redirect = allow_redirects\n        if follow_redirects is not None:\n            redirect = follow_redirects\n        elif allow_redirects is not None and follow_redirects is not None:\n            raise RuntimeError(  # pragma: no cover\n                \"Cannot use both `allow_redirects` and `follow_redirects`.\"\n            )\n        return redirect\n\n    def request(  # type: ignore[override]\n        self,\n        method: str,\n        url: httpx._types.URLTypes,\n        *,\n        content: typing.Optional[httpx._types.RequestContent] = None,\n        data: typing.Optional[_RequestData] = None,\n        files: typing.Optional[httpx._types.RequestFiles] = None,\n        json: typing.Any = None,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        url = self.base_url.join(url)\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().request(\n            method,\n            url,\n            content=content,\n            data=data,  # type: ignore[arg-type]\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def get(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().get(\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def options(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().options(\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def head(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().head(\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def post(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        content: typing.Optional[httpx._types.RequestContent] = None,\n        data: typing.Optional[_RequestData] = None,\n        files: typing.Optional[httpx._types.RequestFiles] = None,\n        json: typing.Any = None,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().post(\n            url,\n            content=content,\n            data=data,  # type: ignore[arg-type]\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def put(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        content: typing.Optional[httpx._types.RequestContent] = None,\n        data: typing.Optional[_RequestData] = None,\n        files: typing.Optional[httpx._types.RequestFiles] = None,\n        json: typing.Any = None,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().put(\n            url,\n            content=content,\n            data=data,  # type: ignore[arg-type]\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def patch(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        content: typing.Optional[httpx._types.RequestContent] = None,\n        data: typing.Optional[_RequestData] = None,\n        files: typing.Optional[httpx._types.RequestFiles] = None,\n        json: typing.Any = None,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().patch(\n            url,\n            content=content,\n            data=data,  # type: ignore[arg-type]\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def delete(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().delete(\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def websocket_connect(\n        self, url: str, subprotocols: typing.Sequence[str] = None, **kwargs: typing.Any\n    ) -> typing.Any:\n        url = urljoin(\"ws://testserver\", url)\n        headers = kwargs.get(\"headers\", {})\n        headers.setdefault(\"connection\", \"upgrade\")\n        headers.setdefault(\"sec-websocket-key\", \"testserver==\")\n        headers.setdefault(\"sec-websocket-version\", \"13\")\n        if subprotocols is not None:\n            headers.setdefault(\"sec-websocket-protocol\", \", \".join(subprotocols))\n        kwargs[\"headers\"] = headers\n        try:\n            super().request(\"GET\", url, **kwargs)\n        except _Upgrade as exc:\n            session = exc.session\n        else:\n            raise RuntimeError(\"Expected WebSocket upgrade\")  # pragma: no cover\n\n        return session\n\n    def __enter__(self) -> \"TestClient\":\n        with contextlib.ExitStack() as stack:\n            self.portal = portal = stack.enter_context(\n                anyio.from_thread.start_blocking_portal(**self.async_backend)\n            )\n\n            @stack.callback\n            def reset_portal() -> None:\n                self.portal = None\n\n            self.stream_send = StapledObjectStream(\n                *anyio.create_memory_object_stream(math.inf)\n            )\n            self.stream_receive = StapledObjectStream(\n                *anyio.create_memory_object_stream(math.inf)\n            )\n            self.task = portal.start_task_soon(self.lifespan)\n            portal.call(self.wait_startup)\n\n            @stack.callback\n            def wait_shutdown() -> None:\n                portal.call(self.wait_shutdown)\n\n            self.exit_stack = stack.pop_all()\n\n        return self\n\n    def __exit__(self, *args: typing.Any) -> None:\n        self.exit_stack.close()\n\n    async def lifespan(self) -> None:\n        scope = {\"type\": \"lifespan\"}\n        try:\n            await self.app(scope, self.stream_receive.receive, self.stream_send.send)\n        finally:\n            await self.stream_send.send(None)\n\n    async def wait_startup(self) -> None:\n        await self.stream_receive.send({\"type\": \"lifespan.startup\"})\n\n        async def receive() -> typing.Any:\n            message = await self.stream_send.receive()\n            if message is None:\n                self.task.result()\n            return message\n\n        message = await receive()\n        assert message[\"type\"] in (\n            \"lifespan.startup.complete\",\n            \"lifespan.startup.failed\",\n        )\n        if message[\"type\"] == \"lifespan.startup.failed\":\n            await receive()\n\n    async def wait_shutdown(self) -> None:\n        async def receive() -> typing.Any:\n            message = await self.stream_send.receive()\n            if message is None:\n                self.task.result()\n            return message\n\n        async with self.stream_send:\n            await self.stream_receive.send({\"type\": \"lifespan.shutdown\"})\n            message = await receive()\n            assert message[\"type\"] in (\n                \"lifespan.shutdown.complete\",\n                \"lifespan.shutdown.failed\",\n            )\n            if message[\"type\"] == \"lifespan.shutdown.failed\":\n                await receive()\n", 790], "/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/locks.py": ["\"\"\"Synchronization primitives.\"\"\"\n\n__all__ = ('Lock', 'Event', 'Condition', 'Semaphore', 'BoundedSemaphore')\n\nimport collections\nimport warnings\n\nfrom . import events\nfrom . import futures\nfrom .coroutines import coroutine\n\n\nclass _ContextManager:\n    \"\"\"Context manager.\n\n    This enables the following idiom for acquiring and releasing a\n    lock around a block:\n\n        with (yield from lock):\n            <block>\n\n    while failing loudly when accidentally using:\n\n        with lock:\n            <block>\n\n    Deprecated, use 'async with' statement:\n        async with lock:\n            <block>\n    \"\"\"\n\n    def __init__(self, lock):\n        self._lock = lock\n\n    def __enter__(self):\n        # We have no use for the \"as ...\"  clause in the with\n        # statement for locks.\n        return None\n\n    def __exit__(self, *args):\n        try:\n            self._lock.release()\n        finally:\n            self._lock = None  # Crudely prevent reuse.\n\n\nclass _ContextManagerMixin:\n    def __enter__(self):\n        raise RuntimeError(\n            '\"yield from\" should be used as context manager expression')\n\n    def __exit__(self, *args):\n        # This must exist because __enter__ exists, even though that\n        # always raises; that's how the with-statement works.\n        pass\n\n    @coroutine\n    def __iter__(self):\n        # This is not a coroutine.  It is meant to enable the idiom:\n        #\n        #     with (yield from lock):\n        #         <block>\n        #\n        # as an alternative to:\n        #\n        #     yield from lock.acquire()\n        #     try:\n        #         <block>\n        #     finally:\n        #         lock.release()\n        # Deprecated, use 'async with' statement:\n        #     async with lock:\n        #         <block>\n        warnings.warn(\"'with (yield from lock)' is deprecated \"\n                      \"use 'async with lock' instead\",\n                      DeprecationWarning, stacklevel=2)\n        yield from self.acquire()\n        return _ContextManager(self)\n\n    async def __acquire_ctx(self):\n        await self.acquire()\n        return _ContextManager(self)\n\n    def __await__(self):\n        warnings.warn(\"'with await lock' is deprecated \"\n                      \"use 'async with lock' instead\",\n                      DeprecationWarning, stacklevel=2)\n        # To make \"with await lock\" work.\n        return self.__acquire_ctx().__await__()\n\n    async def __aenter__(self):\n        await self.acquire()\n        # We have no use for the \"as ...\"  clause in the with\n        # statement for locks.\n        return None\n\n    async def __aexit__(self, exc_type, exc, tb):\n        self.release()\n\n\nclass Lock(_ContextManagerMixin):\n    \"\"\"Primitive lock objects.\n\n    A primitive lock is a synchronization primitive that is not owned\n    by a particular coroutine when locked.  A primitive lock is in one\n    of two states, 'locked' or 'unlocked'.\n\n    It is created in the unlocked state.  It has two basic methods,\n    acquire() and release().  When the state is unlocked, acquire()\n    changes the state to locked and returns immediately.  When the\n    state is locked, acquire() blocks until a call to release() in\n    another coroutine changes it to unlocked, then the acquire() call\n    resets it to locked and returns.  The release() method should only\n    be called in the locked state; it changes the state to unlocked\n    and returns immediately.  If an attempt is made to release an\n    unlocked lock, a RuntimeError will be raised.\n\n    When more than one coroutine is blocked in acquire() waiting for\n    the state to turn to unlocked, only one coroutine proceeds when a\n    release() call resets the state to unlocked; first coroutine which\n    is blocked in acquire() is being processed.\n\n    acquire() is a coroutine and should be called with 'await'.\n\n    Locks also support the asynchronous context management protocol.\n    'async with lock' statement should be used.\n\n    Usage:\n\n        lock = Lock()\n        ...\n        await lock.acquire()\n        try:\n            ...\n        finally:\n            lock.release()\n\n    Context manager usage:\n\n        lock = Lock()\n        ...\n        async with lock:\n             ...\n\n    Lock objects can be tested for locking state:\n\n        if not lock.locked():\n           await lock.acquire()\n        else:\n           # lock is acquired\n           ...\n\n    \"\"\"\n\n    def __init__(self, *, loop=None):\n        self._waiters = collections.deque()\n        self._locked = False\n        if loop is not None:\n            self._loop = loop\n        else:\n            self._loop = events.get_event_loop()\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self._locked else 'unlocked'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def locked(self):\n        \"\"\"Return True if lock is acquired.\"\"\"\n        return self._locked\n\n    async def acquire(self):\n        \"\"\"Acquire a lock.\n\n        This method blocks until the lock is unlocked, then sets it to\n        locked and returns True.\n        \"\"\"\n        if not self._locked and all(w.cancelled() for w in self._waiters):\n            self._locked = True\n            return True\n\n        fut = self._loop.create_future()\n        self._waiters.append(fut)\n\n        # Finally block should be called before the CancelledError\n        # handling as we don't want CancelledError to call\n        # _wake_up_first() and attempt to wake up itself.\n        try:\n            try:\n                await fut\n            finally:\n                self._waiters.remove(fut)\n        except futures.CancelledError:\n            if not self._locked:\n                self._wake_up_first()\n            raise\n\n        self._locked = True\n        return True\n\n    def release(self):\n        \"\"\"Release a lock.\n\n        When the lock is locked, reset it to unlocked, and return.\n        If any other coroutines are blocked waiting for the lock to become\n        unlocked, allow exactly one of them to proceed.\n\n        When invoked on an unlocked lock, a RuntimeError is raised.\n\n        There is no return value.\n        \"\"\"\n        if self._locked:\n            self._locked = False\n            self._wake_up_first()\n        else:\n            raise RuntimeError('Lock is not acquired.')\n\n    def _wake_up_first(self):\n        \"\"\"Wake up the first waiter if it isn't done.\"\"\"\n        try:\n            fut = next(iter(self._waiters))\n        except StopIteration:\n            return\n\n        # .done() necessarily means that a waiter will wake up later on and\n        # either take the lock, or, if it was cancelled and lock wasn't\n        # taken already, will hit this again and wake up a new waiter.\n        if not fut.done():\n            fut.set_result(True)\n\n\nclass Event:\n    \"\"\"Asynchronous equivalent to threading.Event.\n\n    Class implementing event objects. An event manages a flag that can be set\n    to true with the set() method and reset to false with the clear() method.\n    The wait() method blocks until the flag is true. The flag is initially\n    false.\n    \"\"\"\n\n    def __init__(self, *, loop=None):\n        self._waiters = collections.deque()\n        self._value = False\n        if loop is not None:\n            self._loop = loop\n        else:\n            self._loop = events.get_event_loop()\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'set' if self._value else 'unset'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def is_set(self):\n        \"\"\"Return True if and only if the internal flag is true.\"\"\"\n        return self._value\n\n    def set(self):\n        \"\"\"Set the internal flag to true. All coroutines waiting for it to\n        become true are awakened. Coroutine that call wait() once the flag is\n        true will not block at all.\n        \"\"\"\n        if not self._value:\n            self._value = True\n\n            for fut in self._waiters:\n                if not fut.done():\n                    fut.set_result(True)\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false. Subsequently, coroutines calling\n        wait() will block until set() is called to set the internal flag\n        to true again.\"\"\"\n        self._value = False\n\n    async def wait(self):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return True\n        immediately.  Otherwise, block until another coroutine calls\n        set() to set the flag to true, then return True.\n        \"\"\"\n        if self._value:\n            return True\n\n        fut = self._loop.create_future()\n        self._waiters.append(fut)\n        try:\n            await fut\n            return True\n        finally:\n            self._waiters.remove(fut)\n\n\nclass Condition(_ContextManagerMixin):\n    \"\"\"Asynchronous equivalent to threading.Condition.\n\n    This class implements condition variable objects. A condition variable\n    allows one or more coroutines to wait until they are notified by another\n    coroutine.\n\n    A new Lock object is created and used as the underlying lock.\n    \"\"\"\n\n    def __init__(self, lock=None, *, loop=None):\n        if loop is not None:\n            self._loop = loop\n        else:\n            self._loop = events.get_event_loop()\n\n        if lock is None:\n            lock = Lock(loop=self._loop)\n        elif lock._loop is not self._loop:\n            raise ValueError(\"loop argument must agree with lock\")\n\n        self._lock = lock\n        # Export the lock's locked(), acquire() and release() methods.\n        self.locked = lock.locked\n        self.acquire = lock.acquire\n        self.release = lock.release\n\n        self._waiters = collections.deque()\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self.locked() else 'unlocked'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    async def wait(self):\n        \"\"\"Wait until notified.\n\n        If the calling coroutine has not acquired the lock when this\n        method is called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks\n        until it is awakened by a notify() or notify_all() call for\n        the same condition variable in another coroutine.  Once\n        awakened, it re-acquires the lock and returns True.\n        \"\"\"\n        if not self.locked():\n            raise RuntimeError('cannot wait on un-acquired lock')\n\n        self.release()\n        try:\n            fut = self._loop.create_future()\n            self._waiters.append(fut)\n            try:\n                await fut\n                return True\n            finally:\n                self._waiters.remove(fut)\n\n        finally:\n            # Must reacquire lock even if wait is cancelled\n            cancelled = False\n            while True:\n                try:\n                    await self.acquire()\n                    break\n                except futures.CancelledError:\n                    cancelled = True\n\n            if cancelled:\n                raise futures.CancelledError\n\n    async def wait_for(self, predicate):\n        \"\"\"Wait until a predicate becomes true.\n\n        The predicate should be a callable which result will be\n        interpreted as a boolean value.  The final predicate value is\n        the return value.\n        \"\"\"\n        result = predicate()\n        while not result:\n            await self.wait()\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"By default, wake up one coroutine waiting on this condition, if any.\n        If the calling coroutine has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        This method wakes up at most n of the coroutines waiting for the\n        condition variable; it is a no-op if no coroutines are waiting.\n\n        Note: an awakened coroutine does not actually return from its\n        wait() call until it can reacquire the lock. Since notify() does\n        not release the lock, its caller should.\n        \"\"\"\n        if not self.locked():\n            raise RuntimeError('cannot notify on un-acquired lock')\n\n        idx = 0\n        for fut in self._waiters:\n            if idx >= n:\n                break\n\n            if not fut.done():\n                idx += 1\n                fut.set_result(False)\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition. This method acts\n        like notify(), but wakes up all waiting threads instead of one. If the\n        calling thread has not acquired the lock when this method is called,\n        a RuntimeError is raised.\n        \"\"\"\n        self.notify(len(self._waiters))\n\n\nclass Semaphore(_ContextManagerMixin):\n    \"\"\"A Semaphore implementation.\n\n    A semaphore manages an internal counter which is decremented by each\n    acquire() call and incremented by each release() call. The counter\n    can never go below zero; when acquire() finds that it is zero, it blocks,\n    waiting until some other thread calls release().\n\n    Semaphores also support the context management protocol.\n\n    The optional argument gives the initial value for the internal\n    counter; it defaults to 1. If the value given is less than 0,\n    ValueError is raised.\n    \"\"\"\n\n    def __init__(self, value=1, *, loop=None):\n        if value < 0:\n            raise ValueError(\"Semaphore initial value must be >= 0\")\n        self._value = value\n        self._waiters = collections.deque()\n        if loop is not None:\n            self._loop = loop\n        else:\n            self._loop = events.get_event_loop()\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self.locked() else f'unlocked, value:{self._value}'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def _wake_up_next(self):\n        while self._waiters:\n            waiter = self._waiters.popleft()\n            if not waiter.done():\n                waiter.set_result(None)\n                return\n\n    def locked(self):\n        \"\"\"Returns True if semaphore can not be acquired immediately.\"\"\"\n        return self._value == 0\n\n    async def acquire(self):\n        \"\"\"Acquire a semaphore.\n\n        If the internal counter is larger than zero on entry,\n        decrement it by one and return True immediately.  If it is\n        zero on entry, block, waiting until some other coroutine has\n        called release() to make it larger than 0, and then return\n        True.\n        \"\"\"\n        while self._value <= 0:\n            fut = self._loop.create_future()\n            self._waiters.append(fut)\n            try:\n                await fut\n            except:\n                # See the similar code in Queue.get.\n                fut.cancel()\n                if self._value > 0 and not fut.cancelled():\n                    self._wake_up_next()\n                raise\n        self._value -= 1\n        return True\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n        When it was zero on entry and another coroutine is waiting for it to\n        become larger than zero again, wake up that coroutine.\n        \"\"\"\n        self._value += 1\n        self._wake_up_next()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"A bounded semaphore implementation.\n\n    This raises ValueError in release() if it would increase the value\n    above the initial value.\n    \"\"\"\n\n    def __init__(self, value=1, *, loop=None):\n        self._bound_value = value\n        super().__init__(value, loop=loop)\n\n    def release(self):\n        if self._value >= self._bound_value:\n            raise ValueError('BoundedSemaphore released too many times')\n        super().release()\n", 507]}, "functions": {"__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:387)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", 387], "__aenter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:628)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", 628], "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:236)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py", 236], "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:471)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py", 471], "__new__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:814)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py", 814], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:69)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py", 69], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:193)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py", 193], "render (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:58)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py", 58], "init_headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:65)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py", 65], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:43)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py", 43], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:514)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py", 514], "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:98)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py", 98], "__delitem__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:619)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py", 619], "path_params (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:122)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py", 122], "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py:595)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py", 595], "_coerce_args (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py:112)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py", 112], "<listcomp> (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py:739)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py", 739], "parse_qsl (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py:697)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/parse.py", 697], "<dictcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:291)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py", 291], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:257)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py", 257], "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:420)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py", 420], "<dictcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:421)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py", 421], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:397)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py", 397], "query_params (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:116)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py", 116], "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:110)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py", 110], "__getitem__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:563)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py", 563], "get (/usr/local/opt/python@3.7/bin/../Frameworks/Python.framework/Versions/3.7/lib/python3.7/_collections_abc.py:657)": ["/usr/local/opt/python@3.7/bin/../Frameworks/Python.framework/Versions/3.7/lib/python3.7/_collections_abc.py", 657], "cookies (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py:126)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/requests.py", 126], "solve_dependencies (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py:467)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/dependencies/utils.py", 467], "current_task (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:27)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py", 27], "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py:25)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/sniffio/_impl.py", 25], "get_asynclib (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:147)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py", 147], "__sleep0 (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:570)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py", 570], "sleep (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:582)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py", 582], "checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:514)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 514], "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:894)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 894], "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/to_thread.py:10)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/to_thread.py", 10], "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/concurrency.py:35)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/concurrency.py", 35], "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:155)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py", 155], "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:190)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py", 190], "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:63)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py", 63], "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:265)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py", 265], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py:685)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/routing.py", 685], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py:12)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/middleware/asyncexitstack.py", 12], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py:53)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py", 53], "_check_closed (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:477)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py", 477], "get_debug (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1804)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py", 1804], "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:39)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py", 39], "_call_soon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:710)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py", 710], "call_soon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:681)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py", 681], "select (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py:553)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py", 553], "_process_events (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py:557)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py", 557], "time (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:634)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py", 634], "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:60)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py", 60], "__getitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:395)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py", 395], "__setitem__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:408)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py", 408], "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:107)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py", 107], "get (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:131)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py", 131], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:82)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py", 82], "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py:146)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/lowlevel.py", 146], "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1852)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1852], "total_tokens (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1875)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1875], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1855)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1855], "current_default_thread_limiter (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1973)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1973], "cancel_called (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:498)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 498], "shield (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:502)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 502], "checkpoint_if_cancelled (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:518)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 518], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_compat.py:126)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_compat.py", 126], "acquire_on_behalf_of_nowait (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1908)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1908], "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:310)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 310], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:315)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 315], "cast (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py:898)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/typing.py", 898], "_timeout (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:401)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 401], "__enter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:327)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 327], "cancel_shielded_checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:537)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 537], "acquire_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1924)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1924], "acquire (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1921)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1921], "__aenter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1860)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1860], "__call__ (/Users/shopbox/projects/profyle/profyle/middleware/fastapi.py:25)": ["/Users/shopbox/projects/profyle/profyle/middleware/fastapi.py", 25], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/errors.py:147)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/errors.py", 147], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/applications.py:114)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/applications.py", 114], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/applications.py:268)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/applications.py", 268], "_call_func (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/from_thread.py:200)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/from_thread.py", 200], "_run_wrapped_task (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:694)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 694], "_run (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py:86)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py", 86], "_run_once (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1690)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py", 1690], "get (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py:432)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/weakref.py", 432], "_deliver_cancellation_to_parent (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:445)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 445], "__exit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:349)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 349], "__len__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:67)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py", 67], "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:16)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py", 16], "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:20)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py", 20], "__iter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:58)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py", 58], "_commit_removals (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:52)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py", 52], "__exit__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:26)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py", 26], "_get_loop (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py:275)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py", 275], "<setcomp> (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:53)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py", 53], "all_tasks (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py:34)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py", 34], "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:89)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 89], "_get_task_callbacks (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:88)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 88], "find_root_task (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:187)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 187], "current_thread (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:1225)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 1225], "daemon (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:1116)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 1116], "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:216)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 216], "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:499)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 499], "add (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py:81)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_weakrefset.py", 81], "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:763)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 763], "_init (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:205)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py", 205], "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:33)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py", 33], "current_time (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:559)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 559], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:823)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 823], "is_set (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:507)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 507], "__init__ (/Users/shopbox/.vscode/extensions/ms-python.python-2023.4.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:1084)": ["/Users/shopbox/.vscode/extensions/ms-python.python-2023.4.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py", 1084], "pydev_start_new_thread (/Users/shopbox/.vscode/extensions/ms-python.python-2023.4.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:1174)": ["/Users/shopbox/.vscode/extensions/ms-python.python-2023.4.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py", 1174], "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:240)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 240], "_is_owned (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:255)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 255], "_release_save (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:249)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 249], "_acquire_restore (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:252)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 252], "wait (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:264)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 264], "__exit__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:243)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 243], "wait (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:534)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 534], "start (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:834)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 834], "_qsize (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:208)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py", 208], "_put (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:212)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py", 212], "notify (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:335)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 335], "put (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:121)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py", 121], "put_nowait (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:184)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py", 184], "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:81)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", 81], "helper (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:237)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", 237], "claim_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py:137)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_core/_eventloop.py", 137], "__enter__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:107)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", 107], "_get (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:216)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py", 216], "get (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:153)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py", 153], "tarda (/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py:11)": ["/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py", 11], "run_middleware_1 (/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py:22)": ["/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py", 22], "is_closed (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:619)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py", 619], "_write_to_self (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py:128)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py", 128], "call_soon_threadsafe (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:734)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py", 734], "notify_all (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py:358)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", 358], "task_done (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py:56)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py", 56], "_key_from_fd (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py:275)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/selectors.py", 275], "_add_callback (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:1672)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py", 1672], "_report_result (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:840)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 840], "_process_self_data (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/unix_events.py:68)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/unix_events.py", 68], "_read_from_self (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py:116)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/selector_events.py", 116], "release_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1948)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1948], "release (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1945)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1945], "__aexit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1863)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1863], "is_dataclass (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/dataclasses.py:1036)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/dataclasses.py", 1036], "jsonable_encoder (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/encoders.py:29)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/encoders.py", 29], "serialize_response (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py:110)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/routing.py", 110], "__init__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py:104)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py", 104], "iterencode (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py:204)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py", 204], "encode (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py:182)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py", 182], "dumps (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py:183)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py", 183], "render (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:198)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py", 198], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:188)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py", 188], "is_body_allowed_for_status_code (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/utils.py:21)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/fastapi/utils.py", 21], "raw (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py:646)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/datastructures.py", 646], "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/testclient.py:309)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/testclient.py", 309], "send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/testclient.py:300)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/testclient.py", 300], "_send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/errors.py:154)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/errors.py", 154], "sender (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py:60)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/middleware/exceptions.py", 60], "is_set (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/locks.py:258)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/locks.py", 258], "is_set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1838)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1838], "set (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/locks.py:262)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/locks.py", 262], "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py:1834)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/anyio/_backends/_asyncio.py", 1834], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py:163)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.7/lib/python3.7/site-packages/starlette/responses.py", 163], "__aexit__ (/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py:631)": ["/usr/local/opt/python@3.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py", 631]}}}