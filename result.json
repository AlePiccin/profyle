{"traceEvents": [{"ph": "M", "pid": 37441, "tid": 37441, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 37441, "tid": 4277750, "name": "thread_name", "args": {"name": "Thread-6"}}, {"ph": "M", "pid": 37441, "tid": 4277743, "name": "thread_name", "args": {"name": "AnyIO worker thread"}}, {"ph": "M", "pid": 37441, "tid": 4277739, "name": "thread_name", "args": {"name": "ThreadPoolExecutor-0_0"}}, {"ph": "M", "pid": 37441, "tid": 4277630, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 37441, "tid": 15206704, "ts": 1476196176129.0, "dur": 1.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:408)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176146.0, "dur": 0.02, "name": "__aenter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:620)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176182.02, "dur": 0.98, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176182.0, "dur": 2.0, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176187.02, "dur": 0.98, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176187.0, "dur": 1.02, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176190.02, "dur": 0.02, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176190.0, "dur": 0.06, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176192.02, "dur": 0.02, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176192.0, "dur": 1.0, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176212.0, "dur": 0.02, "name": "re.Pattern.match", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176213.0, "dur": 0.02, "name": "re.Match.groupdict", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176213.04, "dur": 0.96, "name": "dict.items", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176214.02, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176215.0, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176211.0, "dur": 5.0, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:236)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176209.0, "dur": 8.0, "name": "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:514)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176217.02, "dur": 0.98, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176275.0, "dur": 1.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:69)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176261.0, "dur": 17.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:193)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176356.0, "dur": 0.02, "name": "render (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176433.0, "dur": 1.0, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176435.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176436.0, "dur": 0.02, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176437.0, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176432.0, "dur": 6.0, "name": "init_headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:65)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176340.0, "dur": 98.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176456.0, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176475.0, "dur": 1.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:514)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176455.0, "dur": 22.0, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:98)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176492.02, "dur": 0.02, "name": "str.lower", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176493.0, "dur": 0.02, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176494.0, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176492.0, "dur": 4.0, "name": "__delitem__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:619)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176513.02, "dur": 0.98, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176513.0, "dur": 1.02, "name": "path_params (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:122)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176531.0, "dur": 1.0, "name": "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py:636)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176546.02, "dur": 0.98, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176564.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176564.06, "dur": 0.94, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176565.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176566.0, "dur": 1.0, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176573.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176572.0, "dur": 2.0, "name": "_coerce_args (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:122)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176575.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176575.0, "dur": 0.06, "name": "_coerce_args (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:122)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176576.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176576.04, "dur": 0.02, "name": "str.split", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176577.0, "dur": 1.0, "name": "<listcomp> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:769)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176578.02, "dur": 0.98, "name": "str.split", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176579.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176579.06, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176580.0, "dur": 0.02, "name": "str.replace", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176582.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176582.0, "dur": 1.0, "name": "unquote (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:656)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176597.0, "dur": 0.02, "name": "_noop (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:111)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176597.04, "dur": 0.02, "name": "str.replace", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176599.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176599.0, "dur": 0.06, "name": "unquote (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:656)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176600.0, "dur": 0.02, "name": "_noop (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:111)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176600.04, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176570.0, "dur": 31.0, "name": "parse_qsl (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:726)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176617.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176617.06, "dur": 0.94, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176618.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176621.0, "dur": 1.0, "name": "inner (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:271)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176626.02, "dur": 0.02, "name": "builtins.hash", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176626.0, "dur": 0.06, "name": "__hash__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:756)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176628.02, "dur": 0.98, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176635.02, "dur": 0.98, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176635.0, "dur": 1.02, "name": "_type_convert (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:128)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176636.04, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176638.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176638.0, "dur": 0.06, "name": "__eq__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:750)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176639.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176639.0, "dur": 0.06, "name": "__eq__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:750)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176640.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176640.0, "dur": 0.06, "name": "__eq__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:750)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176640.08, "dur": 0.92, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176641.04, "dur": 0.96, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176641.02, "dur": 1.0, "name": "__eq__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:750)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176642.06, "dur": 0.94, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176642.04, "dur": 0.98, "name": "__eq__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:750)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176643.04, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176643.08, "dur": 0.92, "name": "builtins.callable", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176633.0, "dur": 11.02, "name": "_type_check (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:137)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176631.0, "dur": 13.04, "name": "<genexpr> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:837)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176645.0, "dur": 0.02, "name": "<genexpr> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:837)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176648.02, "dur": 0.98, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176648.0, "dur": 1.02, "name": "_check_generic (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py:148)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176657.0, "dur": 0.02, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176656.0, "dur": 1.04, "name": "_is_dunder (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:665)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176655.0, "dur": 3.0, "name": "__setattr__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:713)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176660.02, "dur": 0.02, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176660.0, "dur": 0.06, "name": "_is_dunder (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:665)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176659.0, "dur": 2.0, "name": "__setattr__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:713)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176662.04, "dur": 0.02, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176663.0, "dur": 0.02, "name": "str.endswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176662.02, "dur": 1.02, "name": "_is_dunder (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:665)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176662.0, "dur": 1.06, "name": "__setattr__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:713)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176665.02, "dur": 0.02, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176665.06, "dur": 0.02, "name": "str.endswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176665.0, "dur": 0.1, "name": "_is_dunder (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:665)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176664.0, "dur": 2.0, "name": "__setattr__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:713)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176654.0, "dur": 12.02, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:677)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176666.04, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176668.0, "dur": 0.02, "name": "<genexpr> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:743)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176669.0, "dur": 0.02, "name": "<genexpr> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:743)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176671.02, "dur": 0.02, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176671.06, "dur": 0.02, "name": "str.endswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176671.0, "dur": 0.1, "name": "_is_dunder (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:665)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176670.0, "dur": 2.0, "name": "__setattr__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:713)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176674.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176675.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176675.0, "dur": 0.06, "name": "_should_collect_from_parameters (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py:175)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176677.0, "dur": 0.02, "name": "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py:199)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176678.0, "dur": 0.02, "name": "list.extend", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176673.0, "dur": 5.04, "name": "_collect_type_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py:182)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176680.02, "dur": 0.02, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176680.06, "dur": 0.02, "name": "str.endswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176680.0, "dur": 0.1, "name": "_is_dunder (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:665)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176679.0, "dur": 1.12, "name": "__setattr__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:713)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176653.0, "dur": 28.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:739)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176650.0, "dur": 31.02, "name": "copy_with (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:841)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176628.0, "dur": 53.04, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:832)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176624.0, "dur": 58.0, "name": "inner (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:271)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176683.0, "dur": 0.02, "name": "cast (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:1375)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176698.0, "dur": 1.0, "name": "<dictcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:291)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176617.0, "dur": 83.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:257)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176713.0, "dur": 1.0, "name": "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:420)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176714.02, "dur": 0.02, "name": "dict.items", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176727.0, "dur": 1.0, "name": "<dictcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:421)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176564.0, "dur": 164.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:397)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176546.0, "dur": 182.04, "name": "query_params (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:116)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176731.0, "dur": 0.02, "name": "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py:636)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176746.02, "dur": 0.98, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176748.0, "dur": 1.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:514)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176746.0, "dur": 4.0, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:110)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176751.0, "dur": 1.0, "name": "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py:636)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176766.02, "dur": 0.98, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176768.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176768.0, "dur": 0.06, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:110)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176783.02, "dur": 0.02, "name": "str.lower", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176784.0, "dur": 0.02, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176783.0, "dur": 2.0, "name": "__getitem__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:563)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176770.0, "dur": 16.0, "name": "get (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_collections_abc.py:760)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176766.0, "dur": 21.0, "name": "cookies (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:126)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176789.0, "dur": 0.02, "name": "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py:636)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176789.04, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176790.0, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176790.04, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176790.08, "dur": 0.02, "name": "dict.update", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176322.0, "dur": 471.0, "name": "solve_dependencies (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py:508)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176846.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176844.0, "dur": 2.04, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176847.0, "dur": 1.0, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176842.0, "dur": 6.02, "name": "get_async_backend (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176906.0, "dur": 1.0, "name": "__sleep0 (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:621)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176895.0, "dur": 12.02, "name": "sleep (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:633)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176881.0, "dur": 26.04, "name": "checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1982)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176867.0, "dur": 40.06, "name": "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2054)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176841.0, "dur": 66.08, "name": "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/to_thread.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176824.0, "dur": 83.1, "name": "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/concurrency.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176810.0, "dur": 97.12, "name": "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:182)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176298.0, "dur": 609.14, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:217)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176245.0, "dur": 662.16, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176231.0, "dur": 676.18, "name": "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176165.0, "dur": 743.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:697)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176115.0, "dur": 793.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/middleware/asyncexitstack.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196176099.0, "dur": 809.04, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176912.0, "dur": 1.0, "name": "_check_closed (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:513)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176919.0, "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1923)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176916.0, "dur": 3.04, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:31)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176920.0, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176914.0, "dur": 6.04, "name": "_call_soon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:770)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176911.0, "dur": 9.06, "name": "call_soon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:741)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176923.0, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176927.02, "dur": 5.98, "name": "socket.recv", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176935.0, "dur": 1.0, "name": "_process_self_data (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/unix_events.py:71)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176936.02, "dur": 3.98, "name": "socket.recv", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176927.0, "dur": 14.0, "name": "_read_from_self (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:112)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176925.02, "dur": 16.0, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176925.0, "dur": 17.0, "name": "_run (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:78)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176945.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176948.02, "dur": 0.98, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176950.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176950.04, "dur": 0.02, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176957.0, "dur": 37.0, "name": "select.kqueue.control", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176948.0, "dur": 47.0, "name": "select (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py:554)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177003.0, "dur": 0.02, "name": "_process_events (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:592)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177005.02, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177005.0, "dur": 1.0, "name": "time (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:694)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177007.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177008.0, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177063.0, "dur": 0.02, "name": "__sleep0 (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:621)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177062.0, "dur": 2.0, "name": "sleep (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:633)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177060.0, "dur": 5.0, "name": "checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1982)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177129.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177127.0, "dur": 2.04, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177131.0, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177125.0, "dur": 6.04, "name": "get_async_backend (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177234.02, "dur": 0.98, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177234.0, "dur": 1.02, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1970)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177123.0, "dur": 112.04, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177238.0, "dur": 1.0, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177241.0, "dur": 1.0, "name": "__setitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:428)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177110.0, "dur": 132.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177095.0, "dur": 150.0, "name": "get (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:124)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177270.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177269.0, "dur": 1.04, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177271.0, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177267.0, "dur": 4.04, "name": "get_async_backend (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177273.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177273.0, "dur": 0.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1970)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177265.0, "dur": 8.08, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177274.0, "dur": 1.0, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177264.0, "dur": 11.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177275.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177289.0, "dur": 1.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:84)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177262.0, "dur": 28.02, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:139)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177297.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177296.0, "dur": 1.04, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177297.06, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177295.0, "dur": 2.1, "name": "get_async_backend (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177298.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177298.0, "dur": 1.0, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1970)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177294.0, "dur": 5.02, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177299.04, "dur": 0.96, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177293.0, "dur": 7.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177300.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177301.0, "dur": 0.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:84)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177292.0, "dur": 10.0, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:139)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177323.02, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177323.0, "dur": 1.0, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177324.02, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177322.0, "dur": 2.06, "name": "get_async_backend (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177325.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177325.0, "dur": 0.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1970)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177320.02, "dur": 5.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177326.0, "dur": 1.0, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177320.0, "dur": 7.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177319.0, "dur": 9.0, "name": "get (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:124)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177354.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177353.0, "dur": 1.04, "name": "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1630)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177384.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177385.0, "dur": 0.02, "name": "collections.OrderedDict.values", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177384.0, "dur": 2.0, "name": "total_tokens (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1653)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177368.0, "dur": 18.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1633)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177392.02, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177392.0, "dur": 0.06, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177393.0, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177391.0, "dur": 2.04, "name": "get_async_backend (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177394.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177394.0, "dur": 0.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1970)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177389.02, "dur": 5.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177395.0, "dur": 0.02, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177389.0, "dur": 7.0, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177396.02, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177397.0, "dur": 0.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:84)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177388.0, "dur": 9.04, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:139)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177317.0, "dur": 81.0, "name": "current_default_thread_limiter (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2368)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177427.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177427.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177426.0, "dur": 1.08, "name": "current_task (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177455.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177455.06, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177455.0, "dur": 1.0, "name": "current_task (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177457.0, "dur": 0.02, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177471.0, "dur": 0.02, "name": "cancel_called (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:546)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177483.0, "dur": 0.02, "name": "shield (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:554)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177484.0, "dur": 0.02, "name": "cancel_called (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:546)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177485.0, "dur": 0.02, "name": "shield (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:554)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177454.0, "dur": 32.0, "name": "checkpoint_if_cancelled (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1986)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177500.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177500.04, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177499.0, "dur": 1.08, "name": "acquire_on_behalf_of_nowait (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1685)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177516.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177515.0, "dur": 1.04, "name": "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:340)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177517.0, "dur": 2.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:345)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177523.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177523.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177522.0, "dur": 1.08, "name": "current_task (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177524.0, "dur": 0.02, "name": "cast (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:1375)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177524.04, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177525.0, "dur": 1.0, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177527.0, "dur": 1.0, "name": "_timeout (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:444)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177521.0, "dur": 8.0, "name": "__enter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:359)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177532.0, "dur": 0.02, "name": "__sleep0 (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:621)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177531.0, "dur": 1.04, "name": "sleep (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:633)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177514.0, "dur": 18.06, "name": "cancel_shielded_checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2005)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177440.0, "dur": 92.08, "name": "acquire_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1700)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177425.0, "dur": 107.1, "name": "acquire (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1697)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177411.0, "dur": 121.12, "name": "__aenter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1638)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177059.0, "dur": 473.14, "name": "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2054)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177056.0, "dur": 476.16, "name": "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/to_thread.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177055.0, "dur": 478.0, "name": "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/concurrency.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177053.0, "dur": 480.02, "name": "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:182)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177052.0, "dur": 481.04, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:217)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177046.0, "dur": 487.06, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177045.0, "dur": 488.08, "name": "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177043.0, "dur": 490.1, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:697)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177041.0, "dur": 493.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/middleware/asyncexitstack.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177039.0, "dur": 495.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177036.0, "dur": 645.0, "name": "__call__ (/Users/shopbox/projects/profyle/profyle/infrastructure/middleware/fastapi.py:20)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177026.0, "dur": 655.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/errors.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177020.0, "dur": 661.04, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/applications.py:118)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177019.0, "dur": 663.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/applications.py:289)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177017.0, "dur": 665.02, "name": "_call_func (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/from_thread.py:179)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177685.0, "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:513)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177690.0, "dur": 1.0, "name": "get_debug (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1923)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177689.0, "dur": 2.02, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:31)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177691.04, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177687.0, "dur": 5.0, "name": "_call_soon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:770)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177684.0, "dur": 8.02, "name": "call_soon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:741)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177012.0, "dur": 680.04, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177011.0, "dur": 682.0, "name": "_run (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:78)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196176945.0, "dur": 748.02, "name": "_run_once (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1830)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177696.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177698.02, "dur": 0.98, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177699.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177699.06, "dur": 0.02, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177700.0, "dur": 24.0, "name": "select.kqueue.control", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177698.0, "dur": 27.0, "name": "select (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py:554)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177733.0, "dur": 0.02, "name": "_process_events (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:592)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177735.02, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177735.0, "dur": 1.0, "name": "time (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:694)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177737.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177738.0, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177789.0, "dur": 0.02, "name": "__sleep0 (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:621)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177788.0, "dur": 2.0, "name": "sleep (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:633)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177824.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177824.06, "dur": 0.94, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177824.0, "dur": 1.02, "name": "current_task (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177836.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177835.0, "dur": 1.04, "name": "get (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:452)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177838.0, "dur": 0.02, "name": "set.remove", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177854.0, "dur": 1.0, "name": "_deliver_cancellation_to_parent (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:494)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177856.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177822.0, "dur": 34.04, "name": "__exit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:387)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177787.0, "dur": 69.06, "name": "cancel_shielded_checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2005)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177785.0, "dur": 72.0, "name": "acquire_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1700)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177784.0, "dur": 73.02, "name": "acquire (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1697)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177783.0, "dur": 75.0, "name": "__aenter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1638)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177861.0, "dur": 0.02, "name": "type.__new__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177860.0, "dur": 1.04, "name": "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:340)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177862.0, "dur": 2.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:345)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177868.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177868.06, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177868.0, "dur": 0.1, "name": "current_task (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177869.0, "dur": 0.02, "name": "cast (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:1375)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177870.0, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177872.0, "dur": 0.02, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177874.0, "dur": 1.0, "name": "_timeout (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:444)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177866.0, "dur": 10.0, "name": "__enter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:359)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177878.0, "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1923)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177904.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177903.0, "dur": 1.04, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177905.0, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177900.0, "dur": 5.04, "name": "get_async_backend (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177907.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177907.0, "dur": 0.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1970)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177899.0, "dur": 8.08, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177908.0, "dur": 1.0, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177898.0, "dur": 11.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177896.0, "dur": 14.0, "name": "get (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:124)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177926.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177942.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177942.04, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177941.0, "dur": 1.08, "name": "__len__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:72)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177944.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177944.06, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177944.0, "dur": 0.1, "name": "__len__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:72)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178013.0, "dur": 1.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:17)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178030.0, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178029.0, "dur": 1.04, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:21)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177951.0, "dur": 80.0, "name": "__iter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178032.0, "dur": 0.02, "name": "__iter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178041.0, "dur": 0.02, "name": "set.remove", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178048.02, "dur": 0.98, "name": "list.pop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178048.0, "dur": 1.02, "name": "_commit_removals (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178040.0, "dur": 9.04, "name": "__exit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:27)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178033.0, "dur": 17.0, "name": "__iter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178079.0, "dur": 0.02, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178078.0, "dur": 1.04, "name": "_get_loop (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py:296)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178080.0, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178081.02, "dur": 0.02, "name": "_asyncio.Task.get_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178081.0, "dur": 1.0, "name": "_get_loop (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py:296)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178082.02, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178064.0, "dur": 18.06, "name": "<setcomp> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:61)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177926.0, "dur": 156.08, "name": "all_tasks (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:42)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178083.0, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178099.0, "dur": 0.02, "name": "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:287)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178100.0, "dur": 1.0, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178102.0, "dur": 0.02, "name": "_asyncio.Task.done", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178103.0, "dur": 0.02, "name": "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:287)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178111.0, "dur": 0.02, "name": "ContextVar.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178110.0, "dur": 1.04, "name": "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py:25)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178112.0, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178109.0, "dur": 3.04, "name": "get_async_backend (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:146)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178113.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178113.0, "dur": 0.06, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1970)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178108.0, "dur": 5.08, "name": "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:58)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178114.0, "dur": 1.0, "name": "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178107.0, "dur": 8.02, "name": "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:107)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178115.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178117.0, "dur": 0.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:84)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178105.0, "dur": 13.0, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:139)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177895.0, "dur": 223.02, "name": "find_root_task (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:279)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178140.02, "dur": 0.02, "name": "_thread.get_ident", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178140.0, "dur": 1.0, "name": "current_thread (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1358)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178142.0, "dur": 1.0, "name": "daemon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1147)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178146.0, "dur": 0.02, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178148.0, "dur": 4.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:228)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178145.0, "dur": 7.02, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:528)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178156.0, "dur": 1.0, "name": "_make_invoke_excepthook (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1229)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178159.02, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178159.0, "dur": 0.06, "name": "add (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:86)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178137.0, "dur": 23.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:802)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178183.0, "dur": 0.02, "name": "_init (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:206)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178184.0, "dur": 0.02, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178185.0, "dur": 3.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:228)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178189.0, "dur": 1.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:228)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178192.0, "dur": 1.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:228)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178176.0, "dur": 18.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:34)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178209.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178211.0, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178210.0, "dur": 1.04, "name": "time (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:694)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178209.0, "dur": 2.06, "name": "current_time (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1974)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178134.0, "dur": 77.08, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:785)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178217.0, "dur": 0.02, "name": "is_set (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:536)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178218.0, "dur": 0.02, "name": "_thread.RLock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178223.0, "dur": 0.02, "name": "__init__ (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:1084)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178224.0, "dur": 36.0, "name": "_thread.start_new_thread", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178221.0, "dur": 40.0, "name": "pydev_start_new_thread (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:1174)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178264.02, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178264.0, "dur": 0.06, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:256)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178267.02, "dur": 0.98, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178267.0, "dur": 1.02, "name": "_is_owned (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:271)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178268.04, "dur": 0.02, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178268.08, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178269.0, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178270.02, "dur": 0.02, "name": "_thread.lock.release", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178270.0, "dur": 0.06, "name": "_release_save (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178270.08, "dur": 332.92, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178609.02, "dur": 0.98, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178609.0, "dur": 1.02, "name": "_acquire_restore (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:268)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178266.0, "dur": 344.04, "name": "wait (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:280)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178613.0, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178612.0, "dur": 1.04, "name": "__exit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:259)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178262.0, "dur": 351.06, "name": "wait (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:563)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178213.0, "dur": 400.08, "name": "start (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:880)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178614.0, "dur": 0.02, "name": "set.add", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178615.0, "dur": 1.0, "name": "_asyncio.Task.add_done_callback", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178616.02, "dur": 0.02, "name": "_contextvars.copy_context", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178617.0, "dur": 1.0, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178648.02, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178648.0, "dur": 0.06, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:256)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178655.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178655.0, "dur": 1.0, "name": "_qsize (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:209)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178662.02, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178662.0, "dur": 1.0, "name": "_put (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:213)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178666.0, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178665.02, "dur": 1.02, "name": "_is_owned (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:271)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178665.0, "dur": 1.06, "name": "notify (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:351)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178668.0, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178667.0, "dur": 1.04, "name": "__exit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:259)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178646.0, "dur": 22.06, "name": "put (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:122)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178636.0, "dur": 32.08, "name": "put_nowait (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:185)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177782.0, "dur": 887.0, "name": "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2054)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177779.0, "dur": 890.02, "name": "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/to_thread.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177778.0, "dur": 891.04, "name": "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/concurrency.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177777.0, "dur": 892.06, "name": "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:182)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177775.0, "dur": 894.08, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:217)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177771.0, "dur": 899.0, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177770.0, "dur": 900.02, "name": "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177769.0, "dur": 901.04, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:697)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177766.0, "dur": 904.06, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/middleware/asyncexitstack.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177765.0, "dur": 905.08, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177762.0, "dur": 912.0, "name": "__call__ (/Users/shopbox/projects/profyle/profyle/infrastructure/middleware/fastapi.py:20)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177753.0, "dur": 921.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/errors.py:147)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177750.0, "dur": 925.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/applications.py:118)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177749.0, "dur": 926.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/applications.py:289)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196177747.0, "dur": 928.04, "name": "_call_func (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/from_thread.py:179)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177742.0, "dur": 933.06, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177741.0, "dur": 935.0, "name": "_run (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:78)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196177696.0, "dur": 980.02, "name": "_run_once (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1830)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178679.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178682.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178682.04, "dur": 0.02, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178734.0, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178732.0, "dur": 3.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:86)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178730.0, "dur": 5.02, "name": "helper (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:261)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178753.0, "dur": 2.0, "name": "claim_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:133)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178737.0, "dur": 18.02, "name": "builtins.next", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178736.0, "dur": 19.04, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:114)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178763.02, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178763.0, "dur": 0.06, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:256)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178764.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178764.0, "dur": 1.0, "name": "_qsize (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:209)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178771.02, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178771.0, "dur": 0.06, "name": "_get (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:217)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178774.0, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178773.0, "dur": 1.04, "name": "_is_owned (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:271)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178772.0, "dur": 2.06, "name": "notify (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:351)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178775.02, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178775.0, "dur": 0.06, "name": "__exit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:259)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178762.0, "dur": 14.0, "name": "get (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:154)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178776.02, "dur": 0.98, "name": "_asyncio.Future.cancelled", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178796.0, "dur": 0.02, "name": "run_middleware (/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py:11)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178777.02, "dur": 19.02, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178810.0, "dur": 0.02, "name": "is_closed (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:680)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178813.0, "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:513)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178816.02, "dur": 0.98, "name": "_contextvars.copy_context", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178819.0, "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1923)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178816.0, "dur": 3.04, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:31)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178819.06, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178814.0, "dur": 6.0, "name": "_call_soon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:770)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178822.0, "dur": 9.0, "name": "socket.send", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178821.0, "dur": 10.02, "name": "_write_to_self (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:124)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178812.0, "dur": 19.04, "name": "call_soon_threadsafe (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:794)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178840.02, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178840.0, "dur": 0.06, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:256)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178842.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178845.0, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178844.0, "dur": 1.04, "name": "_is_owned (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:271)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178843.0, "dur": 2.06, "name": "notify (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:351)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178842.0, "dur": 3.08, "name": "notify_all (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:381)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178846.02, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178846.0, "dur": 1.0, "name": "__exit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:259)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178839.0, "dur": 8.02, "name": "task_done (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:57)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178849.02, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178849.0, "dur": 0.06, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:256)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178850.02, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178850.0, "dur": 0.06, "name": "_qsize (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:209)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178853.02, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178853.0, "dur": 0.06, "name": "_is_owned (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:271)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178853.08, "dur": 0.92, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178854.02, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178854.06, "dur": 0.94, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178856.02, "dur": 0.02, "name": "_thread.lock.release", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277743, "ts": 1476196178856.0, "dur": 0.06, "name": "_release_save (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178683.0, "dur": 181.0, "name": "select.kqueue.control", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178868.0, "dur": 0.02, "name": "_key_from_fd (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py:276)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178869.0, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178681.0, "dur": 188.04, "name": "select (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py:554)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178874.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178874.04, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178874.08, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178873.0, "dur": 2.0, "name": "_add_callback (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1812)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178871.0, "dur": 4.02, "name": "_process_events (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:592)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178877.0, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178876.0, "dur": 1.04, "name": "time (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:694)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178878.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178879.0, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178899.02, "dur": 0.98, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178901.02, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178901.0, "dur": 0.06, "name": "time (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:694)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178899.0, "dur": 2.08, "name": "current_time (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1974)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178902.0, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178909.0, "dur": 0.02, "name": "_asyncio.Future.cancelled", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178912.0, "dur": 0.02, "name": "_check_closed (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:513)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178916.0, "dur": 0.02, "name": "get_debug (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1923)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178915.0, "dur": 1.04, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:31)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178917.0, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178914.0, "dur": 3.04, "name": "_call_soon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:770)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178911.0, "dur": 6.06, "name": "call_soon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:741)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178910.0, "dur": 7.08, "name": "_asyncio.Future.set_result", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178898.0, "dur": 19.1, "name": "_report_result (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:802)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178881.02, "dur": 36.98, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178881.0, "dur": 37.02, "name": "_run (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:78)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178918.04, "dur": 0.96, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178922.02, "dur": 1.98, "name": "socket.recv", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178926.0, "dur": 0.02, "name": "_process_self_data (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/unix_events.py:71)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178927.0, "dur": 2.0, "name": "socket.recv", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178922.0, "dur": 8.0, "name": "_read_from_self (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:112)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178921.0, "dur": 9.02, "name": "Context.run", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178920.0, "dur": 10.04, "name": "_run (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:78)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178679.0, "dur": 251.06, "name": "_run_once (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1830)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178932.02, "dur": 0.98, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178935.0, "dur": 0.02, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178935.04, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178935.08, "dur": 0.02, "name": "builtins.max", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178936.0, "dur": 19.0, "name": "select.kqueue.control", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178934.0, "dur": 22.0, "name": "select (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py:554)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178962.0, "dur": 0.02, "name": "_process_events (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:592)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178964.0, "dur": 0.02, "name": "time.monotonic", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178963.0, "dur": 1.04, "name": "time (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:694)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178965.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277739, "ts": 1476196178967.0, "dur": 0.02, "name": "collections.deque.popleft", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179014.0, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179014.04, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179013.0, "dur": 1.08, "name": "current_task (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179017.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179016.0, "dur": 1.04, "name": "get (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:452)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179018.0, "dur": 0.02, "name": "set.remove", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179020.0, "dur": 1.0, "name": "_deliver_cancellation_to_parent (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:494)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179021.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179012.0, "dur": 9.06, "name": "__exit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:387)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179069.02, "dur": 0.02, "name": "_asyncio.get_running_loop", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179069.06, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179069.0, "dur": 0.1, "name": "current_task (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179087.0, "dur": 0.02, "name": "set.remove", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179086.0, "dur": 1.04, "name": "release_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1724)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179067.0, "dur": 21.0, "name": "release (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1721)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179053.0, "dur": 35.02, "name": "__aexit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1641)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179009.0, "dur": 79.04, "name": "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2054)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179006.0, "dur": 83.0, "name": "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/to_thread.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179005.0, "dur": 84.02, "name": "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/concurrency.py:35)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179004.0, "dur": 86.0, "name": "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:182)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179090.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179156.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179156.0, "dur": 1.0, "name": "__instancecheck__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/pydantic/_internal/_model_construction.py:222)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179136.0, "dur": 21.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179159.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179160.0, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179159.0, "dur": 1.04, "name": "is_dataclass (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/dataclasses.py:1047)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179161.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179161.04, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179162.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179162.04, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179163.0, "dur": 0.02, "name": "dict.keys", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179164.0, "dur": 0.02, "name": "dict.items", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179165.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179165.04, "dur": 0.02, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179172.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179172.0, "dur": 0.06, "name": "__instancecheck__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/pydantic/_internal/_model_construction.py:222)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179171.0, "dur": 1.08, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179173.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179174.0, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179173.0, "dur": 1.04, "name": "is_dataclass (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/dataclasses.py:1047)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179174.06, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179175.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179175.04, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179170.0, "dur": 5.08, "name": "jsonable_encoder (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/encoders.py:101)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179180.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179180.0, "dur": 0.06, "name": "__instancecheck__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/pydantic/_internal/_model_construction.py:222)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179179.02, "dur": 1.06, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179181.02, "dur": 0.98, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179182.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179181.0, "dur": 1.06, "name": "is_dataclass (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/dataclasses.py:1047)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179182.08, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179182.12, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179183.0, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179179.0, "dur": 4.04, "name": "jsonable_encoder (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/encoders.py:101)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179135.0, "dur": 49.0, "name": "jsonable_encoder (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/encoders.py:101)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179111.0, "dur": 73.02, "name": "serialize_response (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179367.0, "dur": 2.0, "name": "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:104)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179383.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179400.0, "dur": 6.0, "name": "iterencode (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:204)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179406.02, "dur": 0.02, "name": "builtins.isinstance", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179407.0, "dur": 0.02, "name": "str.join", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179383.0, "dur": 24.04, "name": "encode (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:182)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179241.0, "dur": 166.06, "name": "dumps (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/__init__.py:183)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179408.0, "dur": 0.02, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179218.0, "dur": 190.04, "name": "render (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:198)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179416.0, "dur": 0.02, "name": "builtins.getattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179417.0, "dur": 0.02, "name": "builtins.len", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179418.0, "dur": 0.02, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179418.04, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179419.0, "dur": 0.02, "name": "str.startswith", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179419.04, "dur": 0.02, "name": "str.encode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179419.08, "dur": 0.02, "name": "list.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179411.0, "dur": 9.0, "name": "init_headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:65)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179203.0, "dur": 217.02, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:43)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179201.0, "dur": 219.04, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:188)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179436.0, "dur": 1.0, "name": "is_body_allowed_for_status_code (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/utils.py:42)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179439.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179442.0, "dur": 1.0, "name": "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:514)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179439.0, "dur": 5.0, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:98)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179458.0, "dur": 0.02, "name": "raw (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:646)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179460.02, "dur": 0.02, "name": "builtins.hasattr", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179460.0, "dur": 0.06, "name": "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:98)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179461.0, "dur": 0.02, "name": "raw (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:646)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179461.04, "dur": 0.02, "name": "list.extend", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179002.0, "dur": 460.0, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:217)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179573.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179587.0, "dur": 0.02, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179587.04, "dur": 0.02, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179587.08, "dur": 0.02, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179587.12, "dur": 0.88, "name": "bytes.decode", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179586.0, "dur": 2.02, "name": "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/testclient.py:313)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179572.0, "dur": 16.04, "name": "send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/testclient.py:305)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179552.0, "dur": 37.0, "name": "_send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/errors.py:154)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179539.0, "dur": 50.02, "name": "sender (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179599.0, "dur": 0.02, "name": "is_set (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/locks.py:191)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179597.0, "dur": 2.04, "name": "is_set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1614)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179599.06, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179600.0, "dur": 0.02, "name": "dict.get", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179601.0, "dur": 1.0, "name": "_io.BytesIO.write", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179602.02, "dur": 0.98, "name": "_io.BytesIO.seek", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179634.0, "dur": 1.0, "name": "set (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/locks.py:195)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179621.0, "dur": 14.02, "name": "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1611)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179593.0, "dur": 42.04, "name": "send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/testclient.py:305)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179592.0, "dur": 43.06, "name": "_send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/errors.py:154)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179590.0, "dur": 46.0, "name": "sender (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py:60)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179523.0, "dur": 113.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:163)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178998.0, "dur": 638.04, "name": "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:63)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178997.0, "dur": 641.0, "name": "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178995.0, "dur": 643.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:697)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179659.0, "dur": 0.02, "name": "sys.exc_info", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196179658.0, "dur": 2.0, "name": "__aexit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:623)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178993.0, "dur": 668.0, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/middleware/asyncexitstack.py:12)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 15206704, "ts": 1476196178992.0, "dur": 669.02, "name": "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py:53)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253744.02, "dur": 0.98, "name": "py_db (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_daemon_thread.py:32)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253747.0, "dur": 0.02, "name": "sys.settrace", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253746.02, "dur": 1.02, "name": "SetTrace (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd_tracing.py:83)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253746.0, "dur": 1.06, "name": "_stop_trace (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_daemon_thread.py:68)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253748.04, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253748.02, "dur": 0.98, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:256)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253749.06, "dur": 0.94, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253749.04, "dur": 0.98, "name": "_is_owned (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:271)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253750.04, "dur": 0.02, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253750.08, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253750.12, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253751.02, "dur": 0.02, "name": "_thread.lock.release", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253751.0, "dur": 0.06, "name": "_release_save (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:265)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253751.08, "dur": 214.92, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253968.02, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253968.0, "dur": 0.06, "name": "_acquire_restore (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:268)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253749.02, "dur": 219.98, "name": "wait (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:280)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253970.02, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253970.0, "dur": 0.06, "name": "__exit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:259)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253748.0, "dur": 223.0, "name": "wait (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:563)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253973.04, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253973.02, "dur": 0.06, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:256)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253974.02, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253974.0, "dur": 0.06, "name": "__exit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:259)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253973.0, "dur": 1.08, "name": "clear (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:553)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253974.1, "dur": 0.9, "name": "time.time", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253976.0, "dur": 0.02, "name": "exec_on_timeout (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_timeout.py:121)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253976.04, "dur": 0.02, "name": "_thread.lock.__exit__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253972.0, "dur": 4.08, "name": "process_handles (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_timeout.py:51)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253978.02, "dur": 0.02, "name": "_thread.lock.__enter__", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253978.0, "dur": 0.06, "name": "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:256)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253979.0, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253978.1, "dur": 0.94, "name": "_is_owned (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:271)", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253979.06, "dur": 0.94, "name": "_thread.allocate_lock", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253980.02, "dur": 0.02, "name": "_thread.lock.acquire", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253981.0, "dur": 0.02, "name": "collections.deque.append", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253981.06, "dur": 0.02, "name": "_thread.lock.release", "ph": "X", "cat": "FEE"}, {"pid": 37441, "tid": 4277750, "ts": 1476196253981.04, "dur": 0.06, "name": "_release_save (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:265)", "ph": "X", "cat": "FEE"}, {"ph": "M", "pid": 37441, "tid": 15206704, "name": "thread_name", "args": {"name": "anyio.from_thread.BlockingPortal._call_func"}}], "viztracer_metadata": {"version": "0.16.0", "overflow": false}, "file_info": {"files": {"/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py": ["\"\"\"Utilities for with-statement contexts.  See PEP 343.\"\"\"\nimport abc\nimport sys\nimport _collections_abc\nfrom collections import deque\nfrom functools import wraps\nfrom types import MethodType, GenericAlias\n\n__all__ = [\"asynccontextmanager\", \"contextmanager\", \"closing\", \"nullcontext\",\n           \"AbstractContextManager\", \"AbstractAsyncContextManager\",\n           \"AsyncExitStack\", \"ContextDecorator\", \"ExitStack\",\n           \"redirect_stdout\", \"redirect_stderr\", \"suppress\"]\n\n\nclass AbstractContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for context managers.\"\"\"\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def __enter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractContextManager:\n            return _collections_abc._check_methods(C, \"__enter__\", \"__exit__\")\n        return NotImplemented\n\n\nclass AbstractAsyncContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for asynchronous context managers.\"\"\"\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    async def __aenter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractAsyncContextManager:\n            return _collections_abc._check_methods(C, \"__aenter__\",\n                                                   \"__aexit__\")\n        return NotImplemented\n\n\nclass ContextDecorator(object):\n    \"A base class or mixin that enables context managers to work as decorators.\"\n\n    def _recreate_cm(self):\n        \"\"\"Return a recreated instance of self.\n\n        Allows an otherwise one-shot context manager like\n        _GeneratorContextManager to support use as\n        a decorator via implicit recreation.\n\n        This is a private interface just for _GeneratorContextManager.\n        See issue #11647 for details.\n        \"\"\"\n        return self\n\n    def __call__(self, func):\n        @wraps(func)\n        def inner(*args, **kwds):\n            with self._recreate_cm():\n                return func(*args, **kwds)\n        return inner\n\n\nclass _GeneratorContextManagerBase:\n    \"\"\"Shared functionality for @contextmanager and @asynccontextmanager.\"\"\"\n\n    def __init__(self, func, args, kwds):\n        self.gen = func(*args, **kwds)\n        self.func, self.args, self.kwds = func, args, kwds\n        # Issue 19330: ensure context manager instances have good docstrings\n        doc = getattr(func, \"__doc__\", None)\n        if doc is None:\n            doc = type(self).__doc__\n        self.__doc__ = doc\n        # Unfortunately, this still doesn't provide good help output when\n        # inspecting the created context manager instances, since pydoc\n        # currently bypasses the instance docstring and shows the docstring\n        # for the class instead.\n        # See http://bugs.python.org/issue19404 for more details.\n\n    def _recreate_cm(self):\n        # _GCMB instances are one-shot context managers, so the\n        # CM must be recreated each time a decorated function is\n        # called\n        return self.__class__(self.func, self.args, self.kwds)\n\n\nclass _GeneratorContextManager(\n    _GeneratorContextManagerBase,\n    AbstractContextManager,\n    ContextDecorator,\n):\n    \"\"\"Helper for @contextmanager decorator.\"\"\"\n\n    def __enter__(self):\n        # do not keep args and kwds alive unnecessarily\n        # they are only needed for recreation, which is not possible anymore\n        del self.args, self.kwds, self.func\n        try:\n            return next(self.gen)\n        except StopIteration:\n            raise RuntimeError(\"generator didn't yield\") from None\n\n    def __exit__(self, typ, value, traceback):\n        if typ is None:\n            try:\n                next(self.gen)\n            except StopIteration:\n                return False\n            else:\n                raise RuntimeError(\"generator didn't stop\")\n        else:\n            if value is None:\n                # Need to force instantiation so we can reliably\n                # tell if we get the same exception back\n                value = typ()\n            try:\n                self.gen.throw(typ, value, traceback)\n            except StopIteration as exc:\n                # Suppress StopIteration *unless* it's the same exception that\n                # was passed to throw().  This prevents a StopIteration\n                # raised inside the \"with\" statement from being suppressed.\n                return exc is not value\n            except RuntimeError as exc:\n                # Don't re-raise the passed in exception. (issue27122)\n                if exc is value:\n                    return False\n                # Avoid suppressing if a StopIteration exception\n                # was passed to throw() and later wrapped into a RuntimeError\n                # (see PEP 479 for sync generators; async generators also\n                # have this behavior). But do this only if the exception wrapped\n                # by the RuntimeError is actually Stop(Async)Iteration (see\n                # issue29692).\n                if (\n                    isinstance(value, StopIteration)\n                    and exc.__cause__ is value\n                ):\n                    return False\n                raise\n            except BaseException as exc:\n                # only re-raise if it's *not* the exception that was\n                # passed to throw(), because __exit__() must not raise\n                # an exception unless __exit__() itself failed.  But throw()\n                # has to raise the exception to signal propagation, so this\n                # fixes the impedance mismatch between the throw() protocol\n                # and the __exit__() protocol.\n                if exc is not value:\n                    raise\n                return False\n            raise RuntimeError(\"generator didn't stop after throw()\")\n\n\nclass _AsyncGeneratorContextManager(_GeneratorContextManagerBase,\n                                    AbstractAsyncContextManager):\n    \"\"\"Helper for @asynccontextmanager decorator.\"\"\"\n\n    async def __aenter__(self):\n        # do not keep args and kwds alive unnecessarily\n        # they are only needed for recreation, which is not possible anymore\n        del self.args, self.kwds, self.func\n        try:\n            return await self.gen.__anext__()\n        except StopAsyncIteration:\n            raise RuntimeError(\"generator didn't yield\") from None\n\n    async def __aexit__(self, typ, value, traceback):\n        if typ is None:\n            try:\n                await self.gen.__anext__()\n            except StopAsyncIteration:\n                return False\n            else:\n                raise RuntimeError(\"generator didn't stop\")\n        else:\n            if value is None:\n                # Need to force instantiation so we can reliably\n                # tell if we get the same exception back\n                value = typ()\n            try:\n                await self.gen.athrow(typ, value, traceback)\n            except StopAsyncIteration as exc:\n                # Suppress StopIteration *unless* it's the same exception that\n                # was passed to throw().  This prevents a StopIteration\n                # raised inside the \"with\" statement from being suppressed.\n                return exc is not value\n            except RuntimeError as exc:\n                # Don't re-raise the passed in exception. (issue27122)\n                if exc is value:\n                    return False\n                # Avoid suppressing if a Stop(Async)Iteration exception\n                # was passed to athrow() and later wrapped into a RuntimeError\n                # (see PEP 479 for sync generators; async generators also\n                # have this behavior). But do this only if the exception wrapped\n                # by the RuntimeError is actully Stop(Async)Iteration (see\n                # issue29692).\n                if (\n                    isinstance(value, (StopIteration, StopAsyncIteration))\n                    and exc.__cause__ is value\n                ):\n                    return False\n                raise\n            except BaseException as exc:\n                # only re-raise if it's *not* the exception that was\n                # passed to throw(), because __exit__() must not raise\n                # an exception unless __exit__() itself failed.  But throw()\n                # has to raise the exception to signal propagation, so this\n                # fixes the impedance mismatch between the throw() protocol\n                # and the __exit__() protocol.\n                if exc is not value:\n                    raise\n                return False\n            raise RuntimeError(\"generator didn't stop after athrow()\")\n\n\ndef contextmanager(func):\n    \"\"\"@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    \"\"\"\n    @wraps(func)\n    def helper(*args, **kwds):\n        return _GeneratorContextManager(func, args, kwds)\n    return helper\n\n\ndef asynccontextmanager(func):\n    \"\"\"@asynccontextmanager decorator.\n\n    Typical usage:\n\n        @asynccontextmanager\n        async def some_async_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        async with some_async_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    \"\"\"\n    @wraps(func)\n    def helper(*args, **kwds):\n        return _AsyncGeneratorContextManager(func, args, kwds)\n    return helper\n\n\nclass closing(AbstractContextManager):\n    \"\"\"Context to automatically close something at the end of a block.\n\n    Code like this:\n\n        with closing(<module>.open(<arguments>)) as f:\n            <block>\n\n    is equivalent to this:\n\n        f = <module>.open(<arguments>)\n        try:\n            <block>\n        finally:\n            f.close()\n\n    \"\"\"\n    def __init__(self, thing):\n        self.thing = thing\n    def __enter__(self):\n        return self.thing\n    def __exit__(self, *exc_info):\n        self.thing.close()\n\n\nclass _RedirectStream(AbstractContextManager):\n\n    _stream = None\n\n    def __init__(self, new_target):\n        self._new_target = new_target\n        # We use a list of old targets to make this CM re-entrant\n        self._old_targets = []\n\n    def __enter__(self):\n        self._old_targets.append(getattr(sys, self._stream))\n        setattr(sys, self._stream, self._new_target)\n        return self._new_target\n\n    def __exit__(self, exctype, excinst, exctb):\n        setattr(sys, self._stream, self._old_targets.pop())\n\n\nclass redirect_stdout(_RedirectStream):\n    \"\"\"Context manager for temporarily redirecting stdout to another file.\n\n        # How to send help() to stderr\n        with redirect_stdout(sys.stderr):\n            help(dir)\n\n        # How to write help() to a file\n        with open('help.txt', 'w') as f:\n            with redirect_stdout(f):\n                help(pow)\n    \"\"\"\n\n    _stream = \"stdout\"\n\n\nclass redirect_stderr(_RedirectStream):\n    \"\"\"Context manager for temporarily redirecting stderr to another file.\"\"\"\n\n    _stream = \"stderr\"\n\n\nclass suppress(AbstractContextManager):\n    \"\"\"Context manager to suppress specified exceptions\n\n    After the exception is suppressed, execution proceeds with the next\n    statement following the with statement.\n\n         with suppress(FileNotFoundError):\n             os.remove(somefile)\n         # Execution still resumes here if the file was already removed\n    \"\"\"\n\n    def __init__(self, *exceptions):\n        self._exceptions = exceptions\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exctype, excinst, exctb):\n        # Unlike isinstance and issubclass, CPython exception handling\n        # currently only looks at the concrete type hierarchy (ignoring\n        # the instance and subclass checking hooks). While Guido considers\n        # that a bug rather than a feature, it's a fairly hard one to fix\n        # due to various internal implementation details. suppress provides\n        # the simpler issubclass based semantics, rather than trying to\n        # exactly reproduce the limitations of the CPython interpreter.\n        #\n        # See http://bugs.python.org/issue12029 for more details\n        return exctype is not None and issubclass(exctype, self._exceptions)\n\n\nclass _BaseExitStack:\n    \"\"\"A base class for ExitStack and AsyncExitStack.\"\"\"\n\n    @staticmethod\n    def _create_exit_wrapper(cm, cm_exit):\n        return MethodType(cm_exit, cm)\n\n    @staticmethod\n    def _create_cb_wrapper(callback, /, *args, **kwds):\n        def _exit_wrapper(exc_type, exc, tb):\n            callback(*args, **kwds)\n        return _exit_wrapper\n\n    def __init__(self):\n        self._exit_callbacks = deque()\n\n    def pop_all(self):\n        \"\"\"Preserve the context stack by transferring it to a new instance.\"\"\"\n        new_stack = type(self)()\n        new_stack._exit_callbacks = self._exit_callbacks\n        self._exit_callbacks = deque()\n        return new_stack\n\n    def push(self, exit):\n        \"\"\"Registers a callback with the standard __exit__ method signature.\n\n        Can suppress exceptions the same way __exit__ method can.\n        Also accepts any object with an __exit__ method (registering a call\n        to the method instead of the object itself).\n        \"\"\"\n        # We use an unbound method rather than a bound method to follow\n        # the standard lookup behaviour for special methods.\n        _cb_type = type(exit)\n\n        try:\n            exit_method = _cb_type.__exit__\n        except AttributeError:\n            # Not a context manager, so assume it's a callable.\n            self._push_exit_callback(exit)\n        else:\n            self._push_cm_exit(exit, exit_method)\n        return exit  # Allow use as a decorator.\n\n    def enter_context(self, cm):\n        \"\"\"Enters the supplied context manager.\n\n        If successful, also pushes its __exit__ method as a callback and\n        returns the result of the __enter__ method.\n        \"\"\"\n        # We look up the special methods on the type to match the with\n        # statement.\n        _cm_type = type(cm)\n        _exit = _cm_type.__exit__\n        result = _cm_type.__enter__(cm)\n        self._push_cm_exit(cm, _exit)\n        return result\n\n    def callback(self, callback, /, *args, **kwds):\n        \"\"\"Registers an arbitrary callback and arguments.\n\n        Cannot suppress exceptions.\n        \"\"\"\n        _exit_wrapper = self._create_cb_wrapper(callback, *args, **kwds)\n\n        # We changed the signature, so using @wraps is not appropriate, but\n        # setting __wrapped__ may still help with introspection.\n        _exit_wrapper.__wrapped__ = callback\n        self._push_exit_callback(_exit_wrapper)\n        return callback  # Allow use as a decorator\n\n    def _push_cm_exit(self, cm, cm_exit):\n        \"\"\"Helper to correctly register callbacks to __exit__ methods.\"\"\"\n        _exit_wrapper = self._create_exit_wrapper(cm, cm_exit)\n        self._push_exit_callback(_exit_wrapper, True)\n\n    def _push_exit_callback(self, callback, is_sync=True):\n        self._exit_callbacks.append((is_sync, callback))\n\n\n# Inspired by discussions on http://bugs.python.org/issue13585\nclass ExitStack(_BaseExitStack, AbstractContextManager):\n    \"\"\"Context manager for dynamic management of a stack of exit callbacks.\n\n    For example:\n        with ExitStack() as stack:\n            files = [stack.enter_context(open(fname)) for fname in filenames]\n            # All opened files will automatically be closed at the end of\n            # the with statement, even if attempts to open files later\n            # in the list raise an exception.\n    \"\"\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc_details):\n        received_exc = exc_details[0] is not None\n\n        # We manipulate the exception state so it behaves as though\n        # we were actually nesting multiple with statements\n        frame_exc = sys.exc_info()[1]\n        def _fix_exception_context(new_exc, old_exc):\n            # Context may not be correct, so find the end of the chain\n            while 1:\n                exc_context = new_exc.__context__\n                if exc_context is None or exc_context is old_exc:\n                    # Context is already set correctly (see issue 20317)\n                    return\n                if exc_context is frame_exc:\n                    break\n                new_exc = exc_context\n            # Change the end of the chain to point to the exception\n            # we expect it to reference\n            new_exc.__context__ = old_exc\n\n        # Callbacks are invoked in LIFO order to match the behaviour of\n        # nested context managers\n        suppressed_exc = False\n        pending_raise = False\n        while self._exit_callbacks:\n            is_sync, cb = self._exit_callbacks.pop()\n            assert is_sync\n            try:\n                if cb(*exc_details):\n                    suppressed_exc = True\n                    pending_raise = False\n                    exc_details = (None, None, None)\n            except:\n                new_exc_details = sys.exc_info()\n                # simulate the stack of exceptions by setting the context\n                _fix_exception_context(new_exc_details[1], exc_details[1])\n                pending_raise = True\n                exc_details = new_exc_details\n        if pending_raise:\n            try:\n                # bare \"raise exc_details[1]\" replaces our carefully\n                # set-up context\n                fixed_ctx = exc_details[1].__context__\n                raise exc_details[1]\n            except BaseException:\n                exc_details[1].__context__ = fixed_ctx\n                raise\n        return received_exc and suppressed_exc\n\n    def close(self):\n        \"\"\"Immediately unwind the context stack.\"\"\"\n        self.__exit__(None, None, None)\n\n\n# Inspired by discussions on https://bugs.python.org/issue29302\nclass AsyncExitStack(_BaseExitStack, AbstractAsyncContextManager):\n    \"\"\"Async context manager for dynamic management of a stack of exit\n    callbacks.\n\n    For example:\n        async with AsyncExitStack() as stack:\n            connections = [await stack.enter_async_context(get_connection())\n                for i in range(5)]\n            # All opened connections will automatically be released at the\n            # end of the async with statement, even if attempts to open a\n            # connection later in the list raise an exception.\n    \"\"\"\n\n    @staticmethod\n    def _create_async_exit_wrapper(cm, cm_exit):\n        return MethodType(cm_exit, cm)\n\n    @staticmethod\n    def _create_async_cb_wrapper(callback, /, *args, **kwds):\n        async def _exit_wrapper(exc_type, exc, tb):\n            await callback(*args, **kwds)\n        return _exit_wrapper\n\n    async def enter_async_context(self, cm):\n        \"\"\"Enters the supplied async context manager.\n\n        If successful, also pushes its __aexit__ method as a callback and\n        returns the result of the __aenter__ method.\n        \"\"\"\n        _cm_type = type(cm)\n        _exit = _cm_type.__aexit__\n        result = await _cm_type.__aenter__(cm)\n        self._push_async_cm_exit(cm, _exit)\n        return result\n\n    def push_async_exit(self, exit):\n        \"\"\"Registers a coroutine function with the standard __aexit__ method\n        signature.\n\n        Can suppress exceptions the same way __aexit__ method can.\n        Also accepts any object with an __aexit__ method (registering a call\n        to the method instead of the object itself).\n        \"\"\"\n        _cb_type = type(exit)\n        try:\n            exit_method = _cb_type.__aexit__\n        except AttributeError:\n            # Not an async context manager, so assume it's a coroutine function\n            self._push_exit_callback(exit, False)\n        else:\n            self._push_async_cm_exit(exit, exit_method)\n        return exit  # Allow use as a decorator\n\n    def push_async_callback(self, callback, /, *args, **kwds):\n        \"\"\"Registers an arbitrary coroutine function and arguments.\n\n        Cannot suppress exceptions.\n        \"\"\"\n        _exit_wrapper = self._create_async_cb_wrapper(callback, *args, **kwds)\n\n        # We changed the signature, so using @wraps is not appropriate, but\n        # setting __wrapped__ may still help with introspection.\n        _exit_wrapper.__wrapped__ = callback\n        self._push_exit_callback(_exit_wrapper, False)\n        return callback  # Allow use as a decorator\n\n    async def aclose(self):\n        \"\"\"Immediately unwind the context stack.\"\"\"\n        await self.__aexit__(None, None, None)\n\n    def _push_async_cm_exit(self, cm, cm_exit):\n        \"\"\"Helper to correctly register coroutine function to __aexit__\n        method.\"\"\"\n        _exit_wrapper = self._create_async_exit_wrapper(cm, cm_exit)\n        self._push_exit_callback(_exit_wrapper, False)\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc_details):\n        received_exc = exc_details[0] is not None\n\n        # We manipulate the exception state so it behaves as though\n        # we were actually nesting multiple with statements\n        frame_exc = sys.exc_info()[1]\n        def _fix_exception_context(new_exc, old_exc):\n            # Context may not be correct, so find the end of the chain\n            while 1:\n                exc_context = new_exc.__context__\n                if exc_context is None or exc_context is old_exc:\n                    # Context is already set correctly (see issue 20317)\n                    return\n                if exc_context is frame_exc:\n                    break\n                new_exc = exc_context\n            # Change the end of the chain to point to the exception\n            # we expect it to reference\n            new_exc.__context__ = old_exc\n\n        # Callbacks are invoked in LIFO order to match the behaviour of\n        # nested context managers\n        suppressed_exc = False\n        pending_raise = False\n        while self._exit_callbacks:\n            is_sync, cb = self._exit_callbacks.pop()\n            try:\n                if is_sync:\n                    cb_suppress = cb(*exc_details)\n                else:\n                    cb_suppress = await cb(*exc_details)\n\n                if cb_suppress:\n                    suppressed_exc = True\n                    pending_raise = False\n                    exc_details = (None, None, None)\n            except:\n                new_exc_details = sys.exc_info()\n                # simulate the stack of exceptions by setting the context\n                _fix_exception_context(new_exc_details[1], exc_details[1])\n                pending_raise = True\n                exc_details = new_exc_details\n        if pending_raise:\n            try:\n                # bare \"raise exc_details[1]\" replaces our carefully\n                # set-up context\n                fixed_ctx = exc_details[1].__context__\n                raise exc_details[1]\n            except BaseException:\n                exc_details[1].__context__ = fixed_ctx\n                raise\n        return received_exc and suppressed_exc\n\n\nclass nullcontext(AbstractContextManager):\n    \"\"\"Context manager that does no additional processing.\n\n    Used as a stand-in for a normal context manager, when a particular\n    block of code is only sometimes used with a normal context manager:\n\n    cm = optional_cm if condition else nullcontext()\n    with cm:\n        # Perform operation, using optional_cm if condition is True\n    \"\"\"\n\n    def __init__(self, enter_result=None):\n        self.enter_result = enter_result\n\n    def __enter__(self):\n        return self.enter_result\n\n    def __exit__(self, *excinfo):\n        pass\n", 695], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py": ["import contextlib\nimport functools\nimport inspect\nimport re\nimport traceback\nimport types\nimport typing\nimport warnings\nfrom contextlib import asynccontextmanager\nfrom enum import Enum\n\nfrom starlette._utils import is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.convertors import CONVERTOR_TYPES, Convertor\nfrom starlette.datastructures import URL, Headers, URLPath\nfrom starlette.exceptions import HTTPException\nfrom starlette.middleware import Middleware\nfrom starlette.requests import Request\nfrom starlette.responses import PlainTextResponse, RedirectResponse\nfrom starlette.types import ASGIApp, Lifespan, Receive, Scope, Send\nfrom starlette.websockets import WebSocket, WebSocketClose\n\n\nclass NoMatchFound(Exception):\n    \"\"\"\n    Raised by `.url_for(name, **path_params)` and `.url_path_for(name, **path_params)`\n    if no matching route exists.\n    \"\"\"\n\n    def __init__(self, name: str, path_params: typing.Dict[str, typing.Any]) -> None:\n        params = \", \".join(list(path_params.keys()))\n        super().__init__(f'No route exists for name \"{name}\" and params \"{params}\".')\n\n\nclass Match(Enum):\n    NONE = 0\n    PARTIAL = 1\n    FULL = 2\n\n\ndef iscoroutinefunction_or_partial(obj: typing.Any) -> bool:  # pragma: no cover\n    \"\"\"\n    Correctly determines if an object is a coroutine function,\n    including those wrapped in functools.partial objects.\n    \"\"\"\n    warnings.warn(\n        \"iscoroutinefunction_or_partial is deprecated, \"\n        \"and will be removed in a future release.\",\n        DeprecationWarning,\n    )\n    while isinstance(obj, functools.partial):\n        obj = obj.func\n    return inspect.iscoroutinefunction(obj)\n\n\ndef request_response(func: typing.Callable) -> ASGIApp:\n    \"\"\"\n    Takes a function or coroutine `func(request) -> response`,\n    and returns an ASGI application.\n    \"\"\"\n    is_coroutine = is_async_callable(func)\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        request = Request(scope, receive=receive, send=send)\n        if is_coroutine:\n            response = await func(request)\n        else:\n            response = await run_in_threadpool(func, request)\n        await response(scope, receive, send)\n\n    return app\n\n\ndef websocket_session(func: typing.Callable) -> ASGIApp:\n    \"\"\"\n    Takes a coroutine `func(session)`, and returns an ASGI application.\n    \"\"\"\n    # assert asyncio.iscoroutinefunction(func), \"WebSocket endpoints must be async\"\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        session = WebSocket(scope, receive=receive, send=send)\n        await func(session)\n\n    return app\n\n\ndef get_name(endpoint: typing.Callable) -> str:\n    if inspect.isroutine(endpoint) or inspect.isclass(endpoint):\n        return endpoint.__name__\n    return endpoint.__class__.__name__\n\n\ndef replace_params(\n    path: str,\n    param_convertors: typing.Dict[str, Convertor],\n    path_params: typing.Dict[str, str],\n) -> typing.Tuple[str, dict]:\n    for key, value in list(path_params.items()):\n        if \"{\" + key + \"}\" in path:\n            convertor = param_convertors[key]\n            value = convertor.to_string(value)\n            path = path.replace(\"{\" + key + \"}\", value)\n            path_params.pop(key)\n    return path, path_params\n\n\n# Match parameters in URL paths, eg. '{param}', and '{param:int}'\nPARAM_REGEX = re.compile(\"{([a-zA-Z_][a-zA-Z0-9_]*)(:[a-zA-Z_][a-zA-Z0-9_]*)?}\")\n\n\ndef compile_path(\n    path: str,\n) -> typing.Tuple[typing.Pattern, str, typing.Dict[str, Convertor]]:\n    \"\"\"\n    Given a path string, like: \"/{username:str}\",\n    or a host string, like: \"{subdomain}.mydomain.org\", return a three-tuple\n    of (regex, format, {param_name:convertor}).\n\n    regex:      \"/(?P<username>[^/]+)\"\n    format:     \"/{username}\"\n    convertors: {\"username\": StringConvertor()}\n    \"\"\"\n    is_host = not path.startswith(\"/\")\n\n    path_regex = \"^\"\n    path_format = \"\"\n    duplicated_params = set()\n\n    idx = 0\n    param_convertors = {}\n    for match in PARAM_REGEX.finditer(path):\n        param_name, convertor_type = match.groups(\"str\")\n        convertor_type = convertor_type.lstrip(\":\")\n        assert (\n            convertor_type in CONVERTOR_TYPES\n        ), f\"Unknown path convertor '{convertor_type}'\"\n        convertor = CONVERTOR_TYPES[convertor_type]\n\n        path_regex += re.escape(path[idx : match.start()])\n        path_regex += f\"(?P<{param_name}>{convertor.regex})\"\n\n        path_format += path[idx : match.start()]\n        path_format += \"{%s}\" % param_name\n\n        if param_name in param_convertors:\n            duplicated_params.add(param_name)\n\n        param_convertors[param_name] = convertor\n\n        idx = match.end()\n\n    if duplicated_params:\n        names = \", \".join(sorted(duplicated_params))\n        ending = \"s\" if len(duplicated_params) > 1 else \"\"\n        raise ValueError(f\"Duplicated param name{ending} {names} at path {path}\")\n\n    if is_host:\n        # Align with `Host.matches()` behavior, which ignores port.\n        hostname = path[idx:].split(\":\")[0]\n        path_regex += re.escape(hostname) + \"$\"\n    else:\n        path_regex += re.escape(path[idx:]) + \"$\"\n\n    path_format += path[idx:]\n\n    return re.compile(path_regex), path_format, param_convertors\n\n\nclass BaseRoute:\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        raise NotImplementedError()  # pragma: no cover\n\n    def url_path_for(self, __name: str, **path_params: typing.Any) -> URLPath:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        \"\"\"\n        A route may be used in isolation as a stand-alone ASGI app.\n        This is a somewhat contrived case, as they'll almost always be used\n        within a Router, but could be useful for some tooling and minimal apps.\n        \"\"\"\n        match, child_scope = self.matches(scope)\n        if match == Match.NONE:\n            if scope[\"type\"] == \"http\":\n                response = PlainTextResponse(\"Not Found\", status_code=404)\n                await response(scope, receive, send)\n            elif scope[\"type\"] == \"websocket\":\n                websocket_close = WebSocketClose()\n                await websocket_close(scope, receive, send)\n            return\n\n        scope.update(child_scope)\n        await self.handle(scope, receive, send)\n\n\nclass Route(BaseRoute):\n    def __init__(\n        self,\n        path: str,\n        endpoint: typing.Callable,\n        *,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> None:\n        assert path.startswith(\"/\"), \"Routed paths must start with '/'\"\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n        self.include_in_schema = include_in_schema\n\n        endpoint_handler = endpoint\n        while isinstance(endpoint_handler, functools.partial):\n            endpoint_handler = endpoint_handler.func\n        if inspect.isfunction(endpoint_handler) or inspect.ismethod(endpoint_handler):\n            # Endpoint is function or method. Treat it as `func(request) -> response`.\n            self.app = request_response(endpoint)\n            if methods is None:\n                methods = [\"GET\"]\n        else:\n            # Endpoint is a class. Treat it as ASGI.\n            self.app = endpoint\n\n        if methods is None:\n            self.methods = None\n        else:\n            self.methods = {method.upper() for method in methods}\n            if \"GET\" in self.methods:\n                self.methods.add(\"HEAD\")\n\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        if scope[\"type\"] == \"http\":\n            match = self.path_regex.match(scope[\"path\"])\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\"endpoint\": self.endpoint, \"path_params\": path_params}\n                if self.methods and scope[\"method\"] not in self.methods:\n                    return Match.PARTIAL, child_scope\n                else:\n                    return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, __name: str, **path_params: typing.Any) -> URLPath:\n        seen_params = set(path_params.keys())\n        expected_params = set(self.param_convertors.keys())\n\n        if __name != self.name or seen_params != expected_params:\n            raise NoMatchFound(__name, path_params)\n\n        path, remaining_params = replace_params(\n            self.path_format, self.param_convertors, path_params\n        )\n        assert not remaining_params\n        return URLPath(path=path, protocol=\"http\")\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if self.methods and scope[\"method\"] not in self.methods:\n            headers = {\"Allow\": \", \".join(self.methods)}\n            if \"app\" in scope:\n                raise HTTPException(status_code=405, headers=headers)\n            else:\n                response = PlainTextResponse(\n                    \"Method Not Allowed\", status_code=405, headers=headers\n                )\n            await response(scope, receive, send)\n        else:\n            await self.app(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return (\n            isinstance(other, Route)\n            and self.path == other.path\n            and self.endpoint == other.endpoint\n            and self.methods == other.methods\n        )\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        methods = sorted(self.methods or [])\n        path, name = self.path, self.name\n        return f\"{class_name}(path={path!r}, name={name!r}, methods={methods!r})\"\n\n\nclass WebSocketRoute(BaseRoute):\n    def __init__(\n        self, path: str, endpoint: typing.Callable, *, name: typing.Optional[str] = None\n    ) -> None:\n        assert path.startswith(\"/\"), \"Routed paths must start with '/'\"\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n\n        endpoint_handler = endpoint\n        while isinstance(endpoint_handler, functools.partial):\n            endpoint_handler = endpoint_handler.func\n        if inspect.isfunction(endpoint_handler) or inspect.ismethod(endpoint_handler):\n            # Endpoint is function or method. Treat it as `func(websocket)`.\n            self.app = websocket_session(endpoint)\n        else:\n            # Endpoint is a class. Treat it as ASGI.\n            self.app = endpoint\n\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        if scope[\"type\"] == \"websocket\":\n            match = self.path_regex.match(scope[\"path\"])\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\"endpoint\": self.endpoint, \"path_params\": path_params}\n                return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, __name: str, **path_params: typing.Any) -> URLPath:\n        seen_params = set(path_params.keys())\n        expected_params = set(self.param_convertors.keys())\n\n        if __name != self.name or seen_params != expected_params:\n            raise NoMatchFound(__name, path_params)\n\n        path, remaining_params = replace_params(\n            self.path_format, self.param_convertors, path_params\n        )\n        assert not remaining_params\n        return URLPath(path=path, protocol=\"websocket\")\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await self.app(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return (\n            isinstance(other, WebSocketRoute)\n            and self.path == other.path\n            and self.endpoint == other.endpoint\n        )\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(path={self.path!r}, name={self.name!r})\"\n\n\nclass Mount(BaseRoute):\n    def __init__(\n        self,\n        path: str,\n        app: typing.Optional[ASGIApp] = None,\n        routes: typing.Optional[typing.Sequence[BaseRoute]] = None,\n        name: typing.Optional[str] = None,\n        *,\n        middleware: typing.Optional[typing.Sequence[Middleware]] = None,\n    ) -> None:\n        assert path == \"\" or path.startswith(\"/\"), \"Routed paths must start with '/'\"\n        assert (\n            app is not None or routes is not None\n        ), \"Either 'app=...', or 'routes=' must be specified\"\n        self.path = path.rstrip(\"/\")\n        if app is not None:\n            self._base_app: ASGIApp = app\n        else:\n            self._base_app = Router(routes=routes)\n        self.app = self._base_app\n        if middleware is not None:\n            for cls, options in reversed(middleware):\n                self.app = cls(app=self.app, **options)\n        self.name = name\n        self.path_regex, self.path_format, self.param_convertors = compile_path(\n            self.path + \"/{path:path}\"\n        )\n\n    @property\n    def routes(self) -> typing.List[BaseRoute]:\n        return getattr(self._base_app, \"routes\", [])\n\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        if scope[\"type\"] in (\"http\", \"websocket\"):\n            path = scope[\"path\"]\n            match = self.path_regex.match(path)\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                remaining_path = \"/\" + matched_params.pop(\"path\")\n                matched_path = path[: -len(remaining_path)]\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                root_path = scope.get(\"root_path\", \"\")\n                child_scope = {\n                    \"path_params\": path_params,\n                    \"app_root_path\": scope.get(\"app_root_path\", root_path),\n                    \"root_path\": root_path + matched_path,\n                    \"path\": remaining_path,\n                    \"endpoint\": self.app,\n                }\n                return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, __name: str, **path_params: typing.Any) -> URLPath:\n        if self.name is not None and __name == self.name and \"path\" in path_params:\n            # 'name' matches \"<mount_name>\".\n            path_params[\"path\"] = path_params[\"path\"].lstrip(\"/\")\n            path, remaining_params = replace_params(\n                self.path_format, self.param_convertors, path_params\n            )\n            if not remaining_params:\n                return URLPath(path=path)\n        elif self.name is None or __name.startswith(self.name + \":\"):\n            if self.name is None:\n                # No mount name.\n                remaining_name = __name\n            else:\n                # 'name' matches \"<mount_name>:<child_name>\".\n                remaining_name = __name[len(self.name) + 1 :]\n            path_kwarg = path_params.get(\"path\")\n            path_params[\"path\"] = \"\"\n            path_prefix, remaining_params = replace_params(\n                self.path_format, self.param_convertors, path_params\n            )\n            if path_kwarg is not None:\n                remaining_params[\"path\"] = path_kwarg\n            for route in self.routes or []:\n                try:\n                    url = route.url_path_for(remaining_name, **remaining_params)\n                    return URLPath(\n                        path=path_prefix.rstrip(\"/\") + str(url), protocol=url.protocol\n                    )\n                except NoMatchFound:\n                    pass\n        raise NoMatchFound(__name, path_params)\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await self.app(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return (\n            isinstance(other, Mount)\n            and self.path == other.path\n            and self.app == other.app\n        )\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        name = self.name or \"\"\n        return f\"{class_name}(path={self.path!r}, name={name!r}, app={self.app!r})\"\n\n\nclass Host(BaseRoute):\n    def __init__(\n        self, host: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:\n        assert not host.startswith(\"/\"), \"Host must not start with '/'\"\n        self.host = host\n        self.app = app\n        self.name = name\n        self.host_regex, self.host_format, self.param_convertors = compile_path(host)\n\n    @property\n    def routes(self) -> typing.List[BaseRoute]:\n        return getattr(self.app, \"routes\", [])\n\n    def matches(self, scope: Scope) -> typing.Tuple[Match, Scope]:\n        if scope[\"type\"] in (\"http\", \"websocket\"):\n            headers = Headers(scope=scope)\n            host = headers.get(\"host\", \"\").split(\":\")[0]\n            match = self.host_regex.match(host)\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\"path_params\": path_params, \"endpoint\": self.app}\n                return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, __name: str, **path_params: typing.Any) -> URLPath:\n        if self.name is not None and __name == self.name and \"path\" in path_params:\n            # 'name' matches \"<mount_name>\".\n            path = path_params.pop(\"path\")\n            host, remaining_params = replace_params(\n                self.host_format, self.param_convertors, path_params\n            )\n            if not remaining_params:\n                return URLPath(path=path, host=host)\n        elif self.name is None or __name.startswith(self.name + \":\"):\n            if self.name is None:\n                # No mount name.\n                remaining_name = __name\n            else:\n                # 'name' matches \"<mount_name>:<child_name>\".\n                remaining_name = __name[len(self.name) + 1 :]\n            host, remaining_params = replace_params(\n                self.host_format, self.param_convertors, path_params\n            )\n            for route in self.routes or []:\n                try:\n                    url = route.url_path_for(remaining_name, **remaining_params)\n                    return URLPath(path=str(url), protocol=url.protocol, host=host)\n                except NoMatchFound:\n                    pass\n        raise NoMatchFound(__name, path_params)\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await self.app(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return (\n            isinstance(other, Host)\n            and self.host == other.host\n            and self.app == other.app\n        )\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        name = self.name or \"\"\n        return f\"{class_name}(host={self.host!r}, name={name!r}, app={self.app!r})\"\n\n\n_T = typing.TypeVar(\"_T\")\n\n\nclass _AsyncLiftContextManager(typing.AsyncContextManager[_T]):\n    def __init__(self, cm: typing.ContextManager[_T]):\n        self._cm = cm\n\n    async def __aenter__(self) -> _T:\n        return self._cm.__enter__()\n\n    async def __aexit__(\n        self,\n        exc_type: typing.Optional[typing.Type[BaseException]],\n        exc_value: typing.Optional[BaseException],\n        traceback: typing.Optional[types.TracebackType],\n    ) -> typing.Optional[bool]:\n        return self._cm.__exit__(exc_type, exc_value, traceback)\n\n\ndef _wrap_gen_lifespan_context(\n    lifespan_context: typing.Callable[[typing.Any], typing.Generator]\n) -> typing.Callable[[typing.Any], typing.AsyncContextManager]:\n    cmgr = contextlib.contextmanager(lifespan_context)\n\n    @functools.wraps(cmgr)\n    def wrapper(app: typing.Any) -> _AsyncLiftContextManager:\n        return _AsyncLiftContextManager(cmgr(app))\n\n    return wrapper\n\n\nclass _DefaultLifespan:\n    def __init__(self, router: \"Router\"):\n        self._router = router\n\n    async def __aenter__(self) -> None:\n        await self._router.startup()\n\n    async def __aexit__(self, *exc_info: object) -> None:\n        await self._router.shutdown()\n\n    def __call__(self: _T, app: object) -> _T:\n        return self\n\n\nclass Router:\n    def __init__(\n        self,\n        routes: typing.Optional[typing.Sequence[BaseRoute]] = None,\n        redirect_slashes: bool = True,\n        default: typing.Optional[ASGIApp] = None,\n        on_startup: typing.Optional[typing.Sequence[typing.Callable]] = None,\n        on_shutdown: typing.Optional[typing.Sequence[typing.Callable]] = None,\n        # the generic to Lifespan[AppType] is the type of the top level application\n        # which the router cannot know statically, so we use typing.Any\n        lifespan: typing.Optional[Lifespan[typing.Any]] = None,\n    ) -> None:\n        self.routes = [] if routes is None else list(routes)\n        self.redirect_slashes = redirect_slashes\n        self.default = self.not_found if default is None else default\n        self.on_startup = [] if on_startup is None else list(on_startup)\n        self.on_shutdown = [] if on_shutdown is None else list(on_shutdown)\n\n        if on_startup or on_shutdown:\n            warnings.warn(\n                \"The on_startup and on_shutdown parameters are deprecated, and they \"\n                \"will be removed on version 1.0. Use the lifespan parameter instead. \"\n                \"See more about it on https://www.starlette.io/lifespan/.\",\n                DeprecationWarning,\n            )\n\n        if lifespan is None:\n            self.lifespan_context: Lifespan = _DefaultLifespan(self)\n\n        elif inspect.isasyncgenfunction(lifespan):\n            warnings.warn(\n                \"async generator function lifespans are deprecated, \"\n                \"use an @contextlib.asynccontextmanager function instead\",\n                DeprecationWarning,\n            )\n            self.lifespan_context = asynccontextmanager(\n                lifespan,  # type: ignore[arg-type]\n            )\n        elif inspect.isgeneratorfunction(lifespan):\n            warnings.warn(\n                \"generator function lifespans are deprecated, \"\n                \"use an @contextlib.asynccontextmanager function instead\",\n                DeprecationWarning,\n            )\n            self.lifespan_context = _wrap_gen_lifespan_context(\n                lifespan,  # type: ignore[arg-type]\n            )\n        else:\n            self.lifespan_context = lifespan\n\n    async def not_found(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] == \"websocket\":\n            websocket_close = WebSocketClose()\n            await websocket_close(scope, receive, send)\n            return\n\n        # If we're running inside a starlette application then raise an\n        # exception, so that the configurable exception handler can deal with\n        # returning the response. For plain ASGI apps, just return the response.\n        if \"app\" in scope:\n            raise HTTPException(status_code=404)\n        else:\n            response = PlainTextResponse(\"Not Found\", status_code=404)\n        await response(scope, receive, send)\n\n    def url_path_for(self, __name: str, **path_params: typing.Any) -> URLPath:\n        for route in self.routes:\n            try:\n                return route.url_path_for(__name, **path_params)\n            except NoMatchFound:\n                pass\n        raise NoMatchFound(__name, path_params)\n\n    async def startup(self) -> None:\n        \"\"\"\n        Run any `.on_startup` event handlers.\n        \"\"\"\n        for handler in self.on_startup:\n            if is_async_callable(handler):\n                await handler()\n            else:\n                handler()\n\n    async def shutdown(self) -> None:\n        \"\"\"\n        Run any `.on_shutdown` event handlers.\n        \"\"\"\n        for handler in self.on_shutdown:\n            if is_async_callable(handler):\n                await handler()\n            else:\n                handler()\n\n    async def lifespan(self, scope: Scope, receive: Receive, send: Send) -> None:\n        \"\"\"\n        Handle ASGI lifespan messages, which allows us to manage application\n        startup and shutdown events.\n        \"\"\"\n        started = False\n        app: typing.Any = scope.get(\"app\")\n        await receive()\n        try:\n            async with self.lifespan_context(app) as maybe_state:\n                if maybe_state is not None:\n                    if \"state\" not in scope:\n                        raise RuntimeError(\n                            'The server does not support \"state\" in the lifespan scope.'\n                        )\n                    scope[\"state\"].update(maybe_state)\n                await send({\"type\": \"lifespan.startup.complete\"})\n                started = True\n                await receive()\n        except BaseException:\n            exc_text = traceback.format_exc()\n            if started:\n                await send({\"type\": \"lifespan.shutdown.failed\", \"message\": exc_text})\n            else:\n                await send({\"type\": \"lifespan.startup.failed\", \"message\": exc_text})\n            raise\n        else:\n            await send({\"type\": \"lifespan.shutdown.complete\"})\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        \"\"\"\n        The main entry point to the Router class.\n        \"\"\"\n        assert scope[\"type\"] in (\"http\", \"websocket\", \"lifespan\")\n\n        if \"router\" not in scope:\n            scope[\"router\"] = self\n\n        if scope[\"type\"] == \"lifespan\":\n            await self.lifespan(scope, receive, send)\n            return\n\n        partial = None\n\n        for route in self.routes:\n            # Determine if any route matches the incoming scope,\n            # and hand over to the matching route if found.\n            match, child_scope = route.matches(scope)\n            if match == Match.FULL:\n                scope.update(child_scope)\n                await route.handle(scope, receive, send)\n                return\n            elif match == Match.PARTIAL and partial is None:\n                partial = route\n                partial_scope = child_scope\n\n        if partial is not None:\n            # \u00a0Handle partial matches. These are cases where an endpoint is\n            # able to handle the request, but is not a preferred option.\n            # We use this in particular to deal with \"405 Method Not Allowed\".\n            scope.update(partial_scope)\n            await partial.handle(scope, receive, send)\n            return\n\n        if scope[\"type\"] == \"http\" and self.redirect_slashes and scope[\"path\"] != \"/\":\n            redirect_scope = dict(scope)\n            if scope[\"path\"].endswith(\"/\"):\n                redirect_scope[\"path\"] = redirect_scope[\"path\"].rstrip(\"/\")\n            else:\n                redirect_scope[\"path\"] = redirect_scope[\"path\"] + \"/\"\n\n            for route in self.routes:\n                match, child_scope = route.matches(redirect_scope)\n                if match != Match.NONE:\n                    redirect_url = URL(scope=redirect_scope)\n                    response = RedirectResponse(url=str(redirect_url))\n                    await response(scope, receive, send)\n                    return\n\n        await self.default(scope, receive, send)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return isinstance(other, Router) and self.routes == other.routes\n\n    def mount(\n        self, path: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:  # pragma: nocover\n        route = Mount(path, app=app, name=name)\n        self.routes.append(route)\n\n    def host(\n        self, host: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:  # pragma: no cover\n        route = Host(host, app=app, name=name)\n        self.routes.append(route)\n\n    def add_route(\n        self,\n        path: str,\n        endpoint: typing.Callable,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> None:  # pragma: nocover\n        route = Route(\n            path,\n            endpoint=endpoint,\n            methods=methods,\n            name=name,\n            include_in_schema=include_in_schema,\n        )\n        self.routes.append(route)\n\n    def add_websocket_route(\n        self, path: str, endpoint: typing.Callable, name: typing.Optional[str] = None\n    ) -> None:  # pragma: no cover\n        route = WebSocketRoute(path, endpoint=endpoint, name=name)\n        self.routes.append(route)\n\n    def route(\n        self,\n        path: str,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [Route(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `route` decorator is deprecated, and will be removed in version 1.0.0.\"\n            \"Refer to https://www.starlette.io/routing/#http-routing for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_route(\n                path,\n                func,\n                methods=methods,\n                name=name,\n                include_in_schema=include_in_schema,\n            )\n            return func\n\n        return decorator\n\n    def websocket_route(\n        self, path: str, name: typing.Optional[str] = None\n    ) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [WebSocketRoute(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `websocket_route` decorator is deprecated, and will be removed in version 1.0.0. Refer to \"  # noqa: E501\n            \"https://www.starlette.io/routing/#websocket-routing for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def add_event_handler(\n        self, event_type: str, func: typing.Callable\n    ) -> None:  # pragma: no cover\n        assert event_type in (\"startup\", \"shutdown\")\n\n        if event_type == \"startup\":\n            self.on_startup.append(func)\n        else:\n            self.on_shutdown.append(func)\n\n    def on_event(self, event_type: str) -> typing.Callable:\n        warnings.warn(\n            \"The `on_event` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/lifespan/ for recommended approach.\",\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_event_handler(event_type, func)\n            return func\n\n        return decorator\n", 862], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py": ["import asyncio\nimport dataclasses\nimport email.message\nimport inspect\nimport json\nfrom contextlib import AsyncExitStack\nfrom enum import Enum, IntEnum\nfrom typing import (\n    Any,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom fastapi import params\nfrom fastapi._compat import (\n    ModelField,\n    Undefined,\n    _get_model_config,\n    _model_dump,\n    _normalize_errors,\n    lenient_issubclass,\n)\nfrom fastapi.datastructures import Default, DefaultPlaceholder\nfrom fastapi.dependencies.models import Dependant\nfrom fastapi.dependencies.utils import (\n    get_body_field,\n    get_dependant,\n    get_parameterless_sub_dependant,\n    get_typed_return_annotation,\n    solve_dependencies,\n)\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.exceptions import (\n    FastAPIError,\n    RequestValidationError,\n    ResponseValidationError,\n    WebSocketRequestValidationError,\n)\nfrom fastapi.types import DecoratedCallable, IncEx\nfrom fastapi.utils import (\n    create_cloned_field,\n    create_response_field,\n    generate_unique_id,\n    get_value_or_default,\n    is_body_allowed_for_status_code,\n)\nfrom pydantic import BaseModel\nfrom starlette import routing\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.exceptions import HTTPException\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse, Response\nfrom starlette.routing import (\n    BaseRoute,\n    Match,\n    compile_path,\n    get_name,\n    request_response,\n    websocket_session,\n)\nfrom starlette.routing import Mount as Mount  # noqa\nfrom starlette.types import ASGIApp, Lifespan, Scope\nfrom starlette.websockets import WebSocket\n\n\ndef _prepare_response_content(\n    res: Any,\n    *,\n    exclude_unset: bool,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n) -> Any:\n    if isinstance(res, BaseModel):\n        read_with_orm_mode = getattr(_get_model_config(res), \"read_with_orm_mode\", None)\n        if read_with_orm_mode:\n            # Let from_orm extract the data from this model instead of converting\n            # it now to a dict.\n            # Otherwise, there's no way to extract lazy data that requires attribute\n            # access instead of dict iteration, e.g. lazy relationships.\n            return res\n        return _model_dump(\n            res,\n            by_alias=True,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n    elif isinstance(res, list):\n        return [\n            _prepare_response_content(\n                item,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n            for item in res\n        ]\n    elif isinstance(res, dict):\n        return {\n            k: _prepare_response_content(\n                v,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n            for k, v in res.items()\n        }\n    elif dataclasses.is_dataclass(res):\n        return dataclasses.asdict(res)\n    return res\n\n\nasync def serialize_response(\n    *,\n    field: Optional[ModelField] = None,\n    response_content: Any,\n    include: Optional[IncEx] = None,\n    exclude: Optional[IncEx] = None,\n    by_alias: bool = True,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    is_coroutine: bool = True,\n) -> Any:\n    if field:\n        errors = []\n        if not hasattr(field, \"serialize\"):\n            # pydantic v1\n            response_content = _prepare_response_content(\n                response_content,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        if is_coroutine:\n            value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n        else:\n            value, errors_ = await run_in_threadpool(\n                field.validate, response_content, {}, loc=(\"response\",)\n            )\n        if isinstance(errors_, list):\n            errors.extend(errors_)\n        elif errors_:\n            errors.append(errors_)\n        if errors:\n            raise ResponseValidationError(\n                errors=_normalize_errors(errors), body=response_content\n            )\n\n        if hasattr(field, \"serialize\"):\n            return field.serialize(\n                value,\n                include=include,\n                exclude=exclude,\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n\n        return jsonable_encoder(\n            value,\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n    else:\n        return jsonable_encoder(response_content)\n\n\nasync def run_endpoint_function(\n    *, dependant: Dependant, values: Dict[str, Any], is_coroutine: bool\n) -> Any:\n    # Only called by get_request_handler. Has been split into its own function to\n    # facilitate profiling endpoints, since inner functions are harder to profile.\n    assert dependant.call is not None, \"dependant.call must be a function\"\n\n    if is_coroutine:\n        return await dependant.call(**values)\n    else:\n        return await run_in_threadpool(dependant.call, **values)\n\n\ndef get_request_handler(\n    dependant: Dependant,\n    body_field: Optional[ModelField] = None,\n    status_code: Optional[int] = None,\n    response_class: Union[Type[Response], DefaultPlaceholder] = Default(JSONResponse),\n    response_field: Optional[ModelField] = None,\n    response_model_include: Optional[IncEx] = None,\n    response_model_exclude: Optional[IncEx] = None,\n    response_model_by_alias: bool = True,\n    response_model_exclude_unset: bool = False,\n    response_model_exclude_defaults: bool = False,\n    response_model_exclude_none: bool = False,\n    dependency_overrides_provider: Optional[Any] = None,\n) -> Callable[[Request], Coroutine[Any, Any, Response]]:\n    assert dependant.call is not None, \"dependant.call must be a function\"\n    is_coroutine = asyncio.iscoroutinefunction(dependant.call)\n    is_body_form = body_field and isinstance(body_field.field_info, params.Form)\n    if isinstance(response_class, DefaultPlaceholder):\n        actual_response_class: Type[Response] = response_class.value\n    else:\n        actual_response_class = response_class\n\n    async def app(request: Request) -> Response:\n        try:\n            body: Any = None\n            if body_field:\n                if is_body_form:\n                    body = await request.form()\n                    stack = request.scope.get(\"fastapi_astack\")\n                    assert isinstance(stack, AsyncExitStack)\n                    stack.push_async_callback(body.close)\n                else:\n                    body_bytes = await request.body()\n                    if body_bytes:\n                        json_body: Any = Undefined\n                        content_type_value = request.headers.get(\"content-type\")\n                        if not content_type_value:\n                            json_body = await request.json()\n                        else:\n                            message = email.message.Message()\n                            message[\"content-type\"] = content_type_value\n                            if message.get_content_maintype() == \"application\":\n                                subtype = message.get_content_subtype()\n                                if subtype == \"json\" or subtype.endswith(\"+json\"):\n                                    json_body = await request.json()\n                        if json_body != Undefined:\n                            body = json_body\n                        else:\n                            body = body_bytes\n        except json.JSONDecodeError as e:\n            raise RequestValidationError(\n                [\n                    {\n                        \"type\": \"json_invalid\",\n                        \"loc\": (\"body\", e.pos),\n                        \"msg\": \"JSON decode error\",\n                        \"input\": {},\n                        \"ctx\": {\"error\": e.msg},\n                    }\n                ],\n                body=e.doc,\n            ) from e\n        except HTTPException:\n            raise\n        except Exception as e:\n            raise HTTPException(\n                status_code=400, detail=\"There was an error parsing the body\"\n            ) from e\n        solved_result = await solve_dependencies(\n            request=request,\n            dependant=dependant,\n            body=body,\n            dependency_overrides_provider=dependency_overrides_provider,\n        )\n        values, errors, background_tasks, sub_response, _ = solved_result\n        if errors:\n            raise RequestValidationError(_normalize_errors(errors), body=body)\n        else:\n            raw_response = await run_endpoint_function(\n                dependant=dependant, values=values, is_coroutine=is_coroutine\n            )\n\n            if isinstance(raw_response, Response):\n                if raw_response.background is None:\n                    raw_response.background = background_tasks\n                return raw_response\n            response_args: Dict[str, Any] = {\"background\": background_tasks}\n            # If status_code was set, use it, otherwise use the default from the\n            # response class, in the case of redirect it's 307\n            current_status_code = (\n                status_code if status_code else sub_response.status_code\n            )\n            if current_status_code is not None:\n                response_args[\"status_code\"] = current_status_code\n            if sub_response.status_code:\n                response_args[\"status_code\"] = sub_response.status_code\n            content = await serialize_response(\n                field=response_field,\n                response_content=raw_response,\n                include=response_model_include,\n                exclude=response_model_exclude,\n                by_alias=response_model_by_alias,\n                exclude_unset=response_model_exclude_unset,\n                exclude_defaults=response_model_exclude_defaults,\n                exclude_none=response_model_exclude_none,\n                is_coroutine=is_coroutine,\n            )\n            response = actual_response_class(content, **response_args)\n            if not is_body_allowed_for_status_code(response.status_code):\n                response.body = b\"\"\n            response.headers.raw.extend(sub_response.headers.raw)\n            return response\n\n    return app\n\n\ndef get_websocket_app(\n    dependant: Dependant, dependency_overrides_provider: Optional[Any] = None\n) -> Callable[[WebSocket], Coroutine[Any, Any, Any]]:\n    async def app(websocket: WebSocket) -> None:\n        solved_result = await solve_dependencies(\n            request=websocket,\n            dependant=dependant,\n            dependency_overrides_provider=dependency_overrides_provider,\n        )\n        values, errors, _, _2, _3 = solved_result\n        if errors:\n            raise WebSocketRequestValidationError(_normalize_errors(errors))\n        assert dependant.call is not None, \"dependant.call must be a function\"\n        await dependant.call(**values)\n\n    return app\n\n\nclass APIWebSocketRoute(routing.WebSocketRoute):\n    def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        name: Optional[str] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        dependency_overrides_provider: Optional[Any] = None,\n    ) -> None:\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n        self.dependencies = list(dependencies or [])\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n        self.dependant = get_dependant(path=self.path_format, call=self.endpoint)\n        for depends in self.dependencies[::-1]:\n            self.dependant.dependencies.insert(\n                0,\n                get_parameterless_sub_dependant(depends=depends, path=self.path_format),\n            )\n\n        self.app = websocket_session(\n            get_websocket_app(\n                dependant=self.dependant,\n                dependency_overrides_provider=dependency_overrides_provider,\n            )\n        )\n\n    def matches(self, scope: Scope) -> Tuple[Match, Scope]:\n        match, child_scope = super().matches(scope)\n        if match != Match.NONE:\n            child_scope[\"route\"] = self\n        return match, child_scope\n\n\nclass APIRoute(routing.Route):\n    def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        name: Optional[str] = None,\n        methods: Optional[Union[Set[str], List[str]]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        dependency_overrides_provider: Optional[Any] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Union[\n            Callable[[\"APIRoute\"], str], DefaultPlaceholder\n        ] = Default(generate_unique_id),\n    ) -> None:\n        self.path = path\n        self.endpoint = endpoint\n        if isinstance(response_model, DefaultPlaceholder):\n            return_annotation = get_typed_return_annotation(endpoint)\n            if lenient_issubclass(return_annotation, Response):\n                response_model = None\n            else:\n                response_model = return_annotation\n        self.response_model = response_model\n        self.summary = summary\n        self.response_description = response_description\n        self.deprecated = deprecated\n        self.operation_id = operation_id\n        self.response_model_include = response_model_include\n        self.response_model_exclude = response_model_exclude\n        self.response_model_by_alias = response_model_by_alias\n        self.response_model_exclude_unset = response_model_exclude_unset\n        self.response_model_exclude_defaults = response_model_exclude_defaults\n        self.response_model_exclude_none = response_model_exclude_none\n        self.include_in_schema = include_in_schema\n        self.response_class = response_class\n        self.dependency_overrides_provider = dependency_overrides_provider\n        self.callbacks = callbacks\n        self.openapi_extra = openapi_extra\n        self.generate_unique_id_function = generate_unique_id_function\n        self.tags = tags or []\n        self.responses = responses or {}\n        self.name = get_name(endpoint) if name is None else name\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n        if methods is None:\n            methods = [\"GET\"]\n        self.methods: Set[str] = {method.upper() for method in methods}\n        if isinstance(generate_unique_id_function, DefaultPlaceholder):\n            current_generate_unique_id: Callable[\n                [\"APIRoute\"], str\n            ] = generate_unique_id_function.value\n        else:\n            current_generate_unique_id = generate_unique_id_function\n        self.unique_id = self.operation_id or current_generate_unique_id(self)\n        # normalize enums e.g. http.HTTPStatus\n        if isinstance(status_code, IntEnum):\n            status_code = int(status_code)\n        self.status_code = status_code\n        if self.response_model:\n            assert is_body_allowed_for_status_code(\n                status_code\n            ), f\"Status code {status_code} must not have a response body\"\n            response_name = \"Response_\" + self.unique_id\n            self.response_field = create_response_field(\n                name=response_name,\n                type_=self.response_model,\n                mode=\"serialization\",\n            )\n            # Create a clone of the field, so that a Pydantic submodel is not returned\n            # as is just because it's an instance of a subclass of a more limited class\n            # e.g. UserInDB (containing hashed_password) could be a subclass of User\n            # that doesn't have the hashed_password. But because it's a subclass, it\n            # would pass the validation and be returned as is.\n            # By being a new field, no inheritance will be passed as is. A new model\n            # will always be created.\n            # TODO: remove when deprecating Pydantic v1\n            self.secure_cloned_response_field: Optional[\n                ModelField\n            ] = create_cloned_field(self.response_field)\n        else:\n            self.response_field = None  # type: ignore\n            self.secure_cloned_response_field = None\n        self.dependencies = list(dependencies or [])\n        self.description = description or inspect.cleandoc(self.endpoint.__doc__ or \"\")\n        # if a \"form feed\" character (page break) is found in the description text,\n        # truncate description text to the content preceding the first \"form feed\"\n        self.description = self.description.split(\"\\f\")[0].strip()\n        response_fields = {}\n        for additional_status_code, response in self.responses.items():\n            assert isinstance(response, dict), \"An additional response must be a dict\"\n            model = response.get(\"model\")\n            if model:\n                assert is_body_allowed_for_status_code(\n                    additional_status_code\n                ), f\"Status code {additional_status_code} must not have a response body\"\n                response_name = f\"Response_{additional_status_code}_{self.unique_id}\"\n                response_field = create_response_field(name=response_name, type_=model)\n                response_fields[additional_status_code] = response_field\n        if response_fields:\n            self.response_fields: Dict[Union[int, str], ModelField] = response_fields\n        else:\n            self.response_fields = {}\n\n        assert callable(endpoint), \"An endpoint must be a callable\"\n        self.dependant = get_dependant(path=self.path_format, call=self.endpoint)\n        for depends in self.dependencies[::-1]:\n            self.dependant.dependencies.insert(\n                0,\n                get_parameterless_sub_dependant(depends=depends, path=self.path_format),\n            )\n        self.body_field = get_body_field(dependant=self.dependant, name=self.unique_id)\n        self.app = request_response(self.get_route_handler())\n\n    def get_route_handler(self) -> Callable[[Request], Coroutine[Any, Any, Response]]:\n        return get_request_handler(\n            dependant=self.dependant,\n            body_field=self.body_field,\n            status_code=self.status_code,\n            response_class=self.response_class,\n            response_field=self.secure_cloned_response_field,\n            response_model_include=self.response_model_include,\n            response_model_exclude=self.response_model_exclude,\n            response_model_by_alias=self.response_model_by_alias,\n            response_model_exclude_unset=self.response_model_exclude_unset,\n            response_model_exclude_defaults=self.response_model_exclude_defaults,\n            response_model_exclude_none=self.response_model_exclude_none,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n\n    def matches(self, scope: Scope) -> Tuple[Match, Scope]:\n        match, child_scope = super().matches(scope)\n        if match != Match.NONE:\n            child_scope[\"route\"] = self\n        return match, child_scope\n\n\nclass APIRouter(routing.Router):\n    def __init__(\n        self,\n        *,\n        prefix: str = \"\",\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        default_response_class: Type[Response] = Default(JSONResponse),\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        routes: Optional[List[routing.BaseRoute]] = None,\n        redirect_slashes: bool = True,\n        default: Optional[ASGIApp] = None,\n        dependency_overrides_provider: Optional[Any] = None,\n        route_class: Type[APIRoute] = APIRoute,\n        on_startup: Optional[Sequence[Callable[[], Any]]] = None,\n        on_shutdown: Optional[Sequence[Callable[[], Any]]] = None,\n        # the generic to Lifespan[AppType] is the type of the top level application\n        # which the router cannot know statically, so we use typing.Any\n        lifespan: Optional[Lifespan[Any]] = None,\n        deprecated: Optional[bool] = None,\n        include_in_schema: bool = True,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        super().__init__(\n            routes=routes,\n            redirect_slashes=redirect_slashes,\n            default=default,\n            on_startup=on_startup,\n            on_shutdown=on_shutdown,\n            lifespan=lifespan,\n        )\n        if prefix:\n            assert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\n            assert not prefix.endswith(\n                \"/\"\n            ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n        self.prefix = prefix\n        self.tags: List[Union[str, Enum]] = tags or []\n        self.dependencies = list(dependencies or [])\n        self.deprecated = deprecated\n        self.include_in_schema = include_in_schema\n        self.responses = responses or {}\n        self.callbacks = callbacks or []\n        self.dependency_overrides_provider = dependency_overrides_provider\n        self.route_class = route_class\n        self.default_response_class = default_response_class\n        self.generate_unique_id_function = generate_unique_id_function\n\n    def route(\n        self,\n        path: str,\n        methods: Optional[List[str]] = None,\n        name: Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_route(\n                path,\n                func,\n                methods=methods,\n                name=name,\n                include_in_schema=include_in_schema,\n            )\n            return func\n\n        return decorator\n\n    def add_api_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[Union[Set[str], List[str]]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        name: Optional[str] = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Union[\n            Callable[[APIRoute], str], DefaultPlaceholder\n        ] = Default(generate_unique_id),\n    ) -> None:\n        route_class = route_class_override or self.route_class\n        responses = responses or {}\n        combined_responses = {**self.responses, **responses}\n        current_response_class = get_value_or_default(\n            response_class, self.default_response_class\n        )\n        current_tags = self.tags.copy()\n        if tags:\n            current_tags.extend(tags)\n        current_dependencies = self.dependencies.copy()\n        if dependencies:\n            current_dependencies.extend(dependencies)\n        current_callbacks = self.callbacks.copy()\n        if callbacks:\n            current_callbacks.extend(callbacks)\n        current_generate_unique_id = get_value_or_default(\n            generate_unique_id_function, self.generate_unique_id_function\n        )\n        route = route_class(\n            self.prefix + path,\n            endpoint=endpoint,\n            response_model=response_model,\n            status_code=status_code,\n            tags=current_tags,\n            dependencies=current_dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=combined_responses,\n            deprecated=deprecated or self.deprecated,\n            methods=methods,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema and self.include_in_schema,\n            response_class=current_response_class,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n            callbacks=current_callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=current_generate_unique_id,\n        )\n        self.routes.append(route)\n\n    def api_route(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_route(\n                path,\n                func,\n                response_model=response_model,\n                status_code=status_code,\n                tags=tags,\n                dependencies=dependencies,\n                summary=summary,\n                description=description,\n                response_description=response_description,\n                responses=responses,\n                deprecated=deprecated,\n                methods=methods,\n                operation_id=operation_id,\n                response_model_include=response_model_include,\n                response_model_exclude=response_model_exclude,\n                response_model_by_alias=response_model_by_alias,\n                response_model_exclude_unset=response_model_exclude_unset,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                include_in_schema=include_in_schema,\n                response_class=response_class,\n                name=name,\n                callbacks=callbacks,\n                openapi_extra=openapi_extra,\n                generate_unique_id_function=generate_unique_id_function,\n            )\n            return func\n\n        return decorator\n\n    def add_api_websocket_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        name: Optional[str] = None,\n        *,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n    ) -> None:\n        current_dependencies = self.dependencies.copy()\n        if dependencies:\n            current_dependencies.extend(dependencies)\n\n        route = APIWebSocketRoute(\n            self.prefix + path,\n            endpoint=endpoint,\n            name=name,\n            dependencies=current_dependencies,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n        self.routes.append(route)\n\n    def websocket(\n        self,\n        path: str,\n        name: Optional[str] = None,\n        *,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_websocket_route(\n                path, func, name=name, dependencies=dependencies\n            )\n            return func\n\n        return decorator\n\n    def websocket_route(\n        self, path: str, name: Union[str, None] = None\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def include_router(\n        self,\n        router: \"APIRouter\",\n        *,\n        prefix: str = \"\",\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        default_response_class: Type[Response] = Default(JSONResponse),\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        deprecated: Optional[bool] = None,\n        include_in_schema: bool = True,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        if prefix:\n            assert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\n            assert not prefix.endswith(\n                \"/\"\n            ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n        else:\n            for r in router.routes:\n                path = getattr(r, \"path\")  # noqa: B009\n                name = getattr(r, \"name\", \"unknown\")\n                if path is not None and not path:\n                    raise FastAPIError(\n                        f\"Prefix and path cannot be both empty (path operation: {name})\"\n                    )\n        if responses is None:\n            responses = {}\n        for route in router.routes:\n            if isinstance(route, APIRoute):\n                combined_responses = {**responses, **route.responses}\n                use_response_class = get_value_or_default(\n                    route.response_class,\n                    router.default_response_class,\n                    default_response_class,\n                    self.default_response_class,\n                )\n                current_tags = []\n                if tags:\n                    current_tags.extend(tags)\n                if route.tags:\n                    current_tags.extend(route.tags)\n                current_dependencies: List[params.Depends] = []\n                if dependencies:\n                    current_dependencies.extend(dependencies)\n                if route.dependencies:\n                    current_dependencies.extend(route.dependencies)\n                current_callbacks = []\n                if callbacks:\n                    current_callbacks.extend(callbacks)\n                if route.callbacks:\n                    current_callbacks.extend(route.callbacks)\n                current_generate_unique_id = get_value_or_default(\n                    route.generate_unique_id_function,\n                    router.generate_unique_id_function,\n                    generate_unique_id_function,\n                    self.generate_unique_id_function,\n                )\n                self.add_api_route(\n                    prefix + route.path,\n                    route.endpoint,\n                    response_model=route.response_model,\n                    status_code=route.status_code,\n                    tags=current_tags,\n                    dependencies=current_dependencies,\n                    summary=route.summary,\n                    description=route.description,\n                    response_description=route.response_description,\n                    responses=combined_responses,\n                    deprecated=route.deprecated or deprecated or self.deprecated,\n                    methods=route.methods,\n                    operation_id=route.operation_id,\n                    response_model_include=route.response_model_include,\n                    response_model_exclude=route.response_model_exclude,\n                    response_model_by_alias=route.response_model_by_alias,\n                    response_model_exclude_unset=route.response_model_exclude_unset,\n                    response_model_exclude_defaults=route.response_model_exclude_defaults,\n                    response_model_exclude_none=route.response_model_exclude_none,\n                    include_in_schema=route.include_in_schema\n                    and self.include_in_schema\n                    and include_in_schema,\n                    response_class=use_response_class,\n                    name=route.name,\n                    route_class_override=type(route),\n                    callbacks=current_callbacks,\n                    openapi_extra=route.openapi_extra,\n                    generate_unique_id_function=current_generate_unique_id,\n                )\n            elif isinstance(route, routing.Route):\n                methods = list(route.methods or [])\n                self.add_route(\n                    prefix + route.path,\n                    route.endpoint,\n                    methods=methods,\n                    include_in_schema=route.include_in_schema,\n                    name=route.name,\n                )\n            elif isinstance(route, APIWebSocketRoute):\n                current_dependencies = []\n                if dependencies:\n                    current_dependencies.extend(dependencies)\n                if route.dependencies:\n                    current_dependencies.extend(route.dependencies)\n                self.add_api_websocket_route(\n                    prefix + route.path,\n                    route.endpoint,\n                    dependencies=current_dependencies,\n                    name=route.name,\n                )\n            elif isinstance(route, routing.WebSocketRoute):\n                self.add_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n        for handler in router.on_startup:\n            self.add_event_handler(\"startup\", handler)\n        for handler in router.on_shutdown:\n            self.add_event_handler(\"shutdown\", handler)\n\n    def get(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"GET\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def put(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"PUT\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def post(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"POST\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def delete(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"DELETE\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def options(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"OPTIONS\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def head(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"HEAD\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def patch(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"PATCH\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def trace(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"TRACE\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def on_event(\n        self, event_type: str\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_event_handler(event_type, func)\n            return func\n\n        return decorator\n", 1356], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py": ["import json\nimport typing\nfrom http import cookies as http_cookies\n\nimport anyio\n\nfrom starlette._utils import AwaitableOrContextManager, AwaitableOrContextManagerWrapper\nfrom starlette.datastructures import URL, Address, FormData, Headers, QueryParams, State\nfrom starlette.exceptions import HTTPException\nfrom starlette.formparsers import FormParser, MultiPartException, MultiPartParser\nfrom starlette.types import Message, Receive, Scope, Send\n\ntry:\n    from multipart.multipart import parse_options_header\nexcept ModuleNotFoundError:  # pragma: nocover\n    parse_options_header = None\n\n\nif typing.TYPE_CHECKING:\n    from starlette.routing import Router\n\n\nSERVER_PUSH_HEADERS_TO_COPY = {\n    \"accept\",\n    \"accept-encoding\",\n    \"accept-language\",\n    \"cache-control\",\n    \"user-agent\",\n}\n\n\ndef cookie_parser(cookie_string: str) -> typing.Dict[str, str]:\n    \"\"\"\n    This function parses a ``Cookie`` HTTP header into a dict of key/value pairs.\n\n    It attempts to mimic browser cookie parsing behavior: browsers and web servers\n    frequently disregard the spec (RFC 6265) when setting and reading cookies,\n    so we attempt to suit the common scenarios here.\n\n    This function has been adapted from Django 3.1.0.\n    Note: we are explicitly _NOT_ using `SimpleCookie.load` because it is based\n    on an outdated spec and will fail on lots of input we want to support\n    \"\"\"\n    cookie_dict: typing.Dict[str, str] = {}\n    for chunk in cookie_string.split(\";\"):\n        if \"=\" in chunk:\n            key, val = chunk.split(\"=\", 1)\n        else:\n            # Assume an empty name per\n            # https://bugzilla.mozilla.org/show_bug.cgi?id=169091\n            key, val = \"\", chunk\n        key, val = key.strip(), val.strip()\n        if key or val:\n            # unquote using Python's algorithm.\n            cookie_dict[key] = http_cookies._unquote(val)\n    return cookie_dict\n\n\nclass ClientDisconnect(Exception):\n    pass\n\n\nclass HTTPConnection(typing.Mapping[str, typing.Any]):\n    \"\"\"\n    A base class for incoming HTTP connections, that is used to provide\n    any functionality that is common to both `Request` and `WebSocket`.\n    \"\"\"\n\n    def __init__(self, scope: Scope, receive: typing.Optional[Receive] = None) -> None:\n        assert scope[\"type\"] in (\"http\", \"websocket\")\n        self.scope = scope\n\n    def __getitem__(self, key: str) -> typing.Any:\n        return self.scope[key]\n\n    def __iter__(self) -> typing.Iterator[str]:\n        return iter(self.scope)\n\n    def __len__(self) -> int:\n        return len(self.scope)\n\n    # Don't use the `abc.Mapping.__eq__` implementation.\n    # Connection instances should never be considered equal\n    # unless `self is other`.\n    __eq__ = object.__eq__\n    __hash__ = object.__hash__\n\n    @property\n    def app(self) -> typing.Any:\n        return self.scope[\"app\"]\n\n    @property\n    def url(self) -> URL:\n        if not hasattr(self, \"_url\"):\n            self._url = URL(scope=self.scope)\n        return self._url\n\n    @property\n    def base_url(self) -> URL:\n        if not hasattr(self, \"_base_url\"):\n            base_url_scope = dict(self.scope)\n            base_url_scope[\"path\"] = \"/\"\n            base_url_scope[\"query_string\"] = b\"\"\n            base_url_scope[\"root_path\"] = base_url_scope.get(\n                \"app_root_path\", base_url_scope.get(\"root_path\", \"\")\n            )\n            self._base_url = URL(scope=base_url_scope)\n        return self._base_url\n\n    @property\n    def headers(self) -> Headers:\n        if not hasattr(self, \"_headers\"):\n            self._headers = Headers(scope=self.scope)\n        return self._headers\n\n    @property\n    def query_params(self) -> QueryParams:\n        if not hasattr(self, \"_query_params\"):\n            self._query_params = QueryParams(self.scope[\"query_string\"])\n        return self._query_params\n\n    @property\n    def path_params(self) -> typing.Dict[str, typing.Any]:\n        return self.scope.get(\"path_params\", {})\n\n    @property\n    def cookies(self) -> typing.Dict[str, str]:\n        if not hasattr(self, \"_cookies\"):\n            cookies: typing.Dict[str, str] = {}\n            cookie_header = self.headers.get(\"cookie\")\n\n            if cookie_header:\n                cookies = cookie_parser(cookie_header)\n            self._cookies = cookies\n        return self._cookies\n\n    @property\n    def client(self) -> typing.Optional[Address]:\n        # client is a 2 item tuple of (host, port), None or missing\n        host_port = self.scope.get(\"client\")\n        if host_port is not None:\n            return Address(*host_port)\n        return None\n\n    @property\n    def session(self) -> typing.Dict[str, typing.Any]:\n        assert (\n            \"session\" in self.scope\n        ), \"SessionMiddleware must be installed to access request.session\"\n        return self.scope[\"session\"]\n\n    @property\n    def auth(self) -> typing.Any:\n        assert (\n            \"auth\" in self.scope\n        ), \"AuthenticationMiddleware must be installed to access request.auth\"\n        return self.scope[\"auth\"]\n\n    @property\n    def user(self) -> typing.Any:\n        assert (\n            \"user\" in self.scope\n        ), \"AuthenticationMiddleware must be installed to access request.user\"\n        return self.scope[\"user\"]\n\n    @property\n    def state(self) -> State:\n        if not hasattr(self, \"_state\"):\n            # Ensure 'state' has an empty dict if it's not already populated.\n            self.scope.setdefault(\"state\", {})\n            # Create a state instance with a reference to the dict in which it should\n            # store info\n            self._state = State(self.scope[\"state\"])\n        return self._state\n\n    def url_for(self, __name: str, **path_params: typing.Any) -> URL:\n        router: Router = self.scope[\"router\"]\n        url_path = router.url_path_for(__name, **path_params)\n        return url_path.make_absolute_url(base_url=self.base_url)\n\n\nasync def empty_receive() -> typing.NoReturn:\n    raise RuntimeError(\"Receive channel has not been made available\")\n\n\nasync def empty_send(message: Message) -> typing.NoReturn:\n    raise RuntimeError(\"Send channel has not been made available\")\n\n\nclass Request(HTTPConnection):\n    _form: typing.Optional[FormData]\n\n    def __init__(\n        self, scope: Scope, receive: Receive = empty_receive, send: Send = empty_send\n    ):\n        super().__init__(scope)\n        assert scope[\"type\"] == \"http\"\n        self._receive = receive\n        self._send = send\n        self._stream_consumed = False\n        self._is_disconnected = False\n        self._form = None\n\n    @property\n    def method(self) -> str:\n        return self.scope[\"method\"]\n\n    @property\n    def receive(self) -> Receive:\n        return self._receive\n\n    async def stream(self) -> typing.AsyncGenerator[bytes, None]:\n        if hasattr(self, \"_body\"):\n            yield self._body\n            yield b\"\"\n            return\n        if self._stream_consumed:\n            raise RuntimeError(\"Stream consumed\")\n        self._stream_consumed = True\n        while True:\n            message = await self._receive()\n            if message[\"type\"] == \"http.request\":\n                body = message.get(\"body\", b\"\")\n                if body:\n                    yield body\n                if not message.get(\"more_body\", False):\n                    break\n            elif message[\"type\"] == \"http.disconnect\":\n                self._is_disconnected = True\n                raise ClientDisconnect()\n        yield b\"\"\n\n    async def body(self) -> bytes:\n        if not hasattr(self, \"_body\"):\n            chunks: \"typing.List[bytes]\" = []\n            async for chunk in self.stream():\n                chunks.append(chunk)\n            self._body = b\"\".join(chunks)\n        return self._body\n\n    async def json(self) -> typing.Any:\n        if not hasattr(self, \"_json\"):\n            body = await self.body()\n            self._json = json.loads(body)\n        return self._json\n\n    async def _get_form(\n        self,\n        *,\n        max_files: typing.Union[int, float] = 1000,\n        max_fields: typing.Union[int, float] = 1000,\n    ) -> FormData:\n        if self._form is None:\n            assert (\n                parse_options_header is not None\n            ), \"The `python-multipart` library must be installed to use form parsing.\"\n            content_type_header = self.headers.get(\"Content-Type\")\n            content_type: bytes\n            content_type, _ = parse_options_header(content_type_header)\n            if content_type == b\"multipart/form-data\":\n                try:\n                    multipart_parser = MultiPartParser(\n                        self.headers,\n                        self.stream(),\n                        max_files=max_files,\n                        max_fields=max_fields,\n                    )\n                    self._form = await multipart_parser.parse()\n                except MultiPartException as exc:\n                    if \"app\" in self.scope:\n                        raise HTTPException(status_code=400, detail=exc.message)\n                    raise exc\n            elif content_type == b\"application/x-www-form-urlencoded\":\n                form_parser = FormParser(self.headers, self.stream())\n                self._form = await form_parser.parse()\n            else:\n                self._form = FormData()\n        return self._form\n\n    def form(\n        self,\n        *,\n        max_files: typing.Union[int, float] = 1000,\n        max_fields: typing.Union[int, float] = 1000,\n    ) -> AwaitableOrContextManager[FormData]:\n        return AwaitableOrContextManagerWrapper(\n            self._get_form(max_files=max_files, max_fields=max_fields)\n        )\n\n    async def close(self) -> None:\n        if self._form is not None:\n            await self._form.close()\n\n    async def is_disconnected(self) -> bool:\n        if not self._is_disconnected:\n            message: Message = {}\n\n            # If message isn't immediately available, move on\n            with anyio.CancelScope() as cs:\n                cs.cancel()\n                message = await self._receive()\n\n            if message.get(\"type\") == \"http.disconnect\":\n                self._is_disconnected = True\n\n        return self._is_disconnected\n\n    async def send_push_promise(self, path: str) -> None:\n        if \"http.response.push\" in self.scope.get(\"extensions\", {}):\n            raw_headers: \"typing.List[typing.Tuple[bytes, bytes]]\" = []\n            for name in SERVER_PUSH_HEADERS_TO_COPY:\n                for value in self.headers.getlist(name):\n                    raw_headers.append(\n                        (name.encode(\"latin-1\"), value.encode(\"latin-1\"))\n                    )\n            await self._send(\n                {\"type\": \"http.response.push\", \"path\": path, \"headers\": raw_headers}\n            )\n", 318], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py": ["import http.cookies\nimport json\nimport os\nimport stat\nimport sys\nimport typing\nfrom datetime import datetime\nfrom email.utils import format_datetime, formatdate\nfrom functools import partial\nfrom mimetypes import guess_type as mimetypes_guess_type\nfrom urllib.parse import quote\n\nimport anyio\n\nfrom starlette._compat import md5_hexdigest\nfrom starlette.background import BackgroundTask\nfrom starlette.concurrency import iterate_in_threadpool\nfrom starlette.datastructures import URL, MutableHeaders\nfrom starlette.types import Receive, Scope, Send\n\nif sys.version_info >= (3, 8):  # pragma: no cover\n    from typing import Literal\nelse:  # pragma: no cover\n    from typing_extensions import Literal\n\n# Workaround for adding samesite support to pre 3.8 python\nhttp.cookies.Morsel._reserved[\"samesite\"] = \"SameSite\"  # type: ignore[attr-defined]\n\n\n# Compatibility wrapper for `mimetypes.guess_type` to support `os.PathLike` on <py3.8\ndef guess_type(\n    url: typing.Union[str, \"os.PathLike[str]\"], strict: bool = True\n) -> typing.Tuple[typing.Optional[str], typing.Optional[str]]:\n    if sys.version_info < (3, 8):  # pragma: no cover\n        url = os.fspath(url)\n    return mimetypes_guess_type(url, strict)\n\n\nclass Response:\n    media_type = None\n    charset = \"utf-8\"\n\n    def __init__(\n        self,\n        content: typing.Any = None,\n        status_code: int = 200,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        media_type: typing.Optional[str] = None,\n        background: typing.Optional[BackgroundTask] = None,\n    ) -> None:\n        self.status_code = status_code\n        if media_type is not None:\n            self.media_type = media_type\n        self.background = background\n        self.body = self.render(content)\n        self.init_headers(headers)\n\n    def render(self, content: typing.Any) -> bytes:\n        if content is None:\n            return b\"\"\n        if isinstance(content, bytes):\n            return content\n        return content.encode(self.charset)\n\n    def init_headers(\n        self, headers: typing.Optional[typing.Mapping[str, str]] = None\n    ) -> None:\n        if headers is None:\n            raw_headers: typing.List[typing.Tuple[bytes, bytes]] = []\n            populate_content_length = True\n            populate_content_type = True\n        else:\n            raw_headers = [\n                (k.lower().encode(\"latin-1\"), v.encode(\"latin-1\"))\n                for k, v in headers.items()\n            ]\n            keys = [h[0] for h in raw_headers]\n            populate_content_length = b\"content-length\" not in keys\n            populate_content_type = b\"content-type\" not in keys\n\n        body = getattr(self, \"body\", None)\n        if (\n            body is not None\n            and populate_content_length\n            and not (self.status_code < 200 or self.status_code in (204, 304))\n        ):\n            content_length = str(len(body))\n            raw_headers.append((b\"content-length\", content_length.encode(\"latin-1\")))\n\n        content_type = self.media_type\n        if content_type is not None and populate_content_type:\n            if content_type.startswith(\"text/\"):\n                content_type += \"; charset=\" + self.charset\n            raw_headers.append((b\"content-type\", content_type.encode(\"latin-1\")))\n\n        self.raw_headers = raw_headers\n\n    @property\n    def headers(self) -> MutableHeaders:\n        if not hasattr(self, \"_headers\"):\n            self._headers = MutableHeaders(raw=self.raw_headers)\n        return self._headers\n\n    def set_cookie(\n        self,\n        key: str,\n        value: str = \"\",\n        max_age: typing.Optional[int] = None,\n        expires: typing.Optional[typing.Union[datetime, str, int]] = None,\n        path: str = \"/\",\n        domain: typing.Optional[str] = None,\n        secure: bool = False,\n        httponly: bool = False,\n        samesite: typing.Optional[Literal[\"lax\", \"strict\", \"none\"]] = \"lax\",\n    ) -> None:\n        cookie: \"http.cookies.BaseCookie[str]\" = http.cookies.SimpleCookie()\n        cookie[key] = value\n        if max_age is not None:\n            cookie[key][\"max-age\"] = max_age\n        if expires is not None:\n            if isinstance(expires, datetime):\n                cookie[key][\"expires\"] = format_datetime(expires, usegmt=True)\n            else:\n                cookie[key][\"expires\"] = expires\n        if path is not None:\n            cookie[key][\"path\"] = path\n        if domain is not None:\n            cookie[key][\"domain\"] = domain\n        if secure:\n            cookie[key][\"secure\"] = True\n        if httponly:\n            cookie[key][\"httponly\"] = True\n        if samesite is not None:\n            assert samesite.lower() in [\n                \"strict\",\n                \"lax\",\n                \"none\",\n            ], \"samesite must be either 'strict', 'lax' or 'none'\"\n            cookie[key][\"samesite\"] = samesite\n        cookie_val = cookie.output(header=\"\").strip()\n        self.raw_headers.append((b\"set-cookie\", cookie_val.encode(\"latin-1\")))\n\n    def delete_cookie(\n        self,\n        key: str,\n        path: str = \"/\",\n        domain: typing.Optional[str] = None,\n        secure: bool = False,\n        httponly: bool = False,\n        samesite: typing.Optional[Literal[\"lax\", \"strict\", \"none\"]] = \"lax\",\n    ) -> None:\n        self.set_cookie(\n            key,\n            max_age=0,\n            expires=0,\n            path=path,\n            domain=domain,\n            secure=secure,\n            httponly=httponly,\n            samesite=samesite,\n        )\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status_code,\n                \"headers\": self.raw_headers,\n            }\n        )\n        await send({\"type\": \"http.response.body\", \"body\": self.body})\n\n        if self.background is not None:\n            await self.background()\n\n\nclass HTMLResponse(Response):\n    media_type = \"text/html\"\n\n\nclass PlainTextResponse(Response):\n    media_type = \"text/plain\"\n\n\nclass JSONResponse(Response):\n    media_type = \"application/json\"\n\n    def __init__(\n        self,\n        content: typing.Any,\n        status_code: int = 200,\n        headers: typing.Optional[typing.Dict[str, str]] = None,\n        media_type: typing.Optional[str] = None,\n        background: typing.Optional[BackgroundTask] = None,\n    ) -> None:\n        super().__init__(content, status_code, headers, media_type, background)\n\n    def render(self, content: typing.Any) -> bytes:\n        return json.dumps(\n            content,\n            ensure_ascii=False,\n            allow_nan=False,\n            indent=None,\n            separators=(\",\", \":\"),\n        ).encode(\"utf-8\")\n\n\nclass RedirectResponse(Response):\n    def __init__(\n        self,\n        url: typing.Union[str, URL],\n        status_code: int = 307,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        background: typing.Optional[BackgroundTask] = None,\n    ) -> None:\n        super().__init__(\n            content=b\"\", status_code=status_code, headers=headers, background=background\n        )\n        self.headers[\"location\"] = quote(str(url), safe=\":/%#?=@[]!$&'()*+,;\")\n\n\nContent = typing.Union[str, bytes]\nSyncContentStream = typing.Iterator[Content]\nAsyncContentStream = typing.AsyncIterable[Content]\nContentStream = typing.Union[AsyncContentStream, SyncContentStream]\n\n\nclass StreamingResponse(Response):\n    body_iterator: AsyncContentStream\n\n    def __init__(\n        self,\n        content: ContentStream,\n        status_code: int = 200,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        media_type: typing.Optional[str] = None,\n        background: typing.Optional[BackgroundTask] = None,\n    ) -> None:\n        if isinstance(content, typing.AsyncIterable):\n            self.body_iterator = content\n        else:\n            self.body_iterator = iterate_in_threadpool(content)\n        self.status_code = status_code\n        self.media_type = self.media_type if media_type is None else media_type\n        self.background = background\n        self.init_headers(headers)\n\n    async def listen_for_disconnect(self, receive: Receive) -> None:\n        while True:\n            message = await receive()\n            if message[\"type\"] == \"http.disconnect\":\n                break\n\n    async def stream_response(self, send: Send) -> None:\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status_code,\n                \"headers\": self.raw_headers,\n            }\n        )\n        async for chunk in self.body_iterator:\n            if not isinstance(chunk, bytes):\n                chunk = chunk.encode(self.charset)\n            await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": True})\n\n        await send({\"type\": \"http.response.body\", \"body\": b\"\", \"more_body\": False})\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        async with anyio.create_task_group() as task_group:\n\n            async def wrap(func: \"typing.Callable[[], typing.Awaitable[None]]\") -> None:\n                await func()\n                task_group.cancel_scope.cancel()\n\n            task_group.start_soon(wrap, partial(self.stream_response, send))\n            await wrap(partial(self.listen_for_disconnect, receive))\n\n        if self.background is not None:\n            await self.background()\n\n\nclass FileResponse(Response):\n    chunk_size = 64 * 1024\n\n    def __init__(\n        self,\n        path: typing.Union[str, \"os.PathLike[str]\"],\n        status_code: int = 200,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        media_type: typing.Optional[str] = None,\n        background: typing.Optional[BackgroundTask] = None,\n        filename: typing.Optional[str] = None,\n        stat_result: typing.Optional[os.stat_result] = None,\n        method: typing.Optional[str] = None,\n        content_disposition_type: str = \"attachment\",\n    ) -> None:\n        self.path = path\n        self.status_code = status_code\n        self.filename = filename\n        self.send_header_only = method is not None and method.upper() == \"HEAD\"\n        if media_type is None:\n            media_type = guess_type(filename or path)[0] or \"text/plain\"\n        self.media_type = media_type\n        self.background = background\n        self.init_headers(headers)\n        if self.filename is not None:\n            content_disposition_filename = quote(self.filename)\n            if content_disposition_filename != self.filename:\n                content_disposition = \"{}; filename*=utf-8''{}\".format(\n                    content_disposition_type, content_disposition_filename\n                )\n            else:\n                content_disposition = '{}; filename=\"{}\"'.format(\n                    content_disposition_type, self.filename\n                )\n            self.headers.setdefault(\"content-disposition\", content_disposition)\n        self.stat_result = stat_result\n        if stat_result is not None:\n            self.set_stat_headers(stat_result)\n\n    def set_stat_headers(self, stat_result: os.stat_result) -> None:\n        content_length = str(stat_result.st_size)\n        last_modified = formatdate(stat_result.st_mtime, usegmt=True)\n        etag_base = str(stat_result.st_mtime) + \"-\" + str(stat_result.st_size)\n        etag = md5_hexdigest(etag_base.encode(), usedforsecurity=False)\n\n        self.headers.setdefault(\"content-length\", content_length)\n        self.headers.setdefault(\"last-modified\", last_modified)\n        self.headers.setdefault(\"etag\", etag)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if self.stat_result is None:\n            try:\n                stat_result = await anyio.to_thread.run_sync(os.stat, self.path)\n                self.set_stat_headers(stat_result)\n            except FileNotFoundError:\n                raise RuntimeError(f\"File at path {self.path} does not exist.\")\n            else:\n                mode = stat_result.st_mode\n                if not stat.S_ISREG(mode):\n                    raise RuntimeError(f\"File at path {self.path} is not a file.\")\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status_code,\n                \"headers\": self.raw_headers,\n            }\n        )\n        if self.send_header_only:\n            await send({\"type\": \"http.response.body\", \"body\": b\"\", \"more_body\": False})\n        else:\n            async with await anyio.open_file(self.path, mode=\"rb\") as file:\n                more_body = True\n                while more_body:\n                    chunk = await file.read(self.chunk_size)\n                    more_body = len(chunk) == self.chunk_size\n                    await send(\n                        {\n                            \"type\": \"http.response.body\",\n                            \"body\": chunk,\n                            \"more_body\": more_body,\n                        }\n                    )\n        if self.background is not None:\n            await self.background()\n", 366], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py": ["import typing\nfrom collections.abc import Sequence\nfrom shlex import shlex\nfrom urllib.parse import SplitResult, parse_qsl, urlencode, urlsplit\n\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.types import Scope\n\n\nclass Address(typing.NamedTuple):\n    host: str\n    port: int\n\n\n_KeyType = typing.TypeVar(\"_KeyType\")\n# Mapping keys are invariant but their values are covariant since\n# you can only read them\n# that is, you can't do `Mapping[str, Animal]()[\"fido\"] = Dog()`\n_CovariantValueType = typing.TypeVar(\"_CovariantValueType\", covariant=True)\n\n\nclass URL:\n    def __init__(\n        self,\n        url: str = \"\",\n        scope: typing.Optional[Scope] = None,\n        **components: typing.Any,\n    ) -> None:\n        if scope is not None:\n            assert not url, 'Cannot set both \"url\" and \"scope\".'\n            assert not components, 'Cannot set both \"scope\" and \"**components\".'\n            scheme = scope.get(\"scheme\", \"http\")\n            server = scope.get(\"server\", None)\n            path = scope.get(\"root_path\", \"\") + scope[\"path\"]\n            query_string = scope.get(\"query_string\", b\"\")\n\n            host_header = None\n            for key, value in scope[\"headers\"]:\n                if key == b\"host\":\n                    host_header = value.decode(\"latin-1\")\n                    break\n\n            if host_header is not None:\n                url = f\"{scheme}://{host_header}{path}\"\n            elif server is None:\n                url = path\n            else:\n                host, port = server\n                default_port = {\"http\": 80, \"https\": 443, \"ws\": 80, \"wss\": 443}[scheme]\n                if port == default_port:\n                    url = f\"{scheme}://{host}{path}\"\n                else:\n                    url = f\"{scheme}://{host}:{port}{path}\"\n\n            if query_string:\n                url += \"?\" + query_string.decode()\n        elif components:\n            assert not url, 'Cannot set both \"url\" and \"**components\".'\n            url = URL(\"\").replace(**components).components.geturl()\n\n        self._url = url\n\n    @property\n    def components(self) -> SplitResult:\n        if not hasattr(self, \"_components\"):\n            self._components = urlsplit(self._url)\n        return self._components\n\n    @property\n    def scheme(self) -> str:\n        return self.components.scheme\n\n    @property\n    def netloc(self) -> str:\n        return self.components.netloc\n\n    @property\n    def path(self) -> str:\n        return self.components.path\n\n    @property\n    def query(self) -> str:\n        return self.components.query\n\n    @property\n    def fragment(self) -> str:\n        return self.components.fragment\n\n    @property\n    def username(self) -> typing.Union[None, str]:\n        return self.components.username\n\n    @property\n    def password(self) -> typing.Union[None, str]:\n        return self.components.password\n\n    @property\n    def hostname(self) -> typing.Union[None, str]:\n        return self.components.hostname\n\n    @property\n    def port(self) -> typing.Optional[int]:\n        return self.components.port\n\n    @property\n    def is_secure(self) -> bool:\n        return self.scheme in (\"https\", \"wss\")\n\n    def replace(self, **kwargs: typing.Any) -> \"URL\":\n        if (\n            \"username\" in kwargs\n            or \"password\" in kwargs\n            or \"hostname\" in kwargs\n            or \"port\" in kwargs\n        ):\n            hostname = kwargs.pop(\"hostname\", None)\n            port = kwargs.pop(\"port\", self.port)\n            username = kwargs.pop(\"username\", self.username)\n            password = kwargs.pop(\"password\", self.password)\n\n            if hostname is None:\n                netloc = self.netloc\n                _, _, hostname = netloc.rpartition(\"@\")\n\n                if hostname[-1] != \"]\":\n                    hostname = hostname.rsplit(\":\", 1)[0]\n\n            netloc = hostname\n            if port is not None:\n                netloc += f\":{port}\"\n            if username is not None:\n                userpass = username\n                if password is not None:\n                    userpass += f\":{password}\"\n                netloc = f\"{userpass}@{netloc}\"\n\n            kwargs[\"netloc\"] = netloc\n\n        components = self.components._replace(**kwargs)\n        return self.__class__(components.geturl())\n\n    def include_query_params(self, **kwargs: typing.Any) -> \"URL\":\n        params = MultiDict(parse_qsl(self.query, keep_blank_values=True))\n        params.update({str(key): str(value) for key, value in kwargs.items()})\n        query = urlencode(params.multi_items())\n        return self.replace(query=query)\n\n    def replace_query_params(self, **kwargs: typing.Any) -> \"URL\":\n        query = urlencode([(str(key), str(value)) for key, value in kwargs.items()])\n        return self.replace(query=query)\n\n    def remove_query_params(\n        self, keys: typing.Union[str, typing.Sequence[str]]\n    ) -> \"URL\":\n        if isinstance(keys, str):\n            keys = [keys]\n        params = MultiDict(parse_qsl(self.query, keep_blank_values=True))\n        for key in keys:\n            params.pop(key, None)\n        query = urlencode(params.multi_items())\n        return self.replace(query=query)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return str(self) == str(other)\n\n    def __str__(self) -> str:\n        return self._url\n\n    def __repr__(self) -> str:\n        url = str(self)\n        if self.password:\n            url = str(self.replace(password=\"********\"))\n        return f\"{self.__class__.__name__}({repr(url)})\"\n\n\nclass URLPath(str):\n    \"\"\"\n    A URL path string that may also hold an associated protocol and/or host.\n    Used by the routing to return `url_path_for` matches.\n    \"\"\"\n\n    def __new__(cls, path: str, protocol: str = \"\", host: str = \"\") -> \"URLPath\":\n        assert protocol in (\"http\", \"websocket\", \"\")\n        return str.__new__(cls, path)\n\n    def __init__(self, path: str, protocol: str = \"\", host: str = \"\") -> None:\n        self.protocol = protocol\n        self.host = host\n\n    def make_absolute_url(self, base_url: typing.Union[str, URL]) -> URL:\n        if isinstance(base_url, str):\n            base_url = URL(base_url)\n        if self.protocol:\n            scheme = {\n                \"http\": {True: \"https\", False: \"http\"},\n                \"websocket\": {True: \"wss\", False: \"ws\"},\n            }[self.protocol][base_url.is_secure]\n        else:\n            scheme = base_url.scheme\n\n        netloc = self.host or base_url.netloc\n        path = base_url.path.rstrip(\"/\") + str(self)\n        return URL(scheme=scheme, netloc=netloc, path=path)\n\n\nclass Secret:\n    \"\"\"\n    Holds a string value that should not be revealed in tracebacks etc.\n    You should cast the value to `str` at the point it is required.\n    \"\"\"\n\n    def __init__(self, value: str):\n        self._value = value\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        return f\"{class_name}('**********')\"\n\n    def __str__(self) -> str:\n        return self._value\n\n    def __bool__(self) -> bool:\n        return bool(self._value)\n\n\nclass CommaSeparatedStrings(Sequence):\n    def __init__(self, value: typing.Union[str, typing.Sequence[str]]):\n        if isinstance(value, str):\n            splitter = shlex(value, posix=True)\n            splitter.whitespace = \",\"\n            splitter.whitespace_split = True\n            self._items = [item.strip() for item in splitter]\n        else:\n            self._items = list(value)\n\n    def __len__(self) -> int:\n        return len(self._items)\n\n    def __getitem__(self, index: typing.Union[int, slice]) -> typing.Any:\n        return self._items[index]\n\n    def __iter__(self) -> typing.Iterator[str]:\n        return iter(self._items)\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        items = [item for item in self]\n        return f\"{class_name}({items!r})\"\n\n    def __str__(self) -> str:\n        return \", \".join(repr(item) for item in self)\n\n\nclass ImmutableMultiDict(typing.Mapping[_KeyType, _CovariantValueType]):\n    _dict: typing.Dict[_KeyType, _CovariantValueType]\n\n    def __init__(\n        self,\n        *args: typing.Union[\n            \"ImmutableMultiDict[_KeyType, _CovariantValueType]\",\n            typing.Mapping[_KeyType, _CovariantValueType],\n            typing.Iterable[typing.Tuple[_KeyType, _CovariantValueType]],\n        ],\n        **kwargs: typing.Any,\n    ) -> None:\n        assert len(args) < 2, \"Too many arguments.\"\n\n        value: typing.Any = args[0] if args else []\n        if kwargs:\n            value = (\n                ImmutableMultiDict(value).multi_items()\n                + ImmutableMultiDict(kwargs).multi_items()  # type: ignore[operator]\n            )\n\n        if not value:\n            _items: typing.List[typing.Tuple[typing.Any, typing.Any]] = []\n        elif hasattr(value, \"multi_items\"):\n            value = typing.cast(\n                ImmutableMultiDict[_KeyType, _CovariantValueType], value\n            )\n            _items = list(value.multi_items())\n        elif hasattr(value, \"items\"):\n            value = typing.cast(typing.Mapping[_KeyType, _CovariantValueType], value)\n            _items = list(value.items())\n        else:\n            value = typing.cast(\n                typing.List[typing.Tuple[typing.Any, typing.Any]], value\n            )\n            _items = list(value)\n\n        self._dict = {k: v for k, v in _items}\n        self._list = _items\n\n    def getlist(self, key: typing.Any) -> typing.List[_CovariantValueType]:\n        return [item_value for item_key, item_value in self._list if item_key == key]\n\n    def keys(self) -> typing.KeysView[_KeyType]:\n        return self._dict.keys()\n\n    def values(self) -> typing.ValuesView[_CovariantValueType]:\n        return self._dict.values()\n\n    def items(self) -> typing.ItemsView[_KeyType, _CovariantValueType]:\n        return self._dict.items()\n\n    def multi_items(self) -> typing.List[typing.Tuple[_KeyType, _CovariantValueType]]:\n        return list(self._list)\n\n    def __getitem__(self, key: _KeyType) -> _CovariantValueType:\n        return self._dict[key]\n\n    def __contains__(self, key: typing.Any) -> bool:\n        return key in self._dict\n\n    def __iter__(self) -> typing.Iterator[_KeyType]:\n        return iter(self.keys())\n\n    def __len__(self) -> int:\n        return len(self._dict)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        if not isinstance(other, self.__class__):\n            return False\n        return sorted(self._list) == sorted(other._list)\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        items = self.multi_items()\n        return f\"{class_name}({items!r})\"\n\n\nclass MultiDict(ImmutableMultiDict[typing.Any, typing.Any]):\n    def __setitem__(self, key: typing.Any, value: typing.Any) -> None:\n        self.setlist(key, [value])\n\n    def __delitem__(self, key: typing.Any) -> None:\n        self._list = [(k, v) for k, v in self._list if k != key]\n        del self._dict[key]\n\n    def pop(self, key: typing.Any, default: typing.Any = None) -> typing.Any:\n        self._list = [(k, v) for k, v in self._list if k != key]\n        return self._dict.pop(key, default)\n\n    def popitem(self) -> typing.Tuple:\n        key, value = self._dict.popitem()\n        self._list = [(k, v) for k, v in self._list if k != key]\n        return key, value\n\n    def poplist(self, key: typing.Any) -> typing.List:\n        values = [v for k, v in self._list if k == key]\n        self.pop(key)\n        return values\n\n    def clear(self) -> None:\n        self._dict.clear()\n        self._list.clear()\n\n    def setdefault(self, key: typing.Any, default: typing.Any = None) -> typing.Any:\n        if key not in self:\n            self._dict[key] = default\n            self._list.append((key, default))\n\n        return self[key]\n\n    def setlist(self, key: typing.Any, values: typing.List) -> None:\n        if not values:\n            self.pop(key, None)\n        else:\n            existing_items = [(k, v) for (k, v) in self._list if k != key]\n            self._list = existing_items + [(key, value) for value in values]\n            self._dict[key] = values[-1]\n\n    def append(self, key: typing.Any, value: typing.Any) -> None:\n        self._list.append((key, value))\n        self._dict[key] = value\n\n    def update(\n        self,\n        *args: typing.Union[\n            \"MultiDict\",\n            typing.Mapping,\n            typing.List[typing.Tuple[typing.Any, typing.Any]],\n        ],\n        **kwargs: typing.Any,\n    ) -> None:\n        value = MultiDict(*args, **kwargs)\n        existing_items = [(k, v) for (k, v) in self._list if k not in value.keys()]\n        self._list = existing_items + value.multi_items()\n        self._dict.update(value)\n\n\nclass QueryParams(ImmutableMultiDict[str, str]):\n    \"\"\"\n    An immutable multidict.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: typing.Union[\n            \"ImmutableMultiDict\",\n            typing.Mapping,\n            typing.List[typing.Tuple[typing.Any, typing.Any]],\n            str,\n            bytes,\n        ],\n        **kwargs: typing.Any,\n    ) -> None:\n        assert len(args) < 2, \"Too many arguments.\"\n\n        value = args[0] if args else []\n\n        if isinstance(value, str):\n            super().__init__(parse_qsl(value, keep_blank_values=True), **kwargs)\n        elif isinstance(value, bytes):\n            super().__init__(\n                parse_qsl(value.decode(\"latin-1\"), keep_blank_values=True), **kwargs\n            )\n        else:\n            super().__init__(*args, **kwargs)  # type: ignore[arg-type]\n        self._list = [(str(k), str(v)) for k, v in self._list]\n        self._dict = {str(k): str(v) for k, v in self._dict.items()}\n\n    def __str__(self) -> str:\n        return urlencode(self._list)\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        query_string = str(self)\n        return f\"{class_name}({query_string!r})\"\n\n\nclass UploadFile:\n    \"\"\"\n    An uploaded file included as part of the request data.\n    \"\"\"\n\n    def __init__(\n        self,\n        file: typing.BinaryIO,\n        *,\n        size: typing.Optional[int] = None,\n        filename: typing.Optional[str] = None,\n        headers: \"typing.Optional[Headers]\" = None,\n    ) -> None:\n        self.filename = filename\n        self.file = file\n        self.size = size\n        self.headers = headers or Headers()\n\n    @property\n    def content_type(self) -> typing.Optional[str]:\n        return self.headers.get(\"content-type\", None)\n\n    @property\n    def _in_memory(self) -> bool:\n        # check for SpooledTemporaryFile._rolled\n        rolled_to_disk = getattr(self.file, \"_rolled\", True)\n        return not rolled_to_disk\n\n    async def write(self, data: bytes) -> None:\n        if self.size is not None:\n            self.size += len(data)\n\n        if self._in_memory:\n            self.file.write(data)\n        else:\n            await run_in_threadpool(self.file.write, data)\n\n    async def read(self, size: int = -1) -> bytes:\n        if self._in_memory:\n            return self.file.read(size)\n        return await run_in_threadpool(self.file.read, size)\n\n    async def seek(self, offset: int) -> None:\n        if self._in_memory:\n            self.file.seek(offset)\n        else:\n            await run_in_threadpool(self.file.seek, offset)\n\n    async def close(self) -> None:\n        if self._in_memory:\n            self.file.close()\n        else:\n            await run_in_threadpool(self.file.close)\n\n\nclass FormData(ImmutableMultiDict[str, typing.Union[UploadFile, str]]):\n    \"\"\"\n    An immutable multidict, containing both file uploads and text input.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: typing.Union[\n            \"FormData\",\n            typing.Mapping[str, typing.Union[str, UploadFile]],\n            typing.List[typing.Tuple[str, typing.Union[str, UploadFile]]],\n        ],\n        **kwargs: typing.Union[str, UploadFile],\n    ) -> None:\n        super().__init__(*args, **kwargs)\n\n    async def close(self) -> None:\n        for key, value in self.multi_items():\n            if isinstance(value, UploadFile):\n                await value.close()\n\n\nclass Headers(typing.Mapping[str, str]):\n    \"\"\"\n    An immutable, case-insensitive multidict.\n    \"\"\"\n\n    def __init__(\n        self,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        raw: typing.Optional[typing.List[typing.Tuple[bytes, bytes]]] = None,\n        scope: typing.Optional[typing.MutableMapping[str, typing.Any]] = None,\n    ) -> None:\n        self._list: typing.List[typing.Tuple[bytes, bytes]] = []\n        if headers is not None:\n            assert raw is None, 'Cannot set both \"headers\" and \"raw\".'\n            assert scope is None, 'Cannot set both \"headers\" and \"scope\".'\n            self._list = [\n                (key.lower().encode(\"latin-1\"), value.encode(\"latin-1\"))\n                for key, value in headers.items()\n            ]\n        elif raw is not None:\n            assert scope is None, 'Cannot set both \"raw\" and \"scope\".'\n            self._list = raw\n        elif scope is not None:\n            # scope[\"headers\"] isn't necessarily a list\n            # it might be a tuple or other iterable\n            self._list = scope[\"headers\"] = list(scope[\"headers\"])\n\n    @property\n    def raw(self) -> typing.List[typing.Tuple[bytes, bytes]]:\n        return list(self._list)\n\n    def keys(self) -> typing.List[str]:  # type: ignore[override]\n        return [key.decode(\"latin-1\") for key, value in self._list]\n\n    def values(self) -> typing.List[str]:  # type: ignore[override]\n        return [value.decode(\"latin-1\") for key, value in self._list]\n\n    def items(self) -> typing.List[typing.Tuple[str, str]]:  # type: ignore[override]\n        return [\n            (key.decode(\"latin-1\"), value.decode(\"latin-1\"))\n            for key, value in self._list\n        ]\n\n    def getlist(self, key: str) -> typing.List[str]:\n        get_header_key = key.lower().encode(\"latin-1\")\n        return [\n            item_value.decode(\"latin-1\")\n            for item_key, item_value in self._list\n            if item_key == get_header_key\n        ]\n\n    def mutablecopy(self) -> \"MutableHeaders\":\n        return MutableHeaders(raw=self._list[:])\n\n    def __getitem__(self, key: str) -> str:\n        get_header_key = key.lower().encode(\"latin-1\")\n        for header_key, header_value in self._list:\n            if header_key == get_header_key:\n                return header_value.decode(\"latin-1\")\n        raise KeyError(key)\n\n    def __contains__(self, key: typing.Any) -> bool:\n        get_header_key = key.lower().encode(\"latin-1\")\n        for header_key, header_value in self._list:\n            if header_key == get_header_key:\n                return True\n        return False\n\n    def __iter__(self) -> typing.Iterator[typing.Any]:\n        return iter(self.keys())\n\n    def __len__(self) -> int:\n        return len(self._list)\n\n    def __eq__(self, other: typing.Any) -> bool:\n        if not isinstance(other, Headers):\n            return False\n        return sorted(self._list) == sorted(other._list)\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        as_dict = dict(self.items())\n        if len(as_dict) == len(self):\n            return f\"{class_name}({as_dict!r})\"\n        return f\"{class_name}(raw={self.raw!r})\"\n\n\nclass MutableHeaders(Headers):\n    def __setitem__(self, key: str, value: str) -> None:\n        \"\"\"\n        Set the header `key` to `value`, removing any duplicate entries.\n        Retains insertion order.\n        \"\"\"\n        set_key = key.lower().encode(\"latin-1\")\n        set_value = value.encode(\"latin-1\")\n\n        found_indexes: \"typing.List[int]\" = []\n        for idx, (item_key, item_value) in enumerate(self._list):\n            if item_key == set_key:\n                found_indexes.append(idx)\n\n        for idx in reversed(found_indexes[1:]):\n            del self._list[idx]\n\n        if found_indexes:\n            idx = found_indexes[0]\n            self._list[idx] = (set_key, set_value)\n        else:\n            self._list.append((set_key, set_value))\n\n    def __delitem__(self, key: str) -> None:\n        \"\"\"\n        Remove the header `key`.\n        \"\"\"\n        del_key = key.lower().encode(\"latin-1\")\n\n        pop_indexes: \"typing.List[int]\" = []\n        for idx, (item_key, item_value) in enumerate(self._list):\n            if item_key == del_key:\n                pop_indexes.append(idx)\n\n        for idx in reversed(pop_indexes):\n            del self._list[idx]\n\n    def __ior__(self, other: typing.Mapping[str, str]) -> \"MutableHeaders\":\n        if not isinstance(other, typing.Mapping):\n            raise TypeError(f\"Expected a mapping but got {other.__class__.__name__}\")\n        self.update(other)\n        return self\n\n    def __or__(self, other: typing.Mapping[str, str]) -> \"MutableHeaders\":\n        if not isinstance(other, typing.Mapping):\n            raise TypeError(f\"Expected a mapping but got {other.__class__.__name__}\")\n        new = self.mutablecopy()\n        new.update(other)\n        return new\n\n    @property\n    def raw(self) -> typing.List[typing.Tuple[bytes, bytes]]:\n        return self._list\n\n    def setdefault(self, key: str, value: str) -> str:\n        \"\"\"\n        If the header `key` does not exist, then set it to `value`.\n        Returns the header value.\n        \"\"\"\n        set_key = key.lower().encode(\"latin-1\")\n        set_value = value.encode(\"latin-1\")\n\n        for idx, (item_key, item_value) in enumerate(self._list):\n            if item_key == set_key:\n                return item_value.decode(\"latin-1\")\n        self._list.append((set_key, set_value))\n        return value\n\n    def update(self, other: typing.Mapping[str, str]) -> None:\n        for key, val in other.items():\n            self[key] = val\n\n    def append(self, key: str, value: str) -> None:\n        \"\"\"\n        Append a header, preserving any duplicate entries.\n        \"\"\"\n        append_key = key.lower().encode(\"latin-1\")\n        append_value = value.encode(\"latin-1\")\n        self._list.append((append_key, append_value))\n\n    def add_vary_header(self, vary: str) -> None:\n        existing = self.get(\"vary\")\n        if existing is not None:\n            vary = \", \".join([existing, vary])\n        self[\"vary\"] = vary\n\n\nclass State:\n    \"\"\"\n    An object that can be used to store arbitrary state.\n\n    Used for `request.state` and `app.state`.\n    \"\"\"\n\n    _state: typing.Dict[str, typing.Any]\n\n    def __init__(self, state: typing.Optional[typing.Dict[str, typing.Any]] = None):\n        if state is None:\n            state = {}\n        super().__setattr__(\"_state\", state)\n\n    def __setattr__(self, key: typing.Any, value: typing.Any) -> None:\n        self._state[key] = value\n\n    def __getattr__(self, key: typing.Any) -> typing.Any:\n        try:\n            return self._state[key]\n        except KeyError:\n            message = \"'{}' object has no attribute '{}'\"\n            raise AttributeError(message.format(self.__class__.__name__, key))\n\n    def __delattr__(self, key: typing.Any) -> None:\n        del self._state[key]\n", 708], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py": ["import inspect\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nfrom typing import (\n    Any,\n    Callable,\n    Coroutine,\n    Dict,\n    ForwardRef,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport anyio\nfrom fastapi import params\nfrom fastapi._compat import (\n    PYDANTIC_V2,\n    ErrorWrapper,\n    ModelField,\n    Required,\n    Undefined,\n    _regenerate_error_with_loc,\n    copy_field_info,\n    create_body_model,\n    evaluate_forwardref,\n    field_annotation_is_scalar,\n    get_annotation_from_field_info,\n    get_missing_field_error,\n    is_bytes_field,\n    is_bytes_sequence_field,\n    is_scalar_field,\n    is_scalar_sequence_field,\n    is_sequence_field,\n    is_uploadfile_or_nonable_uploadfile_annotation,\n    is_uploadfile_sequence_annotation,\n    lenient_issubclass,\n    sequence_types,\n    serialize_sequence_value,\n    value_is_sequence,\n)\nfrom fastapi.concurrency import (\n    AsyncExitStack,\n    asynccontextmanager,\n    contextmanager_in_threadpool,\n)\nfrom fastapi.dependencies.models import Dependant, SecurityRequirement\nfrom fastapi.logger import logger\nfrom fastapi.security.base import SecurityBase\nfrom fastapi.security.oauth2 import OAuth2, SecurityScopes\nfrom fastapi.security.open_id_connect_url import OpenIdConnect\nfrom fastapi.utils import create_response_field, get_path_param_names\nfrom pydantic.fields import FieldInfo\nfrom starlette.background import BackgroundTasks\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.datastructures import FormData, Headers, QueryParams, UploadFile\nfrom starlette.requests import HTTPConnection, Request\nfrom starlette.responses import Response\nfrom starlette.websockets import WebSocket\nfrom typing_extensions import Annotated, get_args, get_origin\n\nmultipart_not_installed_error = (\n    'Form data requires \"python-multipart\" to be installed. \\n'\n    'You can install \"python-multipart\" with: \\n\\n'\n    \"pip install python-multipart\\n\"\n)\nmultipart_incorrect_install_error = (\n    'Form data requires \"python-multipart\" to be installed. '\n    'It seems you installed \"multipart\" instead. \\n'\n    'You can remove \"multipart\" with: \\n\\n'\n    \"pip uninstall multipart\\n\\n\"\n    'And then install \"python-multipart\" with: \\n\\n'\n    \"pip install python-multipart\\n\"\n)\n\n\ndef check_file_field(field: ModelField) -> None:\n    field_info = field.field_info\n    if isinstance(field_info, params.Form):\n        try:\n            # __version__ is available in both multiparts, and can be mocked\n            from multipart import __version__  # type: ignore\n\n            assert __version__\n            try:\n                # parse_options_header is only available in the right multipart\n                from multipart.multipart import parse_options_header  # type: ignore\n\n                assert parse_options_header\n            except ImportError:\n                logger.error(multipart_incorrect_install_error)\n                raise RuntimeError(multipart_incorrect_install_error) from None\n        except ImportError:\n            logger.error(multipart_not_installed_error)\n            raise RuntimeError(multipart_not_installed_error) from None\n\n\ndef get_param_sub_dependant(\n    *,\n    param_name: str,\n    depends: params.Depends,\n    path: str,\n    security_scopes: Optional[List[str]] = None,\n) -> Dependant:\n    assert depends.dependency\n    return get_sub_dependant(\n        depends=depends,\n        dependency=depends.dependency,\n        path=path,\n        name=param_name,\n        security_scopes=security_scopes,\n    )\n\n\ndef get_parameterless_sub_dependant(*, depends: params.Depends, path: str) -> Dependant:\n    assert callable(\n        depends.dependency\n    ), \"A parameter-less dependency must have a callable dependency\"\n    return get_sub_dependant(depends=depends, dependency=depends.dependency, path=path)\n\n\ndef get_sub_dependant(\n    *,\n    depends: params.Depends,\n    dependency: Callable[..., Any],\n    path: str,\n    name: Optional[str] = None,\n    security_scopes: Optional[List[str]] = None,\n) -> Dependant:\n    security_requirement = None\n    security_scopes = security_scopes or []\n    if isinstance(depends, params.Security):\n        dependency_scopes = depends.scopes\n        security_scopes.extend(dependency_scopes)\n    if isinstance(dependency, SecurityBase):\n        use_scopes: List[str] = []\n        if isinstance(dependency, (OAuth2, OpenIdConnect)):\n            use_scopes = security_scopes\n        security_requirement = SecurityRequirement(\n            security_scheme=dependency, scopes=use_scopes\n        )\n    sub_dependant = get_dependant(\n        path=path,\n        call=dependency,\n        name=name,\n        security_scopes=security_scopes,\n        use_cache=depends.use_cache,\n    )\n    if security_requirement:\n        sub_dependant.security_requirements.append(security_requirement)\n    return sub_dependant\n\n\nCacheKey = Tuple[Optional[Callable[..., Any]], Tuple[str, ...]]\n\n\ndef get_flat_dependant(\n    dependant: Dependant,\n    *,\n    skip_repeats: bool = False,\n    visited: Optional[List[CacheKey]] = None,\n) -> Dependant:\n    if visited is None:\n        visited = []\n    visited.append(dependant.cache_key)\n\n    flat_dependant = Dependant(\n        path_params=dependant.path_params.copy(),\n        query_params=dependant.query_params.copy(),\n        header_params=dependant.header_params.copy(),\n        cookie_params=dependant.cookie_params.copy(),\n        body_params=dependant.body_params.copy(),\n        security_schemes=dependant.security_requirements.copy(),\n        use_cache=dependant.use_cache,\n        path=dependant.path,\n    )\n    for sub_dependant in dependant.dependencies:\n        if skip_repeats and sub_dependant.cache_key in visited:\n            continue\n        flat_sub = get_flat_dependant(\n            sub_dependant, skip_repeats=skip_repeats, visited=visited\n        )\n        flat_dependant.path_params.extend(flat_sub.path_params)\n        flat_dependant.query_params.extend(flat_sub.query_params)\n        flat_dependant.header_params.extend(flat_sub.header_params)\n        flat_dependant.cookie_params.extend(flat_sub.cookie_params)\n        flat_dependant.body_params.extend(flat_sub.body_params)\n        flat_dependant.security_requirements.extend(flat_sub.security_requirements)\n    return flat_dependant\n\n\ndef get_flat_params(dependant: Dependant) -> List[ModelField]:\n    flat_dependant = get_flat_dependant(dependant, skip_repeats=True)\n    return (\n        flat_dependant.path_params\n        + flat_dependant.query_params\n        + flat_dependant.header_params\n        + flat_dependant.cookie_params\n    )\n\n\ndef get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n    signature = inspect.signature(call)\n    globalns = getattr(call, \"__globals__\", {})\n    typed_params = [\n        inspect.Parameter(\n            name=param.name,\n            kind=param.kind,\n            default=param.default,\n            annotation=get_typed_annotation(param.annotation, globalns),\n        )\n        for param in signature.parameters.values()\n    ]\n    typed_signature = inspect.Signature(typed_params)\n    return typed_signature\n\n\ndef get_typed_annotation(annotation: Any, globalns: Dict[str, Any]) -> Any:\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n        annotation = evaluate_forwardref(annotation, globalns, globalns)\n    return annotation\n\n\ndef get_typed_return_annotation(call: Callable[..., Any]) -> Any:\n    signature = inspect.signature(call)\n    annotation = signature.return_annotation\n\n    if annotation is inspect.Signature.empty:\n        return None\n\n    globalns = getattr(call, \"__globals__\", {})\n    return get_typed_annotation(annotation, globalns)\n\n\ndef get_dependant(\n    *,\n    path: str,\n    call: Callable[..., Any],\n    name: Optional[str] = None,\n    security_scopes: Optional[List[str]] = None,\n    use_cache: bool = True,\n) -> Dependant:\n    path_param_names = get_path_param_names(path)\n    endpoint_signature = get_typed_signature(call)\n    signature_params = endpoint_signature.parameters\n    dependant = Dependant(\n        call=call,\n        name=name,\n        path=path,\n        security_scopes=security_scopes,\n        use_cache=use_cache,\n    )\n    for param_name, param in signature_params.items():\n        is_path_param = param_name in path_param_names\n        type_annotation, depends, param_field = analyze_param(\n            param_name=param_name,\n            annotation=param.annotation,\n            value=param.default,\n            is_path_param=is_path_param,\n        )\n        if depends is not None:\n            sub_dependant = get_param_sub_dependant(\n                param_name=param_name,\n                depends=depends,\n                path=path,\n                security_scopes=security_scopes,\n            )\n            dependant.dependencies.append(sub_dependant)\n            continue\n        if add_non_field_param_to_dependency(\n            param_name=param_name,\n            type_annotation=type_annotation,\n            dependant=dependant,\n        ):\n            assert (\n                param_field is None\n            ), f\"Cannot specify multiple FastAPI annotations for {param_name!r}\"\n            continue\n        assert param_field is not None\n        if is_body_param(param_field=param_field, is_path_param=is_path_param):\n            dependant.body_params.append(param_field)\n        else:\n            add_param_to_fields(field=param_field, dependant=dependant)\n    return dependant\n\n\ndef add_non_field_param_to_dependency(\n    *, param_name: str, type_annotation: Any, dependant: Dependant\n) -> Optional[bool]:\n    if lenient_issubclass(type_annotation, Request):\n        dependant.request_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, WebSocket):\n        dependant.websocket_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, HTTPConnection):\n        dependant.http_connection_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, Response):\n        dependant.response_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, BackgroundTasks):\n        dependant.background_tasks_param_name = param_name\n        return True\n    elif lenient_issubclass(type_annotation, SecurityScopes):\n        dependant.security_scopes_param_name = param_name\n        return True\n    return None\n\n\ndef analyze_param(\n    *,\n    param_name: str,\n    annotation: Any,\n    value: Any,\n    is_path_param: bool,\n) -> Tuple[Any, Optional[params.Depends], Optional[ModelField]]:\n    field_info = None\n    depends = None\n    type_annotation: Any = Any\n    if (\n        annotation is not inspect.Signature.empty\n        and get_origin(annotation) is Annotated\n    ):\n        annotated_args = get_args(annotation)\n        type_annotation = annotated_args[0]\n        fastapi_annotations = [\n            arg\n            for arg in annotated_args[1:]\n            if isinstance(arg, (FieldInfo, params.Depends))\n        ]\n        assert (\n            len(fastapi_annotations) <= 1\n        ), f\"Cannot specify multiple `Annotated` FastAPI arguments for {param_name!r}\"\n        fastapi_annotation = next(iter(fastapi_annotations), None)\n        if isinstance(fastapi_annotation, FieldInfo):\n            # Copy `field_info` because we mutate `field_info.default` below.\n            field_info = copy_field_info(\n                field_info=fastapi_annotation, annotation=annotation\n            )\n            assert field_info.default is Undefined or field_info.default is Required, (\n                f\"`{field_info.__class__.__name__}` default value cannot be set in\"\n                f\" `Annotated` for {param_name!r}. Set the default value with `=` instead.\"\n            )\n            if value is not inspect.Signature.empty:\n                assert not is_path_param, \"Path parameters cannot have default values\"\n                field_info.default = value\n            else:\n                field_info.default = Required\n        elif isinstance(fastapi_annotation, params.Depends):\n            depends = fastapi_annotation\n    elif annotation is not inspect.Signature.empty:\n        type_annotation = annotation\n\n    if isinstance(value, params.Depends):\n        assert depends is None, (\n            \"Cannot specify `Depends` in `Annotated` and default value\"\n            f\" together for {param_name!r}\"\n        )\n        assert field_info is None, (\n            \"Cannot specify a FastAPI annotation in `Annotated` and `Depends` as a\"\n            f\" default value together for {param_name!r}\"\n        )\n        depends = value\n    elif isinstance(value, FieldInfo):\n        assert field_info is None, (\n            \"Cannot specify FastAPI annotations in `Annotated` and default value\"\n            f\" together for {param_name!r}\"\n        )\n        field_info = value\n        if PYDANTIC_V2:\n            field_info.annotation = type_annotation\n\n    if depends is not None and depends.dependency is None:\n        depends.dependency = type_annotation\n\n    if lenient_issubclass(\n        type_annotation,\n        (Request, WebSocket, HTTPConnection, Response, BackgroundTasks, SecurityScopes),\n    ):\n        assert depends is None, f\"Cannot specify `Depends` for type {type_annotation!r}\"\n        assert (\n            field_info is None\n        ), f\"Cannot specify FastAPI annotation for type {type_annotation!r}\"\n    elif field_info is None and depends is None:\n        default_value = value if value is not inspect.Signature.empty else Required\n        if is_path_param:\n            # We might check here that `default_value is Required`, but the fact is that the same\n            # parameter might sometimes be a path parameter and sometimes not. See\n            # `tests/test_infer_param_optionality.py` for an example.\n            field_info = params.Path(annotation=type_annotation)\n        elif is_uploadfile_or_nonable_uploadfile_annotation(\n            type_annotation\n        ) or is_uploadfile_sequence_annotation(type_annotation):\n            field_info = params.File(annotation=type_annotation, default=default_value)\n        elif not field_annotation_is_scalar(annotation=type_annotation):\n            field_info = params.Body(annotation=type_annotation, default=default_value)\n        else:\n            field_info = params.Query(annotation=type_annotation, default=default_value)\n\n    field = None\n    if field_info is not None:\n        if is_path_param:\n            assert isinstance(field_info, params.Path), (\n                f\"Cannot use `{field_info.__class__.__name__}` for path param\"\n                f\" {param_name!r}\"\n            )\n        elif (\n            isinstance(field_info, params.Param)\n            and getattr(field_info, \"in_\", None) is None\n        ):\n            field_info.in_ = params.ParamTypes.query\n        use_annotation = get_annotation_from_field_info(\n            type_annotation,\n            field_info,\n            param_name,\n        )\n        if not field_info.alias and getattr(field_info, \"convert_underscores\", None):\n            alias = param_name.replace(\"_\", \"-\")\n        else:\n            alias = field_info.alias or param_name\n        field_info.alias = alias\n        field = create_response_field(\n            name=param_name,\n            type_=use_annotation,\n            default=field_info.default,\n            alias=alias,\n            required=field_info.default in (Required, Undefined),\n            field_info=field_info,\n        )\n\n    return type_annotation, depends, field\n\n\ndef is_body_param(*, param_field: ModelField, is_path_param: bool) -> bool:\n    if is_path_param:\n        assert is_scalar_field(\n            field=param_field\n        ), \"Path params must be of one of the supported types\"\n        return False\n    elif is_scalar_field(field=param_field):\n        return False\n    elif isinstance(\n        param_field.field_info, (params.Query, params.Header)\n    ) and is_scalar_sequence_field(param_field):\n        return False\n    else:\n        assert isinstance(\n            param_field.field_info, params.Body\n        ), f\"Param: {param_field.name} can only be a request body, using Body()\"\n        return True\n\n\ndef add_param_to_fields(*, field: ModelField, dependant: Dependant) -> None:\n    field_info = cast(params.Param, field.field_info)\n    if field_info.in_ == params.ParamTypes.path:\n        dependant.path_params.append(field)\n    elif field_info.in_ == params.ParamTypes.query:\n        dependant.query_params.append(field)\n    elif field_info.in_ == params.ParamTypes.header:\n        dependant.header_params.append(field)\n    else:\n        assert (\n            field_info.in_ == params.ParamTypes.cookie\n        ), f\"non-body parameters must be in path, query, header or cookie: {field.name}\"\n        dependant.cookie_params.append(field)\n\n\ndef is_coroutine_callable(call: Callable[..., Any]) -> bool:\n    if inspect.isroutine(call):\n        return inspect.iscoroutinefunction(call)\n    if inspect.isclass(call):\n        return False\n    dunder_call = getattr(call, \"__call__\", None)  # noqa: B004\n    return inspect.iscoroutinefunction(dunder_call)\n\n\ndef is_async_gen_callable(call: Callable[..., Any]) -> bool:\n    if inspect.isasyncgenfunction(call):\n        return True\n    dunder_call = getattr(call, \"__call__\", None)  # noqa: B004\n    return inspect.isasyncgenfunction(dunder_call)\n\n\ndef is_gen_callable(call: Callable[..., Any]) -> bool:\n    if inspect.isgeneratorfunction(call):\n        return True\n    dunder_call = getattr(call, \"__call__\", None)  # noqa: B004\n    return inspect.isgeneratorfunction(dunder_call)\n\n\nasync def solve_generator(\n    *, call: Callable[..., Any], stack: AsyncExitStack, sub_values: Dict[str, Any]\n) -> Any:\n    if is_gen_callable(call):\n        cm = contextmanager_in_threadpool(contextmanager(call)(**sub_values))\n    elif is_async_gen_callable(call):\n        cm = asynccontextmanager(call)(**sub_values)\n    return await stack.enter_async_context(cm)\n\n\nasync def solve_dependencies(\n    *,\n    request: Union[Request, WebSocket],\n    dependant: Dependant,\n    body: Optional[Union[Dict[str, Any], FormData]] = None,\n    background_tasks: Optional[BackgroundTasks] = None,\n    response: Optional[Response] = None,\n    dependency_overrides_provider: Optional[Any] = None,\n    dependency_cache: Optional[Dict[Tuple[Callable[..., Any], Tuple[str]], Any]] = None,\n) -> Tuple[\n    Dict[str, Any],\n    List[Any],\n    Optional[BackgroundTasks],\n    Response,\n    Dict[Tuple[Callable[..., Any], Tuple[str]], Any],\n]:\n    values: Dict[str, Any] = {}\n    errors: List[Any] = []\n    if response is None:\n        response = Response()\n        del response.headers[\"content-length\"]\n        response.status_code = None  # type: ignore\n    dependency_cache = dependency_cache or {}\n    sub_dependant: Dependant\n    for sub_dependant in dependant.dependencies:\n        sub_dependant.call = cast(Callable[..., Any], sub_dependant.call)\n        sub_dependant.cache_key = cast(\n            Tuple[Callable[..., Any], Tuple[str]], sub_dependant.cache_key\n        )\n        call = sub_dependant.call\n        use_sub_dependant = sub_dependant\n        if (\n            dependency_overrides_provider\n            and dependency_overrides_provider.dependency_overrides\n        ):\n            original_call = sub_dependant.call\n            call = getattr(\n                dependency_overrides_provider, \"dependency_overrides\", {}\n            ).get(original_call, original_call)\n            use_path: str = sub_dependant.path  # type: ignore\n            use_sub_dependant = get_dependant(\n                path=use_path,\n                call=call,\n                name=sub_dependant.name,\n                security_scopes=sub_dependant.security_scopes,\n            )\n\n        solved_result = await solve_dependencies(\n            request=request,\n            dependant=use_sub_dependant,\n            body=body,\n            background_tasks=background_tasks,\n            response=response,\n            dependency_overrides_provider=dependency_overrides_provider,\n            dependency_cache=dependency_cache,\n        )\n        (\n            sub_values,\n            sub_errors,\n            background_tasks,\n            _,  # the subdependency returns the same response we have\n            sub_dependency_cache,\n        ) = solved_result\n        dependency_cache.update(sub_dependency_cache)\n        if sub_errors:\n            errors.extend(sub_errors)\n            continue\n        if sub_dependant.use_cache and sub_dependant.cache_key in dependency_cache:\n            solved = dependency_cache[sub_dependant.cache_key]\n        elif is_gen_callable(call) or is_async_gen_callable(call):\n            stack = request.scope.get(\"fastapi_astack\")\n            assert isinstance(stack, AsyncExitStack)\n            solved = await solve_generator(\n                call=call, stack=stack, sub_values=sub_values\n            )\n        elif is_coroutine_callable(call):\n            solved = await call(**sub_values)\n        else:\n            solved = await run_in_threadpool(call, **sub_values)\n        if sub_dependant.name is not None:\n            values[sub_dependant.name] = solved\n        if sub_dependant.cache_key not in dependency_cache:\n            dependency_cache[sub_dependant.cache_key] = solved\n    path_values, path_errors = request_params_to_args(\n        dependant.path_params, request.path_params\n    )\n    query_values, query_errors = request_params_to_args(\n        dependant.query_params, request.query_params\n    )\n    header_values, header_errors = request_params_to_args(\n        dependant.header_params, request.headers\n    )\n    cookie_values, cookie_errors = request_params_to_args(\n        dependant.cookie_params, request.cookies\n    )\n    values.update(path_values)\n    values.update(query_values)\n    values.update(header_values)\n    values.update(cookie_values)\n    errors += path_errors + query_errors + header_errors + cookie_errors\n    if dependant.body_params:\n        (\n            body_values,\n            body_errors,\n        ) = await request_body_to_args(  # body_params checked above\n            required_params=dependant.body_params, received_body=body\n        )\n        values.update(body_values)\n        errors.extend(body_errors)\n    if dependant.http_connection_param_name:\n        values[dependant.http_connection_param_name] = request\n    if dependant.request_param_name and isinstance(request, Request):\n        values[dependant.request_param_name] = request\n    elif dependant.websocket_param_name and isinstance(request, WebSocket):\n        values[dependant.websocket_param_name] = request\n    if dependant.background_tasks_param_name:\n        if background_tasks is None:\n            background_tasks = BackgroundTasks()\n        values[dependant.background_tasks_param_name] = background_tasks\n    if dependant.response_param_name:\n        values[dependant.response_param_name] = response\n    if dependant.security_scopes_param_name:\n        values[dependant.security_scopes_param_name] = SecurityScopes(\n            scopes=dependant.security_scopes\n        )\n    return values, errors, background_tasks, response, dependency_cache\n\n\ndef request_params_to_args(\n    required_params: Sequence[ModelField],\n    received_params: Union[Mapping[str, Any], QueryParams, Headers],\n) -> Tuple[Dict[str, Any], List[Any]]:\n    values = {}\n    errors = []\n    for field in required_params:\n        if is_scalar_sequence_field(field) and isinstance(\n            received_params, (QueryParams, Headers)\n        ):\n            value = received_params.getlist(field.alias) or field.default\n        else:\n            value = received_params.get(field.alias)\n        field_info = field.field_info\n        assert isinstance(\n            field_info, params.Param\n        ), \"Params must be subclasses of Param\"\n        loc = (field_info.in_.value, field.alias)\n        if value is None:\n            if field.required:\n                errors.append(get_missing_field_error(loc=loc))\n            else:\n                values[field.name] = deepcopy(field.default)\n            continue\n        v_, errors_ = field.validate(value, values, loc=loc)\n        if isinstance(errors_, ErrorWrapper):\n            errors.append(errors_)\n        elif isinstance(errors_, list):\n            new_errors = _regenerate_error_with_loc(errors=errors_, loc_prefix=())\n            errors.extend(new_errors)\n        else:\n            values[field.name] = v_\n    return values, errors\n\n\nasync def request_body_to_args(\n    required_params: List[ModelField],\n    received_body: Optional[Union[Dict[str, Any], FormData]],\n) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:\n    values = {}\n    errors: List[Dict[str, Any]] = []\n    if required_params:\n        field = required_params[0]\n        field_info = field.field_info\n        embed = getattr(field_info, \"embed\", None)\n        field_alias_omitted = len(required_params) == 1 and not embed\n        if field_alias_omitted:\n            received_body = {field.alias: received_body}\n\n        for field in required_params:\n            loc: Tuple[str, ...]\n            if field_alias_omitted:\n                loc = (\"body\",)\n            else:\n                loc = (\"body\", field.alias)\n\n            value: Optional[Any] = None\n            if received_body is not None:\n                if (is_sequence_field(field)) and isinstance(received_body, FormData):\n                    value = received_body.getlist(field.alias)\n                else:\n                    try:\n                        value = received_body.get(field.alias)\n                    except AttributeError:\n                        errors.append(get_missing_field_error(loc))\n                        continue\n            if (\n                value is None\n                or (isinstance(field_info, params.Form) and value == \"\")\n                or (\n                    isinstance(field_info, params.Form)\n                    and is_sequence_field(field)\n                    and len(value) == 0\n                )\n            ):\n                if field.required:\n                    errors.append(get_missing_field_error(loc))\n                else:\n                    values[field.name] = deepcopy(field.default)\n                continue\n            if (\n                isinstance(field_info, params.File)\n                and is_bytes_field(field)\n                and isinstance(value, UploadFile)\n            ):\n                value = await value.read()\n            elif (\n                is_bytes_sequence_field(field)\n                and isinstance(field_info, params.File)\n                and value_is_sequence(value)\n            ):\n                # For types\n                assert isinstance(value, sequence_types)  # type: ignore[arg-type]\n                results: List[Union[bytes, str]] = []\n\n                async def process_fn(\n                    fn: Callable[[], Coroutine[Any, Any, Any]]\n                ) -> None:\n                    result = await fn()\n                    results.append(result)  # noqa: B023\n\n                async with anyio.create_task_group() as tg:\n                    for sub_value in value:\n                        tg.start_soon(process_fn, sub_value.read)\n                value = serialize_sequence_value(field=field, value=results)\n\n            v_, errors_ = field.validate(value, values, loc=loc)\n\n            if isinstance(errors_, list):\n                errors.extend(errors_)\n            elif errors_:\n                errors.append(errors_)\n            else:\n                values[field.name] = v_\n    return values, errors\n\n\ndef get_body_field(*, dependant: Dependant, name: str) -> Optional[ModelField]:\n    flat_dependant = get_flat_dependant(dependant)\n    if not flat_dependant.body_params:\n        return None\n    first_param = flat_dependant.body_params[0]\n    field_info = first_param.field_info\n    embed = getattr(field_info, \"embed\", None)\n    body_param_names_set = {param.name for param in flat_dependant.body_params}\n    if len(body_param_names_set) == 1 and not embed:\n        check_file_field(first_param)\n        return first_param\n    # If one field requires to embed, all have to be embedded\n    # in case a sub-dependency is evaluated with a single unique body field\n    # That is combined (embedded) with other body fields\n    for param in flat_dependant.body_params:\n        setattr(param.field_info, \"embed\", True)  # noqa: B010\n    model_name = \"Body_\" + name\n    BodyModel = create_body_model(\n        fields=flat_dependant.body_params, model_name=model_name\n    )\n    required = any(True for f in flat_dependant.body_params if f.required)\n    BodyFieldInfo_kwargs: Dict[str, Any] = {\n        \"annotation\": BodyModel,\n        \"alias\": \"body\",\n    }\n    if not required:\n        BodyFieldInfo_kwargs[\"default\"] = None\n    if any(isinstance(f.field_info, params.File) for f in flat_dependant.body_params):\n        BodyFieldInfo: Type[params.Body] = params.File\n    elif any(isinstance(f.field_info, params.Form) for f in flat_dependant.body_params):\n        BodyFieldInfo = params.Form\n    else:\n        BodyFieldInfo = params.Body\n\n        body_param_media_types = [\n            f.field_info.media_type\n            for f in flat_dependant.body_params\n            if isinstance(f.field_info, params.Body)\n        ]\n        if len(set(body_param_media_types)) == 1:\n            BodyFieldInfo_kwargs[\"media_type\"] = body_param_media_types[0]\n    final_field = create_response_field(\n        name=\"body\",\n        type_=BodyModel,\n        required=required,\n        alias=\"body\",\n        field_info=BodyFieldInfo(**BodyFieldInfo_kwargs),\n    )\n    check_file_field(final_field)\n    return final_field\n", 802], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py": ["\"\"\"Parse (absolute and relative) URLs.\n\nurlparse module is based upon the following RFC specifications.\n\nRFC 3986 (STD66): \"Uniform Resource Identifiers\" by T. Berners-Lee, R. Fielding\nand L.  Masinter, January 2005.\n\nRFC 2732 : \"Format for Literal IPv6 Addresses in URL's by R.Hinden, B.Carpenter\nand L.Masinter, December 1999.\n\nRFC 2396:  \"Uniform Resource Identifiers (URI)\": Generic Syntax by T.\nBerners-Lee, R. Fielding, and L. Masinter, August 1998.\n\nRFC 2368: \"The mailto URL scheme\", by P.Hoffman , L Masinter, J. Zawinski, July 1998.\n\nRFC 1808: \"Relative Uniform Resource Locators\", by R. Fielding, UC Irvine, June\n1995.\n\nRFC 1738: \"Uniform Resource Locators (URL)\" by T. Berners-Lee, L. Masinter, M.\nMcCahill, December 1994\n\nRFC 3986 is considered the current standard and any future changes to\nurlparse module should conform with it.  The urlparse module is\ncurrently not entirely compliant with this RFC due to defacto\nscenarios for parsing, and for backward compatibility purposes, some\nparsing quirks from older RFCs are retained. The testcases in\ntest_urlparse.py provides a good indicator of parsing behavior.\n\nThe WHATWG URL Parser spec should also be considered.  We are not compliant with\nit either due to existing user code API behavior expectations (Hyrum's Law).\nIt serves as a useful guide when making changes.\n\"\"\"\n\nimport re\nimport sys\nimport types\nimport collections\nimport warnings\n\n__all__ = [\"urlparse\", \"urlunparse\", \"urljoin\", \"urldefrag\",\n           \"urlsplit\", \"urlunsplit\", \"urlencode\", \"parse_qs\",\n           \"parse_qsl\", \"quote\", \"quote_plus\", \"quote_from_bytes\",\n           \"unquote\", \"unquote_plus\", \"unquote_to_bytes\",\n           \"DefragResult\", \"ParseResult\", \"SplitResult\",\n           \"DefragResultBytes\", \"ParseResultBytes\", \"SplitResultBytes\"]\n\n# A classification of schemes.\n# The empty string classifies URLs with no scheme specified,\n# being the default value returned by \u201curlsplit\u201d and \u201curlparse\u201d.\n\nuses_relative = ['', 'ftp', 'http', 'gopher', 'nntp', 'imap',\n                 'wais', 'file', 'https', 'shttp', 'mms',\n                 'prospero', 'rtsp', 'rtspu', 'sftp',\n                 'svn', 'svn+ssh', 'ws', 'wss']\n\nuses_netloc = ['', 'ftp', 'http', 'gopher', 'nntp', 'telnet',\n               'imap', 'wais', 'file', 'mms', 'https', 'shttp',\n               'snews', 'prospero', 'rtsp', 'rtspu', 'rsync',\n               'svn', 'svn+ssh', 'sftp', 'nfs', 'git', 'git+ssh',\n               'ws', 'wss']\n\nuses_params = ['', 'ftp', 'hdl', 'prospero', 'http', 'imap',\n               'https', 'shttp', 'rtsp', 'rtspu', 'sip', 'sips',\n               'mms', 'sftp', 'tel']\n\n# These are not actually used anymore, but should stay for backwards\n# compatibility.  (They are undocumented, but have a public-looking name.)\n\nnon_hierarchical = ['gopher', 'hdl', 'mailto', 'news',\n                    'telnet', 'wais', 'imap', 'snews', 'sip', 'sips']\n\nuses_query = ['', 'http', 'wais', 'imap', 'https', 'shttp', 'mms',\n              'gopher', 'rtsp', 'rtspu', 'sip', 'sips']\n\nuses_fragment = ['', 'ftp', 'hdl', 'http', 'gopher', 'news',\n                 'nntp', 'wais', 'https', 'shttp', 'snews',\n                 'file', 'prospero']\n\n# Characters valid in scheme names\nscheme_chars = ('abcdefghijklmnopqrstuvwxyz'\n                'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                '0123456789'\n                '+-.')\n\n# Leading and trailing C0 control and space to be stripped per WHATWG spec.\n# == \"\".join([chr(i) for i in range(0, 0x20 + 1)])\n_WHATWG_C0_CONTROL_OR_SPACE = '\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\x0c\\r\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f '\n\n# Unsafe bytes to be removed per WHATWG spec\n_UNSAFE_URL_BYTES_TO_REMOVE = ['\\t', '\\r', '\\n']\n\n# XXX: Consider replacing with functools.lru_cache\nMAX_CACHE_SIZE = 20\n_parse_cache = {}\n\ndef clear_cache():\n    \"\"\"Clear the parse cache and the quoters cache.\"\"\"\n    _parse_cache.clear()\n    _safe_quoters.clear()\n\n\n# Helpers for bytes handling\n# For 3.2, we deliberately require applications that\n# handle improperly quoted URLs to do their own\n# decoding and encoding. If valid use cases are\n# presented, we may relax this by using latin-1\n# decoding internally for 3.3\n_implicit_encoding = 'ascii'\n_implicit_errors = 'strict'\n\ndef _noop(obj):\n    return obj\n\ndef _encode_result(obj, encoding=_implicit_encoding,\n                        errors=_implicit_errors):\n    return obj.encode(encoding, errors)\n\ndef _decode_args(args, encoding=_implicit_encoding,\n                       errors=_implicit_errors):\n    return tuple(x.decode(encoding, errors) if x else '' for x in args)\n\ndef _coerce_args(*args):\n    # Invokes decode if necessary to create str args\n    # and returns the coerced inputs along with\n    # an appropriate result coercion function\n    #   - noop for str inputs\n    #   - encoding function otherwise\n    str_input = isinstance(args[0], str)\n    for arg in args[1:]:\n        # We special-case the empty string to support the\n        # \"scheme=''\" default argument to some functions\n        if arg and isinstance(arg, str) != str_input:\n            raise TypeError(\"Cannot mix str and non-str arguments\")\n    if str_input:\n        return args + (_noop,)\n    return _decode_args(args) + (_encode_result,)\n\n# Result objects are more helpful than simple tuples\nclass _ResultMixinStr(object):\n    \"\"\"Standard approach to encoding parsed results from str to bytes\"\"\"\n    __slots__ = ()\n\n    def encode(self, encoding='ascii', errors='strict'):\n        return self._encoded_counterpart(*(x.encode(encoding, errors) for x in self))\n\n\nclass _ResultMixinBytes(object):\n    \"\"\"Standard approach to decoding parsed results from bytes to str\"\"\"\n    __slots__ = ()\n\n    def decode(self, encoding='ascii', errors='strict'):\n        return self._decoded_counterpart(*(x.decode(encoding, errors) for x in self))\n\n\nclass _NetlocResultMixinBase(object):\n    \"\"\"Shared methods for the parsed result objects containing a netloc element\"\"\"\n    __slots__ = ()\n\n    @property\n    def username(self):\n        return self._userinfo[0]\n\n    @property\n    def password(self):\n        return self._userinfo[1]\n\n    @property\n    def hostname(self):\n        hostname = self._hostinfo[0]\n        if not hostname:\n            return None\n        # Scoped IPv6 address may have zone info, which must not be lowercased\n        # like http://[fe80::822a:a8ff:fe49:470c%tESt]:1234/keys\n        separator = '%' if isinstance(hostname, str) else b'%'\n        hostname, percent, zone = hostname.partition(separator)\n        return hostname.lower() + percent + zone\n\n    @property\n    def port(self):\n        port = self._hostinfo[1]\n        if port is not None:\n            try:\n                port = int(port, 10)\n            except ValueError:\n                message = f'Port could not be cast to integer value as {port!r}'\n                raise ValueError(message) from None\n            if not ( 0 <= port <= 65535):\n                raise ValueError(\"Port out of range 0-65535\")\n        return port\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\n\nclass _NetlocResultMixinStr(_NetlocResultMixinBase, _ResultMixinStr):\n    __slots__ = ()\n\n    @property\n    def _userinfo(self):\n        netloc = self.netloc\n        userinfo, have_info, hostinfo = netloc.rpartition('@')\n        if have_info:\n            username, have_password, password = userinfo.partition(':')\n            if not have_password:\n                password = None\n        else:\n            username = password = None\n        return username, password\n\n    @property\n    def _hostinfo(self):\n        netloc = self.netloc\n        _, _, hostinfo = netloc.rpartition('@')\n        _, have_open_br, bracketed = hostinfo.partition('[')\n        if have_open_br:\n            hostname, _, port = bracketed.partition(']')\n            _, _, port = port.partition(':')\n        else:\n            hostname, _, port = hostinfo.partition(':')\n        if not port:\n            port = None\n        return hostname, port\n\n\nclass _NetlocResultMixinBytes(_NetlocResultMixinBase, _ResultMixinBytes):\n    __slots__ = ()\n\n    @property\n    def _userinfo(self):\n        netloc = self.netloc\n        userinfo, have_info, hostinfo = netloc.rpartition(b'@')\n        if have_info:\n            username, have_password, password = userinfo.partition(b':')\n            if not have_password:\n                password = None\n        else:\n            username = password = None\n        return username, password\n\n    @property\n    def _hostinfo(self):\n        netloc = self.netloc\n        _, _, hostinfo = netloc.rpartition(b'@')\n        _, have_open_br, bracketed = hostinfo.partition(b'[')\n        if have_open_br:\n            hostname, _, port = bracketed.partition(b']')\n            _, _, port = port.partition(b':')\n        else:\n            hostname, _, port = hostinfo.partition(b':')\n        if not port:\n            port = None\n        return hostname, port\n\n\nfrom collections import namedtuple\n\n_DefragResultBase = namedtuple('DefragResult', 'url fragment')\n_SplitResultBase = namedtuple(\n    'SplitResult', 'scheme netloc path query fragment')\n_ParseResultBase = namedtuple(\n    'ParseResult', 'scheme netloc path params query fragment')\n\n_DefragResultBase.__doc__ = \"\"\"\nDefragResult(url, fragment)\n\nA 2-tuple that contains the url without fragment identifier and the fragment\nidentifier as a separate argument.\n\"\"\"\n\n_DefragResultBase.url.__doc__ = \"\"\"The URL with no fragment identifier.\"\"\"\n\n_DefragResultBase.fragment.__doc__ = \"\"\"\nFragment identifier separated from URL, that allows indirect identification of a\nsecondary resource by reference to a primary resource and additional identifying\ninformation.\n\"\"\"\n\n_SplitResultBase.__doc__ = \"\"\"\nSplitResult(scheme, netloc, path, query, fragment)\n\nA 5-tuple that contains the different components of a URL. Similar to\nParseResult, but does not split params.\n\"\"\"\n\n_SplitResultBase.scheme.__doc__ = \"\"\"Specifies URL scheme for the request.\"\"\"\n\n_SplitResultBase.netloc.__doc__ = \"\"\"\nNetwork location where the request is made to.\n\"\"\"\n\n_SplitResultBase.path.__doc__ = \"\"\"\nThe hierarchical path, such as the path to a file to download.\n\"\"\"\n\n_SplitResultBase.query.__doc__ = \"\"\"\nThe query component, that contains non-hierarchical data, that along with data\nin path component, identifies a resource in the scope of URI's scheme and\nnetwork location.\n\"\"\"\n\n_SplitResultBase.fragment.__doc__ = \"\"\"\nFragment identifier, that allows indirect identification of a secondary resource\nby reference to a primary resource and additional identifying information.\n\"\"\"\n\n_ParseResultBase.__doc__ = \"\"\"\nParseResult(scheme, netloc, path, params, query, fragment)\n\nA 6-tuple that contains components of a parsed URL.\n\"\"\"\n\n_ParseResultBase.scheme.__doc__ = _SplitResultBase.scheme.__doc__\n_ParseResultBase.netloc.__doc__ = _SplitResultBase.netloc.__doc__\n_ParseResultBase.path.__doc__ = _SplitResultBase.path.__doc__\n_ParseResultBase.params.__doc__ = \"\"\"\nParameters for last path element used to dereference the URI in order to provide\naccess to perform some operation on the resource.\n\"\"\"\n\n_ParseResultBase.query.__doc__ = _SplitResultBase.query.__doc__\n_ParseResultBase.fragment.__doc__ = _SplitResultBase.fragment.__doc__\n\n\n# For backwards compatibility, alias _NetlocResultMixinStr\n# ResultBase is no longer part of the documented API, but it is\n# retained since deprecating it isn't worth the hassle\nResultBase = _NetlocResultMixinStr\n\n# Structured result objects for string data\nclass DefragResult(_DefragResultBase, _ResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        if self.fragment:\n            return self.url + '#' + self.fragment\n        else:\n            return self.url\n\nclass SplitResult(_SplitResultBase, _NetlocResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        return urlunsplit(self)\n\nclass ParseResult(_ParseResultBase, _NetlocResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        return urlunparse(self)\n\n# Structured result objects for bytes data\nclass DefragResultBytes(_DefragResultBase, _ResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        if self.fragment:\n            return self.url + b'#' + self.fragment\n        else:\n            return self.url\n\nclass SplitResultBytes(_SplitResultBase, _NetlocResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        return urlunsplit(self)\n\nclass ParseResultBytes(_ParseResultBase, _NetlocResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        return urlunparse(self)\n\n# Set up the encode/decode result pairs\ndef _fix_result_transcoding():\n    _result_pairs = (\n        (DefragResult, DefragResultBytes),\n        (SplitResult, SplitResultBytes),\n        (ParseResult, ParseResultBytes),\n    )\n    for _decoded, _encoded in _result_pairs:\n        _decoded._encoded_counterpart = _encoded\n        _encoded._decoded_counterpart = _decoded\n\n_fix_result_transcoding()\ndel _fix_result_transcoding\n\ndef urlparse(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 6 components:\n    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n\n    The result is a named 6-tuple with fields corresponding to the\n    above. It is either a ParseResult or ParseResultBytes object,\n    depending on the type of the url parameter.\n\n    The username, password, hostname, and port sub-components of netloc\n    can also be accessed as attributes of the returned object.\n\n    The scheme argument provides the default value of the scheme\n    component when no scheme is found in url.\n\n    If allow_fragments is False, no attempt is made to separate the\n    fragment component from the previous component, which can be either\n    path or query.\n\n    Note that % escapes are not expanded.\n    \"\"\"\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    splitresult = urlsplit(url, scheme, allow_fragments)\n    scheme, netloc, url, query, fragment = splitresult\n    if scheme in uses_params and ';' in url:\n        url, params = _splitparams(url)\n    else:\n        params = ''\n    result = ParseResult(scheme, netloc, url, params, query, fragment)\n    return _coerce_result(result)\n\ndef _splitparams(url):\n    if '/'  in url:\n        i = url.find(';', url.rfind('/'))\n        if i < 0:\n            return url, ''\n    else:\n        i = url.find(';')\n    return url[:i], url[i+1:]\n\ndef _splitnetloc(url, start=0):\n    delim = len(url)   # position of end of domain part of url, default is end\n    for c in '/?#':    # look for delimiters; the order is NOT important\n        wdelim = url.find(c, start)        # find first of this delim\n        if wdelim >= 0:                    # if found\n            delim = min(delim, wdelim)     # use earliest delim position\n    return url[start:delim], url[delim:]   # return (domain, rest)\n\ndef _checknetloc(netloc):\n    if not netloc or netloc.isascii():\n        return\n    # looking for characters like \\u2100 that expand to 'a/c'\n    # IDNA uses NFKC equivalence, so normalize for this check\n    import unicodedata\n    n = netloc.replace('@', '')   # ignore characters already included\n    n = n.replace(':', '')        # but not the surrounding text\n    n = n.replace('#', '')\n    n = n.replace('?', '')\n    netloc2 = unicodedata.normalize('NFKC', n)\n    if n == netloc2:\n        return\n    for c in '/?#@:':\n        if c in netloc2:\n            raise ValueError(\"netloc '\" + netloc + \"' contains invalid \" +\n                             \"characters under NFKC normalization\")\n\ndef urlsplit(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 5 components:\n    <scheme>://<netloc>/<path>?<query>#<fragment>\n\n    The result is a named 5-tuple with fields corresponding to the\n    above. It is either a SplitResult or SplitResultBytes object,\n    depending on the type of the url parameter.\n\n    The username, password, hostname, and port sub-components of netloc\n    can also be accessed as attributes of the returned object.\n\n    The scheme argument provides the default value of the scheme\n    component when no scheme is found in url.\n\n    If allow_fragments is False, no attempt is made to separate the\n    fragment component from the previous component, which can be either\n    path or query.\n\n    Note that % escapes are not expanded.\n    \"\"\"\n\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    # Only lstrip url as some applications rely on preserving trailing space.\n    # (https://url.spec.whatwg.org/#concept-basic-url-parser would strip both)\n    url = url.lstrip(_WHATWG_C0_CONTROL_OR_SPACE)\n    scheme = scheme.strip(_WHATWG_C0_CONTROL_OR_SPACE)\n\n    for b in _UNSAFE_URL_BYTES_TO_REMOVE:\n        url = url.replace(b, \"\")\n        scheme = scheme.replace(b, \"\")\n\n    allow_fragments = bool(allow_fragments)\n    key = url, scheme, allow_fragments, type(url), type(scheme)\n    cached = _parse_cache.get(key, None)\n    if cached:\n        return _coerce_result(cached)\n    if len(_parse_cache) >= MAX_CACHE_SIZE: # avoid runaway growth\n        clear_cache()\n    netloc = query = fragment = ''\n    i = url.find(':')\n    if i > 0:\n        for c in url[:i]:\n            if c not in scheme_chars:\n                break\n        else:\n            scheme, url = url[:i].lower(), url[i+1:]\n\n    if url[:2] == '//':\n        netloc, url = _splitnetloc(url, 2)\n        if (('[' in netloc and ']' not in netloc) or\n                (']' in netloc and '[' not in netloc)):\n            raise ValueError(\"Invalid IPv6 URL\")\n    if allow_fragments and '#' in url:\n        url, fragment = url.split('#', 1)\n    if '?' in url:\n        url, query = url.split('?', 1)\n    _checknetloc(netloc)\n    v = SplitResult(scheme, netloc, url, query, fragment)\n    _parse_cache[key] = v\n    return _coerce_result(v)\n\ndef urlunparse(components):\n    \"\"\"Put a parsed URL back together again.  This may result in a\n    slightly different, but equivalent URL, if the URL that was parsed\n    originally had redundant delimiters, e.g. a ? with an empty query\n    (the draft states that these are equivalent).\"\"\"\n    scheme, netloc, url, params, query, fragment, _coerce_result = (\n                                                  _coerce_args(*components))\n    if params:\n        url = \"%s;%s\" % (url, params)\n    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))\n\ndef urlunsplit(components):\n    \"\"\"Combine the elements of a tuple as returned by urlsplit() into a\n    complete URL as a string. The data argument can be any five-item iterable.\n    This may result in a slightly different, but equivalent URL, if the URL that\n    was parsed originally had unnecessary delimiters (for example, a ? with an\n    empty query; the RFC states that these are equivalent).\"\"\"\n    scheme, netloc, url, query, fragment, _coerce_result = (\n                                          _coerce_args(*components))\n    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):\n        if url and url[:1] != '/': url = '/' + url\n        url = '//' + (netloc or '') + url\n    if scheme:\n        url = scheme + ':' + url\n    if query:\n        url = url + '?' + query\n    if fragment:\n        url = url + '#' + fragment\n    return _coerce_result(url)\n\ndef urljoin(base, url, allow_fragments=True):\n    \"\"\"Join a base URL and a possibly relative URL to form an absolute\n    interpretation of the latter.\"\"\"\n    if not base:\n        return url\n    if not url:\n        return base\n\n    base, url, _coerce_result = _coerce_args(base, url)\n    bscheme, bnetloc, bpath, bparams, bquery, bfragment = \\\n            urlparse(base, '', allow_fragments)\n    scheme, netloc, path, params, query, fragment = \\\n            urlparse(url, bscheme, allow_fragments)\n\n    if scheme != bscheme or scheme not in uses_relative:\n        return _coerce_result(url)\n    if scheme in uses_netloc:\n        if netloc:\n            return _coerce_result(urlunparse((scheme, netloc, path,\n                                              params, query, fragment)))\n        netloc = bnetloc\n\n    if not path and not params:\n        path = bpath\n        params = bparams\n        if not query:\n            query = bquery\n        return _coerce_result(urlunparse((scheme, netloc, path,\n                                          params, query, fragment)))\n\n    base_parts = bpath.split('/')\n    if base_parts[-1] != '':\n        # the last item is not a directory, so will not be taken into account\n        # in resolving the relative path\n        del base_parts[-1]\n\n    # for rfc3986, ignore all base path should the first character be root.\n    if path[:1] == '/':\n        segments = path.split('/')\n    else:\n        segments = base_parts + path.split('/')\n        # filter out elements that would cause redundant slashes on re-joining\n        # the resolved_path\n        segments[1:-1] = filter(None, segments[1:-1])\n\n    resolved_path = []\n\n    for seg in segments:\n        if seg == '..':\n            try:\n                resolved_path.pop()\n            except IndexError:\n                # ignore any .. segments that would otherwise cause an IndexError\n                # when popped from resolved_path if resolving for rfc3986\n                pass\n        elif seg == '.':\n            continue\n        else:\n            resolved_path.append(seg)\n\n    if segments[-1] in ('.', '..'):\n        # do some post-processing here. if the last segment was a relative dir,\n        # then we need to append the trailing '/'\n        resolved_path.append('')\n\n    return _coerce_result(urlunparse((scheme, netloc, '/'.join(\n        resolved_path) or '/', params, query, fragment)))\n\n\ndef urldefrag(url):\n    \"\"\"Removes any existing fragment from URL.\n\n    Returns a tuple of the defragmented URL and the fragment.  If\n    the URL contained no fragments, the second element is the\n    empty string.\n    \"\"\"\n    url, _coerce_result = _coerce_args(url)\n    if '#' in url:\n        s, n, p, a, q, frag = urlparse(url)\n        defrag = urlunparse((s, n, p, a, q, ''))\n    else:\n        frag = ''\n        defrag = url\n    return _coerce_result(DefragResult(defrag, frag))\n\n_hexdig = '0123456789ABCDEFabcdef'\n_hextobyte = None\n\ndef unquote_to_bytes(string):\n    \"\"\"unquote_to_bytes('abc%20def') -> b'abc def'.\"\"\"\n    # Note: strings are encoded as UTF-8. This is only an issue if it contains\n    # unescaped non-ASCII characters, which URIs should not.\n    if not string:\n        # Is it a string-like object?\n        string.split\n        return b''\n    if isinstance(string, str):\n        string = string.encode('utf-8')\n    bits = string.split(b'%')\n    if len(bits) == 1:\n        return string\n    res = [bits[0]]\n    append = res.append\n    # Delay the initialization of the table to not waste memory\n    # if the function is never called\n    global _hextobyte\n    if _hextobyte is None:\n        _hextobyte = {(a + b).encode(): bytes.fromhex(a + b)\n                      for a in _hexdig for b in _hexdig}\n    for item in bits[1:]:\n        try:\n            append(_hextobyte[item[:2]])\n            append(item[2:])\n        except KeyError:\n            append(b'%')\n            append(item)\n    return b''.join(res)\n\n_asciire = re.compile('([\\x00-\\x7f]+)')\n\ndef unquote(string, encoding='utf-8', errors='replace'):\n    \"\"\"Replace %xx escapes by their single-character equivalent. The optional\n    encoding and errors parameters specify how to decode percent-encoded\n    sequences into Unicode characters, as accepted by the bytes.decode()\n    method.\n    By default, percent-encoded sequences are decoded with UTF-8, and invalid\n    sequences are replaced by a placeholder character.\n\n    unquote('abc%20def') -> 'abc def'.\n    \"\"\"\n    if isinstance(string, bytes):\n        return unquote_to_bytes(string).decode(encoding, errors)\n    if '%' not in string:\n        string.split\n        return string\n    if encoding is None:\n        encoding = 'utf-8'\n    if errors is None:\n        errors = 'replace'\n    bits = _asciire.split(string)\n    res = [bits[0]]\n    append = res.append\n    for i in range(1, len(bits), 2):\n        append(unquote_to_bytes(bits[i]).decode(encoding, errors))\n        append(bits[i + 1])\n    return ''.join(res)\n\n\ndef parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n             encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as\n            blank strings.  The default false value indicates that\n            blank values are to be ignored and treated as if they were\n            not included.\n\n        strict_parsing: flag indicating what to do with parsing errors.\n            If false (the default), errors are silently ignored.\n            If true, errors raise a ValueError exception.\n\n        encoding and errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n\n        max_num_fields: int. If set, then throws a ValueError if there\n            are more than n fields read by parse_qsl().\n\n        separator: str. The symbol to use for separating the query arguments.\n            Defaults to &.\n\n        Returns a dictionary.\n    \"\"\"\n    parsed_result = {}\n    pairs = parse_qsl(qs, keep_blank_values, strict_parsing,\n                      encoding=encoding, errors=errors,\n                      max_num_fields=max_num_fields, separator=separator)\n    for name, value in pairs:\n        if name in parsed_result:\n            parsed_result[name].append(value)\n        else:\n            parsed_result[name] = [value]\n    return parsed_result\n\n\ndef parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n              encoding='utf-8', errors='replace', max_num_fields=None, separator='&'):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception.\n\n        encoding and errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n\n        max_num_fields: int. If set, then throws a ValueError\n            if there are more than n fields read by parse_qsl().\n\n        separator: str. The symbol to use for separating the query arguments.\n            Defaults to &.\n\n        Returns a list, as G-d intended.\n    \"\"\"\n    qs, _coerce_result = _coerce_args(qs)\n    separator, _ = _coerce_args(separator)\n\n    if not separator or (not isinstance(separator, (str, bytes))):\n        raise ValueError(\"Separator must be of type string or bytes.\")\n\n    # If max_num_fields is defined then check that the number of fields\n    # is less than max_num_fields. This prevents a memory exhaustion DOS\n    # attack via post bodies with many fields.\n    if max_num_fields is not None:\n        num_fields = 1 + qs.count(separator)\n        if max_num_fields < num_fields:\n            raise ValueError('Max number of fields exceeded')\n\n    pairs = [s1 for s1 in qs.split(separator)]\n    r = []\n    for name_value in pairs:\n        if not name_value and not strict_parsing:\n            continue\n        nv = name_value.split('=', 1)\n        if len(nv) != 2:\n            if strict_parsing:\n                raise ValueError(\"bad query field: %r\" % (name_value,))\n            # Handle case of a control-name with no equal sign\n            if keep_blank_values:\n                nv.append('')\n            else:\n                continue\n        if len(nv[1]) or keep_blank_values:\n            name = nv[0].replace('+', ' ')\n            name = unquote(name, encoding=encoding, errors=errors)\n            name = _coerce_result(name)\n            value = nv[1].replace('+', ' ')\n            value = unquote(value, encoding=encoding, errors=errors)\n            value = _coerce_result(value)\n            r.append((name, value))\n    return r\n\ndef unquote_plus(string, encoding='utf-8', errors='replace'):\n    \"\"\"Like unquote(), but also replace plus signs by spaces, as required for\n    unquoting HTML form values.\n\n    unquote_plus('%7e/abc+def') -> '~/abc def'\n    \"\"\"\n    string = string.replace('+', ' ')\n    return unquote(string, encoding, errors)\n\n_ALWAYS_SAFE = frozenset(b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                         b'abcdefghijklmnopqrstuvwxyz'\n                         b'0123456789'\n                         b'_.-~')\n_ALWAYS_SAFE_BYTES = bytes(_ALWAYS_SAFE)\n_safe_quoters = {}\n\nclass Quoter(collections.defaultdict):\n    \"\"\"A mapping from bytes (in range(0,256)) to strings.\n\n    String values are percent-encoded byte values, unless the key < 128, and\n    in the \"safe\" set (either the specified safe set, or default set).\n    \"\"\"\n    # Keeps a cache internally, using defaultdict, for efficiency (lookups\n    # of cached keys don't call Python code at all).\n    def __init__(self, safe):\n        \"\"\"safe: bytes object.\"\"\"\n        self.safe = _ALWAYS_SAFE.union(safe)\n\n    def __repr__(self):\n        # Without this, will just display as a defaultdict\n        return \"<%s %r>\" % (self.__class__.__name__, dict(self))\n\n    def __missing__(self, b):\n        # Handle a cache miss. Store quoted string in cache and return.\n        res = chr(b) if b in self.safe else '%{:02X}'.format(b)\n        self[b] = res\n        return res\n\ndef quote(string, safe='/', encoding=None, errors=None):\n    \"\"\"quote('abc def') -> 'abc%20def'\n\n    Each part of a URL, e.g. the path info, the query, etc., has a\n    different set of reserved characters that must be quoted. The\n    quote function offers a cautious (not minimal) way to quote a\n    string for most of these parts.\n\n    RFC 3986 Uniform Resource Identifier (URI): Generic Syntax lists\n    the following (un)reserved characters.\n\n    unreserved    = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n    reserved      = gen-delims / sub-delims\n    gen-delims    = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\"\n    sub-delims    = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\"\n                  / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n\n    Each of the reserved characters is reserved in some component of a URL,\n    but not necessarily in all of them.\n\n    The quote function %-escapes all characters that are neither in the\n    unreserved chars (\"always safe\") nor the additional chars set via the\n    safe arg.\n\n    The default for the safe arg is '/'. The character is reserved, but in\n    typical usage the quote function is being called on a path where the\n    existing slash characters are to be preserved.\n\n    Python 3.7 updates from using RFC 2396 to RFC 3986 to quote URL strings.\n    Now, \"~\" is included in the set of unreserved characters.\n\n    string and safe may be either str or bytes objects. encoding and errors\n    must not be specified if string is a bytes object.\n\n    The optional encoding and errors parameters specify how to deal with\n    non-ASCII characters, as accepted by the str.encode method.\n    By default, encoding='utf-8' (characters are encoded with UTF-8), and\n    errors='strict' (unsupported characters raise a UnicodeEncodeError).\n    \"\"\"\n    if isinstance(string, str):\n        if not string:\n            return string\n        if encoding is None:\n            encoding = 'utf-8'\n        if errors is None:\n            errors = 'strict'\n        string = string.encode(encoding, errors)\n    else:\n        if encoding is not None:\n            raise TypeError(\"quote() doesn't support 'encoding' for bytes\")\n        if errors is not None:\n            raise TypeError(\"quote() doesn't support 'errors' for bytes\")\n    return quote_from_bytes(string, safe)\n\ndef quote_plus(string, safe='', encoding=None, errors=None):\n    \"\"\"Like quote(), but also replace ' ' with '+', as required for quoting\n    HTML form values. Plus signs in the original string are escaped unless\n    they are included in safe. It also does not have safe default to '/'.\n    \"\"\"\n    # Check if ' ' in string, where string may either be a str or bytes.  If\n    # there are no spaces, the regular quote will produce the right answer.\n    if ((isinstance(string, str) and ' ' not in string) or\n        (isinstance(string, bytes) and b' ' not in string)):\n        return quote(string, safe, encoding, errors)\n    if isinstance(safe, str):\n        space = ' '\n    else:\n        space = b' '\n    string = quote(string, safe + space, encoding, errors)\n    return string.replace(' ', '+')\n\ndef quote_from_bytes(bs, safe='/'):\n    \"\"\"Like quote(), but accepts a bytes object rather than a str, and does\n    not perform string-to-bytes encoding.  It always returns an ASCII string.\n    quote_from_bytes(b'abc def\\x3f') -> 'abc%20def%3f'\n    \"\"\"\n    if not isinstance(bs, (bytes, bytearray)):\n        raise TypeError(\"quote_from_bytes() expected bytes\")\n    if not bs:\n        return ''\n    if isinstance(safe, str):\n        # Normalize 'safe' by converting to bytes and removing non-ASCII chars\n        safe = safe.encode('ascii', 'ignore')\n    else:\n        safe = bytes([c for c in safe if c < 128])\n    if not bs.rstrip(_ALWAYS_SAFE_BYTES + safe):\n        return bs.decode()\n    try:\n        quoter = _safe_quoters[safe]\n    except KeyError:\n        _safe_quoters[safe] = quoter = Quoter(safe).__getitem__\n    return ''.join([quoter(char) for char in bs])\n\ndef urlencode(query, doseq=False, safe='', encoding=None, errors=None,\n              quote_via=quote_plus):\n    \"\"\"Encode a dict or sequence of two-element tuples into a URL query string.\n\n    If any values in the query arg are sequences and doseq is true, each\n    sequence element is converted to a separate parameter.\n\n    If the query arg is a sequence of two-element tuples, the order of the\n    parameters in the output will match the order of parameters in the\n    input.\n\n    The components of a query arg may each be either a string or a bytes type.\n\n    The safe, encoding, and errors parameters are passed down to the function\n    specified by quote_via (encoding and errors only if a component is a str).\n    \"\"\"\n\n    if hasattr(query, \"items\"):\n        query = query.items()\n    else:\n        # It's a bother at times that strings and string-like objects are\n        # sequences.\n        try:\n            # non-sequence items should not work with len()\n            # non-empty strings will fail this\n            if len(query) and not isinstance(query[0], tuple):\n                raise TypeError\n            # Zero-length sequences of all types will get here and succeed,\n            # but that's a minor nit.  Since the original implementation\n            # allowed empty dicts that type of behavior probably should be\n            # preserved for consistency\n        except TypeError:\n            ty, va, tb = sys.exc_info()\n            raise TypeError(\"not a valid non-string sequence \"\n                            \"or mapping object\").with_traceback(tb)\n\n    l = []\n    if not doseq:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_via(k, safe)\n            else:\n                k = quote_via(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_via(v, safe)\n            else:\n                v = quote_via(str(v), safe, encoding, errors)\n            l.append(k + '=' + v)\n    else:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_via(k, safe)\n            else:\n                k = quote_via(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_via(v, safe)\n                l.append(k + '=' + v)\n            elif isinstance(v, str):\n                v = quote_via(v, safe, encoding, errors)\n                l.append(k + '=' + v)\n            else:\n                try:\n                    # Is this a sufficient test for sequence-ness?\n                    x = len(v)\n                except TypeError:\n                    # not a sequence\n                    v = quote_via(str(v), safe, encoding, errors)\n                    l.append(k + '=' + v)\n                else:\n                    # loop over the sequence\n                    for elt in v:\n                        if isinstance(elt, bytes):\n                            elt = quote_via(elt, safe)\n                        else:\n                            elt = quote_via(str(elt), safe, encoding, errors)\n                        l.append(k + '=' + elt)\n    return '&'.join(l)\n\n\ndef to_bytes(url):\n    warnings.warn(\"urllib.parse.to_bytes() is deprecated as of 3.8\",\n                  DeprecationWarning, stacklevel=2)\n    return _to_bytes(url)\n\n\ndef _to_bytes(url):\n    \"\"\"to_bytes(u\"URL\") --> 'URL'.\"\"\"\n    # Most URL schemes require ASCII. If that changes, the conversion\n    # can be relaxed.\n    # XXX get rid of to_bytes()\n    if isinstance(url, str):\n        try:\n            url = url.encode(\"ASCII\").decode()\n        except UnicodeError:\n            raise UnicodeError(\"URL \" + repr(url) +\n                               \" contains non-ASCII characters\")\n    return url\n\n\ndef unwrap(url):\n    \"\"\"Transform a string like '<URL:scheme://host/path>' into 'scheme://host/path'.\n\n    The string is returned unchanged if it's not a wrapped URL.\n    \"\"\"\n    url = str(url).strip()\n    if url[:1] == '<' and url[-1:] == '>':\n        url = url[1:-1].strip()\n    if url[:4] == 'URL:':\n        url = url[4:].strip()\n    return url\n\n\ndef splittype(url):\n    warnings.warn(\"urllib.parse.splittype() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splittype(url)\n\n\n_typeprog = None\ndef _splittype(url):\n    \"\"\"splittype('type:opaquestring') --> 'type', 'opaquestring'.\"\"\"\n    global _typeprog\n    if _typeprog is None:\n        _typeprog = re.compile('([^/:]+):(.*)', re.DOTALL)\n\n    match = _typeprog.match(url)\n    if match:\n        scheme, data = match.groups()\n        return scheme.lower(), data\n    return None, url\n\n\ndef splithost(url):\n    warnings.warn(\"urllib.parse.splithost() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splithost(url)\n\n\n_hostprog = None\ndef _splithost(url):\n    \"\"\"splithost('//host[:port]/path') --> 'host[:port]', '/path'.\"\"\"\n    global _hostprog\n    if _hostprog is None:\n        _hostprog = re.compile('//([^/#?]*)(.*)', re.DOTALL)\n\n    match = _hostprog.match(url)\n    if match:\n        host_port, path = match.groups()\n        if path and path[0] != '/':\n            path = '/' + path\n        return host_port, path\n    return None, url\n\n\ndef splituser(host):\n    warnings.warn(\"urllib.parse.splituser() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splituser(host)\n\n\ndef _splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    user, delim, host = host.rpartition('@')\n    return (user if delim else None), host\n\n\ndef splitpasswd(user):\n    warnings.warn(\"urllib.parse.splitpasswd() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitpasswd(user)\n\n\ndef _splitpasswd(user):\n    \"\"\"splitpasswd('user:passwd') -> 'user', 'passwd'.\"\"\"\n    user, delim, passwd = user.partition(':')\n    return user, (passwd if delim else None)\n\n\ndef splitport(host):\n    warnings.warn(\"urllib.parse.splitport() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitport(host)\n\n\n# splittag('/path#tag') --> '/path', 'tag'\n_portprog = None\ndef _splitport(host):\n    \"\"\"splitport('host:port') --> 'host', 'port'.\"\"\"\n    global _portprog\n    if _portprog is None:\n        _portprog = re.compile('(.*):([0-9]*)', re.DOTALL)\n\n    match = _portprog.fullmatch(host)\n    if match:\n        host, port = match.groups()\n        if port:\n            return host, port\n    return host, None\n\n\ndef splitnport(host, defport=-1):\n    warnings.warn(\"urllib.parse.splitnport() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitnport(host, defport)\n\n\ndef _splitnport(host, defport=-1):\n    \"\"\"Split host and port, returning numeric port.\n    Return given default port if no ':' found; defaults to -1.\n    Return numerical port if a valid number are found after ':'.\n    Return None if ':' but not a valid number.\"\"\"\n    host, delim, port = host.rpartition(':')\n    if not delim:\n        host = port\n    elif port:\n        try:\n            nport = int(port)\n        except ValueError:\n            nport = None\n        return host, nport\n    return host, defport\n\n\ndef splitquery(url):\n    warnings.warn(\"urllib.parse.splitquery() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitquery(url)\n\n\ndef _splitquery(url):\n    \"\"\"splitquery('/path?query') --> '/path', 'query'.\"\"\"\n    path, delim, query = url.rpartition('?')\n    if delim:\n        return path, query\n    return url, None\n\n\ndef splittag(url):\n    warnings.warn(\"urllib.parse.splittag() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splittag(url)\n\n\ndef _splittag(url):\n    \"\"\"splittag('/path#tag') --> '/path', 'tag'.\"\"\"\n    path, delim, tag = url.rpartition('#')\n    if delim:\n        return path, tag\n    return url, None\n\n\ndef splitattr(url):\n    warnings.warn(\"urllib.parse.splitattr() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitattr(url)\n\n\ndef _splitattr(url):\n    \"\"\"splitattr('/path;attr1=value1;attr2=value2;...') ->\n        '/path', ['attr1=value1', 'attr2=value2', ...].\"\"\"\n    words = url.split(';')\n    return words[0], words[1:]\n\n\ndef splitvalue(attr):\n    warnings.warn(\"urllib.parse.splitvalue() is deprecated as of 3.8, \"\n                  \"use urllib.parse.parse_qsl() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitvalue(attr)\n\n\ndef _splitvalue(attr):\n    \"\"\"splitvalue('attr=value') --> 'attr', 'value'.\"\"\"\n    attr, delim, value = attr.partition('=')\n    return attr, (value if delim else None)\n", 1209], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py": ["\"\"\"\nThe typing module: Support for gradual typing as defined by PEP 484.\n\nAt large scale, the structure of the module is following:\n* Imports and exports, all public names should be explicitly added to __all__.\n* Internal helper functions: these should never be used in code outside this module.\n* _SpecialForm and its instances (special forms): Any, NoReturn, ClassVar, Union, Optional\n* Two classes whose instances can be type arguments in addition to types: ForwardRef and TypeVar\n* The core of internal generics API: _GenericAlias and _VariadicGenericAlias, the latter is\n  currently only used by Tuple and Callable. All subscripted types like X[int], Union[int, str],\n  etc., are instances of either of these classes.\n* The public counterpart of the generics API consists of two classes: Generic and Protocol.\n* Public helper functions: get_type_hints, overload, cast, no_type_check,\n  no_type_check_decorator.\n* Generic aliases for collections.abc ABCs and few additional protocols.\n* Special types: NewType, NamedTuple, TypedDict.\n* Wrapper submodules for re and io related types.\n\"\"\"\n\nfrom abc import abstractmethod, ABCMeta\nimport collections\nimport collections.abc\nimport contextlib\nimport functools\nimport operator\nimport re as stdlib_re  # Avoid confusion with the re we export.\nimport sys\nimport types\nfrom types import WrapperDescriptorType, MethodWrapperType, MethodDescriptorType, GenericAlias\n\n# Please keep __all__ alphabetized within each category.\n__all__ = [\n    # Super-special typing primitives.\n    'Annotated',\n    'Any',\n    'Callable',\n    'ClassVar',\n    'Final',\n    'ForwardRef',\n    'Generic',\n    'Literal',\n    'Optional',\n    'Protocol',\n    'Tuple',\n    'Type',\n    'TypeVar',\n    'Union',\n\n    # ABCs (from collections.abc).\n    'AbstractSet',  # collections.abc.Set.\n    'ByteString',\n    'Container',\n    'ContextManager',\n    'Hashable',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'Mapping',\n    'MappingView',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'Sequence',\n    'Sized',\n    'ValuesView',\n    'Awaitable',\n    'AsyncIterator',\n    'AsyncIterable',\n    'Coroutine',\n    'Collection',\n    'AsyncGenerator',\n    'AsyncContextManager',\n\n    # Structural checks, a.k.a. protocols.\n    'Reversible',\n    'SupportsAbs',\n    'SupportsBytes',\n    'SupportsComplex',\n    'SupportsFloat',\n    'SupportsIndex',\n    'SupportsInt',\n    'SupportsRound',\n\n    # Concrete collection types.\n    'ChainMap',\n    'Counter',\n    'Deque',\n    'Dict',\n    'DefaultDict',\n    'List',\n    'OrderedDict',\n    'Set',\n    'FrozenSet',\n    'NamedTuple',  # Not really a type.\n    'TypedDict',  # Not really a type.\n    'Generator',\n\n    # Other concrete types.\n    'BinaryIO',\n    'IO',\n    'Match',\n    'Pattern',\n    'TextIO',\n\n    # One-off things.\n    'AnyStr',\n    'cast',\n    'final',\n    'get_args',\n    'get_origin',\n    'get_type_hints',\n    'NewType',\n    'no_type_check',\n    'no_type_check_decorator',\n    'NoReturn',\n    'overload',\n    'runtime_checkable',\n    'Text',\n    'TYPE_CHECKING',\n]\n\n# The pseudo-submodules 're' and 'io' are part of the public\n# namespace, but excluded from __all__ because they might stomp on\n# legitimate imports of those modules.\n\n\ndef _type_convert(arg, module=None, *, allow_special_forms=False):\n    \"\"\"For converting None to type(None), and strings to ForwardRef.\"\"\"\n    if arg is None:\n        return type(None)\n    if isinstance(arg, str):\n        return ForwardRef(arg, module=module, is_class=allow_special_forms)\n    return arg\n\n\ndef _type_check(arg, msg, is_argument=True, module=None, *, allow_special_forms=False):\n    \"\"\"Check that the argument is a type, and return it (internal helper).\n\n    As a special case, accept None and return type(None) instead. Also wrap strings\n    into ForwardRef instances. Consider several corner cases, for example plain\n    special forms like Union are not valid, while Union[int, str] is OK, etc.\n    The msg argument is a human-readable error message, e.g::\n\n        \"Union[arg, ...]: arg should be a type.\"\n\n    We append the repr() of the actual value (truncated to 100 chars).\n    \"\"\"\n    invalid_generic_forms = (Generic, Protocol)\n    if not allow_special_forms:\n        invalid_generic_forms += (ClassVar,)\n        if is_argument:\n            invalid_generic_forms += (Final,)\n\n    arg = _type_convert(arg, module=module, allow_special_forms=allow_special_forms)\n    if (isinstance(arg, _GenericAlias) and\n            arg.__origin__ in invalid_generic_forms):\n        raise TypeError(f\"{arg} is not valid as type argument\")\n    if arg in (Any, NoReturn, Final):\n        return arg\n    if isinstance(arg, _SpecialForm) or arg in (Generic, Protocol):\n        raise TypeError(f\"Plain {arg} is not valid as type argument\")\n    if isinstance(arg, (type, TypeVar, ForwardRef)):\n        return arg\n    if not callable(arg):\n        raise TypeError(f\"{msg} Got {arg!r:.100}.\")\n    return arg\n\n\ndef _type_repr(obj):\n    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n\n    If obj is a type, we return a shorter version than the default\n    type.__repr__, based on the module and qualified name, which is\n    typically enough to uniquely identify a type.  For everything\n    else, we fall back on repr(obj).\n    \"\"\"\n    if isinstance(obj, types.GenericAlias):\n        return repr(obj)\n    if isinstance(obj, type):\n        if obj.__module__ == 'builtins':\n            return obj.__qualname__\n        return f'{obj.__module__}.{obj.__qualname__}'\n    if obj is ...:\n        return('...')\n    if isinstance(obj, types.FunctionType):\n        return obj.__name__\n    return repr(obj)\n\n\ndef _collect_type_vars(types):\n    \"\"\"Collect all type variable contained in types in order of\n    first appearance (lexicographic order). For example::\n\n        _collect_type_vars((T, List[S, T])) == (T, S)\n    \"\"\"\n    tvars = []\n    for t in types:\n        if isinstance(t, TypeVar) and t not in tvars:\n            tvars.append(t)\n        if isinstance(t, (_GenericAlias, GenericAlias)):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)\n\n\ndef _check_generic(cls, parameters, elen):\n    \"\"\"Check correct count for parameters of a generic cls (internal helper).\n    This gives a nice error message in case of count mismatch.\n    \"\"\"\n    if not elen:\n        raise TypeError(f\"{cls} is not a generic class\")\n    alen = len(parameters)\n    if alen != elen:\n        raise TypeError(f\"Too {'many' if alen > elen else 'few'} parameters for {cls};\"\n                        f\" actual {alen}, expected {elen}\")\n\n\ndef _deduplicate(params):\n    # Weed out strict duplicates, preserving the first of each occurrence.\n    all_params = set(params)\n    if len(all_params) < len(params):\n        new_params = []\n        for t in params:\n            if t in all_params:\n                new_params.append(t)\n                all_params.remove(t)\n        params = new_params\n        assert not all_params, all_params\n    return params\n\n\ndef _remove_dups_flatten(parameters):\n    \"\"\"An internal helper for Union creation and substitution: flatten Unions\n    among parameters, then remove duplicates.\n    \"\"\"\n    # Flatten out Union[Union[...], ...].\n    params = []\n    for p in parameters:\n        if isinstance(p, _UnionGenericAlias):\n            params.extend(p.__args__)\n        elif isinstance(p, tuple) and len(p) > 0 and p[0] is Union:\n            params.extend(p[1:])\n        else:\n            params.append(p)\n\n    return tuple(_deduplicate(params))\n\n\ndef _flatten_literal_params(parameters):\n    \"\"\"An internal helper for Literal creation: flatten Literals among parameters\"\"\"\n    params = []\n    for p in parameters:\n        if isinstance(p, _LiteralGenericAlias):\n            params.extend(p.__args__)\n        else:\n            params.append(p)\n    return tuple(params)\n\n\n_cleanups = []\n\n\ndef _tp_cache(func=None, /, *, typed=False):\n    \"\"\"Internal wrapper caching __getitem__ of generic types with a fallback to\n    original function for non-hashable arguments.\n    \"\"\"\n    def decorator(func):\n        cached = functools.lru_cache(typed=typed)(func)\n        _cleanups.append(cached.cache_clear)\n\n        @functools.wraps(func)\n        def inner(*args, **kwds):\n            try:\n                return cached(*args, **kwds)\n            except TypeError:\n                pass  # All real errors (not unhashable args) are raised below.\n            return func(*args, **kwds)\n        return inner\n\n    if func is not None:\n        return decorator(func)\n\n    return decorator\n\ndef _eval_type(t, globalns, localns, recursive_guard=frozenset()):\n    \"\"\"Evaluate all forward references in the given type t.\n    For use of globalns and localns see the docstring for get_type_hints().\n    recursive_guard is used to prevent infinite recursion with a recursive\n    ForwardRef.\n    \"\"\"\n    if isinstance(t, ForwardRef):\n        return t._evaluate(globalns, localns, recursive_guard)\n    if isinstance(t, (_GenericAlias, GenericAlias)):\n        ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n        if ev_args == t.__args__:\n            return t\n        if isinstance(t, GenericAlias):\n            return GenericAlias(t.__origin__, ev_args)\n        else:\n            return t.copy_with(ev_args)\n    return t\n\n\nclass _Final:\n    \"\"\"Mixin to prohibit subclassing\"\"\"\n\n    __slots__ = ('__weakref__',)\n\n    def __init_subclass__(self, /, *args, **kwds):\n        if '_root' not in kwds:\n            raise TypeError(\"Cannot subclass special typing classes\")\n\nclass _Immutable:\n    \"\"\"Mixin to indicate that object should not be copied.\"\"\"\n    __slots__ = ()\n\n    def __copy__(self):\n        return self\n\n    def __deepcopy__(self, memo):\n        return self\n\n\n# Internal indicator of special typing constructs.\n# See __doc__ instance attribute for specific docs.\nclass _SpecialForm(_Final, _root=True):\n    __slots__ = ('_name', '__doc__', '_getitem')\n\n    def __init__(self, getitem):\n        self._getitem = getitem\n        self._name = getitem.__name__\n        self.__doc__ = getitem.__doc__\n\n    def __mro_entries__(self, bases):\n        raise TypeError(f\"Cannot subclass {self!r}\")\n\n    def __repr__(self):\n        return 'typing.' + self._name\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f\"Cannot instantiate {self!r}\")\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f\"{self} cannot be used with isinstance()\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f\"{self} cannot be used with issubclass()\")\n\n    @_tp_cache\n    def __getitem__(self, parameters):\n        return self._getitem(self, parameters)\n\n\nclass _LiteralSpecialForm(_SpecialForm, _root=True):\n    def __getitem__(self, parameters):\n        if not isinstance(parameters, tuple):\n            parameters = (parameters,)\n        return self._getitem(self, *parameters)\n\n\n@_SpecialForm\ndef Any(self, parameters):\n    \"\"\"Special type indicating an unconstrained type.\n\n    - Any is compatible with every type.\n    - Any assumed to have all methods.\n    - All values assumed to be instances of Any.\n\n    Note that all the above statements are true from the point of view of\n    static type checkers. At runtime, Any should not be used with instance\n    or class checks.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n@_SpecialForm\ndef NoReturn(self, parameters):\n    \"\"\"Special type indicating functions that never return.\n    Example::\n\n      from typing import NoReturn\n\n      def stop() -> NoReturn:\n          raise Exception('no way')\n\n    This type is invalid in other positions, e.g., ``List[NoReturn]``\n    will fail in static type checkers.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n@_SpecialForm\ndef ClassVar(self, parameters):\n    \"\"\"Special type construct to mark class variables.\n\n    An annotation wrapped in ClassVar indicates that a given\n    attribute is intended to be used as a class variable and\n    should not be set on instances of that class. Usage::\n\n      class Starship:\n          stats: ClassVar[Dict[str, int]] = {} # class variable\n          damage: int = 10                     # instance variable\n\n    ClassVar accepts only types and cannot be further subscribed.\n\n    Note that ClassVar is not a class itself, and should not\n    be used with isinstance() or issubclass().\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n@_SpecialForm\ndef Final(self, parameters):\n    \"\"\"Special typing construct to indicate final names to type checkers.\n\n    A final name cannot be re-assigned or overridden in a subclass.\n    For example:\n\n      MAX_SIZE: Final = 9000\n      MAX_SIZE += 1  # Error reported by type checker\n\n      class Connection:\n          TIMEOUT: Final[int] = 10\n\n      class FastConnector(Connection):\n          TIMEOUT = 1  # Error reported by type checker\n\n    There is no runtime checking of these properties.\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n@_SpecialForm\ndef Union(self, parameters):\n    \"\"\"Union type; Union[X, Y] means either X or Y.\n\n    To define a union, use e.g. Union[int, str].  Details:\n    - The arguments must be types and there must be at least one.\n    - None as an argument is a special case and is replaced by\n      type(None).\n    - Unions of unions are flattened, e.g.::\n\n        Union[Union[int, str], float] == Union[int, str, float]\n\n    - Unions of a single argument vanish, e.g.::\n\n        Union[int] == int  # The constructor actually returns int\n\n    - Redundant arguments are skipped, e.g.::\n\n        Union[int, str, int] == Union[int, str]\n\n    - When comparing unions, the argument order is ignored, e.g.::\n\n        Union[int, str] == Union[str, int]\n\n    - You cannot subclass or instantiate a union.\n    - You can use Optional[X] as a shorthand for Union[X, None].\n    \"\"\"\n    if parameters == ():\n        raise TypeError(\"Cannot take a Union of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    msg = \"Union[arg, ...]: each arg must be a type.\"\n    parameters = tuple(_type_check(p, msg) for p in parameters)\n    parameters = _remove_dups_flatten(parameters)\n    if len(parameters) == 1:\n        return parameters[0]\n    return _UnionGenericAlias(self, parameters)\n\n@_SpecialForm\ndef Optional(self, parameters):\n    \"\"\"Optional type.\n\n    Optional[X] is equivalent to Union[X, None].\n    \"\"\"\n    arg = _type_check(parameters, f\"{self} requires a single type.\")\n    return Union[arg, type(None)]\n\n@_LiteralSpecialForm\n@_tp_cache(typed=True)\ndef Literal(self, *parameters):\n    \"\"\"Special typing form to define literal types (a.k.a. value types).\n\n    This form can be used to indicate to type checkers that the corresponding\n    variable or function parameter has a value equivalent to the provided\n    literal (or one of several literals):\n\n      def validate_simple(data: Any) -> Literal[True]:  # always returns True\n          ...\n\n      MODE = Literal['r', 'rb', 'w', 'wb']\n      def open_helper(file: str, mode: MODE) -> str:\n          ...\n\n      open_helper('/some/path', 'r')  # Passes type check\n      open_helper('/other/path', 'typo')  # Error in type checker\n\n    Literal[...] cannot be subclassed. At runtime, an arbitrary value\n    is allowed as type argument to Literal[...], but type checkers may\n    impose restrictions.\n    \"\"\"\n    # There is no '_type_check' call because arguments to Literal[...] are\n    # values, not types.\n    parameters = _flatten_literal_params(parameters)\n\n    try:\n        parameters = tuple(p for p, _ in _deduplicate(list(_value_and_type_iter(parameters))))\n    except TypeError:  # unhashable parameters\n        pass\n\n    return _LiteralGenericAlias(self, parameters)\n\n\nclass ForwardRef(_Final, _root=True):\n    \"\"\"Internal wrapper to hold a forward reference.\"\"\"\n\n    __slots__ = ('__forward_arg__', '__forward_code__',\n                 '__forward_evaluated__', '__forward_value__',\n                 '__forward_is_argument__', '__forward_is_class__',\n                 '__forward_module__')\n\n    def __init__(self, arg, is_argument=True, module=None, *, is_class=False):\n        if not isinstance(arg, str):\n            raise TypeError(f\"Forward reference must be a string -- got {arg!r}\")\n        try:\n            code = compile(arg, '<string>', 'eval')\n        except SyntaxError:\n            raise SyntaxError(f\"Forward reference must be an expression -- got {arg!r}\")\n        self.__forward_arg__ = arg\n        self.__forward_code__ = code\n        self.__forward_evaluated__ = False\n        self.__forward_value__ = None\n        self.__forward_is_argument__ = is_argument\n        self.__forward_is_class__ = is_class\n        self.__forward_module__ = module\n\n    def _evaluate(self, globalns, localns, recursive_guard):\n        if self.__forward_arg__ in recursive_guard:\n            return self\n        if not self.__forward_evaluated__ or localns is not globalns:\n            if globalns is None and localns is None:\n                globalns = localns = {}\n            elif globalns is None:\n                globalns = localns\n            elif localns is None:\n                localns = globalns\n            if self.__forward_module__ is not None:\n                globalns = getattr(\n                    sys.modules.get(self.__forward_module__, None), '__dict__', globalns\n                )\n            type_ = _type_check(\n                eval(self.__forward_code__, globalns, localns),\n                \"Forward references must evaluate to types.\",\n                is_argument=self.__forward_is_argument__,\n                allow_special_forms=self.__forward_is_class__,\n            )\n            self.__forward_value__ = _eval_type(\n                type_, globalns, localns, recursive_guard | {self.__forward_arg__}\n            )\n            self.__forward_evaluated__ = True\n        return self.__forward_value__\n\n    def __eq__(self, other):\n        if not isinstance(other, ForwardRef):\n            return NotImplemented\n        if self.__forward_evaluated__ and other.__forward_evaluated__:\n            return (self.__forward_arg__ == other.__forward_arg__ and\n                    self.__forward_value__ == other.__forward_value__)\n        return (self.__forward_arg__ == other.__forward_arg__ and\n                self.__forward_module__ == other.__forward_module__)\n\n    def __hash__(self):\n        return hash((self.__forward_arg__, self.__forward_module__))\n\n    def __repr__(self):\n        return f'ForwardRef({self.__forward_arg__!r})'\n\n\nclass TypeVar(_Final, _Immutable, _root=True):\n    \"\"\"Type variable.\n\n    Usage::\n\n      T = TypeVar('T')  # Can be anything\n      A = TypeVar('A', str, bytes)  # Must be str or bytes\n\n    Type variables exist primarily for the benefit of static type\n    checkers.  They serve as the parameters for generic types as well\n    as for generic function definitions.  See class Generic for more\n    information on generic types.  Generic functions work as follows:\n\n      def repeat(x: T, n: int) -> List[T]:\n          '''Return a list containing n references to x.'''\n          return [x]*n\n\n      def longest(x: A, y: A) -> A:\n          '''Return the longest of two strings.'''\n          return x if len(x) >= len(y) else y\n\n    The latter example's signature is essentially the overloading\n    of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n    that if the arguments are instances of some subclass of str,\n    the return type is still plain str.\n\n    At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n\n    Type variables defined with covariant=True or contravariant=True\n    can be used to declare covariant or contravariant generic types.\n    See PEP 484 for more details. By default generic types are invariant\n    in all type variables.\n\n    Type variables can be introspected. e.g.:\n\n      T.__name__ == 'T'\n      T.__constraints__ == ()\n      T.__covariant__ == False\n      T.__contravariant__ = False\n      A.__constraints__ == (str, bytes)\n\n    Note that only type variables defined in global scope can be pickled.\n    \"\"\"\n\n    __slots__ = ('__name__', '__bound__', '__constraints__',\n                 '__covariant__', '__contravariant__', '__dict__')\n\n    def __init__(self, name, *constraints, bound=None,\n                 covariant=False, contravariant=False):\n        self.__name__ = name\n        if covariant and contravariant:\n            raise ValueError(\"Bivariant types are not supported.\")\n        self.__covariant__ = bool(covariant)\n        self.__contravariant__ = bool(contravariant)\n        if constraints and bound is not None:\n            raise TypeError(\"Constraints cannot be combined with bound=...\")\n        if constraints and len(constraints) == 1:\n            raise TypeError(\"A single constraint is not allowed\")\n        msg = \"TypeVar(name, constraint, ...): constraints must be types.\"\n        self.__constraints__ = tuple(_type_check(t, msg) for t in constraints)\n        if bound:\n            self.__bound__ = _type_check(bound, \"Bound must be a type.\")\n        else:\n            self.__bound__ = None\n        try:\n            def_mod = sys._getframe(1).f_globals.get('__name__', '__main__')  # for pickling\n        except (AttributeError, ValueError):\n            def_mod = None\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n    def __repr__(self):\n        if self.__covariant__:\n            prefix = '+'\n        elif self.__contravariant__:\n            prefix = '-'\n        else:\n            prefix = '~'\n        return prefix + self.__name__\n\n    def __reduce__(self):\n        return self.__name__\n\n\ndef _is_dunder(attr):\n    return attr.startswith('__') and attr.endswith('__')\n\nclass _BaseGenericAlias(_Final, _root=True):\n    \"\"\"The central part of internal API.\n\n    This represents a generic version of type 'origin' with type arguments 'params'.\n    There are two kind of these aliases: user defined and special. The special ones\n    are wrappers around builtin collections and ABCs in collections.abc. These must\n    have 'name' always set. If 'inst' is False, then the alias can't be instantiated,\n    this is used by e.g. typing.List and typing.Dict.\n    \"\"\"\n    def __init__(self, origin, *, inst=True, name=None):\n        self._inst = inst\n        self._name = name\n        self.__origin__ = origin\n        self.__slots__ = None  # This is not documented.\n\n    def __call__(self, *args, **kwargs):\n        if not self._inst:\n            raise TypeError(f\"Type {self._name} cannot be instantiated; \"\n                            f\"use {self.__origin__.__name__}() instead\")\n        result = self.__origin__(*args, **kwargs)\n        try:\n            result.__orig_class__ = self\n        except AttributeError:\n            pass\n        return result\n\n    def __mro_entries__(self, bases):\n        res = []\n        if self.__origin__ not in bases:\n            res.append(self.__origin__)\n        i = bases.index(self)\n        for b in bases[i+1:]:\n            if isinstance(b, _BaseGenericAlias) or issubclass(b, Generic):\n                break\n        else:\n            res.append(Generic)\n        return tuple(res)\n\n    def __getattr__(self, attr):\n        # We are careful for copy and pickle.\n        # Also for simplicity we don't relay any dunder names\n        if '__origin__' in self.__dict__ and not _is_dunder(attr):\n            return getattr(self.__origin__, attr)\n        raise AttributeError(attr)\n\n    def __setattr__(self, attr, val):\n        if _is_dunder(attr) or attr in ('_name', '_inst', '_nparams'):\n            super().__setattr__(attr, val)\n        else:\n            setattr(self.__origin__, attr, val)\n\n    def __instancecheck__(self, obj):\n        return self.__subclasscheck__(type(obj))\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(\"Subscripted generics cannot be used with\"\n                        \" class and instance checks\")\n\n\n# Special typing constructs Union, Optional, Generic, Callable and Tuple\n# use three special attributes for internal bookkeeping of generic types:\n# * __parameters__ is a tuple of unique free type parameters of a generic\n#   type, for example, Dict[T, T].__parameters__ == (T,);\n# * __origin__ keeps a reference to a type that was subscripted,\n#   e.g., Union[T, int].__origin__ == Union, or the non-generic version of\n#   the type.\n# * __args__ is a tuple of all arguments used in subscripting,\n#   e.g., Dict[T, int].__args__ == (T, int).\n\n\nclass _GenericAlias(_BaseGenericAlias, _root=True):\n    def __init__(self, origin, params, *, inst=True, name=None):\n        super().__init__(origin, inst=inst, name=name)\n        if not isinstance(params, tuple):\n            params = (params,)\n        self.__args__ = tuple(... if a is _TypingEllipsis else\n                              () if a is _TypingEmpty else\n                              a for a in params)\n        self.__parameters__ = _collect_type_vars(params)\n        if not name:\n            self.__module__ = origin.__module__\n\n    def __eq__(self, other):\n        if not isinstance(other, _GenericAlias):\n            return NotImplemented\n        return (self.__origin__ == other.__origin__\n                and self.__args__ == other.__args__)\n\n    def __hash__(self):\n        return hash((self.__origin__, self.__args__))\n\n    @_tp_cache\n    def __getitem__(self, params):\n        if self.__origin__ in (Generic, Protocol):\n            # Can't subscript Generic[...] or Protocol[...].\n            raise TypeError(f\"Cannot subscript already-subscripted {self}\")\n        if not isinstance(params, tuple):\n            params = (params,)\n        msg = \"Parameters to generic types must be types.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        _check_generic(self, params, len(self.__parameters__))\n\n        subst = dict(zip(self.__parameters__, params))\n        new_args = []\n        for arg in self.__args__:\n            if isinstance(arg, TypeVar):\n                arg = subst[arg]\n            elif isinstance(arg, (_GenericAlias, GenericAlias)):\n                subparams = arg.__parameters__\n                if subparams:\n                    subargs = tuple(subst[x] for x in subparams)\n                    arg = arg[subargs]\n            new_args.append(arg)\n        return self.copy_with(tuple(new_args))\n\n    def copy_with(self, params):\n        return self.__class__(self.__origin__, params, name=self._name, inst=self._inst)\n\n    def __repr__(self):\n        if self._name:\n            name = 'typing.' + self._name\n        else:\n            name = _type_repr(self.__origin__)\n        args = \", \".join([_type_repr(a) for a in self.__args__])\n        return f'{name}[{args}]'\n\n    def __reduce__(self):\n        if self._name:\n            origin = globals()[self._name]\n        else:\n            origin = self.__origin__\n        args = tuple(self.__args__)\n        if len(args) == 1 and not isinstance(args[0], tuple):\n            args, = args\n        return operator.getitem, (origin, args)\n\n    def __mro_entries__(self, bases):\n        if self._name:  # generic version of an ABC or built-in class\n            return super().__mro_entries__(bases)\n        if self.__origin__ is Generic:\n            if Protocol in bases:\n                return ()\n            i = bases.index(self)\n            for b in bases[i+1:]:\n                if isinstance(b, _BaseGenericAlias) and b is not self:\n                    return ()\n        return (self.__origin__,)\n\n\n# _nparams is the number of accepted parameters, e.g. 0 for Hashable,\n# 1 for List and 2 for Dict.  It may be -1 if variable number of\n# parameters are accepted (needs custom __getitem__).\n\nclass _SpecialGenericAlias(_BaseGenericAlias, _root=True):\n    def __init__(self, origin, nparams, *, inst=True, name=None):\n        if name is None:\n            name = origin.__name__\n        super().__init__(origin, inst=inst, name=name)\n        self._nparams = nparams\n        if origin.__module__ == 'builtins':\n            self.__doc__ = f'A generic version of {origin.__qualname__}.'\n        else:\n            self.__doc__ = f'A generic version of {origin.__module__}.{origin.__qualname__}.'\n\n    @_tp_cache\n    def __getitem__(self, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        msg = \"Parameters to generic types must be types.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        _check_generic(self, params, self._nparams)\n        return self.copy_with(params)\n\n    def copy_with(self, params):\n        return _GenericAlias(self.__origin__, params,\n                             name=self._name, inst=self._inst)\n\n    def __repr__(self):\n        return 'typing.' + self._name\n\n    def __subclasscheck__(self, cls):\n        if isinstance(cls, _SpecialGenericAlias):\n            return issubclass(cls.__origin__, self.__origin__)\n        if not isinstance(cls, _GenericAlias):\n            return issubclass(cls, self.__origin__)\n        return super().__subclasscheck__(cls)\n\n    def __reduce__(self):\n        return self._name\n\n\nclass _CallableGenericAlias(_GenericAlias, _root=True):\n    def __repr__(self):\n        assert self._name == 'Callable'\n        if len(self.__args__) == 2 and self.__args__[0] is Ellipsis:\n            return super().__repr__()\n        return (f'typing.Callable'\n                f'[[{\", \".join([_type_repr(a) for a in self.__args__[:-1]])}], '\n                f'{_type_repr(self.__args__[-1])}]')\n\n    def __reduce__(self):\n        args = self.__args__\n        if not (len(args) == 2 and args[0] is ...):\n            args = list(args[:-1]), args[-1]\n        return operator.getitem, (Callable, args)\n\n\nclass _CallableType(_SpecialGenericAlias, _root=True):\n    def copy_with(self, params):\n        return _CallableGenericAlias(self.__origin__, params,\n                                     name=self._name, inst=self._inst)\n\n    def __getitem__(self, params):\n        if not isinstance(params, tuple) or len(params) != 2:\n            raise TypeError(\"Callable must be used as \"\n                            \"Callable[[arg, ...], result].\")\n        args, result = params\n        # This relaxes what args can be on purpose to allow things like\n        # PEP 612 ParamSpec.  Responsibility for whether a user is using\n        # Callable[...] properly is deferred to static type checkers.\n        if isinstance(args, list):\n            params = (tuple(args), result)\n        else:\n            params = (args, result)\n        return self.__getitem_inner__(params)\n\n    @_tp_cache\n    def __getitem_inner__(self, params):\n        args, result = params\n        msg = \"Callable[args, result]: result must be a type.\"\n        result = _type_check(result, msg)\n        if args is Ellipsis:\n            return self.copy_with((_TypingEllipsis, result))\n        if not isinstance(args, tuple):\n            args = (args,)\n        args = tuple(_type_convert(arg) for arg in args)\n        params = args + (result,)\n        return self.copy_with(params)\n\n\nclass _TupleType(_SpecialGenericAlias, _root=True):\n    @_tp_cache\n    def __getitem__(self, params):\n        if params == ():\n            return self.copy_with((_TypingEmpty,))\n        if not isinstance(params, tuple):\n            params = (params,)\n        if len(params) == 2 and params[1] is ...:\n            msg = \"Tuple[t, ...]: t must be a type.\"\n            p = _type_check(params[0], msg)\n            return self.copy_with((p, _TypingEllipsis))\n        msg = \"Tuple[t0, t1, ...]: each t must be a type.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        return self.copy_with(params)\n\n\nclass _UnionGenericAlias(_GenericAlias, _root=True):\n    def copy_with(self, params):\n        return Union[params]\n\n    def __eq__(self, other):\n        if not isinstance(other, _UnionGenericAlias):\n            return NotImplemented\n        return set(self.__args__) == set(other.__args__)\n\n    def __hash__(self):\n        return hash(frozenset(self.__args__))\n\n    def __repr__(self):\n        args = self.__args__\n        if len(args) == 2:\n            if args[0] is type(None):\n                return f'typing.Optional[{_type_repr(args[1])}]'\n            elif args[1] is type(None):\n                return f'typing.Optional[{_type_repr(args[0])}]'\n        return super().__repr__()\n\n\ndef _value_and_type_iter(parameters):\n    return ((p, type(p)) for p in parameters)\n\n\nclass _LiteralGenericAlias(_GenericAlias, _root=True):\n\n    def __eq__(self, other):\n        if not isinstance(other, _LiteralGenericAlias):\n            return NotImplemented\n\n        return set(_value_and_type_iter(self.__args__)) == set(_value_and_type_iter(other.__args__))\n\n    def __hash__(self):\n        return hash(frozenset(_value_and_type_iter(self.__args__)))\n\n\nclass Generic:\n    \"\"\"Abstract base class for generic types.\n\n    A generic type is typically declared by inheriting from\n    this class parameterized with one or more type variables.\n    For example, a generic mapping type might be defined as::\n\n      class Mapping(Generic[KT, VT]):\n          def __getitem__(self, key: KT) -> VT:\n              ...\n          # Etc.\n\n    This class can then be used as follows::\n\n      def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT:\n          try:\n              return mapping[key]\n          except KeyError:\n              return default\n    \"\"\"\n    __slots__ = ()\n    _is_protocol = False\n\n    @_tp_cache\n    def __class_getitem__(cls, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        if not params and cls is not Tuple:\n            raise TypeError(\n                f\"Parameter list to {cls.__qualname__}[...] cannot be empty\")\n        msg = \"Parameters to generic types must be types.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        if cls in (Generic, Protocol):\n            # Generic and Protocol can only be subscripted with unique type variables.\n            if not all(isinstance(p, TypeVar) for p in params):\n                raise TypeError(\n                    f\"Parameters to {cls.__name__}[...] must all be type variables\")\n            if len(set(params)) != len(params):\n                raise TypeError(\n                    f\"Parameters to {cls.__name__}[...] must all be unique\")\n        else:\n            # Subscripting a regular Generic subclass.\n            _check_generic(cls, params, len(cls.__parameters__))\n        return _GenericAlias(cls, params)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n        tvars = []\n        if '__orig_bases__' in cls.__dict__:\n            error = Generic in cls.__orig_bases__\n        else:\n            error = Generic in cls.__bases__ and cls.__name__ != 'Protocol'\n        if error:\n            raise TypeError(\"Cannot inherit from plain Generic\")\n        if '__orig_bases__' in cls.__dict__:\n            tvars = _collect_type_vars(cls.__orig_bases__)\n            # Look for Generic[T1, ..., Tn].\n            # If found, tvars must be a subset of it.\n            # If not found, tvars is it.\n            # Also check for and reject plain Generic,\n            # and reject multiple Generic[...].\n            gvars = None\n            for base in cls.__orig_bases__:\n                if (isinstance(base, _GenericAlias) and\n                        base.__origin__ is Generic):\n                    if gvars is not None:\n                        raise TypeError(\n                            \"Cannot inherit from Generic[...] multiple types.\")\n                    gvars = base.__parameters__\n            if gvars is not None:\n                tvarset = set(tvars)\n                gvarset = set(gvars)\n                if not tvarset <= gvarset:\n                    s_vars = ', '.join(str(t) for t in tvars if t not in gvarset)\n                    s_args = ', '.join(str(g) for g in gvars)\n                    raise TypeError(f\"Some type variables ({s_vars}) are\"\n                                    f\" not listed in Generic[{s_args}]\")\n                tvars = gvars\n        cls.__parameters__ = tuple(tvars)\n\n\nclass _TypingEmpty:\n    \"\"\"Internal placeholder for () or []. Used by TupleMeta and CallableMeta\n    to allow empty list/tuple in specific places, without allowing them\n    to sneak in where prohibited.\n    \"\"\"\n\n\nclass _TypingEllipsis:\n    \"\"\"Internal placeholder for ... (ellipsis).\"\"\"\n\n\n_TYPING_INTERNALS = ['__parameters__', '__orig_bases__',  '__orig_class__',\n                     '_is_protocol', '_is_runtime_protocol']\n\n_SPECIAL_NAMES = ['__abstractmethods__', '__annotations__', '__dict__', '__doc__',\n                  '__init__', '__module__', '__new__', '__slots__',\n                  '__subclasshook__', '__weakref__', '__class_getitem__']\n\n# These special attributes will be not collected as protocol members.\nEXCLUDED_ATTRIBUTES = _TYPING_INTERNALS + _SPECIAL_NAMES + ['_MutableMapping__marker']\n\n\ndef _get_protocol_attrs(cls):\n    \"\"\"Collect protocol members from a protocol class objects.\n\n    This includes names actually defined in the class dictionary, as well\n    as names that appear in annotations. Special names (above) are skipped.\n    \"\"\"\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in ('Protocol', 'Generic'):\n            continue\n        annotations = getattr(base, '__annotations__', {})\n        for attr in list(base.__dict__.keys()) + list(annotations.keys()):\n            if not attr.startswith('_abc_') and attr not in EXCLUDED_ATTRIBUTES:\n                attrs.add(attr)\n    return attrs\n\n\ndef _is_callable_members_only(cls):\n    # PEP 544 prohibits using issubclass() with protocols that have non-method members.\n    return all(callable(getattr(cls, attr, None)) for attr in _get_protocol_attrs(cls))\n\n\ndef _no_init_or_replace_init(self, *args, **kwargs):\n    cls = type(self)\n\n    if cls._is_protocol:\n        raise TypeError('Protocols cannot be instantiated')\n\n    # Already using a custom `__init__`. No need to calculate correct\n    # `__init__` to call. This can lead to RecursionError. See bpo-45121.\n    if cls.__init__ is not _no_init_or_replace_init:\n        return\n\n    # Initially, `__init__` of a protocol subclass is set to `_no_init_or_replace_init`.\n    # The first instantiation of the subclass will call `_no_init_or_replace_init` which\n    # searches for a proper new `__init__` in the MRO. The new `__init__`\n    # replaces the subclass' old `__init__` (ie `_no_init_or_replace_init`). Subsequent\n    # instantiation of the protocol subclass will thus use the new\n    # `__init__` and no longer call `_no_init_or_replace_init`.\n    for base in cls.__mro__:\n        init = base.__dict__.get('__init__', _no_init_or_replace_init)\n        if init is not _no_init_or_replace_init:\n            cls.__init__ = init\n            break\n    else:\n        # should not happen\n        cls.__init__ = object.__init__\n\n    cls.__init__(self, *args, **kwargs)\n\n\n\ndef _allow_reckless_class_cheks():\n    \"\"\"Allow instance and class checks for special stdlib modules.\n\n    The abc and functools modules indiscriminately call isinstance() and\n    issubclass() on the whole MRO of a user class, which may contain protocols.\n    \"\"\"\n    try:\n        return sys._getframe(3).f_globals['__name__'] in ['abc', 'functools']\n    except (AttributeError, ValueError):  # For platforms without _getframe().\n        return True\n\n\n_PROTO_WHITELIST = {\n    'collections.abc': [\n        'Callable', 'Awaitable', 'Iterable', 'Iterator', 'AsyncIterable',\n        'Hashable', 'Sized', 'Container', 'Collection', 'Reversible',\n    ],\n    'contextlib': ['AbstractContextManager', 'AbstractAsyncContextManager'],\n}\n\n\nclass _ProtocolMeta(ABCMeta):\n    # This metaclass is really unfortunate and exists only because of\n    # the lack of __instancehook__.\n    def __instancecheck__(cls, instance):\n        # We need this method for situations where attributes are\n        # assigned in __init__.\n        if ((not getattr(cls, '_is_protocol', False) or\n                _is_callable_members_only(cls)) and\n                issubclass(instance.__class__, cls)):\n            return True\n        if cls._is_protocol:\n            if all(hasattr(instance, attr) and\n                    # All *methods* can be blocked by setting them to None.\n                    (not callable(getattr(cls, attr, None)) or\n                     getattr(instance, attr) is not None)\n                    for attr in _get_protocol_attrs(cls)):\n                return True\n        return super().__instancecheck__(instance)\n\n\nclass Protocol(Generic, metaclass=_ProtocolMeta):\n    \"\"\"Base class for protocol classes.\n\n    Protocol classes are defined as::\n\n        class Proto(Protocol):\n            def meth(self) -> int:\n                ...\n\n    Such classes are primarily used with static type checkers that recognize\n    structural subtyping (static duck-typing), for example::\n\n        class C:\n            def meth(self) -> int:\n                return 0\n\n        def func(x: Proto) -> int:\n            return x.meth()\n\n        func(C())  # Passes static type check\n\n    See PEP 544 for details. Protocol classes decorated with\n    @typing.runtime_checkable act as simple-minded runtime protocols that check\n    only the presence of given attributes, ignoring their type signatures.\n    Protocol classes can be generic, they are defined as::\n\n        class GenProto(Protocol[T]):\n            def meth(self) -> T:\n                ...\n    \"\"\"\n    __slots__ = ()\n    _is_protocol = True\n    _is_runtime_protocol = False\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n\n        # Determine if this is a protocol or a concrete subclass.\n        if not cls.__dict__.get('_is_protocol', False):\n            cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n        # Set (or override) the protocol subclass hook.\n        def _proto_hook(other):\n            if not cls.__dict__.get('_is_protocol', False):\n                return NotImplemented\n\n            # First, perform various sanity checks.\n            if not getattr(cls, '_is_runtime_protocol', False):\n                if _allow_reckless_class_cheks():\n                    return NotImplemented\n                raise TypeError(\"Instance and class checks can only be used with\"\n                                \" @runtime_checkable protocols\")\n            if not _is_callable_members_only(cls):\n                if _allow_reckless_class_cheks():\n                    return NotImplemented\n                raise TypeError(\"Protocols with non-method members\"\n                                \" don't support issubclass()\")\n            if not isinstance(other, type):\n                # Same error message as for issubclass(1, int).\n                raise TypeError('issubclass() arg 1 must be a class')\n\n            # Second, perform the actual structural compatibility check.\n            for attr in _get_protocol_attrs(cls):\n                for base in other.__mro__:\n                    # Check if the members appears in the class dictionary...\n                    if attr in base.__dict__:\n                        if base.__dict__[attr] is None:\n                            return NotImplemented\n                        break\n\n                    # ...or in annotations, if it is a sub-protocol.\n                    annotations = getattr(base, '__annotations__', {})\n                    if (isinstance(annotations, collections.abc.Mapping) and\n                            attr in annotations and\n                            issubclass(other, Generic) and other._is_protocol):\n                        break\n                else:\n                    return NotImplemented\n            return True\n\n        if '__subclasshook__' not in cls.__dict__:\n            cls.__subclasshook__ = _proto_hook\n\n        # We have nothing more to do for non-protocols...\n        if not cls._is_protocol:\n            return\n\n        # ... otherwise check consistency of bases, and prohibit instantiation.\n        for base in cls.__bases__:\n            if not (base in (object, Generic) or\n                    base.__module__ in _PROTO_WHITELIST and\n                    base.__name__ in _PROTO_WHITELIST[base.__module__] or\n                    issubclass(base, Generic) and base._is_protocol):\n                raise TypeError('Protocols can only inherit from other'\n                                ' protocols, got %r' % base)\n        cls.__init__ = _no_init_or_replace_init\n\n\nclass _AnnotatedAlias(_GenericAlias, _root=True):\n    \"\"\"Runtime representation of an annotated type.\n\n    At its core 'Annotated[t, dec1, dec2, ...]' is an alias for the type 't'\n    with extra annotations. The alias behaves like a normal typing alias,\n    instantiating is the same as instantiating the underlying type, binding\n    it to types is also the same.\n    \"\"\"\n    def __init__(self, origin, metadata):\n        if isinstance(origin, _AnnotatedAlias):\n            metadata = origin.__metadata__ + metadata\n            origin = origin.__origin__\n        super().__init__(origin, origin)\n        self.__metadata__ = metadata\n\n    def copy_with(self, params):\n        assert len(params) == 1\n        new_type = params[0]\n        return _AnnotatedAlias(new_type, self.__metadata__)\n\n    def __repr__(self):\n        return \"typing.Annotated[{}, {}]\".format(\n            _type_repr(self.__origin__),\n            \", \".join(repr(a) for a in self.__metadata__)\n        )\n\n    def __reduce__(self):\n        return operator.getitem, (\n            Annotated, (self.__origin__,) + self.__metadata__\n        )\n\n    def __eq__(self, other):\n        if not isinstance(other, _AnnotatedAlias):\n            return NotImplemented\n        return (self.__origin__ == other.__origin__\n                and self.__metadata__ == other.__metadata__)\n\n    def __hash__(self):\n        return hash((self.__origin__, self.__metadata__))\n\n\nclass Annotated:\n    \"\"\"Add context specific metadata to a type.\n\n    Example: Annotated[int, runtime_check.Unsigned] indicates to the\n    hypothetical runtime_check module that this type is an unsigned int.\n    Every other consumer of this type can ignore this metadata and treat\n    this type as int.\n\n    The first argument to Annotated must be a valid type.\n\n    Details:\n\n    - It's an error to call `Annotated` with less than two arguments.\n    - Nested Annotated are flattened::\n\n        Annotated[Annotated[T, Ann1, Ann2], Ann3] == Annotated[T, Ann1, Ann2, Ann3]\n\n    - Instantiating an annotated type is equivalent to instantiating the\n    underlying type::\n\n        Annotated[C, Ann1](5) == C(5)\n\n    - Annotated can be used as a generic type alias::\n\n        Optimized = Annotated[T, runtime.Optimize()]\n        Optimized[int] == Annotated[int, runtime.Optimize()]\n\n        OptimizedList = Annotated[List[T], runtime.Optimize()]\n        OptimizedList[int] == Annotated[List[int], runtime.Optimize()]\n    \"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, *args, **kwargs):\n        raise TypeError(\"Type Annotated cannot be instantiated.\")\n\n    @_tp_cache\n    def __class_getitem__(cls, params):\n        if not isinstance(params, tuple) or len(params) < 2:\n            raise TypeError(\"Annotated[...] should be used \"\n                            \"with at least two arguments (a type and an \"\n                            \"annotation).\")\n        msg = \"Annotated[t, ...]: t must be a type.\"\n        origin = _type_check(params[0], msg, allow_special_forms=True)\n        metadata = tuple(params[1:])\n        return _AnnotatedAlias(origin, metadata)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        raise TypeError(\n            \"Cannot subclass {}.Annotated\".format(cls.__module__)\n        )\n\n\ndef runtime_checkable(cls):\n    \"\"\"Mark a protocol class as a runtime protocol.\n\n    Such protocol can be used with isinstance() and issubclass().\n    Raise TypeError if applied to a non-protocol class.\n    This allows a simple-minded structural check very similar to\n    one trick ponies in collections.abc such as Iterable.\n    For example::\n\n        @runtime_checkable\n        class Closable(Protocol):\n            def close(self): ...\n\n        assert isinstance(open('/some/file'), Closable)\n\n    Warning: this will check only the presence of the required methods,\n    not their type signatures!\n    \"\"\"\n    if not issubclass(cls, Generic) or not cls._is_protocol:\n        raise TypeError('@runtime_checkable can be only applied to protocol classes,'\n                        ' got %r' % cls)\n    cls._is_runtime_protocol = True\n    return cls\n\n\ndef cast(typ, val):\n    \"\"\"Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    \"\"\"\n    return val\n\n\ndef _get_defaults(func):\n    \"\"\"Internal helper to extract the default arguments, by name.\"\"\"\n    try:\n        code = func.__code__\n    except AttributeError:\n        # Some built-in functions don't have __code__, __defaults__, etc.\n        return {}\n    pos_count = code.co_argcount\n    arg_names = code.co_varnames\n    arg_names = arg_names[:pos_count]\n    defaults = func.__defaults__ or ()\n    kwdefaults = func.__kwdefaults__\n    res = dict(kwdefaults) if kwdefaults else {}\n    pos_offset = pos_count - len(defaults)\n    for name, value in zip(arg_names[pos_offset:], defaults):\n        assert name not in res\n        res[name] = value\n    return res\n\n\n_allowed_types = (types.FunctionType, types.BuiltinFunctionType,\n                  types.MethodType, types.ModuleType,\n                  WrapperDescriptorType, MethodWrapperType, MethodDescriptorType)\n\n\ndef get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n    \"\"\"Return type hints for an object.\n\n    This is often the same as obj.__annotations__, but it handles\n    forward references encoded as string literals, adds Optional[t] if a\n    default value equal to None is set and recursively replaces all\n    'Annotated[T, ...]' with 'T' (unless 'include_extras=True').\n\n    The argument may be a module, class, method, or function. The annotations\n    are returned as a dictionary. For classes, annotations include also\n    inherited members.\n\n    TypeError is raised if the argument is not of a type that can contain\n    annotations, and an empty dictionary is returned if no annotations are\n    present.\n\n    BEWARE -- the behavior of globalns and localns is counterintuitive\n    (unless you are familiar with how eval() and exec() work).  The\n    search order is locals first, then globals.\n\n    - If no dict arguments are passed, an attempt is made to use the\n      globals from obj (or the respective module's globals for classes),\n      and these are also used as the locals.  If the object does not appear\n      to have globals, an empty dictionary is used.\n\n    - If one dict argument is passed, it is used for both globals and\n      locals.\n\n    - If two dict arguments are passed, they specify globals and\n      locals, respectively.\n    \"\"\"\n\n    if getattr(obj, '__no_type_check__', None):\n        return {}\n    # Classes require a special treatment.\n    if isinstance(obj, type):\n        hints = {}\n        for base in reversed(obj.__mro__):\n            if globalns is None:\n                base_globals = sys.modules[base.__module__].__dict__\n            else:\n                base_globals = globalns\n            ann = base.__dict__.get('__annotations__', {})\n            for name, value in ann.items():\n                if value is None:\n                    value = type(None)\n                if isinstance(value, str):\n                    value = ForwardRef(value, is_argument=False, is_class=True)\n                value = _eval_type(value, base_globals, localns)\n                hints[name] = value\n        return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\n\n    if globalns is None:\n        if isinstance(obj, types.ModuleType):\n            globalns = obj.__dict__\n        else:\n            nsobj = obj\n            # Find globalns for the unwrapped object.\n            while hasattr(nsobj, '__wrapped__'):\n                nsobj = nsobj.__wrapped__\n            globalns = getattr(nsobj, '__globals__', {})\n        if localns is None:\n            localns = globalns\n    elif localns is None:\n        localns = globalns\n    hints = getattr(obj, '__annotations__', None)\n    if hints is None:\n        # Return empty annotations for something that _could_ have them.\n        if isinstance(obj, _allowed_types):\n            return {}\n        else:\n            raise TypeError('{!r} is not a module, class, method, '\n                            'or function.'.format(obj))\n    defaults = _get_defaults(obj)\n    hints = dict(hints)\n    for name, value in hints.items():\n        if value is None:\n            value = type(None)\n        if isinstance(value, str):\n            # class-level forward refs were handled above, this must be either\n            # a module-level annotation or a function argument annotation\n            value = ForwardRef(\n                value,\n                is_argument=not isinstance(obj, types.ModuleType),\n                is_class=False,\n            )\n        value = _eval_type(value, globalns, localns)\n        if name in defaults and defaults[name] is None:\n            value = Optional[value]\n        hints[name] = value\n    return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\n\n\ndef _strip_annotations(t):\n    \"\"\"Strips the annotations from a given type.\n    \"\"\"\n    if isinstance(t, _AnnotatedAlias):\n        return _strip_annotations(t.__origin__)\n    if isinstance(t, _GenericAlias):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return t.copy_with(stripped_args)\n    if isinstance(t, GenericAlias):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return GenericAlias(t.__origin__, stripped_args)\n    return t\n\n\ndef get_origin(tp):\n    \"\"\"Get the unsubscripted version of a type.\n\n    This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar\n    and Annotated. Return None for unsupported types. Examples::\n\n        get_origin(Literal[42]) is Literal\n        get_origin(int) is None\n        get_origin(ClassVar[int]) is ClassVar\n        get_origin(Generic) is Generic\n        get_origin(Generic[T]) is Generic\n        get_origin(Union[T, int]) is Union\n        get_origin(List[Tuple[T, T]][int]) == list\n    \"\"\"\n    if isinstance(tp, _AnnotatedAlias):\n        return Annotated\n    if isinstance(tp, (_BaseGenericAlias, GenericAlias)):\n        return tp.__origin__\n    if tp is Generic:\n        return Generic\n    return None\n\n\ndef get_args(tp):\n    \"\"\"Get type arguments with all substitutions performed.\n\n    For unions, basic simplifications used by Union constructor are performed.\n    Examples::\n        get_args(Dict[str, int]) == (str, int)\n        get_args(int) == ()\n        get_args(Union[int, Union[T, int], str][int]) == (int, str)\n        get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n        get_args(Callable[[], T][int]) == ([], int)\n    \"\"\"\n    if isinstance(tp, _AnnotatedAlias):\n        return (tp.__origin__,) + tp.__metadata__\n    if isinstance(tp, (_GenericAlias, GenericAlias)):\n        res = tp.__args__\n        if tp.__origin__ is collections.abc.Callable and res[0] is not Ellipsis:\n            res = (list(res[:-1]), res[-1])\n        return res\n    return ()\n\n\ndef no_type_check(arg):\n    \"\"\"Decorator to indicate that annotations are not type hints.\n\n    The argument must be a class or function; if it is a class, it\n    applies recursively to all methods and classes defined in that class\n    (but not to methods defined in its superclasses or subclasses).\n\n    This mutates the function(s) or class(es) in place.\n    \"\"\"\n    if isinstance(arg, type):\n        arg_attrs = arg.__dict__.copy()\n        for attr, val in arg.__dict__.items():\n            if val in arg.__bases__ + (arg,):\n                arg_attrs.pop(attr)\n        for obj in arg_attrs.values():\n            if isinstance(obj, types.FunctionType):\n                obj.__no_type_check__ = True\n            if isinstance(obj, type):\n                no_type_check(obj)\n    try:\n        arg.__no_type_check__ = True\n    except TypeError:  # built-in classes\n        pass\n    return arg\n\n\ndef no_type_check_decorator(decorator):\n    \"\"\"Decorator to give another decorator the @no_type_check effect.\n\n    This wraps the decorator with something that wraps the decorated\n    function in @no_type_check.\n    \"\"\"\n\n    @functools.wraps(decorator)\n    def wrapped_decorator(*args, **kwds):\n        func = decorator(*args, **kwds)\n        func = no_type_check(func)\n        return func\n\n    return wrapped_decorator\n\n\ndef _overload_dummy(*args, **kwds):\n    \"\"\"Helper for @overload to raise when called.\"\"\"\n    raise NotImplementedError(\n        \"You should not call an overloaded function. \"\n        \"A series of @overload-decorated functions \"\n        \"outside a stub module should always be followed \"\n        \"by an implementation that is not @overload-ed.\")\n\n\ndef overload(func):\n    \"\"\"Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n      def utf8(value):\n          # implementation goes here\n    \"\"\"\n    return _overload_dummy\n\n\ndef final(f):\n    \"\"\"A decorator to indicate final methods and final classes.\n\n    Use this decorator to indicate to type checkers that the decorated\n    method cannot be overridden, and decorated class cannot be subclassed.\n    For example:\n\n      class Base:\n          @final\n          def done(self) -> None:\n              ...\n      class Sub(Base):\n          def done(self) -> None:  # Error reported by type checker\n                ...\n\n      @final\n      class Leaf:\n          ...\n      class Other(Leaf):  # Error reported by type checker\n          ...\n\n    There is no runtime checking of these properties.\n    \"\"\"\n    return f\n\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = TypeVar('T')  # Any type.\nKT = TypeVar('KT')  # Key type.\nVT = TypeVar('VT')  # Value type.\nT_co = TypeVar('T_co', covariant=True)  # Any type covariant containers.\nV_co = TypeVar('V_co', covariant=True)  # Any type covariant containers.\nVT_co = TypeVar('VT_co', covariant=True)  # Value type covariant containers.\nT_contra = TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n# Internal type variable used for Type[].\nCT_co = TypeVar('CT_co', covariant=True, bound=type)\n\n# A useful type variable with constraints.  This represents string types.\n# (This one *is* for export!)\nAnyStr = TypeVar('AnyStr', bytes, str)\n\n\n# Various ABCs mimicking those in collections.abc.\n_alias = _SpecialGenericAlias\n\nHashable = _alias(collections.abc.Hashable, 0)  # Not generic.\nAwaitable = _alias(collections.abc.Awaitable, 1)\nCoroutine = _alias(collections.abc.Coroutine, 3)\nAsyncIterable = _alias(collections.abc.AsyncIterable, 1)\nAsyncIterator = _alias(collections.abc.AsyncIterator, 1)\nIterable = _alias(collections.abc.Iterable, 1)\nIterator = _alias(collections.abc.Iterator, 1)\nReversible = _alias(collections.abc.Reversible, 1)\nSized = _alias(collections.abc.Sized, 0)  # Not generic.\nContainer = _alias(collections.abc.Container, 1)\nCollection = _alias(collections.abc.Collection, 1)\nCallable = _CallableType(collections.abc.Callable, 2)\nCallable.__doc__ = \\\n    \"\"\"Callable type; Callable[[int], str] is a function of (int) -> str.\n\n    The subscription syntax must always be used with exactly two\n    values: the argument list and the return type.  The argument list\n    must be a list of types or ellipsis; the return type must be a single type.\n\n    There is no syntax to indicate optional or keyword arguments,\n    such function types are rarely used as callback types.\n    \"\"\"\nAbstractSet = _alias(collections.abc.Set, 1, name='AbstractSet')\nMutableSet = _alias(collections.abc.MutableSet, 1)\n# NOTE: Mapping is only covariant in the value type.\nMapping = _alias(collections.abc.Mapping, 2)\nMutableMapping = _alias(collections.abc.MutableMapping, 2)\nSequence = _alias(collections.abc.Sequence, 1)\nMutableSequence = _alias(collections.abc.MutableSequence, 1)\nByteString = _alias(collections.abc.ByteString, 0)  # Not generic\n# Tuple accepts variable number of parameters.\nTuple = _TupleType(tuple, -1, inst=False, name='Tuple')\nTuple.__doc__ = \\\n    \"\"\"Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n\n    Example: Tuple[T1, T2] is a tuple of two elements corresponding\n    to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n    of an int, a float and a string.\n\n    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n    \"\"\"\nList = _alias(list, 1, inst=False, name='List')\nDeque = _alias(collections.deque, 1, name='Deque')\nSet = _alias(set, 1, inst=False, name='Set')\nFrozenSet = _alias(frozenset, 1, inst=False, name='FrozenSet')\nMappingView = _alias(collections.abc.MappingView, 1)\nKeysView = _alias(collections.abc.KeysView, 1)\nItemsView = _alias(collections.abc.ItemsView, 2)\nValuesView = _alias(collections.abc.ValuesView, 1)\nContextManager = _alias(contextlib.AbstractContextManager, 1, name='ContextManager')\nAsyncContextManager = _alias(contextlib.AbstractAsyncContextManager, 1, name='AsyncContextManager')\nDict = _alias(dict, 2, inst=False, name='Dict')\nDefaultDict = _alias(collections.defaultdict, 2, name='DefaultDict')\nOrderedDict = _alias(collections.OrderedDict, 2)\nCounter = _alias(collections.Counter, 1)\nChainMap = _alias(collections.ChainMap, 2)\nGenerator = _alias(collections.abc.Generator, 3)\nAsyncGenerator = _alias(collections.abc.AsyncGenerator, 2)\nType = _alias(type, 1, inst=False, name='Type')\nType.__doc__ = \\\n    \"\"\"A special construct usable to annotate class objects.\n\n    For example, suppose we have the following classes::\n\n      class User: ...  # Abstract base for User classes\n      class BasicUser(User): ...\n      class ProUser(User): ...\n      class TeamUser(User): ...\n\n    And a function that takes a class argument that's a subclass of\n    User and returns an instance of the corresponding class::\n\n      U = TypeVar('U', bound=User)\n      def new_user(user_class: Type[U]) -> U:\n          user = user_class()\n          # (Here we could write the user object to a database)\n          return user\n\n      joe = new_user(BasicUser)\n\n    At this point the type checker knows that joe has type BasicUser.\n    \"\"\"\n\n\n@runtime_checkable\nclass SupportsInt(Protocol):\n    \"\"\"An ABC with one abstract method __int__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __int__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsFloat(Protocol):\n    \"\"\"An ABC with one abstract method __float__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __float__(self) -> float:\n        pass\n\n\n@runtime_checkable\nclass SupportsComplex(Protocol):\n    \"\"\"An ABC with one abstract method __complex__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __complex__(self) -> complex:\n        pass\n\n\n@runtime_checkable\nclass SupportsBytes(Protocol):\n    \"\"\"An ABC with one abstract method __bytes__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __bytes__(self) -> bytes:\n        pass\n\n\n@runtime_checkable\nclass SupportsIndex(Protocol):\n    \"\"\"An ABC with one abstract method __index__.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __index__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsAbs(Protocol[T_co]):\n    \"\"\"An ABC with one abstract method __abs__ that is covariant in its return type.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __abs__(self) -> T_co:\n        pass\n\n\n@runtime_checkable\nclass SupportsRound(Protocol[T_co]):\n    \"\"\"An ABC with one abstract method __round__ that is covariant in its return type.\"\"\"\n    __slots__ = ()\n\n    @abstractmethod\n    def __round__(self, ndigits: int = 0) -> T_co:\n        pass\n\n\ndef _make_nmtuple(name, types, module, defaults = ()):\n    fields = [n for n, t in types]\n    types = {n: _type_check(t, f\"field {n} annotation must be a type\")\n             for n, t in types}\n    nm_tpl = collections.namedtuple(name, fields,\n                                    defaults=defaults, module=module)\n    nm_tpl.__annotations__ = nm_tpl.__new__.__annotations__ = types\n    return nm_tpl\n\n\n# attributes prohibited to set in NamedTuple class syntax\n_prohibited = frozenset({'__new__', '__init__', '__slots__', '__getnewargs__',\n                         '_fields', '_field_defaults',\n                         '_make', '_replace', '_asdict', '_source'})\n\n_special = frozenset({'__module__', '__name__', '__annotations__'})\n\n\nclass NamedTupleMeta(type):\n\n    def __new__(cls, typename, bases, ns):\n        assert bases[0] is _NamedTuple\n        types = ns.get('__annotations__', {})\n        default_names = []\n        for field_name in types:\n            if field_name in ns:\n                default_names.append(field_name)\n            elif default_names:\n                raise TypeError(f\"Non-default namedtuple field {field_name} \"\n                                f\"cannot follow default field\"\n                                f\"{'s' if len(default_names) > 1 else ''} \"\n                                f\"{', '.join(default_names)}\")\n        nm_tpl = _make_nmtuple(typename, types.items(),\n                               defaults=[ns[n] for n in default_names],\n                               module=ns['__module__'])\n        # update from user namespace without overriding special namedtuple attributes\n        for key in ns:\n            if key in _prohibited:\n                raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n            elif key not in _special and key not in nm_tpl._fields:\n                setattr(nm_tpl, key, ns[key])\n        return nm_tpl\n\n\ndef NamedTuple(typename, fields=None, /, **kwargs):\n    \"\"\"Typed version of namedtuple.\n\n    Usage in Python versions >= 3.6::\n\n        class Employee(NamedTuple):\n            name: str\n            id: int\n\n    This is equivalent to::\n\n        Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n    The resulting class has an extra __annotations__ attribute, giving a\n    dict that maps field names to types.  (The field names are also in\n    the _fields attribute, which is part of the namedtuple API.)\n    Alternative equivalent keyword syntax is also accepted::\n\n        Employee = NamedTuple('Employee', name=str, id=int)\n\n    In Python versions <= 3.5 use::\n\n        Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n    \"\"\"\n    if fields is None:\n        fields = kwargs.items()\n    elif kwargs:\n        raise TypeError(\"Either list of fields or keywords\"\n                        \" can be provided to NamedTuple, not both\")\n    try:\n        module = sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        module = None\n    return _make_nmtuple(typename, fields, module=module)\n\n_NamedTuple = type.__new__(NamedTupleMeta, 'NamedTuple', (), {})\n\ndef _namedtuple_mro_entries(bases):\n    if len(bases) > 1:\n        raise TypeError(\"Multiple inheritance with NamedTuple is not supported\")\n    assert bases[0] is NamedTuple\n    return (_NamedTuple,)\n\nNamedTuple.__mro_entries__ = _namedtuple_mro_entries\n\n\nclass _TypedDictMeta(type):\n    def __new__(cls, name, bases, ns, total=True):\n        \"\"\"Create new typed dict class object.\n\n        This method is called when TypedDict is subclassed,\n        or when TypedDict is instantiated. This way\n        TypedDict supports all three syntax forms described in its docstring.\n        Subclasses and instances of TypedDict return actual dictionaries.\n        \"\"\"\n        for base in bases:\n            if type(base) is not _TypedDictMeta:\n                raise TypeError('cannot inherit from both a TypedDict type '\n                                'and a non-TypedDict base class')\n        tp_dict = type.__new__(_TypedDictMeta, name, (dict,), ns)\n\n        annotations = {}\n        own_annotations = ns.get('__annotations__', {})\n        own_annotation_keys = set(own_annotations.keys())\n        msg = \"TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type\"\n        own_annotations = {\n            n: _type_check(tp, msg, module=tp_dict.__module__)\n            for n, tp in own_annotations.items()\n        }\n        required_keys = set()\n        optional_keys = set()\n\n        for base in bases:\n            annotations.update(base.__dict__.get('__annotations__', {}))\n            required_keys.update(base.__dict__.get('__required_keys__', ()))\n            optional_keys.update(base.__dict__.get('__optional_keys__', ()))\n\n        annotations.update(own_annotations)\n        if total:\n            required_keys.update(own_annotation_keys)\n        else:\n            optional_keys.update(own_annotation_keys)\n\n        tp_dict.__annotations__ = annotations\n        tp_dict.__required_keys__ = frozenset(required_keys)\n        tp_dict.__optional_keys__ = frozenset(optional_keys)\n        if not hasattr(tp_dict, '__total__'):\n            tp_dict.__total__ = total\n        return tp_dict\n\n    __call__ = dict  # static method\n\n    def __subclasscheck__(cls, other):\n        # Typed dicts are only for static structural subtyping.\n        raise TypeError('TypedDict does not support instance and class checks')\n\n    __instancecheck__ = __subclasscheck__\n\n\ndef TypedDict(typename, fields=None, /, *, total=True, **kwargs):\n    \"\"\"A simple typed namespace. At runtime it is equivalent to a plain dict.\n\n    TypedDict creates a dictionary type that expects all of its\n    instances to have a certain set of keys, where each key is\n    associated with a value of a consistent type. This expectation\n    is not checked at runtime but is only enforced by type checkers.\n    Usage::\n\n        class Point2D(TypedDict):\n            x: int\n            y: int\n            label: str\n\n        a: Point2D = {'x': 1, 'y': 2, 'label': 'good'}  # OK\n        b: Point2D = {'z': 3, 'label': 'bad'}           # Fails type check\n\n        assert Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first')\n\n    The type info can be accessed via the Point2D.__annotations__ dict, and\n    the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.\n    TypedDict supports two additional equivalent forms::\n\n        Point2D = TypedDict('Point2D', x=int, y=int, label=str)\n        Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str})\n\n    By default, all keys must be present in a TypedDict. It is possible\n    to override this by specifying totality.\n    Usage::\n\n        class point2D(TypedDict, total=False):\n            x: int\n            y: int\n\n    This means that a point2D TypedDict can have any of the keys omitted.A type\n    checker is only expected to support a literal False or True as the value of\n    the total argument. True is the default, and makes all items defined in the\n    class body be required.\n\n    The class syntax is only supported in Python 3.6+, while two other\n    syntax forms work for Python 2.7 and 3.2+\n    \"\"\"\n    if fields is None:\n        fields = kwargs\n    elif kwargs:\n        raise TypeError(\"TypedDict takes either a dict or keyword arguments,\"\n                        \" but not both\")\n\n    ns = {'__annotations__': dict(fields)}\n    try:\n        # Setting correct module is necessary to make typed dict classes pickleable.\n        ns['__module__'] = sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        pass\n\n    return _TypedDictMeta(typename, (), ns, total=total)\n\n_TypedDict = type.__new__(_TypedDictMeta, 'TypedDict', (), {})\nTypedDict.__mro_entries__ = lambda bases: (_TypedDict,)\n\n\ndef NewType(name, tp):\n    \"\"\"NewType creates simple unique types with almost zero\n    runtime overhead. NewType(name, tp) is considered a subtype of tp\n    by static type checkers. At runtime, NewType(name, tp) returns\n    a dummy function that simply returns its argument. Usage::\n\n        UserId = NewType('UserId', int)\n\n        def name_by_id(user_id: UserId) -> str:\n            ...\n\n        UserId('user')          # Fails type check\n\n        name_by_id(42)          # Fails type check\n        name_by_id(UserId(42))  # OK\n\n        num = UserId(5) + 1     # type: int\n    \"\"\"\n\n    def new_type(x):\n        return x\n\n    new_type.__name__ = name\n    new_type.__supertype__ = tp\n    return new_type\n\n\n# Python-version-specific alias (Python 2: unicode; Python 3: str)\nText = str\n\n\n# Constant that's True when type checking, but False here.\nTYPE_CHECKING = False\n\n\nclass IO(Generic[AnyStr]):\n    \"\"\"Generic base class for TextIO and BinaryIO.\n\n    This is an abstract, generic version of the return of open().\n\n    NOTE: This does not distinguish between the different possible\n    classes (text vs. binary, read vs. write vs. read/write,\n    append-only, unbuffered).  The TextIO and BinaryIO subclasses\n    below capture the distinctions between text vs. binary, which is\n    pervasive in the interface; however we currently do not offer a\n    way to track the other distinctions in the type system.\n    \"\"\"\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def mode(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass\n\n    @abstractmethod\n    def close(self) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def closed(self) -> bool:\n        pass\n\n    @abstractmethod\n    def fileno(self) -> int:\n        pass\n\n    @abstractmethod\n    def flush(self) -> None:\n        pass\n\n    @abstractmethod\n    def isatty(self) -> bool:\n        pass\n\n    @abstractmethod\n    def read(self, n: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def readline(self, limit: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readlines(self, hint: int = -1) -> List[AnyStr]:\n        pass\n\n    @abstractmethod\n    def seek(self, offset: int, whence: int = 0) -> int:\n        pass\n\n    @abstractmethod\n    def seekable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def tell(self) -> int:\n        pass\n\n    @abstractmethod\n    def truncate(self, size: int = None) -> int:\n        pass\n\n    @abstractmethod\n    def writable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def write(self, s: AnyStr) -> int:\n        pass\n\n    @abstractmethod\n    def writelines(self, lines: List[AnyStr]) -> None:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'IO[AnyStr]':\n        pass\n\n    @abstractmethod\n    def __exit__(self, type, value, traceback) -> None:\n        pass\n\n\nclass BinaryIO(IO[bytes]):\n    \"\"\"Typed version of the return of open() in binary mode.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def write(self, s: Union[bytes, bytearray]) -> int:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'BinaryIO':\n        pass\n\n\nclass TextIO(IO[str]):\n    \"\"\"Typed version of the return of open() in text mode.\"\"\"\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def buffer(self) -> BinaryIO:\n        pass\n\n    @property\n    @abstractmethod\n    def encoding(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def errors(self) -> Optional[str]:\n        pass\n\n    @property\n    @abstractmethod\n    def line_buffering(self) -> bool:\n        pass\n\n    @property\n    @abstractmethod\n    def newlines(self) -> Any:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'TextIO':\n        pass\n\n\nclass io:\n    \"\"\"Wrapper namespace for IO generic classes.\"\"\"\n\n    __all__ = ['IO', 'TextIO', 'BinaryIO']\n    IO = IO\n    TextIO = TextIO\n    BinaryIO = BinaryIO\n\n\nio.__name__ = __name__ + '.io'\nsys.modules[io.__name__] = io\n\nPattern = _alias(stdlib_re.Pattern, 1)\nMatch = _alias(stdlib_re.Match, 1)\n\nclass re:\n    \"\"\"Wrapper namespace for re type aliases.\"\"\"\n\n    __all__ = ['Pattern', 'Match']\n    Pattern = Pattern\n    Match = Match\n\n\nre.__name__ = __name__ + '.re'\nsys.modules[re.__name__] = re\n", 2257], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py": ["import abc\nimport collections\nimport collections.abc\nimport functools\nimport inspect\nimport operator\nimport sys\nimport types as _types\nimport typing\nimport warnings\n\n__all__ = [\n    # Super-special typing primitives.\n    'Any',\n    'ClassVar',\n    'Concatenate',\n    'Final',\n    'LiteralString',\n    'ParamSpec',\n    'ParamSpecArgs',\n    'ParamSpecKwargs',\n    'Self',\n    'Type',\n    'TypeVar',\n    'TypeVarTuple',\n    'Unpack',\n\n    # ABCs (from collections.abc).\n    'Awaitable',\n    'AsyncIterator',\n    'AsyncIterable',\n    'Coroutine',\n    'AsyncGenerator',\n    'AsyncContextManager',\n    'Buffer',\n    'ChainMap',\n\n    # Concrete collection types.\n    'ContextManager',\n    'Counter',\n    'Deque',\n    'DefaultDict',\n    'NamedTuple',\n    'OrderedDict',\n    'TypedDict',\n\n    # Structural checks, a.k.a. protocols.\n    'SupportsAbs',\n    'SupportsBytes',\n    'SupportsComplex',\n    'SupportsFloat',\n    'SupportsIndex',\n    'SupportsInt',\n    'SupportsRound',\n\n    # One-off things.\n    'Annotated',\n    'assert_never',\n    'assert_type',\n    'clear_overloads',\n    'dataclass_transform',\n    'deprecated',\n    'Doc',\n    'get_overloads',\n    'final',\n    'get_args',\n    'get_origin',\n    'get_original_bases',\n    'get_protocol_members',\n    'get_type_hints',\n    'IntVar',\n    'is_protocol',\n    'is_typeddict',\n    'Literal',\n    'NewType',\n    'overload',\n    'override',\n    'Protocol',\n    'reveal_type',\n    'runtime',\n    'runtime_checkable',\n    'Text',\n    'TypeAlias',\n    'TypeAliasType',\n    'TypeGuard',\n    'TYPE_CHECKING',\n    'Never',\n    'NoReturn',\n    'Required',\n    'NotRequired',\n\n    # Pure aliases, have always been in typing\n    'AbstractSet',\n    'AnyStr',\n    'BinaryIO',\n    'Callable',\n    'Collection',\n    'Container',\n    'Dict',\n    'ForwardRef',\n    'FrozenSet',\n    'Generator',\n    'Generic',\n    'Hashable',\n    'IO',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'List',\n    'Mapping',\n    'MappingView',\n    'Match',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'Optional',\n    'Pattern',\n    'Reversible',\n    'Sequence',\n    'Set',\n    'Sized',\n    'TextIO',\n    'Tuple',\n    'Union',\n    'ValuesView',\n    'cast',\n    'no_type_check',\n    'no_type_check_decorator',\n]\n\n# for backward compatibility\nPEP_560 = True\nGenericMeta = type\n\n# The functions below are modified copies of typing internal helpers.\n# They are needed by _ProtocolMeta and they provide support for PEP 646.\n\n\nclass _Sentinel:\n    def __repr__(self):\n        return \"<sentinel>\"\n\n\n_marker = _Sentinel()\n\n\ndef _check_generic(cls, parameters, elen=_marker):\n    \"\"\"Check correct count for parameters of a generic cls (internal helper).\n    This gives a nice error message in case of count mismatch.\n    \"\"\"\n    if not elen:\n        raise TypeError(f\"{cls} is not a generic class\")\n    if elen is _marker:\n        if not hasattr(cls, \"__parameters__\") or not cls.__parameters__:\n            raise TypeError(f\"{cls} is not a generic class\")\n        elen = len(cls.__parameters__)\n    alen = len(parameters)\n    if alen != elen:\n        if hasattr(cls, \"__parameters__\"):\n            parameters = [p for p in cls.__parameters__ if not _is_unpack(p)]\n            num_tv_tuples = sum(isinstance(p, TypeVarTuple) for p in parameters)\n            if (num_tv_tuples > 0) and (alen >= elen - num_tv_tuples):\n                return\n        raise TypeError(f\"Too {'many' if alen > elen else 'few'} parameters for {cls};\"\n                        f\" actual {alen}, expected {elen}\")\n\n\nif sys.version_info >= (3, 10):\n    def _should_collect_from_parameters(t):\n        return isinstance(\n            t, (typing._GenericAlias, _types.GenericAlias, _types.UnionType)\n        )\nelif sys.version_info >= (3, 9):\n    def _should_collect_from_parameters(t):\n        return isinstance(t, (typing._GenericAlias, _types.GenericAlias))\nelse:\n    def _should_collect_from_parameters(t):\n        return isinstance(t, typing._GenericAlias) and not t._special\n\n\ndef _collect_type_vars(types, typevar_types=None):\n    \"\"\"Collect all type variable contained in types in order of\n    first appearance (lexicographic order). For example::\n\n        _collect_type_vars((T, List[S, T])) == (T, S)\n    \"\"\"\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if (\n            isinstance(t, typevar_types) and\n            t not in tvars and\n            not _is_unpack(t)\n        ):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)\n\n\nNoReturn = typing.NoReturn\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = typing.TypeVar('T')  # Any type.\nKT = typing.TypeVar('KT')  # Key type.\nVT = typing.TypeVar('VT')  # Value type.\nT_co = typing.TypeVar('T_co', covariant=True)  # Any type covariant containers.\nT_contra = typing.TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n\n\nif sys.version_info >= (3, 11):\n    from typing import Any\nelse:\n\n    class _AnyMeta(type):\n        def __instancecheck__(self, obj):\n            if self is Any:\n                raise TypeError(\"typing_extensions.Any cannot be used with isinstance()\")\n            return super().__instancecheck__(obj)\n\n        def __repr__(self):\n            if self is Any:\n                return \"typing_extensions.Any\"\n            return super().__repr__()\n\n    class Any(metaclass=_AnyMeta):\n        \"\"\"Special type indicating an unconstrained type.\n        - Any is compatible with every type.\n        - Any assumed to have all methods.\n        - All values assumed to be instances of Any.\n        Note that all the above statements are true from the point of view of\n        static type checkers. At runtime, Any should not be used with instance\n        checks.\n        \"\"\"\n        def __new__(cls, *args, **kwargs):\n            if cls is Any:\n                raise TypeError(\"Any cannot be instantiated\")\n            return super().__new__(cls, *args, **kwargs)\n\n\nClassVar = typing.ClassVar\n\n\nclass _ExtensionsSpecialForm(typing._SpecialForm, _root=True):\n    def __repr__(self):\n        return 'typing_extensions.' + self._name\n\n\nFinal = typing.Final\n\nif sys.version_info >= (3, 11):\n    final = typing.final\nelse:\n    # @final exists in 3.8+, but we backport it for all versions\n    # before 3.11 to keep support for the __final__ attribute.\n    # See https://bugs.python.org/issue46342\n    def final(f):\n        \"\"\"This decorator can be used to indicate to type checkers that\n        the decorated method cannot be overridden, and decorated class\n        cannot be subclassed. For example:\n\n            class Base:\n                @final\n                def done(self) -> None:\n                    ...\n            class Sub(Base):\n                def done(self) -> None:  # Error reported by type checker\n                    ...\n            @final\n            class Leaf:\n                ...\n            class Other(Leaf):  # Error reported by type checker\n                ...\n\n        There is no runtime checking of these properties. The decorator\n        sets the ``__final__`` attribute to ``True`` on the decorated object\n        to allow runtime introspection.\n        \"\"\"\n        try:\n            f.__final__ = True\n        except (AttributeError, TypeError):\n            # Skip the attribute silently if it is not writable.\n            # AttributeError happens if the object has __slots__ or a\n            # read-only property, TypeError if it's a builtin class.\n            pass\n        return f\n\n\ndef IntVar(name):\n    return typing.TypeVar(name)\n\n\n# A Literal bug was fixed in 3.11.0, 3.10.1 and 3.9.8\nif sys.version_info >= (3, 10, 1):\n    Literal = typing.Literal\nelse:\n    def _flatten_literal_params(parameters):\n        \"\"\"An internal helper for Literal creation: flatten Literals among parameters\"\"\"\n        params = []\n        for p in parameters:\n            if isinstance(p, _LiteralGenericAlias):\n                params.extend(p.__args__)\n            else:\n                params.append(p)\n        return tuple(params)\n\n    def _value_and_type_iter(params):\n        for p in params:\n            yield p, type(p)\n\n    class _LiteralGenericAlias(typing._GenericAlias, _root=True):\n        def __eq__(self, other):\n            if not isinstance(other, _LiteralGenericAlias):\n                return NotImplemented\n            these_args_deduped = set(_value_and_type_iter(self.__args__))\n            other_args_deduped = set(_value_and_type_iter(other.__args__))\n            return these_args_deduped == other_args_deduped\n\n        def __hash__(self):\n            return hash(frozenset(_value_and_type_iter(self.__args__)))\n\n    class _LiteralForm(_ExtensionsSpecialForm, _root=True):\n        def __init__(self, doc: str):\n            self._name = 'Literal'\n            self._doc = self.__doc__ = doc\n\n        def __getitem__(self, parameters):\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n\n            parameters = _flatten_literal_params(parameters)\n\n            val_type_pairs = list(_value_and_type_iter(parameters))\n            try:\n                deduped_pairs = set(val_type_pairs)\n            except TypeError:\n                # unhashable parameters\n                pass\n            else:\n                # similar logic to typing._deduplicate on Python 3.9+\n                if len(deduped_pairs) < len(val_type_pairs):\n                    new_parameters = []\n                    for pair in val_type_pairs:\n                        if pair in deduped_pairs:\n                            new_parameters.append(pair[0])\n                            deduped_pairs.remove(pair)\n                    assert not deduped_pairs, deduped_pairs\n                    parameters = tuple(new_parameters)\n\n            return _LiteralGenericAlias(self, parameters)\n\n    Literal = _LiteralForm(doc=\"\"\"\\\n                           A type that can be used to indicate to type checkers\n                           that the corresponding value has a value literally equivalent\n                           to the provided parameter. For example:\n\n                               var: Literal[4] = 4\n\n                           The type checker understands that 'var' is literally equal to\n                           the value 4 and no other value.\n\n                           Literal[...] cannot be subclassed. There is no runtime\n                           checking verifying that the parameter is actually a value\n                           instead of a type.\"\"\")\n\n\n_overload_dummy = typing._overload_dummy\n\n\nif hasattr(typing, \"get_overloads\"):  # 3.11+\n    overload = typing.overload\n    get_overloads = typing.get_overloads\n    clear_overloads = typing.clear_overloads\nelse:\n    # {module: {qualname: {firstlineno: func}}}\n    _overload_registry = collections.defaultdict(\n        functools.partial(collections.defaultdict, dict)\n    )\n\n    def overload(func):\n        \"\"\"Decorator for overloaded functions/methods.\n\n        In a stub file, place two or more stub definitions for the same\n        function in a row, each decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n        In a non-stub file (i.e. a regular .py file), do the same but\n        follow it with an implementation.  The implementation should *not*\n        be decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            # implementation goes here\n\n        The overloads for a function can be retrieved at runtime using the\n        get_overloads() function.\n        \"\"\"\n        # classmethod and staticmethod\n        f = getattr(func, \"__func__\", func)\n        try:\n            _overload_registry[f.__module__][f.__qualname__][\n                f.__code__.co_firstlineno\n            ] = func\n        except AttributeError:\n            # Not a normal function; ignore.\n            pass\n        return _overload_dummy\n\n    def get_overloads(func):\n        \"\"\"Return all defined overloads for *func* as a sequence.\"\"\"\n        # classmethod and staticmethod\n        f = getattr(func, \"__func__\", func)\n        if f.__module__ not in _overload_registry:\n            return []\n        mod_dict = _overload_registry[f.__module__]\n        if f.__qualname__ not in mod_dict:\n            return []\n        return list(mod_dict[f.__qualname__].values())\n\n    def clear_overloads():\n        \"\"\"Clear all overloads in the registry.\"\"\"\n        _overload_registry.clear()\n\n\n# This is not a real generic class.  Don't use outside annotations.\nType = typing.Type\n\n# Various ABCs mimicking those in collections.abc.\n# A few are simply re-exported for completeness.\nAwaitable = typing.Awaitable\nCoroutine = typing.Coroutine\nAsyncIterable = typing.AsyncIterable\nAsyncIterator = typing.AsyncIterator\nDeque = typing.Deque\nContextManager = typing.ContextManager\nAsyncContextManager = typing.AsyncContextManager\nDefaultDict = typing.DefaultDict\nOrderedDict = typing.OrderedDict\nCounter = typing.Counter\nChainMap = typing.ChainMap\nAsyncGenerator = typing.AsyncGenerator\nText = typing.Text\nTYPE_CHECKING = typing.TYPE_CHECKING\n\n\n_PROTO_ALLOWLIST = {\n    'collections.abc': [\n        'Callable', 'Awaitable', 'Iterable', 'Iterator', 'AsyncIterable',\n        'Hashable', 'Sized', 'Container', 'Collection', 'Reversible', 'Buffer',\n    ],\n    'contextlib': ['AbstractContextManager', 'AbstractAsyncContextManager'],\n    'typing_extensions': ['Buffer'],\n}\n\n\n_EXCLUDED_ATTRS = {\n    \"__abstractmethods__\", \"__annotations__\", \"__weakref__\", \"_is_protocol\",\n    \"_is_runtime_protocol\", \"__dict__\", \"__slots__\", \"__parameters__\",\n    \"__orig_bases__\", \"__module__\", \"_MutableMapping__marker\", \"__doc__\",\n    \"__subclasshook__\", \"__orig_class__\", \"__init__\", \"__new__\",\n    \"__protocol_attrs__\", \"__callable_proto_members_only__\",\n}\n\nif sys.version_info >= (3, 9):\n    _EXCLUDED_ATTRS.add(\"__class_getitem__\")\n\nif sys.version_info >= (3, 12):\n    _EXCLUDED_ATTRS.add(\"__type_params__\")\n\n_EXCLUDED_ATTRS = frozenset(_EXCLUDED_ATTRS)\n\n\ndef _get_protocol_attrs(cls):\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in {'Protocol', 'Generic'}:\n            continue\n        annotations = getattr(base, '__annotations__', {})\n        for attr in (*base.__dict__, *annotations):\n            if (not attr.startswith('_abc_') and attr not in _EXCLUDED_ATTRS):\n                attrs.add(attr)\n    return attrs\n\n\ndef _caller(depth=2):\n    try:\n        return sys._getframe(depth).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):  # For platforms without _getframe()\n        return None\n\n\n# The performance of runtime-checkable protocols is significantly improved on Python 3.12,\n# so we backport the 3.12 version of Protocol to Python <=3.11\nif sys.version_info >= (3, 12):\n    Protocol = typing.Protocol\nelse:\n    def _allow_reckless_class_checks(depth=3):\n        \"\"\"Allow instance and class checks for special stdlib modules.\n        The abc and functools modules indiscriminately call isinstance() and\n        issubclass() on the whole MRO of a user class, which may contain protocols.\n        \"\"\"\n        return _caller(depth) in {'abc', 'functools', None}\n\n    def _no_init(self, *args, **kwargs):\n        if type(self)._is_protocol:\n            raise TypeError('Protocols cannot be instantiated')\n\n    # Inheriting from typing._ProtocolMeta isn't actually desirable,\n    # but is necessary to allow typing.Protocol and typing_extensions.Protocol\n    # to mix without getting TypeErrors about \"metaclass conflict\"\n    class _ProtocolMeta(type(typing.Protocol)):\n        # This metaclass is somewhat unfortunate,\n        # but is necessary for several reasons...\n        #\n        # NOTE: DO NOT call super() in any methods in this class\n        # That would call the methods on typing._ProtocolMeta on Python 3.8-3.11\n        # and those are slow\n        def __new__(mcls, name, bases, namespace, **kwargs):\n            if name == \"Protocol\" and len(bases) < 2:\n                pass\n            elif {Protocol, typing.Protocol} & set(bases):\n                for base in bases:\n                    if not (\n                        base in {object, typing.Generic, Protocol, typing.Protocol}\n                        or base.__name__ in _PROTO_ALLOWLIST.get(base.__module__, [])\n                        or is_protocol(base)\n                    ):\n                        raise TypeError(\n                            f\"Protocols can only inherit from other protocols, \"\n                            f\"got {base!r}\"\n                        )\n            return abc.ABCMeta.__new__(mcls, name, bases, namespace, **kwargs)\n\n        def __init__(cls, *args, **kwargs):\n            abc.ABCMeta.__init__(cls, *args, **kwargs)\n            if getattr(cls, \"_is_protocol\", False):\n                cls.__protocol_attrs__ = _get_protocol_attrs(cls)\n                # PEP 544 prohibits using issubclass()\n                # with protocols that have non-method members.\n                cls.__callable_proto_members_only__ = all(\n                    callable(getattr(cls, attr, None)) for attr in cls.__protocol_attrs__\n                )\n\n        def __subclasscheck__(cls, other):\n            if cls is Protocol:\n                return type.__subclasscheck__(cls, other)\n            if (\n                getattr(cls, '_is_protocol', False)\n                and not _allow_reckless_class_checks()\n            ):\n                if not isinstance(other, type):\n                    # Same error message as for issubclass(1, int).\n                    raise TypeError('issubclass() arg 1 must be a class')\n                if (\n                    not cls.__callable_proto_members_only__\n                    and cls.__dict__.get(\"__subclasshook__\") is _proto_hook\n                ):\n                    raise TypeError(\n                        \"Protocols with non-method members don't support issubclass()\"\n                    )\n                if not getattr(cls, '_is_runtime_protocol', False):\n                    raise TypeError(\n                        \"Instance and class checks can only be used with \"\n                        \"@runtime_checkable protocols\"\n                    )\n            return abc.ABCMeta.__subclasscheck__(cls, other)\n\n        def __instancecheck__(cls, instance):\n            # We need this method for situations where attributes are\n            # assigned in __init__.\n            if cls is Protocol:\n                return type.__instancecheck__(cls, instance)\n            if not getattr(cls, \"_is_protocol\", False):\n                # i.e., it's a concrete subclass of a protocol\n                return abc.ABCMeta.__instancecheck__(cls, instance)\n\n            if (\n                not getattr(cls, '_is_runtime_protocol', False) and\n                not _allow_reckless_class_checks()\n            ):\n                raise TypeError(\"Instance and class checks can only be used with\"\n                                \" @runtime_checkable protocols\")\n\n            if abc.ABCMeta.__instancecheck__(cls, instance):\n                return True\n\n            for attr in cls.__protocol_attrs__:\n                try:\n                    val = inspect.getattr_static(instance, attr)\n                except AttributeError:\n                    break\n                if val is None and callable(getattr(cls, attr, None)):\n                    break\n            else:\n                return True\n\n            return False\n\n        def __eq__(cls, other):\n            # Hack so that typing.Generic.__class_getitem__\n            # treats typing_extensions.Protocol\n            # as equivalent to typing.Protocol\n            if abc.ABCMeta.__eq__(cls, other) is True:\n                return True\n            return cls is Protocol and other is typing.Protocol\n\n        # This has to be defined, or the abc-module cache\n        # complains about classes with this metaclass being unhashable,\n        # if we define only __eq__!\n        def __hash__(cls) -> int:\n            return type.__hash__(cls)\n\n    @classmethod\n    def _proto_hook(cls, other):\n        if not cls.__dict__.get('_is_protocol', False):\n            return NotImplemented\n\n        for attr in cls.__protocol_attrs__:\n            for base in other.__mro__:\n                # Check if the members appears in the class dictionary...\n                if attr in base.__dict__:\n                    if base.__dict__[attr] is None:\n                        return NotImplemented\n                    break\n\n                # ...or in annotations, if it is a sub-protocol.\n                annotations = getattr(base, '__annotations__', {})\n                if (\n                    isinstance(annotations, collections.abc.Mapping)\n                    and attr in annotations\n                    and is_protocol(other)\n                ):\n                    break\n            else:\n                return NotImplemented\n        return True\n\n    class Protocol(typing.Generic, metaclass=_ProtocolMeta):\n        __doc__ = typing.Protocol.__doc__\n        __slots__ = ()\n        _is_protocol = True\n        _is_runtime_protocol = False\n\n        def __init_subclass__(cls, *args, **kwargs):\n            super().__init_subclass__(*args, **kwargs)\n\n            # Determine if this is a protocol or a concrete subclass.\n            if not cls.__dict__.get('_is_protocol', False):\n                cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n            # Set (or override) the protocol subclass hook.\n            if '__subclasshook__' not in cls.__dict__:\n                cls.__subclasshook__ = _proto_hook\n\n            # Prohibit instantiation for protocol classes\n            if cls._is_protocol and cls.__init__ is Protocol.__init__:\n                cls.__init__ = _no_init\n\n\n# The \"runtime\" alias exists for backwards compatibility.\nruntime = runtime_checkable = typing.runtime_checkable\n\n\n# Our version of runtime-checkable protocols is faster on Python 3.8-3.11\nif sys.version_info >= (3, 12):\n    SupportsInt = typing.SupportsInt\n    SupportsFloat = typing.SupportsFloat\n    SupportsComplex = typing.SupportsComplex\n    SupportsBytes = typing.SupportsBytes\n    SupportsIndex = typing.SupportsIndex\n    SupportsAbs = typing.SupportsAbs\n    SupportsRound = typing.SupportsRound\nelse:\n    @runtime_checkable\n    class SupportsInt(Protocol):\n        \"\"\"An ABC with one abstract method __int__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __int__(self) -> int:\n            pass\n\n    @runtime_checkable\n    class SupportsFloat(Protocol):\n        \"\"\"An ABC with one abstract method __float__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __float__(self) -> float:\n            pass\n\n    @runtime_checkable\n    class SupportsComplex(Protocol):\n        \"\"\"An ABC with one abstract method __complex__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __complex__(self) -> complex:\n            pass\n\n    @runtime_checkable\n    class SupportsBytes(Protocol):\n        \"\"\"An ABC with one abstract method __bytes__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __bytes__(self) -> bytes:\n            pass\n\n    @runtime_checkable\n    class SupportsIndex(Protocol):\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __index__(self) -> int:\n            pass\n\n    @runtime_checkable\n    class SupportsAbs(Protocol[T_co]):\n        \"\"\"\n        An ABC with one abstract method __abs__ that is covariant in its return type.\n        \"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __abs__(self) -> T_co:\n            pass\n\n    @runtime_checkable\n    class SupportsRound(Protocol[T_co]):\n        \"\"\"\n        An ABC with one abstract method __round__ that is covariant in its return type.\n        \"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __round__(self, ndigits: int = 0) -> T_co:\n            pass\n\n\ndef _ensure_subclassable(mro_entries):\n    def inner(func):\n        if sys.implementation.name == \"pypy\" and sys.version_info < (3, 9):\n            cls_dict = {\n                \"__call__\": staticmethod(func),\n                \"__mro_entries__\": staticmethod(mro_entries)\n            }\n            t = type(func.__name__, (), cls_dict)\n            return functools.update_wrapper(t(), func)\n        else:\n            func.__mro_entries__ = mro_entries\n            return func\n    return inner\n\n\nif sys.version_info >= (3, 13):\n    # The standard library TypedDict in Python 3.8 does not store runtime information\n    # about which (if any) keys are optional.  See https://bugs.python.org/issue38834\n    # The standard library TypedDict in Python 3.9.0/1 does not honour the \"total\"\n    # keyword with old-style TypedDict().  See https://bugs.python.org/issue42059\n    # The standard library TypedDict below Python 3.11 does not store runtime\n    # information about optional and required keys when using Required or NotRequired.\n    # Generic TypedDicts are also impossible using typing.TypedDict on Python <3.11.\n    # Aaaand on 3.12 we add __orig_bases__ to TypedDict\n    # to enable better runtime introspection.\n    # On 3.13 we deprecate some odd ways of creating TypedDicts.\n    TypedDict = typing.TypedDict\n    _TypedDictMeta = typing._TypedDictMeta\n    is_typeddict = typing.is_typeddict\nelse:\n    # 3.10.0 and later\n    _TAKES_MODULE = \"module\" in inspect.signature(typing._type_check).parameters\n\n    class _TypedDictMeta(type):\n        def __new__(cls, name, bases, ns, total=True):\n            \"\"\"Create new typed dict class object.\n\n            This method is called when TypedDict is subclassed,\n            or when TypedDict is instantiated. This way\n            TypedDict supports all three syntax forms described in its docstring.\n            Subclasses and instances of TypedDict return actual dictionaries.\n            \"\"\"\n            for base in bases:\n                if type(base) is not _TypedDictMeta and base is not typing.Generic:\n                    raise TypeError('cannot inherit from both a TypedDict type '\n                                    'and a non-TypedDict base class')\n\n            if any(issubclass(b, typing.Generic) for b in bases):\n                generic_base = (typing.Generic,)\n            else:\n                generic_base = ()\n\n            # typing.py generally doesn't let you inherit from plain Generic, unless\n            # the name of the class happens to be \"Protocol\"\n            tp_dict = type.__new__(_TypedDictMeta, \"Protocol\", (*generic_base, dict), ns)\n            tp_dict.__name__ = name\n            if tp_dict.__qualname__ == \"Protocol\":\n                tp_dict.__qualname__ = name\n\n            if not hasattr(tp_dict, '__orig_bases__'):\n                tp_dict.__orig_bases__ = bases\n\n            annotations = {}\n            own_annotations = ns.get('__annotations__', {})\n            msg = \"TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type\"\n            if _TAKES_MODULE:\n                own_annotations = {\n                    n: typing._type_check(tp, msg, module=tp_dict.__module__)\n                    for n, tp in own_annotations.items()\n                }\n            else:\n                own_annotations = {\n                    n: typing._type_check(tp, msg)\n                    for n, tp in own_annotations.items()\n                }\n            required_keys = set()\n            optional_keys = set()\n\n            for base in bases:\n                annotations.update(base.__dict__.get('__annotations__', {}))\n                required_keys.update(base.__dict__.get('__required_keys__', ()))\n                optional_keys.update(base.__dict__.get('__optional_keys__', ()))\n\n            annotations.update(own_annotations)\n            for annotation_key, annotation_type in own_annotations.items():\n                annotation_origin = get_origin(annotation_type)\n                if annotation_origin is Annotated:\n                    annotation_args = get_args(annotation_type)\n                    if annotation_args:\n                        annotation_type = annotation_args[0]\n                        annotation_origin = get_origin(annotation_type)\n\n                if annotation_origin is Required:\n                    required_keys.add(annotation_key)\n                elif annotation_origin is NotRequired:\n                    optional_keys.add(annotation_key)\n                elif total:\n                    required_keys.add(annotation_key)\n                else:\n                    optional_keys.add(annotation_key)\n\n            tp_dict.__annotations__ = annotations\n            tp_dict.__required_keys__ = frozenset(required_keys)\n            tp_dict.__optional_keys__ = frozenset(optional_keys)\n            if not hasattr(tp_dict, '__total__'):\n                tp_dict.__total__ = total\n            return tp_dict\n\n        __call__ = dict  # static method\n\n        def __subclasscheck__(cls, other):\n            # Typed dicts are only for static structural subtyping.\n            raise TypeError('TypedDict does not support instance and class checks')\n\n        __instancecheck__ = __subclasscheck__\n\n    _TypedDict = type.__new__(_TypedDictMeta, 'TypedDict', (), {})\n\n    @_ensure_subclassable(lambda bases: (_TypedDict,))\n    def TypedDict(typename, fields=_marker, /, *, total=True, **kwargs):\n        \"\"\"A simple typed namespace. At runtime it is equivalent to a plain dict.\n\n        TypedDict creates a dictionary type such that a type checker will expect all\n        instances to have a certain set of keys, where each key is\n        associated with a value of a consistent type. This expectation\n        is not checked at runtime.\n\n        Usage::\n\n            class Point2D(TypedDict):\n                x: int\n                y: int\n                label: str\n\n            a: Point2D = {'x': 1, 'y': 2, 'label': 'good'}  # OK\n            b: Point2D = {'z': 3, 'label': 'bad'}           # Fails type check\n\n            assert Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first')\n\n        The type info can be accessed via the Point2D.__annotations__ dict, and\n        the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.\n        TypedDict supports an additional equivalent form::\n\n            Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str})\n\n        By default, all keys must be present in a TypedDict. It is possible\n        to override this by specifying totality::\n\n            class Point2D(TypedDict, total=False):\n                x: int\n                y: int\n\n        This means that a Point2D TypedDict can have any of the keys omitted. A type\n        checker is only expected to support a literal False or True as the value of\n        the total argument. True is the default, and makes all items defined in the\n        class body be required.\n\n        The Required and NotRequired special forms can also be used to mark\n        individual keys as being required or not required::\n\n            class Point2D(TypedDict):\n                x: int  # the \"x\" key must always be present (Required is the default)\n                y: NotRequired[int]  # the \"y\" key can be omitted\n\n        See PEP 655 for more details on Required and NotRequired.\n        \"\"\"\n        if fields is _marker or fields is None:\n            if fields is _marker:\n                deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n            else:\n                deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n\n            example = f\"`{typename} = TypedDict({typename!r}, {{}})`\"\n            deprecation_msg = (\n                f\"{deprecated_thing} is deprecated and will be disallowed in \"\n                \"Python 3.15. To create a TypedDict class with 0 fields \"\n                \"using the functional syntax, pass an empty dictionary, e.g. \"\n            ) + example + \".\"\n            warnings.warn(deprecation_msg, DeprecationWarning, stacklevel=2)\n            fields = kwargs\n        elif kwargs:\n            raise TypeError(\"TypedDict takes either a dict or keyword arguments,\"\n                            \" but not both\")\n        if kwargs:\n            warnings.warn(\n                \"The kwargs-based syntax for TypedDict definitions is deprecated \"\n                \"in Python 3.11, will be removed in Python 3.13, and may not be \"\n                \"understood by third-party type checkers.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        ns = {'__annotations__': dict(fields)}\n        module = _caller()\n        if module is not None:\n            # Setting correct module is necessary to make typed dict classes pickleable.\n            ns['__module__'] = module\n\n        td = _TypedDictMeta(typename, (), ns, total=total)\n        td.__orig_bases__ = (TypedDict,)\n        return td\n\n    if hasattr(typing, \"_TypedDictMeta\"):\n        _TYPEDDICT_TYPES = (typing._TypedDictMeta, _TypedDictMeta)\n    else:\n        _TYPEDDICT_TYPES = (_TypedDictMeta,)\n\n    def is_typeddict(tp):\n        \"\"\"Check if an annotation is a TypedDict class\n\n        For example::\n            class Film(TypedDict):\n                title: str\n                year: int\n\n            is_typeddict(Film)  # => True\n            is_typeddict(Union[list, str])  # => False\n        \"\"\"\n        # On 3.8, this would otherwise return True\n        if hasattr(typing, \"TypedDict\") and tp is typing.TypedDict:\n            return False\n        return isinstance(tp, _TYPEDDICT_TYPES)\n\n\nif hasattr(typing, \"assert_type\"):\n    assert_type = typing.assert_type\n\nelse:\n    def assert_type(val, typ, /):\n        \"\"\"Assert (to the type checker) that the value is of the given type.\n\n        When the type checker encounters a call to assert_type(), it\n        emits an error if the value is not of the specified type::\n\n            def greet(name: str) -> None:\n                assert_type(name, str)  # ok\n                assert_type(name, int)  # type checker error\n\n        At runtime this returns the first argument unchanged and otherwise\n        does nothing.\n        \"\"\"\n        return val\n\n\nif hasattr(typing, \"Required\"):  # 3.11+\n    get_type_hints = typing.get_type_hints\nelse:  # <=3.10\n    # replaces _strip_annotations()\n    def _strip_extras(t):\n        \"\"\"Strips Annotated, Required and NotRequired from a given type.\"\"\"\n        if isinstance(t, _AnnotatedAlias):\n            return _strip_extras(t.__origin__)\n        if hasattr(t, \"__origin__\") and t.__origin__ in (Required, NotRequired):\n            return _strip_extras(t.__args__[0])\n        if isinstance(t, typing._GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return t.copy_with(stripped_args)\n        if hasattr(_types, \"GenericAlias\") and isinstance(t, _types.GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return _types.GenericAlias(t.__origin__, stripped_args)\n        if hasattr(_types, \"UnionType\") and isinstance(t, _types.UnionType):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return functools.reduce(operator.or_, stripped_args)\n\n        return t\n\n    def get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n        \"\"\"Return type hints for an object.\n\n        This is often the same as obj.__annotations__, but it handles\n        forward references encoded as string literals, adds Optional[t] if a\n        default value equal to None is set and recursively replaces all\n        'Annotated[T, ...]', 'Required[T]' or 'NotRequired[T]' with 'T'\n        (unless 'include_extras=True').\n\n        The argument may be a module, class, method, or function. The annotations\n        are returned as a dictionary. For classes, annotations include also\n        inherited members.\n\n        TypeError is raised if the argument is not of a type that can contain\n        annotations, and an empty dictionary is returned if no annotations are\n        present.\n\n        BEWARE -- the behavior of globalns and localns is counterintuitive\n        (unless you are familiar with how eval() and exec() work).  The\n        search order is locals first, then globals.\n\n        - If no dict arguments are passed, an attempt is made to use the\n          globals from obj (or the respective module's globals for classes),\n          and these are also used as the locals.  If the object does not appear\n          to have globals, an empty dictionary is used.\n\n        - If one dict argument is passed, it is used for both globals and\n          locals.\n\n        - If two dict arguments are passed, they specify globals and\n          locals, respectively.\n        \"\"\"\n        if hasattr(typing, \"Annotated\"):  # 3.9+\n            hint = typing.get_type_hints(\n                obj, globalns=globalns, localns=localns, include_extras=True\n            )\n        else:  # 3.8\n            hint = typing.get_type_hints(obj, globalns=globalns, localns=localns)\n        if include_extras:\n            return hint\n        return {k: _strip_extras(t) for k, t in hint.items()}\n\n\n# Python 3.9+ has PEP 593 (Annotated)\nif hasattr(typing, 'Annotated'):\n    Annotated = typing.Annotated\n    # Not exported and not a public API, but needed for get_origin() and get_args()\n    # to work.\n    _AnnotatedAlias = typing._AnnotatedAlias\n# 3.8\nelse:\n    class _AnnotatedAlias(typing._GenericAlias, _root=True):\n        \"\"\"Runtime representation of an annotated type.\n\n        At its core 'Annotated[t, dec1, dec2, ...]' is an alias for the type 't'\n        with extra annotations. The alias behaves like a normal typing alias,\n        instantiating is the same as instantiating the underlying type, binding\n        it to types is also the same.\n        \"\"\"\n        def __init__(self, origin, metadata):\n            if isinstance(origin, _AnnotatedAlias):\n                metadata = origin.__metadata__ + metadata\n                origin = origin.__origin__\n            super().__init__(origin, origin)\n            self.__metadata__ = metadata\n\n        def copy_with(self, params):\n            assert len(params) == 1\n            new_type = params[0]\n            return _AnnotatedAlias(new_type, self.__metadata__)\n\n        def __repr__(self):\n            return (f\"typing_extensions.Annotated[{typing._type_repr(self.__origin__)}, \"\n                    f\"{', '.join(repr(a) for a in self.__metadata__)}]\")\n\n        def __reduce__(self):\n            return operator.getitem, (\n                Annotated, (self.__origin__,) + self.__metadata__\n            )\n\n        def __eq__(self, other):\n            if not isinstance(other, _AnnotatedAlias):\n                return NotImplemented\n            if self.__origin__ != other.__origin__:\n                return False\n            return self.__metadata__ == other.__metadata__\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__metadata__))\n\n    class Annotated:\n        \"\"\"Add context specific metadata to a type.\n\n        Example: Annotated[int, runtime_check.Unsigned] indicates to the\n        hypothetical runtime_check module that this type is an unsigned int.\n        Every other consumer of this type can ignore this metadata and treat\n        this type as int.\n\n        The first argument to Annotated must be a valid type (and will be in\n        the __origin__ field), the remaining arguments are kept as a tuple in\n        the __extra__ field.\n\n        Details:\n\n        - It's an error to call `Annotated` with less than two arguments.\n        - Nested Annotated are flattened::\n\n            Annotated[Annotated[T, Ann1, Ann2], Ann3] == Annotated[T, Ann1, Ann2, Ann3]\n\n        - Instantiating an annotated type is equivalent to instantiating the\n        underlying type::\n\n            Annotated[C, Ann1](5) == C(5)\n\n        - Annotated can be used as a generic type alias::\n\n            Optimized = Annotated[T, runtime.Optimize()]\n            Optimized[int] == Annotated[int, runtime.Optimize()]\n\n            OptimizedList = Annotated[List[T], runtime.Optimize()]\n            OptimizedList[int] == Annotated[List[int], runtime.Optimize()]\n        \"\"\"\n\n        __slots__ = ()\n\n        def __new__(cls, *args, **kwargs):\n            raise TypeError(\"Type Annotated cannot be instantiated.\")\n\n        @typing._tp_cache\n        def __class_getitem__(cls, params):\n            if not isinstance(params, tuple) or len(params) < 2:\n                raise TypeError(\"Annotated[...] should be used \"\n                                \"with at least two arguments (a type and an \"\n                                \"annotation).\")\n            allowed_special_forms = (ClassVar, Final)\n            if get_origin(params[0]) in allowed_special_forms:\n                origin = params[0]\n            else:\n                msg = \"Annotated[t, ...]: t must be a type.\"\n                origin = typing._type_check(params[0], msg)\n            metadata = tuple(params[1:])\n            return _AnnotatedAlias(origin, metadata)\n\n        def __init_subclass__(cls, *args, **kwargs):\n            raise TypeError(\n                f\"Cannot subclass {cls.__module__}.Annotated\"\n            )\n\n# Python 3.8 has get_origin() and get_args() but those implementations aren't\n# Annotated-aware, so we can't use those. Python 3.9's versions don't support\n# ParamSpecArgs and ParamSpecKwargs, so only Python 3.10's versions will do.\nif sys.version_info[:2] >= (3, 10):\n    get_origin = typing.get_origin\n    get_args = typing.get_args\n# 3.8-3.9\nelse:\n    try:\n        # 3.9+\n        from typing import _BaseGenericAlias\n    except ImportError:\n        _BaseGenericAlias = typing._GenericAlias\n    try:\n        # 3.9+\n        from typing import GenericAlias as _typing_GenericAlias\n    except ImportError:\n        _typing_GenericAlias = typing._GenericAlias\n\n    def get_origin(tp):\n        \"\"\"Get the unsubscripted version of a type.\n\n        This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar\n        and Annotated. Return None for unsupported types. Examples::\n\n            get_origin(Literal[42]) is Literal\n            get_origin(int) is None\n            get_origin(ClassVar[int]) is ClassVar\n            get_origin(Generic) is Generic\n            get_origin(Generic[T]) is Generic\n            get_origin(Union[T, int]) is Union\n            get_origin(List[Tuple[T, T]][int]) == list\n            get_origin(P.args) is P\n        \"\"\"\n        if isinstance(tp, _AnnotatedAlias):\n            return Annotated\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias, _BaseGenericAlias,\n                           ParamSpecArgs, ParamSpecKwargs)):\n            return tp.__origin__\n        if tp is typing.Generic:\n            return typing.Generic\n        return None\n\n    def get_args(tp):\n        \"\"\"Get type arguments with all substitutions performed.\n\n        For unions, basic simplifications used by Union constructor are performed.\n        Examples::\n            get_args(Dict[str, int]) == (str, int)\n            get_args(int) == ()\n            get_args(Union[int, Union[T, int], str][int]) == (int, str)\n            get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n            get_args(Callable[[], T][int]) == ([], int)\n        \"\"\"\n        if isinstance(tp, _AnnotatedAlias):\n            return (tp.__origin__,) + tp.__metadata__\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias)):\n            if getattr(tp, \"_special\", False):\n                return ()\n            res = tp.__args__\n            if get_origin(tp) is collections.abc.Callable and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return ()\n\n\n# 3.10+\nif hasattr(typing, 'TypeAlias'):\n    TypeAlias = typing.TypeAlias\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def TypeAlias(self, parameters):\n        \"\"\"Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It's invalid when used anywhere except as in the example above.\n        \"\"\"\n        raise TypeError(f\"{self} is not subscriptable\")\n# 3.8\nelse:\n    TypeAlias = _ExtensionsSpecialForm(\n        'TypeAlias',\n        doc=\"\"\"Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It's invalid when used anywhere except as in the example\n        above.\"\"\"\n    )\n\n\ndef _set_default(type_param, default):\n    if isinstance(default, (tuple, list)):\n        type_param.__default__ = tuple((typing._type_check(d, \"Default must be a type\")\n                                        for d in default))\n    elif default != _marker:\n        if isinstance(type_param, ParamSpec) and default is ...:  # ... not valid <3.11\n            type_param.__default__ = default\n        else:\n            type_param.__default__ = typing._type_check(default, \"Default must be a type\")\n    else:\n        type_param.__default__ = None\n\n\ndef _set_module(typevarlike):\n    # for pickling:\n    def_mod = _caller(depth=3)\n    if def_mod != 'typing_extensions':\n        typevarlike.__module__ = def_mod\n\n\nclass _DefaultMixin:\n    \"\"\"Mixin for TypeVarLike defaults.\"\"\"\n\n    __slots__ = ()\n    __init__ = _set_default\n\n\n# Classes using this metaclass must provide a _backported_typevarlike ClassVar\nclass _TypeVarLikeMeta(type):\n    def __instancecheck__(cls, __instance: Any) -> bool:\n        return isinstance(__instance, cls._backported_typevarlike)\n\n\n# Add default and infer_variance parameters from PEP 696 and 695\nclass TypeVar(metaclass=_TypeVarLikeMeta):\n    \"\"\"Type variable.\"\"\"\n\n    _backported_typevarlike = typing.TypeVar\n\n    def __new__(cls, name, *constraints, bound=None,\n                covariant=False, contravariant=False,\n                default=_marker, infer_variance=False):\n        if hasattr(typing, \"TypeAliasType\"):\n            # PEP 695 implemented (3.12+), can pass infer_variance to typing.TypeVar\n            typevar = typing.TypeVar(name, *constraints, bound=bound,\n                                     covariant=covariant, contravariant=contravariant,\n                                     infer_variance=infer_variance)\n        else:\n            typevar = typing.TypeVar(name, *constraints, bound=bound,\n                                     covariant=covariant, contravariant=contravariant)\n            if infer_variance and (covariant or contravariant):\n                raise ValueError(\"Variance cannot be specified with infer_variance.\")\n            typevar.__infer_variance__ = infer_variance\n        _set_default(typevar, default)\n        _set_module(typevar)\n        return typevar\n\n    def __init_subclass__(cls) -> None:\n        raise TypeError(f\"type '{__name__}.TypeVar' is not an acceptable base type\")\n\n\n# Python 3.10+ has PEP 612\nif hasattr(typing, 'ParamSpecArgs'):\n    ParamSpecArgs = typing.ParamSpecArgs\n    ParamSpecKwargs = typing.ParamSpecKwargs\n# 3.8-3.9\nelse:\n    class _Immutable:\n        \"\"\"Mixin to indicate that object should not be copied.\"\"\"\n        __slots__ = ()\n\n        def __copy__(self):\n            return self\n\n        def __deepcopy__(self, memo):\n            return self\n\n    class ParamSpecArgs(_Immutable):\n        \"\"\"The args for a ParamSpec object.\n\n        Given a ParamSpec object P, P.args is an instance of ParamSpecArgs.\n\n        ParamSpecArgs objects have a reference back to their ParamSpec:\n\n        P.args.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        \"\"\"\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f\"{self.__origin__.__name__}.args\"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecArgs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n    class ParamSpecKwargs(_Immutable):\n        \"\"\"The kwargs for a ParamSpec object.\n\n        Given a ParamSpec object P, P.kwargs is an instance of ParamSpecKwargs.\n\n        ParamSpecKwargs objects have a reference back to their ParamSpec:\n\n        P.kwargs.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        \"\"\"\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f\"{self.__origin__.__name__}.kwargs\"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecKwargs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n# 3.10+\nif hasattr(typing, 'ParamSpec'):\n\n    # Add default parameter - PEP 696\n    class ParamSpec(metaclass=_TypeVarLikeMeta):\n        \"\"\"Parameter specification.\"\"\"\n\n        _backported_typevarlike = typing.ParamSpec\n\n        def __new__(cls, name, *, bound=None,\n                    covariant=False, contravariant=False,\n                    infer_variance=False, default=_marker):\n            if hasattr(typing, \"TypeAliasType\"):\n                # PEP 695 implemented, can pass infer_variance to typing.TypeVar\n                paramspec = typing.ParamSpec(name, bound=bound,\n                                             covariant=covariant,\n                                             contravariant=contravariant,\n                                             infer_variance=infer_variance)\n            else:\n                paramspec = typing.ParamSpec(name, bound=bound,\n                                             covariant=covariant,\n                                             contravariant=contravariant)\n                paramspec.__infer_variance__ = infer_variance\n\n            _set_default(paramspec, default)\n            _set_module(paramspec)\n            return paramspec\n\n        def __init_subclass__(cls) -> None:\n            raise TypeError(f\"type '{__name__}.ParamSpec' is not an acceptable base type\")\n\n# 3.8-3.9\nelse:\n\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class ParamSpec(list, _DefaultMixin):\n        \"\"\"Parameter specification variable.\n\n        Usage::\n\n           P = ParamSpec('P')\n\n        Parameter specification variables exist primarily for the benefit of static\n        type checkers.  They are used to forward the parameter types of one\n        callable to another callable, a pattern commonly found in higher order\n        functions and decorators.  They are only valid when used in ``Concatenate``,\n        or s the first argument to ``Callable``. In Python 3.10 and higher,\n        they are also supported in user-defined Generics at runtime.\n        See class Generic for more information on generic types.  An\n        example for annotating a decorator::\n\n           T = TypeVar('T')\n           P = ParamSpec('P')\n\n           def add_logging(f: Callable[P, T]) -> Callable[P, T]:\n               '''A type-safe decorator to add logging to a function.'''\n               def inner(*args: P.args, **kwargs: P.kwargs) -> T:\n                   logging.info(f'{f.__name__} was called')\n                   return f(*args, **kwargs)\n               return inner\n\n           @add_logging\n           def add_two(x: float, y: float) -> float:\n               '''Add two numbers together.'''\n               return x + y\n\n        Parameter specification variables defined with covariant=True or\n        contravariant=True can be used to declare covariant or contravariant\n        generic types.  These keyword arguments are valid, but their actual semantics\n        are yet to be decided.  See PEP 612 for details.\n\n        Parameter specification variables can be introspected. e.g.:\n\n           P.__name__ == 'T'\n           P.__bound__ == None\n           P.__covariant__ == False\n           P.__contravariant__ == False\n\n        Note that only parameter specification variables defined in global scope can\n        be pickled.\n        \"\"\"\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        @property\n        def args(self):\n            return ParamSpecArgs(self)\n\n        @property\n        def kwargs(self):\n            return ParamSpecKwargs(self)\n\n        def __init__(self, name, *, bound=None, covariant=False, contravariant=False,\n                     infer_variance=False, default=_marker):\n            super().__init__([self])\n            self.__name__ = name\n            self.__covariant__ = bool(covariant)\n            self.__contravariant__ = bool(contravariant)\n            self.__infer_variance__ = bool(infer_variance)\n            if bound:\n                self.__bound__ = typing._type_check(bound, 'Bound must be a type.')\n            else:\n                self.__bound__ = None\n            _DefaultMixin.__init__(self, default)\n\n            # for pickling:\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n        def __repr__(self):\n            if self.__infer_variance__:\n                prefix = ''\n            elif self.__covariant__:\n                prefix = '+'\n            elif self.__contravariant__:\n                prefix = '-'\n            else:\n                prefix = '~'\n            return prefix + self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        # Hack to get typing._type_check to pass.\n        def __call__(self, *args, **kwargs):\n            pass\n\n\n# 3.8-3.9\nif not hasattr(typing, 'Concatenate'):\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class _ConcatenateGenericAlias(list):\n\n        # Trick Generic into looking into this for __parameters__.\n        __class__ = typing._GenericAlias\n\n        # Flag in 3.8.\n        _special = False\n\n        def __init__(self, origin, args):\n            super().__init__(args)\n            self.__origin__ = origin\n            self.__args__ = args\n\n        def __repr__(self):\n            _type_repr = typing._type_repr\n            return (f'{_type_repr(self.__origin__)}'\n                    f'[{\", \".join(_type_repr(arg) for arg in self.__args__)}]')\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__args__))\n\n        # Hack to get typing._type_check to pass in Generic.\n        def __call__(self, *args, **kwargs):\n            pass\n\n        @property\n        def __parameters__(self):\n            return tuple(\n                tp for tp in self.__args__ if isinstance(tp, (typing.TypeVar, ParamSpec))\n            )\n\n\n# 3.8-3.9\n@typing._tp_cache\ndef _concatenate_getitem(self, parameters):\n    if parameters == ():\n        raise TypeError(\"Cannot take a Concatenate of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    if not isinstance(parameters[-1], ParamSpec):\n        raise TypeError(\"The last parameter to Concatenate should be a \"\n                        \"ParamSpec variable.\")\n    msg = \"Concatenate[arg, ...]: each arg must be a type.\"\n    parameters = tuple(typing._type_check(p, msg) for p in parameters)\n    return _ConcatenateGenericAlias(self, parameters)\n\n\n# 3.10+\nif hasattr(typing, 'Concatenate'):\n    Concatenate = typing.Concatenate\n    _ConcatenateGenericAlias = typing._ConcatenateGenericAlias  # noqa: F811\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def Concatenate(self, parameters):\n        \"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        \"\"\"\n        return _concatenate_getitem(self, parameters)\n# 3.8\nelse:\n    class _ConcatenateForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            return _concatenate_getitem(self, parameters)\n\n    Concatenate = _ConcatenateForm(\n        'Concatenate',\n        doc=\"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        \"\"\")\n\n# 3.10+\nif hasattr(typing, 'TypeGuard'):\n    TypeGuard = typing.TypeGuard\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def TypeGuard(self, parameters):\n        \"\"\"Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # \"isinstance\" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        \"\"\"\n        item = typing._type_check(parameters, f'{self} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n# 3.8\nelse:\n    class _TypeGuardForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type')\n            return typing._GenericAlias(self, (item,))\n\n    TypeGuard = _TypeGuardForm(\n        'TypeGuard',\n        doc=\"\"\"Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # \"isinstance\" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        \"\"\")\n\n\n# Vendored from cpython typing._SpecialFrom\nclass _SpecialForm(typing._Final, _root=True):\n    __slots__ = ('_name', '__doc__', '_getitem')\n\n    def __init__(self, getitem):\n        self._getitem = getitem\n        self._name = getitem.__name__\n        self.__doc__ = getitem.__doc__\n\n    def __getattr__(self, item):\n        if item in {'__name__', '__qualname__'}:\n            return self._name\n\n        raise AttributeError(item)\n\n    def __mro_entries__(self, bases):\n        raise TypeError(f\"Cannot subclass {self!r}\")\n\n    def __repr__(self):\n        return f'typing_extensions.{self._name}'\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f\"Cannot instantiate {self!r}\")\n\n    def __or__(self, other):\n        return typing.Union[self, other]\n\n    def __ror__(self, other):\n        return typing.Union[other, self]\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f\"{self} cannot be used with isinstance()\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f\"{self} cannot be used with issubclass()\")\n\n    @typing._tp_cache\n    def __getitem__(self, parameters):\n        return self._getitem(self, parameters)\n\n\nif hasattr(typing, \"LiteralString\"):  # 3.11+\n    LiteralString = typing.LiteralString\nelse:\n    @_SpecialForm\n    def LiteralString(self, params):\n        \"\"\"Represents an arbitrary literal string.\n\n        Example::\n\n          from typing_extensions import LiteralString\n\n          def query(sql: LiteralString) -> ...:\n              ...\n\n          query(\"SELECT * FROM table\")  # ok\n          query(f\"SELECT * FROM {input()}\")  # not ok\n\n        See PEP 675 for details.\n\n        \"\"\"\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, \"Self\"):  # 3.11+\n    Self = typing.Self\nelse:\n    @_SpecialForm\n    def Self(self, params):\n        \"\"\"Used to spell the type of \"self\" in classes.\n\n        Example::\n\n          from typing import Self\n\n          class ReturnsSelf:\n              def parse(self, data: bytes) -> Self:\n                  ...\n                  return self\n\n        \"\"\"\n\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, \"Never\"):  # 3.11+\n    Never = typing.Never\nelse:\n    @_SpecialForm\n    def Never(self, params):\n        \"\"\"The bottom type, a type that has no members.\n\n        This can be used to define a function that should never be\n        called, or a function that never returns::\n\n            from typing_extensions import Never\n\n            def never_call_me(arg: Never) -> None:\n                pass\n\n            def int_or_str(arg: int | str) -> None:\n                never_call_me(arg)  # type checker error\n                match arg:\n                    case int():\n                        print(\"It's an int\")\n                    case str():\n                        print(\"It's a str\")\n                    case _:\n                        never_call_me(arg)  # ok, arg is of type Never\n\n        \"\"\"\n\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, 'Required'):  # 3.11+\n    Required = typing.Required\n    NotRequired = typing.NotRequired\nelif sys.version_info[:2] >= (3, 9):  # 3.9-3.10\n    @_ExtensionsSpecialForm\n    def Required(self, parameters):\n        \"\"\"A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        \"\"\"\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n\n    @_ExtensionsSpecialForm\n    def NotRequired(self, parameters):\n        \"\"\"A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n        \"\"\"\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n\nelse:  # 3.8\n    class _RequiredForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return typing._GenericAlias(self, (item,))\n\n    Required = _RequiredForm(\n        'Required',\n        doc=\"\"\"A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        \"\"\")\n    NotRequired = _RequiredForm(\n        'NotRequired',\n        doc=\"\"\"A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n        \"\"\")\n\n\n_UNPACK_DOC = \"\"\"\\\nType unpack operator.\n\nThe type unpack operator takes the child types from some container type,\nsuch as `tuple[int, str]` or a `TypeVarTuple`, and 'pulls them out'. For\nexample:\n\n  # For some generic class `Foo`:\n  Foo[Unpack[tuple[int, str]]]  # Equivalent to Foo[int, str]\n\n  Ts = TypeVarTuple('Ts')\n  # Specifies that `Bar` is generic in an arbitrary number of types.\n  # (Think of `Ts` as a tuple of an arbitrary number of individual\n  #  `TypeVar`s, which the `Unpack` is 'pulling out' directly into the\n  #  `Generic[]`.)\n  class Bar(Generic[Unpack[Ts]]): ...\n  Bar[int]  # Valid\n  Bar[int, str]  # Also valid\n\nFrom Python 3.11, this can also be done using the `*` operator:\n\n    Foo[*tuple[int, str]]\n    class Bar(Generic[*Ts]): ...\n\nThe operator can also be used along with a `TypedDict` to annotate\n`**kwargs` in a function signature. For instance:\n\n  class Movie(TypedDict):\n    name: str\n    year: int\n\n  # This function expects two keyword arguments - *name* of type `str` and\n  # *year* of type `int`.\n  def foo(**kwargs: Unpack[Movie]): ...\n\nNote that there is only some runtime checking of this operator. Not\neverything the runtime allows may be accepted by static type checkers.\n\nFor more information, see PEP 646 and PEP 692.\n\"\"\"\n\n\nif sys.version_info >= (3, 12):  # PEP 692 changed the repr of Unpack[]\n    Unpack = typing.Unpack\n\n    def _is_unpack(obj):\n        return get_origin(obj) is Unpack\n\nelif sys.version_info[:2] >= (3, 9):  # 3.9+\n    class _UnpackSpecialForm(_ExtensionsSpecialForm, _root=True):\n        def __init__(self, getitem):\n            super().__init__(getitem)\n            self.__doc__ = _UNPACK_DOC\n\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n    @_UnpackSpecialForm\n    def Unpack(self, parameters):\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return _UnpackAlias(self, (item,))\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\nelse:  # 3.8\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n    class _UnpackForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return _UnpackAlias(self, (item,))\n\n    Unpack = _UnpackForm('Unpack', doc=_UNPACK_DOC)\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\n\nif hasattr(typing, \"TypeVarTuple\"):  # 3.11+\n\n    # Add default parameter - PEP 696\n    class TypeVarTuple(metaclass=_TypeVarLikeMeta):\n        \"\"\"Type variable tuple.\"\"\"\n\n        _backported_typevarlike = typing.TypeVarTuple\n\n        def __new__(cls, name, *, default=_marker):\n            tvt = typing.TypeVarTuple(name)\n            _set_default(tvt, default)\n            _set_module(tvt)\n            return tvt\n\n        def __init_subclass__(self, *args, **kwds):\n            raise TypeError(\"Cannot subclass special typing classes\")\n\nelse:  # <=3.10\n    class TypeVarTuple(_DefaultMixin):\n        \"\"\"Type variable tuple.\n\n        Usage::\n\n            Ts = TypeVarTuple('Ts')\n\n        In the same way that a normal type variable is a stand-in for a single\n        type such as ``int``, a type variable *tuple* is a stand-in for a *tuple*\n        type such as ``Tuple[int, str]``.\n\n        Type variable tuples can be used in ``Generic`` declarations.\n        Consider the following example::\n\n            class Array(Generic[*Ts]): ...\n\n        The ``Ts`` type variable tuple here behaves like ``tuple[T1, T2]``,\n        where ``T1`` and ``T2`` are type variables. To use these type variables\n        as type parameters of ``Array``, we must *unpack* the type variable tuple using\n        the star operator: ``*Ts``. The signature of ``Array`` then behaves\n        as if we had simply written ``class Array(Generic[T1, T2]): ...``.\n        In contrast to ``Generic[T1, T2]``, however, ``Generic[*Shape]`` allows\n        us to parameterise the class with an *arbitrary* number of type parameters.\n\n        Type variable tuples can be used anywhere a normal ``TypeVar`` can.\n        This includes class definitions, as shown above, as well as function\n        signatures and variable annotations::\n\n            class Array(Generic[*Ts]):\n\n                def __init__(self, shape: Tuple[*Ts]):\n                    self._shape: Tuple[*Ts] = shape\n\n                def get_shape(self) -> Tuple[*Ts]:\n                    return self._shape\n\n            shape = (Height(480), Width(640))\n            x: Array[Height, Width] = Array(shape)\n            y = abs(x)  # Inferred type is Array[Height, Width]\n            z = x + x   #        ...    is Array[Height, Width]\n            x.get_shape()  #     ...    is tuple[Height, Width]\n\n        \"\"\"\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        def __iter__(self):\n            yield self.__unpacked__\n\n        def __init__(self, name, *, default=_marker):\n            self.__name__ = name\n            _DefaultMixin.__init__(self, default)\n\n            # for pickling:\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n            self.__unpacked__ = Unpack[self]\n\n        def __repr__(self):\n            return self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        def __init_subclass__(self, *args, **kwds):\n            if '_root' not in kwds:\n                raise TypeError(\"Cannot subclass special typing classes\")\n\n\nif hasattr(typing, \"reveal_type\"):  # 3.11+\n    reveal_type = typing.reveal_type\nelse:  # <=3.10\n    def reveal_type(obj: T, /) -> T:\n        \"\"\"Reveal the inferred type of a variable.\n\n        When a static type checker encounters a call to ``reveal_type()``,\n        it will emit the inferred type of the argument::\n\n            x: int = 1\n            reveal_type(x)\n\n        Running a static type checker (e.g., ``mypy``) on this example\n        will produce output similar to 'Revealed type is \"builtins.int\"'.\n\n        At runtime, the function prints the runtime type of the\n        argument and returns it unchanged.\n\n        \"\"\"\n        print(f\"Runtime type is {type(obj).__name__!r}\", file=sys.stderr)\n        return obj\n\n\nif hasattr(typing, \"assert_never\"):  # 3.11+\n    assert_never = typing.assert_never\nelse:  # <=3.10\n    def assert_never(arg: Never, /) -> Never:\n        \"\"\"Assert to the type checker that a line of code is unreachable.\n\n        Example::\n\n            def int_or_str(arg: int | str) -> None:\n                match arg:\n                    case int():\n                        print(\"It's an int\")\n                    case str():\n                        print(\"It's a str\")\n                    case _:\n                        assert_never(arg)\n\n        If a type checker finds that a call to assert_never() is\n        reachable, it will emit an error.\n\n        At runtime, this throws an exception when called.\n\n        \"\"\"\n        raise AssertionError(\"Expected code to be unreachable\")\n\n\nif sys.version_info >= (3, 12):  # 3.12+\n    # dataclass_transform exists in 3.11 but lacks the frozen_default parameter\n    dataclass_transform = typing.dataclass_transform\nelse:  # <=3.11\n    def dataclass_transform(\n        *,\n        eq_default: bool = True,\n        order_default: bool = False,\n        kw_only_default: bool = False,\n        frozen_default: bool = False,\n        field_specifiers: typing.Tuple[\n            typing.Union[typing.Type[typing.Any], typing.Callable[..., typing.Any]],\n            ...\n        ] = (),\n        **kwargs: typing.Any,\n    ) -> typing.Callable[[T], T]:\n        \"\"\"Decorator that marks a function, class, or metaclass as providing\n        dataclass-like behavior.\n\n        Example:\n\n            from typing_extensions import dataclass_transform\n\n            _T = TypeVar(\"_T\")\n\n            # Used on a decorator function\n            @dataclass_transform()\n            def create_model(cls: type[_T]) -> type[_T]:\n                ...\n                return cls\n\n            @create_model\n            class CustomerModel:\n                id: int\n                name: str\n\n            # Used on a base class\n            @dataclass_transform()\n            class ModelBase: ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n            # Used on a metaclass\n            @dataclass_transform()\n            class ModelMeta(type): ...\n\n            class ModelBase(metaclass=ModelMeta): ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n        Each of the ``CustomerModel`` classes defined in this example will now\n        behave similarly to a dataclass created with the ``@dataclasses.dataclass``\n        decorator. For example, the type checker will synthesize an ``__init__``\n        method.\n\n        The arguments to this decorator can be used to customize this behavior:\n        - ``eq_default`` indicates whether the ``eq`` parameter is assumed to be\n          True or False if it is omitted by the caller.\n        - ``order_default`` indicates whether the ``order`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``kw_only_default`` indicates whether the ``kw_only`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``frozen_default`` indicates whether the ``frozen`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``field_specifiers`` specifies a static list of supported classes\n          or functions that describe fields, similar to ``dataclasses.field()``.\n\n        At runtime, this decorator records its arguments in the\n        ``__dataclass_transform__`` attribute on the decorated object.\n\n        See PEP 681 for details.\n\n        \"\"\"\n        def decorator(cls_or_fn):\n            cls_or_fn.__dataclass_transform__ = {\n                \"eq_default\": eq_default,\n                \"order_default\": order_default,\n                \"kw_only_default\": kw_only_default,\n                \"frozen_default\": frozen_default,\n                \"field_specifiers\": field_specifiers,\n                \"kwargs\": kwargs,\n            }\n            return cls_or_fn\n        return decorator\n\n\nif hasattr(typing, \"override\"):  # 3.12+\n    override = typing.override\nelse:  # <=3.11\n    _F = typing.TypeVar(\"_F\", bound=typing.Callable[..., typing.Any])\n\n    def override(arg: _F, /) -> _F:\n        \"\"\"Indicate that a method is intended to override a method in a base class.\n\n        Usage:\n\n            class Base:\n                def method(self) -> None: ...\n                    pass\n\n            class Child(Base):\n                @override\n                def method(self) -> None:\n                    super().method()\n\n        When this decorator is applied to a method, the type checker will\n        validate that it overrides a method with the same name on a base class.\n        This helps prevent bugs that may occur when a base class is changed\n        without an equivalent change to a child class.\n\n        There is no runtime checking of these properties. The decorator\n        sets the ``__override__`` attribute to ``True`` on the decorated object\n        to allow runtime introspection.\n\n        See PEP 698 for details.\n\n        \"\"\"\n        try:\n            arg.__override__ = True\n        except (AttributeError, TypeError):\n            # Skip the attribute silently if it is not writable.\n            # AttributeError happens if the object has __slots__ or a\n            # read-only property, TypeError if it's a builtin class.\n            pass\n        return arg\n\n\nif hasattr(typing, \"deprecated\"):\n    deprecated = typing.deprecated\nelse:\n    _T = typing.TypeVar(\"_T\")\n\n    def deprecated(\n        msg: str,\n        /,\n        *,\n        category: typing.Optional[typing.Type[Warning]] = DeprecationWarning,\n        stacklevel: int = 1,\n    ) -> typing.Callable[[_T], _T]:\n        \"\"\"Indicate that a class, function or overload is deprecated.\n\n        Usage:\n\n            @deprecated(\"Use B instead\")\n            class A:\n                pass\n\n            @deprecated(\"Use g instead\")\n            def f():\n                pass\n\n            @overload\n            @deprecated(\"int support is deprecated\")\n            def g(x: int) -> int: ...\n            @overload\n            def g(x: str) -> int: ...\n\n        When this decorator is applied to an object, the type checker\n        will generate a diagnostic on usage of the deprecated object.\n\n        The warning specified by ``category`` will be emitted on use\n        of deprecated objects. For functions, that happens on calls;\n        for classes, on instantiation. If the ``category`` is ``None``,\n        no warning is emitted. The ``stacklevel`` determines where the\n        warning is emitted. If it is ``1`` (the default), the warning\n        is emitted at the direct caller of the deprecated object; if it\n        is higher, it is emitted further up the stack.\n\n        The decorator sets the ``__deprecated__``\n        attribute on the decorated object to the deprecation message\n        passed to the decorator. If applied to an overload, the decorator\n        must be after the ``@overload`` decorator for the attribute to\n        exist on the overload as returned by ``get_overloads()``.\n\n        See PEP 702 for details.\n\n        \"\"\"\n        def decorator(arg: _T, /) -> _T:\n            if category is None:\n                arg.__deprecated__ = msg\n                return arg\n            elif isinstance(arg, type):\n                original_new = arg.__new__\n                has_init = arg.__init__ is not object.__init__\n\n                @functools.wraps(original_new)\n                def __new__(cls, *args, **kwargs):\n                    warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n                    if original_new is not object.__new__:\n                        return original_new(cls, *args, **kwargs)\n                    # Mirrors a similar check in object.__new__.\n                    elif not has_init and (args or kwargs):\n                        raise TypeError(f\"{cls.__name__}() takes no arguments\")\n                    else:\n                        return original_new(cls)\n\n                arg.__new__ = staticmethod(__new__)\n                arg.__deprecated__ = __new__.__deprecated__ = msg\n                return arg\n            elif callable(arg):\n                @functools.wraps(arg)\n                def wrapper(*args, **kwargs):\n                    warnings.warn(msg, category=category, stacklevel=stacklevel + 1)\n                    return arg(*args, **kwargs)\n\n                arg.__deprecated__ = wrapper.__deprecated__ = msg\n                return wrapper\n            else:\n                raise TypeError(\n                    \"@deprecated decorator with non-None category must be applied to \"\n                    f\"a class or callable, not {arg!r}\"\n                )\n\n        return decorator\n\n\n# We have to do some monkey patching to deal with the dual nature of\n# Unpack/TypeVarTuple:\n# - We want Unpack to be a kind of TypeVar so it gets accepted in\n#   Generic[Unpack[Ts]]\n# - We want it to *not* be treated as a TypeVar for the purposes of\n#   counting generic parameters, so that when we subscript a generic,\n#   the runtime doesn't try to substitute the Unpack with the subscripted type.\nif not hasattr(typing, \"TypeVarTuple\"):\n    typing._collect_type_vars = _collect_type_vars\n    typing._check_generic = _check_generic\n\n\n# Backport typing.NamedTuple as it exists in Python 3.13.\n# In 3.11, the ability to define generic `NamedTuple`s was supported.\n# This was explicitly disallowed in 3.9-3.10, and only half-worked in <=3.8.\n# On 3.12, we added __orig_bases__ to call-based NamedTuples\n# On 3.13, we deprecated kwargs-based NamedTuples\nif sys.version_info >= (3, 13):\n    NamedTuple = typing.NamedTuple\nelse:\n    def _make_nmtuple(name, types, module, defaults=()):\n        fields = [n for n, t in types]\n        annotations = {n: typing._type_check(t, f\"field {n} annotation must be a type\")\n                       for n, t in types}\n        nm_tpl = collections.namedtuple(name, fields,\n                                        defaults=defaults, module=module)\n        nm_tpl.__annotations__ = nm_tpl.__new__.__annotations__ = annotations\n        # The `_field_types` attribute was removed in 3.9;\n        # in earlier versions, it is the same as the `__annotations__` attribute\n        if sys.version_info < (3, 9):\n            nm_tpl._field_types = annotations\n        return nm_tpl\n\n    _prohibited_namedtuple_fields = typing._prohibited\n    _special_namedtuple_fields = frozenset({'__module__', '__name__', '__annotations__'})\n\n    class _NamedTupleMeta(type):\n        def __new__(cls, typename, bases, ns):\n            assert _NamedTuple in bases\n            for base in bases:\n                if base is not _NamedTuple and base is not typing.Generic:\n                    raise TypeError(\n                        'can only inherit from a NamedTuple type and Generic')\n            bases = tuple(tuple if base is _NamedTuple else base for base in bases)\n            types = ns.get('__annotations__', {})\n            default_names = []\n            for field_name in types:\n                if field_name in ns:\n                    default_names.append(field_name)\n                elif default_names:\n                    raise TypeError(f\"Non-default namedtuple field {field_name} \"\n                                    f\"cannot follow default field\"\n                                    f\"{'s' if len(default_names) > 1 else ''} \"\n                                    f\"{', '.join(default_names)}\")\n            nm_tpl = _make_nmtuple(\n                typename, types.items(),\n                defaults=[ns[n] for n in default_names],\n                module=ns['__module__']\n            )\n            nm_tpl.__bases__ = bases\n            if typing.Generic in bases:\n                if hasattr(typing, '_generic_class_getitem'):  # 3.12+\n                    nm_tpl.__class_getitem__ = classmethod(typing._generic_class_getitem)\n                else:\n                    class_getitem = typing.Generic.__class_getitem__.__func__\n                    nm_tpl.__class_getitem__ = classmethod(class_getitem)\n            # update from user namespace without overriding special namedtuple attributes\n            for key in ns:\n                if key in _prohibited_namedtuple_fields:\n                    raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n                elif key not in _special_namedtuple_fields and key not in nm_tpl._fields:\n                    setattr(nm_tpl, key, ns[key])\n            if typing.Generic in bases:\n                nm_tpl.__init_subclass__()\n            return nm_tpl\n\n    _NamedTuple = type.__new__(_NamedTupleMeta, 'NamedTuple', (), {})\n\n    def _namedtuple_mro_entries(bases):\n        assert NamedTuple in bases\n        return (_NamedTuple,)\n\n    @_ensure_subclassable(_namedtuple_mro_entries)\n    def NamedTuple(typename, fields=_marker, /, **kwargs):\n        \"\"\"Typed version of namedtuple.\n\n        Usage::\n\n            class Employee(NamedTuple):\n                name: str\n                id: int\n\n        This is equivalent to::\n\n            Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n        The resulting class has an extra __annotations__ attribute, giving a\n        dict that maps field names to types.  (The field names are also in\n        the _fields attribute, which is part of the namedtuple API.)\n        An alternative equivalent functional syntax is also accepted::\n\n            Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n        \"\"\"\n        if fields is _marker:\n            if kwargs:\n                deprecated_thing = \"Creating NamedTuple classes using keyword arguments\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"Use the class-based or functional syntax instead.\"\n                )\n            else:\n                deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n                example = f\"`{typename} = NamedTuple({typename!r}, [])`\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"To create a NamedTuple class with 0 fields \"\n                    \"using the functional syntax, \"\n                    \"pass an empty list, e.g. \"\n                ) + example + \".\"\n        elif fields is None:\n            if kwargs:\n                raise TypeError(\n                    \"Cannot pass `None` as the 'fields' parameter \"\n                    \"and also specify fields using keyword arguments\"\n                )\n            else:\n                deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n                example = f\"`{typename} = NamedTuple({typename!r}, [])`\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"To create a NamedTuple class with 0 fields \"\n                    \"using the functional syntax, \"\n                    \"pass an empty list, e.g. \"\n                ) + example + \".\"\n        elif kwargs:\n            raise TypeError(\"Either list of fields or keywords\"\n                            \" can be provided to NamedTuple, not both\")\n        if fields is _marker or fields is None:\n            warnings.warn(\n                deprecation_msg.format(name=deprecated_thing, remove=\"3.15\"),\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            fields = kwargs.items()\n        nt = _make_nmtuple(typename, fields, module=_caller())\n        nt.__orig_bases__ = (NamedTuple,)\n        return nt\n\n\nif hasattr(collections.abc, \"Buffer\"):\n    Buffer = collections.abc.Buffer\nelse:\n    class Buffer(abc.ABC):\n        \"\"\"Base class for classes that implement the buffer protocol.\n\n        The buffer protocol allows Python objects to expose a low-level\n        memory buffer interface. Before Python 3.12, it is not possible\n        to implement the buffer protocol in pure Python code, or even\n        to check whether a class implements the buffer protocol. In\n        Python 3.12 and higher, the ``__buffer__`` method allows access\n        to the buffer protocol from Python code, and the\n        ``collections.abc.Buffer`` ABC allows checking whether a class\n        implements the buffer protocol.\n\n        To indicate support for the buffer protocol in earlier versions,\n        inherit from this ABC, either in a stub file or at runtime,\n        or use ABC registration. This ABC provides no methods, because\n        there is no Python-accessible methods shared by pre-3.12 buffer\n        classes. It is useful primarily for static checks.\n\n        \"\"\"\n\n    # As a courtesy, register the most common stdlib buffer classes.\n    Buffer.register(memoryview)\n    Buffer.register(bytearray)\n    Buffer.register(bytes)\n\n\n# Backport of types.get_original_bases, available on 3.12+ in CPython\nif hasattr(_types, \"get_original_bases\"):\n    get_original_bases = _types.get_original_bases\nelse:\n    def get_original_bases(cls, /):\n        \"\"\"Return the class's \"original\" bases prior to modification by `__mro_entries__`.\n\n        Examples::\n\n            from typing import TypeVar, Generic\n            from typing_extensions import NamedTuple, TypedDict\n\n            T = TypeVar(\"T\")\n            class Foo(Generic[T]): ...\n            class Bar(Foo[int], float): ...\n            class Baz(list[str]): ...\n            Eggs = NamedTuple(\"Eggs\", [(\"a\", int), (\"b\", str)])\n            Spam = TypedDict(\"Spam\", {\"a\": int, \"b\": str})\n\n            assert get_original_bases(Bar) == (Foo[int], float)\n            assert get_original_bases(Baz) == (list[str],)\n            assert get_original_bases(Eggs) == (NamedTuple,)\n            assert get_original_bases(Spam) == (TypedDict,)\n            assert get_original_bases(int) == (object,)\n        \"\"\"\n        try:\n            return cls.__dict__.get(\"__orig_bases__\", cls.__bases__)\n        except AttributeError:\n            raise TypeError(\n                f'Expected an instance of type, not {type(cls).__name__!r}'\n            ) from None\n\n\n# NewType is a class on Python 3.10+, making it pickleable\n# The error message for subclassing instances of NewType was improved on 3.11+\nif sys.version_info >= (3, 11):\n    NewType = typing.NewType\nelse:\n    class NewType:\n        \"\"\"NewType creates simple unique types with almost zero\n        runtime overhead. NewType(name, tp) is considered a subtype of tp\n        by static type checkers. At runtime, NewType(name, tp) returns\n        a dummy callable that simply returns its argument. Usage::\n            UserId = NewType('UserId', int)\n            def name_by_id(user_id: UserId) -> str:\n                ...\n            UserId('user')          # Fails type check\n            name_by_id(42)          # Fails type check\n            name_by_id(UserId(42))  # OK\n            num = UserId(5) + 1     # type: int\n        \"\"\"\n\n        def __call__(self, obj):\n            return obj\n\n        def __init__(self, name, tp):\n            self.__qualname__ = name\n            if '.' in name:\n                name = name.rpartition('.')[-1]\n            self.__name__ = name\n            self.__supertype__ = tp\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n        def __mro_entries__(self, bases):\n            # We defined __mro_entries__ to get a better error message\n            # if a user attempts to subclass a NewType instance. bpo-46170\n            supercls_name = self.__name__\n\n            class Dummy:\n                def __init_subclass__(cls):\n                    subcls_name = cls.__name__\n                    raise TypeError(\n                        f\"Cannot subclass an instance of NewType. \"\n                        f\"Perhaps you were looking for: \"\n                        f\"`{subcls_name} = NewType({subcls_name!r}, {supercls_name})`\"\n                    )\n\n            return (Dummy,)\n\n        def __repr__(self):\n            return f'{self.__module__}.{self.__qualname__}'\n\n        def __reduce__(self):\n            return self.__qualname__\n\n        if sys.version_info >= (3, 10):\n            # PEP 604 methods\n            # It doesn't make sense to have these methods on Python <3.10\n\n            def __or__(self, other):\n                return typing.Union[self, other]\n\n            def __ror__(self, other):\n                return typing.Union[other, self]\n\n\nif hasattr(typing, \"TypeAliasType\"):\n    TypeAliasType = typing.TypeAliasType\nelse:\n    def _is_unionable(obj):\n        \"\"\"Corresponds to is_unionable() in unionobject.c in CPython.\"\"\"\n        return obj is None or isinstance(obj, (\n            type,\n            _types.GenericAlias,\n            _types.UnionType,\n            TypeAliasType,\n        ))\n\n    class TypeAliasType:\n        \"\"\"Create named, parameterized type aliases.\n\n        This provides a backport of the new `type` statement in Python 3.12:\n\n            type ListOrSet[T] = list[T] | set[T]\n\n        is equivalent to:\n\n            T = TypeVar(\"T\")\n            ListOrSet = TypeAliasType(\"ListOrSet\", list[T] | set[T], type_params=(T,))\n\n        The name ListOrSet can then be used as an alias for the type it refers to.\n\n        The type_params argument should contain all the type parameters used\n        in the value of the type alias. If the alias is not generic, this\n        argument is omitted.\n\n        Static type checkers should only support type aliases declared using\n        TypeAliasType that follow these rules:\n\n        - The first argument (the name) must be a string literal.\n        - The TypeAliasType instance must be immediately assigned to a variable\n          of the same name. (For example, 'X = TypeAliasType(\"Y\", int)' is invalid,\n          as is 'X, Y = TypeAliasType(\"X\", int), TypeAliasType(\"Y\", int)').\n\n        \"\"\"\n\n        def __init__(self, name: str, value, *, type_params=()):\n            if not isinstance(name, str):\n                raise TypeError(\"TypeAliasType name must be a string\")\n            self.__value__ = value\n            self.__type_params__ = type_params\n\n            parameters = []\n            for type_param in type_params:\n                if isinstance(type_param, TypeVarTuple):\n                    parameters.extend(type_param)\n                else:\n                    parameters.append(type_param)\n            self.__parameters__ = tuple(parameters)\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n            # Setting this attribute closes the TypeAliasType from further modification\n            self.__name__ = name\n\n        def __setattr__(self, name: str, value: object, /) -> None:\n            if hasattr(self, \"__name__\"):\n                self._raise_attribute_error(name)\n            super().__setattr__(name, value)\n\n        def __delattr__(self, name: str, /) -> Never:\n            self._raise_attribute_error(name)\n\n        def _raise_attribute_error(self, name: str) -> Never:\n            # Match the Python 3.12 error messages exactly\n            if name == \"__name__\":\n                raise AttributeError(\"readonly attribute\")\n            elif name in {\"__value__\", \"__type_params__\", \"__parameters__\", \"__module__\"}:\n                raise AttributeError(\n                    f\"attribute '{name}' of 'typing.TypeAliasType' objects \"\n                    \"is not writable\"\n                )\n            else:\n                raise AttributeError(\n                    f\"'typing.TypeAliasType' object has no attribute '{name}'\"\n                )\n\n        def __repr__(self) -> str:\n            return self.__name__\n\n        def __getitem__(self, parameters):\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n            parameters = [\n                typing._type_check(\n                    item, f'Subscripting {self.__name__} requires a type.'\n                )\n                for item in parameters\n            ]\n            return typing._GenericAlias(self, tuple(parameters))\n\n        def __reduce__(self):\n            return self.__name__\n\n        def __init_subclass__(cls, *args, **kwargs):\n            raise TypeError(\n                \"type 'typing_extensions.TypeAliasType' is not an acceptable base type\"\n            )\n\n        # The presence of this method convinces typing._type_check\n        # that TypeAliasTypes are types.\n        def __call__(self):\n            raise TypeError(\"Type alias is not callable\")\n\n        if sys.version_info >= (3, 10):\n            def __or__(self, right):\n                # For forward compatibility with 3.12, reject Unions\n                # that are not accepted by the built-in Union.\n                if not _is_unionable(right):\n                    return NotImplemented\n                return typing.Union[self, right]\n\n            def __ror__(self, left):\n                if not _is_unionable(left):\n                    return NotImplemented\n                return typing.Union[left, self]\n\n\nif hasattr(typing, \"is_protocol\"):\n    is_protocol = typing.is_protocol\n    get_protocol_members = typing.get_protocol_members\nelse:\n    def is_protocol(tp: type, /) -> bool:\n        \"\"\"Return True if the given type is a Protocol.\n\n        Example::\n\n            >>> from typing_extensions import Protocol, is_protocol\n            >>> class P(Protocol):\n            ...     def a(self) -> str: ...\n            ...     b: int\n            >>> is_protocol(P)\n            True\n            >>> is_protocol(int)\n            False\n        \"\"\"\n        return (\n            isinstance(tp, type)\n            and getattr(tp, '_is_protocol', False)\n            and tp is not Protocol\n            and tp is not typing.Protocol\n        )\n\n    def get_protocol_members(tp: type, /) -> typing.FrozenSet[str]:\n        \"\"\"Return the set of members defined in a Protocol.\n\n        Example::\n\n            >>> from typing_extensions import Protocol, get_protocol_members\n            >>> class P(Protocol):\n            ...     def a(self) -> str: ...\n            ...     b: int\n            >>> get_protocol_members(P)\n            frozenset({'a', 'b'})\n\n        Raise a TypeError for arguments that are not Protocols.\n        \"\"\"\n        if not is_protocol(tp):\n            raise TypeError(f'{tp!r} is not a Protocol')\n        if hasattr(tp, '__protocol_attrs__'):\n            return frozenset(tp.__protocol_attrs__)\n        return frozenset(_get_protocol_attrs(tp))\n\n\nif hasattr(typing, \"Doc\"):\n    Doc = typing.Doc\nelse:\n    class Doc:\n        \"\"\"Define the documentation of a type annotation using ``Annotated``, to be\n         used in class attributes, function and method parameters, return values,\n         and variables.\n\n        The value should be a positional-only string literal to allow static tools\n        like editors and documentation generators to use it.\n\n        This complements docstrings.\n\n        The string value passed is available in the attribute ``documentation``.\n\n        Example::\n\n            >>> from typing_extensions import Annotated, Doc\n            >>> def hi(to: Annotated[str, Doc(\"Who to say hi to\")]) -> None: ...\n        \"\"\"\n        def __init__(self, documentation: str, /) -> None:\n            self.documentation = documentation\n\n        def __repr__(self) -> str:\n            return f\"Doc({self.documentation!r})\"\n\n        def __hash__(self) -> int:\n            return hash(self.documentation)\n\n        def __eq__(self, other: object) -> bool:\n            if not isinstance(other, Doc):\n                return NotImplemented\n            return self.documentation == other.documentation\n\n\n# Aliases for items that have always been in typing.\n# Explicitly assign these (rather than using `from typing import *` at the top),\n# so that we get a CI error if one of these is deleted from typing.py\n# in a future version of Python\nAbstractSet = typing.AbstractSet\nAnyStr = typing.AnyStr\nBinaryIO = typing.BinaryIO\nCallable = typing.Callable\nCollection = typing.Collection\nContainer = typing.Container\nDict = typing.Dict\nForwardRef = typing.ForwardRef\nFrozenSet = typing.FrozenSet\nGenerator = typing.Generator\nGeneric = typing.Generic\nHashable = typing.Hashable\nIO = typing.IO\nItemsView = typing.ItemsView\nIterable = typing.Iterable\nIterator = typing.Iterator\nKeysView = typing.KeysView\nList = typing.List\nMapping = typing.Mapping\nMappingView = typing.MappingView\nMatch = typing.Match\nMutableMapping = typing.MutableMapping\nMutableSequence = typing.MutableSequence\nMutableSet = typing.MutableSet\nOptional = typing.Optional\nPattern = typing.Pattern\nReversible = typing.Reversible\nSequence = typing.Sequence\nSet = typing.Set\nSized = typing.Sized\nTextIO = typing.TextIO\nTuple = typing.Tuple\nUnion = typing.Union\nValuesView = typing.ValuesView\ncast = typing.cast\nno_type_check = typing.no_type_check\nno_type_check_decorator = typing.no_type_check_decorator\n", 2892], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_collections_abc.py": ["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nUnit tests are in test_collections.\n\"\"\"\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\nGenericAlias = type(list[int])\nEllipsisType = type(...)\ndef _f(): pass\nFunctionType = type(_f)\ndel _f\n\n__all__ = [\"Awaitable\", \"Coroutine\",\n           \"AsyncIterable\", \"AsyncIterator\", \"AsyncGenerator\",\n           \"Hashable\", \"Iterable\", \"Iterator\", \"Generator\", \"Reversible\",\n           \"Sized\", \"Container\", \"Callable\", \"Collection\",\n           \"Set\", \"MutableSet\",\n           \"Mapping\", \"MutableMapping\",\n           \"MappingView\", \"KeysView\", \"ItemsView\", \"ValuesView\",\n           \"Sequence\", \"MutableSequence\",\n           \"ByteString\",\n           ]\n\n# This module has been renamed from collections.abc to _collections_abc to\n# speed up interpreter startup. Some of the types such as MutableMapping are\n# required early but collections module imports a lot of other modules.\n# See issue #19218\n__name__ = \"collections.abc\"\n\n# Private list of types that we want to register with the various ABCs\n# so that they will pass tests like:\n#       it = iter(somebytearray)\n#       assert isinstance(it, Iterable)\n# Note:  in other implementations, these types might not be distinct\n# and they may have their own implementation specific types that\n# are not included on this list.\nbytes_iterator = type(iter(b''))\nbytearray_iterator = type(iter(bytearray()))\n#callable_iterator = ???\ndict_keyiterator = type(iter({}.keys()))\ndict_valueiterator = type(iter({}.values()))\ndict_itemiterator = type(iter({}.items()))\nlist_iterator = type(iter([]))\nlist_reverseiterator = type(iter(reversed([])))\nrange_iterator = type(iter(range(0)))\nlongrange_iterator = type(iter(range(1 << 1000)))\nset_iterator = type(iter(set()))\nstr_iterator = type(iter(\"\"))\ntuple_iterator = type(iter(()))\nzip_iterator = type(iter(zip()))\n## views ##\ndict_keys = type({}.keys())\ndict_values = type({}.values())\ndict_items = type({}.items())\n## misc ##\nmappingproxy = type(type.__dict__)\ngenerator = type((lambda: (yield))())\n## coroutine ##\nasync def _coro(): pass\n_coro = _coro()\ncoroutine = type(_coro)\n_coro.close()  # Prevent ResourceWarning\ndel _coro\n## asynchronous generator ##\nasync def _ag(): yield\n_ag = _ag()\nasync_generator = type(_ag)\ndel _ag\n\n\n### ONE-TRICK PONIES ###\n\ndef _check_methods(C, *methods):\n    mro = C.__mro__\n    for method in methods:\n        for B in mro:\n            if method in B.__dict__:\n                if B.__dict__[method] is None:\n                    return NotImplemented\n                break\n        else:\n            return NotImplemented\n    return True\n\nclass Hashable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            return _check_methods(C, \"__hash__\")\n        return NotImplemented\n\n\nclass Awaitable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __await__(self):\n        yield\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Awaitable:\n            return _check_methods(C, \"__await__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass Coroutine(Awaitable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"coroutine ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Coroutine:\n            return _check_methods(C, '__await__', 'send', 'throw', 'close')\n        return NotImplemented\n\n\nCoroutine.register(coroutine)\n\n\nclass AsyncIterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __aiter__(self):\n        return AsyncIterator()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterable:\n            return _check_methods(C, \"__aiter__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass AsyncIterator(AsyncIterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    async def __anext__(self):\n        \"\"\"Return the next item or raise StopAsyncIteration when exhausted.\"\"\"\n        raise StopAsyncIteration\n\n    def __aiter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterator:\n            return _check_methods(C, \"__anext__\", \"__aiter__\")\n        return NotImplemented\n\n\nclass AsyncGenerator(AsyncIterator):\n\n    __slots__ = ()\n\n    async def __anext__(self):\n        \"\"\"Return the next item from the asynchronous generator.\n        When exhausted, raise StopAsyncIteration.\n        \"\"\"\n        return await self.asend(None)\n\n    @abstractmethod\n    async def asend(self, value):\n        \"\"\"Send a value into the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        raise StopAsyncIteration\n\n    @abstractmethod\n    async def athrow(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    async def aclose(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            await self.athrow(GeneratorExit)\n        except (GeneratorExit, StopAsyncIteration):\n            pass\n        else:\n            raise RuntimeError(\"asynchronous generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncGenerator:\n            return _check_methods(C, '__aiter__', '__anext__',\n                                  'asend', 'athrow', 'aclose')\n        return NotImplemented\n\n\nAsyncGenerator.register(async_generator)\n\n\nclass Iterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            return _check_methods(C, \"__iter__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass Iterator(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __next__(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            return _check_methods(C, '__iter__', '__next__')\n        return NotImplemented\n\n\nIterator.register(bytes_iterator)\nIterator.register(bytearray_iterator)\n#Iterator.register(callable_iterator)\nIterator.register(dict_keyiterator)\nIterator.register(dict_valueiterator)\nIterator.register(dict_itemiterator)\nIterator.register(list_iterator)\nIterator.register(list_reverseiterator)\nIterator.register(range_iterator)\nIterator.register(longrange_iterator)\nIterator.register(set_iterator)\nIterator.register(str_iterator)\nIterator.register(tuple_iterator)\nIterator.register(zip_iterator)\n\n\nclass Reversible(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __reversed__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Reversible:\n            return _check_methods(C, \"__reversed__\", \"__iter__\")\n        return NotImplemented\n\n\nclass Generator(Iterator):\n\n    __slots__ = ()\n\n    def __next__(self):\n        \"\"\"Return the next item from the generator.\n        When exhausted, raise StopIteration.\n        \"\"\"\n        return self.send(None)\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside generator.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Generator:\n            return _check_methods(C, '__iter__', '__next__',\n                                  'send', 'throw', 'close')\n        return NotImplemented\n\n\nGenerator.register(generator)\n\n\nclass Sized(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            return _check_methods(C, \"__len__\")\n        return NotImplemented\n\n\nclass Container(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            return _check_methods(C, \"__contains__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass Collection(Sized, Iterable, Container):\n\n    __slots__ = ()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Collection:\n            return _check_methods(C,  \"__len__\", \"__iter__\", \"__contains__\")\n        return NotImplemented\n\n\nclass _CallableGenericAlias(GenericAlias):\n    \"\"\" Represent `Callable[argtypes, resulttype]`.\n\n    This sets ``__args__`` to a tuple containing the flattened``argtypes``\n    followed by ``resulttype``.\n\n    Example: ``Callable[[int, str], float]`` sets ``__args__`` to\n    ``(int, str, float)``.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, origin, args):\n        try:\n            return cls.__create_ga(origin, args)\n        except TypeError as exc:\n            import warnings\n            warnings.warn(f'{str(exc)} '\n                          f'(This will raise a TypeError in Python 3.10.)',\n                          DeprecationWarning)\n            return GenericAlias(origin, args)\n\n    @classmethod\n    def __create_ga(cls, origin, args):\n        if not isinstance(args, tuple) or len(args) != 2:\n            raise TypeError(\n                \"Callable must be used as Callable[[arg, ...], result].\")\n        t_args, t_result = args\n        if isinstance(t_args, (list, tuple)):\n            ga_args = tuple(t_args) + (t_result,)\n        # This relaxes what t_args can be on purpose to allow things like\n        # PEP 612 ParamSpec.  Responsibility for whether a user is using\n        # Callable[...] properly is deferred to static type checkers.\n        else:\n            ga_args = args\n        return super().__new__(cls, origin, ga_args)\n\n    def __repr__(self):\n        if len(self.__args__) == 2 and self.__args__[0] is Ellipsis:\n            return super().__repr__()\n        return (f'collections.abc.Callable'\n                f'[[{\", \".join([_type_repr(a) for a in self.__args__[:-1]])}], '\n                f'{_type_repr(self.__args__[-1])}]')\n\n    def __reduce__(self):\n        args = self.__args__\n        if not (len(args) == 2 and args[0] is Ellipsis):\n            args = list(args[:-1]), args[-1]\n        return _CallableGenericAlias, (Callable, args)\n\n    def __getitem__(self, item):\n        # Called during TypeVar substitution, returns the custom subclass\n        # rather than the default types.GenericAlias object.\n        ga = super().__getitem__(item)\n        args = ga.__args__\n        t_result = args[-1]\n        t_args = args[:-1]\n        args = (t_args, t_result)\n        return _CallableGenericAlias(Callable, args)\n\n\ndef _type_repr(obj):\n    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n\n    Copied from :mod:`typing` since collections.abc\n    shouldn't depend on that module.\n    \"\"\"\n    if isinstance(obj, GenericAlias):\n        return repr(obj)\n    if isinstance(obj, type):\n        if obj.__module__ == 'builtins':\n            return obj.__qualname__\n        return f'{obj.__module__}.{obj.__qualname__}'\n    if obj is Ellipsis:\n        return '...'\n    if isinstance(obj, FunctionType):\n        return obj.__name__\n    return repr(obj)\n\n\nclass Callable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            return _check_methods(C, \"__call__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(_CallableGenericAlias)\n\n\n### SETS ###\n\n\nclass Set(Collection):\n\n    \"\"\"A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), redefine __le__ and __ge__,\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        '''Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        '''\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        'Return True if two sets have a null intersection.'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    def _hash(self):\n        \"\"\"Compute the hash value of a set.\n\n        Note that we don't define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there's not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        \"\"\"\n        MAX = sys.maxsize\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h ^= (h >> 11) ^ (h >> 25)\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    \"\"\"A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def add(self, value):\n        \"\"\"Add an element.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n        raise NotImplementedError\n\n    def remove(self, value):\n        \"\"\"Remove an element. If not a member, raise a KeyError.\"\"\"\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        \"\"\"Return the popped value.  Raise KeyError if empty.\"\"\"\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError from None\n        self.discard(value)\n        return value\n\n    def clear(self):\n        \"\"\"This is slow (creates N new iterators!) but effective.\"\"\"\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\n\nclass Mapping(Collection):\n\n    __slots__ = ()\n\n    \"\"\"A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def keys(self):\n        \"D.keys() -> a set-like object providing a view on D's keys\"\n        return KeysView(self)\n\n    def items(self):\n        \"D.items() -> a set-like object providing a view on D's items\"\n        return ItemsView(self)\n\n    def values(self):\n        \"D.values() -> an object providing a view on D's values\"\n        return ValuesView(self)\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    __reversed__ = None\n\n\nMapping.register(mappingproxy)\n\n\nclass MappingView(Sized):\n\n    __slots__ = '_mapping',\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0._mapping!r})'.format(self)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass KeysView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(cls, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        yield from self._mapping\n\n\nKeysView.register(dict_keys)\n\n\nclass ItemsView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(cls, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v is value or v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\n\nItemsView.register(dict_items)\n\n\nclass ValuesView(MappingView, Collection):\n\n    __slots__ = ()\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            v = self._mapping[key]\n            if v is value or v == value:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\n\nValuesView.register(dict_values)\n\n\nclass MutableMapping(Mapping):\n\n    __slots__ = ()\n\n    \"\"\"A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        '''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        '''\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        '''D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        '''\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError from None\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        'D.clear() -> None.  Remove all items from D.'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(self, other=(), /, **kwds):\n        ''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        '''\n        if isinstance(other, Mapping):\n            for key in other:\n                self[key] = other[key]\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\n\nclass Sequence(Reversible, Collection):\n\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v is value or v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value, start=0, stop=None):\n        '''S.index(value, [start, [stop]]) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n\n           Supporting start and stop arguments is optional, but\n           recommended.\n        '''\n        if start is not None and start < 0:\n            start = max(len(self) + start, 0)\n        if stop is not None and stop < 0:\n            stop += len(self)\n\n        i = start\n        while stop is None or i < stop:\n            try:\n                v = self[i]\n                if v is value or v == value:\n                    return i\n            except IndexError:\n                break\n            i += 1\n        raise ValueError\n\n    def count(self, value):\n        'S.count(value) -> integer -- return number of occurrences of value'\n        return sum(1 for v in self if v is value or v == value)\n\n\nSequence.register(tuple)\nSequence.register(str)\nSequence.register(range)\nSequence.register(memoryview)\n\n\nclass ByteString(Sequence):\n\n    \"\"\"This unifies bytes and bytearray.\n\n    XXX Should add all their methods.\n    \"\"\"\n\n    __slots__ = ()\n\nByteString.register(bytes)\nByteString.register(bytearray)\n\n\nclass MutableSequence(Sequence):\n\n    __slots__ = ()\n\n    \"\"\"All the operations on a read-write sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        'S.insert(index, value) -- insert value before index'\n        raise IndexError\n\n    def append(self, value):\n        'S.append(value) -- append value to the end of the sequence'\n        self.insert(len(self), value)\n\n    def clear(self):\n        'S.clear() -> None -- remove all items from S'\n        try:\n            while True:\n                self.pop()\n        except IndexError:\n            pass\n\n    def reverse(self):\n        'S.reverse() -- reverse *IN PLACE*'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        if values is self:\n            values = list(values)\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        '''S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        '''\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        '''S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        '''\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\n\nMutableSequence.register(list)\nMutableSequence.register(bytearray)  # Multiply inheriting, see ByteString\n", 1116], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py": ["from contextvars import ContextVar\nfrom typing import Optional\nimport sys\nimport threading\n\ncurrent_async_library_cvar = ContextVar(\n    \"current_async_library_cvar\", default=None\n)  # type: ContextVar[Optional[str]]\n\n\nclass _ThreadLocal(threading.local):\n    # Since threading.local provides no explicit mechanism is for setting\n    # a default for a value, a custom class with a class attribute is used\n    # instead.\n    name = None  # type: Optional[str]\n\n\nthread_local = _ThreadLocal()\n\n\nclass AsyncLibraryNotFoundError(RuntimeError):\n    pass\n\n\ndef current_async_library() -> str:\n    \"\"\"Detect which async library is currently running.\n\n    The following libraries are currently supported:\n\n    ================   ===========  ============================\n    Library             Requires     Magic string\n    ================   ===========  ============================\n    **Trio**            Trio v0.6+   ``\"trio\"``\n    **Curio**           -            ``\"curio\"``\n    **asyncio**                      ``\"asyncio\"``\n    **Trio-asyncio**    v0.8.2+     ``\"trio\"`` or ``\"asyncio\"``,\n                                    depending on current mode\n    ================   ===========  ============================\n\n    Returns:\n      A string like ``\"trio\"``.\n\n    Raises:\n      AsyncLibraryNotFoundError: if called from synchronous context,\n        or if the current async library was not recognized.\n\n    Examples:\n\n        .. code-block:: python3\n\n           from sniffio import current_async_library\n\n           async def generic_sleep(seconds):\n               library = current_async_library()\n               if library == \"trio\":\n                   import trio\n                   await trio.sleep(seconds)\n               elif library == \"asyncio\":\n                   import asyncio\n                   await asyncio.sleep(seconds)\n               # ... and so on ...\n               else:\n                   raise RuntimeError(f\"Unsupported library {library!r}\")\n\n    \"\"\"\n    value = thread_local.name\n    if value is not None:\n        return value\n\n    value = current_async_library_cvar.get()\n    if value is not None:\n        return value\n\n    # Need to sniff for asyncio\n    if \"asyncio\" in sys.modules:\n        import asyncio\n        try:\n            current_task = asyncio.current_task  # type: ignore[attr-defined]\n        except AttributeError:\n            current_task = asyncio.Task.current_task  # type: ignore[attr-defined]\n        try:\n            if current_task() is not None:\n                return \"asyncio\"\n        except RuntimeError:\n            pass\n\n    # Sniff for curio (for now)\n    if 'curio' in sys.modules:\n        from curio.meta import curio_running\n        if curio_running():\n            return 'curio'\n\n    raise AsyncLibraryNotFoundError(\n        \"unknown async library, or not in async context\"\n    )\n", 95], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py": ["from __future__ import annotations\n\nimport math\nimport sys\nimport threading\nfrom collections.abc import Awaitable, Callable, Generator\nfrom contextlib import contextmanager\nfrom importlib import import_module\nfrom typing import TYPE_CHECKING, Any, TypeVar\n\nimport sniffio\n\nif TYPE_CHECKING:\n    from ..abc import AsyncBackend\n\n# This must be updated when new backends are introduced\nBACKENDS = \"asyncio\", \"trio\"\n\nT_Retval = TypeVar(\"T_Retval\")\nthreadlocals = threading.local()\n\n\ndef run(\n    func: Callable[..., Awaitable[T_Retval]],\n    *args: object,\n    backend: str = \"asyncio\",\n    backend_options: dict[str, Any] | None = None,\n) -> T_Retval:\n    \"\"\"\n    Run the given coroutine function in an asynchronous event loop.\n\n    The current thread must not be already running an event loop.\n\n    :param func: a coroutine function\n    :param args: positional arguments to ``func``\n    :param backend: name of the asynchronous event loop implementation \u2013 currently\n        either ``asyncio`` or ``trio``\n    :param backend_options: keyword arguments to call the backend ``run()``\n        implementation with (documented :ref:`here <backend options>`)\n    :return: the return value of the coroutine function\n    :raises RuntimeError: if an asynchronous event loop is already running in this\n        thread\n    :raises LookupError: if the named backend is not found\n\n    \"\"\"\n    try:\n        asynclib_name = sniffio.current_async_library()\n    except sniffio.AsyncLibraryNotFoundError:\n        pass\n    else:\n        raise RuntimeError(f\"Already running {asynclib_name} in this thread\")\n\n    try:\n        async_backend = get_async_backend(backend)\n    except ImportError as exc:\n        raise LookupError(f\"No such backend: {backend}\") from exc\n\n    token = None\n    if sniffio.current_async_library_cvar.get(None) is None:\n        # Since we're in control of the event loop, we can cache the name of the async\n        # library\n        token = sniffio.current_async_library_cvar.set(backend)\n\n    try:\n        backend_options = backend_options or {}\n        return async_backend.run(func, args, {}, backend_options)\n    finally:\n        if token:\n            sniffio.current_async_library_cvar.reset(token)\n\n\nasync def sleep(delay: float) -> None:\n    \"\"\"\n    Pause the current task for the specified duration.\n\n    :param delay: the duration, in seconds\n\n    \"\"\"\n    return await get_async_backend().sleep(delay)\n\n\nasync def sleep_forever() -> None:\n    \"\"\"\n    Pause the current task until it's cancelled.\n\n    This is a shortcut for ``sleep(math.inf)``.\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    await sleep(math.inf)\n\n\nasync def sleep_until(deadline: float) -> None:\n    \"\"\"\n    Pause the current task until the given time.\n\n    :param deadline: the absolute time to wake up at (according to the internal\n        monotonic clock of the event loop)\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    now = current_time()\n    await sleep(max(deadline - now, 0))\n\n\ndef current_time() -> float:\n    \"\"\"\n    Return the current value of the event loop's internal clock.\n\n    :return: the clock value (seconds)\n\n    \"\"\"\n    return get_async_backend().current_time()\n\n\ndef get_all_backends() -> tuple[str, ...]:\n    \"\"\"Return a tuple of the names of all built-in backends.\"\"\"\n    return BACKENDS\n\n\ndef get_cancelled_exc_class() -> type[BaseException]:\n    \"\"\"Return the current async library's cancellation exception class.\"\"\"\n    return get_async_backend().cancelled_exception_class()\n\n\n#\n# Private API\n#\n\n\n@contextmanager\ndef claim_worker_thread(\n    backend_class: type[AsyncBackend], token: object\n) -> Generator[Any, None, None]:\n    threadlocals.current_async_backend = backend_class\n    threadlocals.current_token = token\n    try:\n        yield\n    finally:\n        del threadlocals.current_async_backend\n        del threadlocals.current_token\n\n\ndef get_async_backend(asynclib_name: str | None = None) -> AsyncBackend:\n    if asynclib_name is None:\n        asynclib_name = sniffio.current_async_library()\n\n    modulename = \"anyio._backends._\" + asynclib_name\n    try:\n        module = sys.modules[modulename]\n    except KeyError:\n        module = import_module(modulename)\n\n    return getattr(module, \"backend_class\")\n", 156], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py": ["\"\"\"Support for tasks, coroutines and the scheduler.\"\"\"\n\n__all__ = (\n    'Task', 'create_task',\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\n    'wait', 'wait_for', 'as_completed', 'sleep',\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\n    'current_task', 'all_tasks',\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\n)\n\nimport concurrent.futures\nimport contextvars\nimport functools\nimport inspect\nimport itertools\nimport types\nimport warnings\nimport weakref\nfrom types import GenericAlias\n\nfrom . import base_tasks\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom .coroutines import _is_coroutine\n\n# Helper to generate new task names\n# This uses itertools.count() instead of a \"+= 1\" operation because the latter\n# is not thread safe. See bpo-11866 for a longer explanation.\n_task_name_counter = itertools.count(1).__next__\n\n\ndef current_task(loop=None):\n    \"\"\"Return a currently executed task.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    return _current_tasks.get(loop)\n\n\ndef all_tasks(loop=None):\n    \"\"\"Return a set of all tasks for the loop.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks\n            if futures._get_loop(t) is loop and not t.done()}\n\n\ndef _all_tasks_compat(loop=None):\n    # Different from \"all_task()\" by returning *all* Tasks, including\n    # the completed ones.  Used to implement deprecated \"Tasks.all_task()\"\n    # method.\n    if loop is None:\n        loop = events.get_event_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks if futures._get_loop(t) is loop}\n\n\ndef _set_task_name(task, name):\n    if name is not None:\n        try:\n            set_name = task.set_name\n        except AttributeError:\n            pass\n        else:\n            set_name(name)\n\n\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\n                                # from a Python Future implementation.\n\n    \"\"\"A coroutine wrapped in a Future.\"\"\"\n\n    # An important invariant maintained while a Task not done:\n    #\n    # - Either _fut_waiter is None, and _step() is scheduled;\n    # - or _fut_waiter is some Future, and _step() is *not* scheduled.\n    #\n    # The only transition from the latter to the former is through\n    # _wakeup().  When _fut_waiter is not None, one of its callbacks\n    # must be _wakeup().\n\n    # If False, don't log a message if the task is destroyed whereas its\n    # status is still pending\n    _log_destroy_pending = True\n\n    def __init__(self, coro, *, loop=None, name=None):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        if not coroutines.iscoroutine(coro):\n            # raise after Future.__init__(), attrs are required for __del__\n            # prevent logging for pending task in __del__\n            self._log_destroy_pending = False\n            raise TypeError(f\"a coroutine was expected, got {coro!r}\")\n\n        if name is None:\n            self._name = f'Task-{_task_name_counter()}'\n        else:\n            self._name = str(name)\n\n        self._must_cancel = False\n        self._fut_waiter = None\n        self._coro = coro\n        self._context = contextvars.copy_context()\n\n        self._loop.call_soon(self.__step, context=self._context)\n        _register_task(self)\n\n    def __del__(self):\n        if self._state == futures._PENDING and self._log_destroy_pending:\n            context = {\n                'task': self,\n                'message': 'Task was destroyed but it is pending!',\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        super().__del__()\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def _repr_info(self):\n        return base_tasks._task_repr_info(self)\n\n    def get_coro(self):\n        return self._coro\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, value):\n        self._name = str(value)\n\n    def set_result(self, result):\n        raise RuntimeError('Task does not support set_result operation')\n\n    def set_exception(self, exception):\n        raise RuntimeError('Task does not support set_exception operation')\n\n    def get_stack(self, *, limit=None):\n        \"\"\"Return the list of stack frames for this task's coroutine.\n\n        If the coroutine is not done, this returns the stack where it is\n        suspended.  If the coroutine has completed successfully or was\n        cancelled, this returns an empty list.  If the coroutine was\n        terminated by an exception, this returns the list of traceback\n        frames.\n\n        The frames are always ordered from oldest to newest.\n\n        The optional limit gives the maximum number of frames to\n        return; by default all available frames are returned.  Its\n        meaning differs depending on whether a stack or a traceback is\n        returned: the newest frames of a stack are returned, but the\n        oldest frames of a traceback are returned.  (This matches the\n        behavior of the traceback module.)\n\n        For reasons beyond our control, only one stack frame is\n        returned for a suspended coroutine.\n        \"\"\"\n        return base_tasks._task_get_stack(self, limit)\n\n    def print_stack(self, *, limit=None, file=None):\n        \"\"\"Print the stack or traceback for this task's coroutine.\n\n        This produces output similar to that of the traceback module,\n        for the frames retrieved by get_stack().  The limit argument\n        is passed to get_stack().  The file argument is an I/O stream\n        to which the output is written; by default output is written\n        to sys.stderr.\n        \"\"\"\n        return base_tasks._task_print_stack(self, limit, file)\n\n    def cancel(self, msg=None):\n        \"\"\"Request that this task cancel itself.\n\n        This arranges for a CancelledError to be thrown into the\n        wrapped coroutine on the next cycle through the event loop.\n        The coroutine then has a chance to clean up or even deny\n        the request using try/except/finally.\n\n        Unlike Future.cancel, this does not guarantee that the\n        task will be cancelled: the exception might be caught and\n        acted upon, delaying cancellation of the task or preventing\n        cancellation completely.  The task may also return a value or\n        raise a different exception.\n\n        Immediately after this method is called, Task.cancelled() will\n        not return True (unless the task was already cancelled).  A\n        task will be marked as cancelled when the wrapped coroutine\n        terminates with a CancelledError exception (even if cancel()\n        was not called).\n        \"\"\"\n        self._log_traceback = False\n        if self.done():\n            return False\n        if self._fut_waiter is not None:\n            if self._fut_waiter.cancel(msg=msg):\n                # Leave self._fut_waiter; it may be a Task that\n                # catches and ignores the cancellation so we may have\n                # to cancel it again later.\n                return True\n        # It must be the case that self.__step is already scheduled.\n        self._must_cancel = True\n        self._cancel_message = msg\n        return True\n\n    def __step(self, exc=None):\n        if self.done():\n            raise exceptions.InvalidStateError(\n                f'_step(): already done: {self!r}, {exc!r}')\n        if self._must_cancel:\n            if not isinstance(exc, exceptions.CancelledError):\n                exc = self._make_cancelled_error()\n            self._must_cancel = False\n        coro = self._coro\n        self._fut_waiter = None\n\n        _enter_task(self._loop, self)\n        # Call either coro.throw(exc) or coro.send(None).\n        try:\n            if exc is None:\n                # We use the `send` method directly, because coroutines\n                # don't have `__iter__` and `__next__` methods.\n                result = coro.send(None)\n            else:\n                result = coro.throw(exc)\n        except StopIteration as exc:\n            if self._must_cancel:\n                # Task is cancelled right before coro stops.\n                self._must_cancel = False\n                super().cancel(msg=self._cancel_message)\n            else:\n                super().set_result(exc.value)\n        except exceptions.CancelledError as exc:\n            # Save the original exception so we can chain it later.\n            self._cancelled_exc = exc\n            super().cancel()  # I.e., Future.cancel(self).\n        except (KeyboardInterrupt, SystemExit) as exc:\n            super().set_exception(exc)\n            raise\n        except BaseException as exc:\n            super().set_exception(exc)\n        else:\n            blocking = getattr(result, '_asyncio_future_blocking', None)\n            if blocking is not None:\n                # Yielded Future must come from Future.__iter__().\n                if futures._get_loop(result) is not self._loop:\n                    new_exc = RuntimeError(\n                        f'Task {self!r} got Future '\n                        f'{result!r} attached to a different loop')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n                elif blocking:\n                    if result is self:\n                        new_exc = RuntimeError(\n                            f'Task cannot await on itself: {self!r}')\n                        self._loop.call_soon(\n                            self.__step, new_exc, context=self._context)\n                    else:\n                        result._asyncio_future_blocking = False\n                        result.add_done_callback(\n                            self.__wakeup, context=self._context)\n                        self._fut_waiter = result\n                        if self._must_cancel:\n                            if self._fut_waiter.cancel(\n                                    msg=self._cancel_message):\n                                self._must_cancel = False\n                else:\n                    new_exc = RuntimeError(\n                        f'yield was used instead of yield from '\n                        f'in task {self!r} with {result!r}')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n\n            elif result is None:\n                # Bare yield relinquishes control for one event loop iteration.\n                self._loop.call_soon(self.__step, context=self._context)\n            elif inspect.isgenerator(result):\n                # Yielding a generator is just wrong.\n                new_exc = RuntimeError(\n                    f'yield was used instead of yield from for '\n                    f'generator in task {self!r} with {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n            else:\n                # Yielding something else is an error.\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n        finally:\n            _leave_task(self._loop, self)\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __wakeup(self, future):\n        try:\n            future.result()\n        except BaseException as exc:\n            # This may also be a cancellation.\n            self.__step(exc)\n        else:\n            # Don't pass the value of `future.result()` explicitly,\n            # as `Future.__iter__` and `Future.__await__` don't need it.\n            # If we call `_step(value, None)` instead of `_step()`,\n            # Python eval loop would use `.send(value)` method call,\n            # instead of `__next__()`, which is slower for futures\n            # that return non-generator iterators from their `__iter__`.\n            self.__step()\n        self = None  # Needed to break cycles when an exception occurs.\n\n\n_PyTask = Task\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CTask is needed for tests.\n    Task = _CTask = _asyncio.Task\n\n\ndef create_task(coro, *, name=None):\n    \"\"\"Schedule the execution of a coroutine object in a spawn task.\n\n    Return a Task object.\n    \"\"\"\n    loop = events.get_running_loop()\n    task = loop.create_task(coro)\n    _set_task_name(task, name)\n    return task\n\n\n# wait() and as_completed() similar to those in PEP 3148.\n\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\n\n\nasync def wait(fs, *, loop=None, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the Futures and coroutines given by fs to complete.\n\n    The fs iterable must not be empty.\n\n    Coroutines will be wrapped in Tasks.\n\n    Returns two sets of Future: (done, pending).\n\n    Usage:\n\n        done, pending = await asyncio.wait(fs)\n\n    Note: This does not raise TimeoutError! Futures that aren't done\n    when the timeout occurs are returned in the second set.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    if not fs:\n        raise ValueError('Set of coroutines/Futures is empty.')\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\n        raise ValueError(f'Invalid return_when value: {return_when}')\n\n    if loop is None:\n        loop = events.get_running_loop()\n    else:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n\n    fs = set(fs)\n\n    if any(coroutines.iscoroutine(f) for f in fs):\n        warnings.warn(\"The explicit passing of coroutine objects to \"\n                      \"asyncio.wait() is deprecated since Python 3.8, and \"\n                      \"scheduled for removal in Python 3.11.\",\n                      DeprecationWarning, stacklevel=2)\n\n    fs = {ensure_future(f, loop=loop) for f in fs}\n\n    return await _wait(fs, timeout, return_when, loop)\n\n\ndef _release_waiter(waiter, *args):\n    if not waiter.done():\n        waiter.set_result(None)\n\n\nasync def wait_for(fut, timeout, *, loop=None):\n    \"\"\"Wait for the single Future or coroutine to complete, with timeout.\n\n    Coroutine will be wrapped in Task.\n\n    Returns result of the Future or coroutine.  When a timeout occurs,\n    it cancels the task and raises TimeoutError.  To avoid the task\n    cancellation, wrap it in shield().\n\n    If the wait is cancelled, the task is also cancelled.\n\n    This function is a coroutine.\n    \"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    else:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n\n    if timeout is None:\n        return await fut\n\n    if timeout <= 0:\n        fut = ensure_future(fut, loop=loop)\n\n        if fut.done():\n            return fut.result()\n\n        await _cancel_and_wait(fut, loop=loop)\n        try:\n            return fut.result()\n        except exceptions.CancelledError as exc:\n            raise exceptions.TimeoutError() from exc\n\n    waiter = loop.create_future()\n    timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    cb = functools.partial(_release_waiter, waiter)\n\n    fut = ensure_future(fut, loop=loop)\n    fut.add_done_callback(cb)\n\n    try:\n        # wait until the future completes or the timeout\n        try:\n            await waiter\n        except exceptions.CancelledError:\n            if fut.done():\n                return fut.result()\n            else:\n                fut.remove_done_callback(cb)\n                # We must ensure that the task is not running\n                # after wait_for() returns.\n                # See https://bugs.python.org/issue32751\n                await _cancel_and_wait(fut, loop=loop)\n                raise\n\n        if fut.done():\n            return fut.result()\n        else:\n            fut.remove_done_callback(cb)\n            # We must ensure that the task is not running\n            # after wait_for() returns.\n            # See https://bugs.python.org/issue32751\n            await _cancel_and_wait(fut, loop=loop)\n            # In case task cancellation failed with some\n            # exception, we should re-raise it\n            # See https://bugs.python.org/issue40607\n            try:\n                return fut.result()\n            except exceptions.CancelledError as exc:\n                raise exceptions.TimeoutError() from exc\n    finally:\n        timeout_handle.cancel()\n\n\nasync def _wait(fs, timeout, return_when, loop):\n    \"\"\"Internal helper for wait().\n\n    The fs argument must be a collection of Futures.\n    \"\"\"\n    assert fs, 'Set of Futures is empty.'\n    waiter = loop.create_future()\n    timeout_handle = None\n    if timeout is not None:\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    counter = len(fs)\n\n    def _on_completion(f):\n        nonlocal counter\n        counter -= 1\n        if (counter <= 0 or\n            return_when == FIRST_COMPLETED or\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\n                                                f.exception() is not None)):\n            if timeout_handle is not None:\n                timeout_handle.cancel()\n            if not waiter.done():\n                waiter.set_result(None)\n\n    for f in fs:\n        f.add_done_callback(_on_completion)\n\n    try:\n        await waiter\n    finally:\n        if timeout_handle is not None:\n            timeout_handle.cancel()\n        for f in fs:\n            f.remove_done_callback(_on_completion)\n\n    done, pending = set(), set()\n    for f in fs:\n        if f.done():\n            done.add(f)\n        else:\n            pending.add(f)\n    return done, pending\n\n\nasync def _cancel_and_wait(fut, loop):\n    \"\"\"Cancel the *fut* future or task and wait until it completes.\"\"\"\n\n    waiter = loop.create_future()\n    cb = functools.partial(_release_waiter, waiter)\n    fut.add_done_callback(cb)\n\n    try:\n        fut.cancel()\n        # We cannot wait on *fut* directly to make\n        # sure _cancel_and_wait itself is reliably cancellable.\n        await waiter\n    finally:\n        fut.remove_done_callback(cb)\n\n\n# This is *not* a @coroutine!  It is just an iterator (yielding Futures).\ndef as_completed(fs, *, loop=None, timeout=None):\n    \"\"\"Return an iterator whose values are coroutines.\n\n    When waiting for the yielded coroutines you'll get the results (or\n    exceptions!) of the original Futures (or coroutines), in the order\n    in which and as soon as they complete.\n\n    This differs from PEP 3148; the proper way to use this is:\n\n        for f in as_completed(fs):\n            result = await f  # The 'await' may raise.\n            # Use result.\n\n    If a timeout is specified, the 'await' will raise\n    TimeoutError when the timeout occurs before all Futures are done.\n\n    Note: The futures 'f' are not necessarily members of fs.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect an iterable of futures, not {type(fs).__name__}\")\n\n    if loop is not None:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n\n    from .queues import Queue  # Import here to avoid circular import problem.\n    done = Queue(loop=loop)\n\n    if loop is None:\n        loop = events.get_event_loop()\n    todo = {ensure_future(f, loop=loop) for f in set(fs)}\n    timeout_handle = None\n\n    def _on_timeout():\n        for f in todo:\n            f.remove_done_callback(_on_completion)\n            done.put_nowait(None)  # Queue a dummy value for _wait_for_one().\n        todo.clear()  # Can't do todo.remove(f) in the loop.\n\n    def _on_completion(f):\n        if not todo:\n            return  # _on_timeout() was here first.\n        todo.remove(f)\n        done.put_nowait(f)\n        if not todo and timeout_handle is not None:\n            timeout_handle.cancel()\n\n    async def _wait_for_one():\n        f = await done.get()\n        if f is None:\n            # Dummy value from _on_timeout().\n            raise exceptions.TimeoutError\n        return f.result()  # May raise f.exception().\n\n    for f in todo:\n        f.add_done_callback(_on_completion)\n    if todo and timeout is not None:\n        timeout_handle = loop.call_later(timeout, _on_timeout)\n    for _ in range(len(todo)):\n        yield _wait_for_one()\n\n\n@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\n\n\nasync def sleep(delay, result=None, *, loop=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if loop is not None:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n\n    if delay <= 0:\n        await __sleep0()\n        return result\n\n    if loop is None:\n        loop = events.get_running_loop()\n\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()\n\n\ndef ensure_future(coro_or_future, *, loop=None):\n    \"\"\"Wrap a coroutine or an awaitable in a future.\n\n    If the argument is a Future, it is returned directly.\n    \"\"\"\n    if coroutines.iscoroutine(coro_or_future):\n        if loop is None:\n            loop = events.get_event_loop()\n        task = loop.create_task(coro_or_future)\n        if task._source_traceback:\n            del task._source_traceback[-1]\n        return task\n    elif futures.isfuture(coro_or_future):\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\n            raise ValueError('The future belongs to a different loop than '\n                             'the one specified as the loop argument')\n        return coro_or_future\n    elif inspect.isawaitable(coro_or_future):\n        return ensure_future(_wrap_awaitable(coro_or_future), loop=loop)\n    else:\n        raise TypeError('An asyncio.Future, a coroutine or an awaitable is '\n                        'required')\n\n\n@types.coroutine\ndef _wrap_awaitable(awaitable):\n    \"\"\"Helper for asyncio.ensure_future().\n\n    Wraps awaitable (an object with __await__) into a coroutine\n    that will later be wrapped in a Task by ensure_future().\n    \"\"\"\n    return (yield from awaitable.__await__())\n\n_wrap_awaitable._is_coroutine = _is_coroutine\n\n\nclass _GatheringFuture(futures.Future):\n    \"\"\"Helper for gather().\n\n    This overrides cancel() to cancel all the children and act more\n    like Task.cancel(), which doesn't immediately mark itself as\n    cancelled.\n    \"\"\"\n\n    def __init__(self, children, *, loop=None):\n        super().__init__(loop=loop)\n        self._children = children\n        self._cancel_requested = False\n\n    def cancel(self, msg=None):\n        if self.done():\n            return False\n        ret = False\n        for child in self._children:\n            if child.cancel(msg=msg):\n                ret = True\n        if ret:\n            # If any child tasks were actually cancelled, we should\n            # propagate the cancellation request regardless of\n            # *return_exceptions* argument.  See issue 32684.\n            self._cancel_requested = True\n        return ret\n\n\ndef gather(*coros_or_futures, loop=None, return_exceptions=False):\n    \"\"\"Return a future aggregating results from the given coroutines/futures.\n\n    Coroutines will be wrapped in a future and scheduled in the event\n    loop. They will not necessarily be scheduled in the same order as\n    passed in.\n\n    All futures must share the same event loop.  If all the tasks are\n    done successfully, the returned future's result is the list of\n    results (in the order of the original sequence, not necessarily\n    the order of results arrival).  If *return_exceptions* is True,\n    exceptions in the tasks are treated the same as successful\n    results, and gathered in the result list; otherwise, the first\n    raised exception will be immediately propagated to the returned\n    future.\n\n    Cancellation: if the outer Future is cancelled, all children (that\n    have not completed yet) are also cancelled.  If any child is\n    cancelled, this is treated as if it raised CancelledError --\n    the outer Future is *not* cancelled in this case.  (This is to\n    prevent the cancellation of one child to cause other children to\n    be cancelled.)\n\n    If *return_exceptions* is False, cancelling gather() after it\n    has been marked done won't cancel any submitted awaitables.\n    For instance, gather can be marked done after propagating an\n    exception to the caller, therefore, calling ``gather.cancel()``\n    after catching an exception (raised by one of the awaitables) from\n    gather won't cancel any other awaitables.\n    \"\"\"\n    if loop is not None:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n\n    return _gather(*coros_or_futures, loop=loop, return_exceptions=return_exceptions)\n\n\ndef _gather(*coros_or_futures, loop=None, return_exceptions=False):\n    if not coros_or_futures:\n        if loop is None:\n            loop = events.get_event_loop()\n        outer = loop.create_future()\n        outer.set_result([])\n        return outer\n\n    def _done_callback(fut):\n        nonlocal nfinished\n        nfinished += 1\n\n        if outer is None or outer.done():\n            if not fut.cancelled():\n                # Mark exception retrieved.\n                fut.exception()\n            return\n\n        if not return_exceptions:\n            if fut.cancelled():\n                # Check if 'fut' is cancelled first, as\n                # 'fut.exception()' will *raise* a CancelledError\n                # instead of returning it.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n                return\n            else:\n                exc = fut.exception()\n                if exc is not None:\n                    outer.set_exception(exc)\n                    return\n\n        if nfinished == nfuts:\n            # All futures are done; create a list of results\n            # and set it to the 'outer' future.\n            results = []\n\n            for fut in children:\n                if fut.cancelled():\n                    # Check if 'fut' is cancelled first, as 'fut.exception()'\n                    # will *raise* a CancelledError instead of returning it.\n                    # Also, since we're adding the exception return value\n                    # to 'results' instead of raising it, don't bother\n                    # setting __context__.  This also lets us preserve\n                    # calling '_make_cancelled_error()' at most once.\n                    res = exceptions.CancelledError(\n                        '' if fut._cancel_message is None else\n                        fut._cancel_message)\n                else:\n                    res = fut.exception()\n                    if res is None:\n                        res = fut.result()\n                results.append(res)\n\n            if outer._cancel_requested:\n                # If gather is being cancelled we must propagate the\n                # cancellation regardless of *return_exceptions* argument.\n                # See issue 32684.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n            else:\n                outer.set_result(results)\n\n    arg_to_fut = {}\n    children = []\n    nfuts = 0\n    nfinished = 0\n    outer = None  # bpo-46672\n    for arg in coros_or_futures:\n        if arg not in arg_to_fut:\n            fut = ensure_future(arg, loop=loop)\n            if loop is None:\n                loop = futures._get_loop(fut)\n            if fut is not arg:\n                # 'arg' was not a Future, therefore, 'fut' is a new\n                # Future created specifically for 'arg'.  Since the caller\n                # can't control it, disable the \"destroy pending task\"\n                # warning.\n                fut._log_destroy_pending = False\n\n            nfuts += 1\n            arg_to_fut[arg] = fut\n            fut.add_done_callback(_done_callback)\n\n        else:\n            # There's a duplicate Future object in coros_or_futures.\n            fut = arg_to_fut[arg]\n\n        children.append(fut)\n\n    outer = _GatheringFuture(children, loop=loop)\n    return outer\n\n\ndef shield(arg, *, loop=None):\n    \"\"\"Wait for a future, shielding it from cancellation.\n\n    The statement\n\n        res = await shield(something())\n\n    is exactly equivalent to the statement\n\n        res = await something()\n\n    *except* that if the coroutine containing it is cancelled, the\n    task running in something() is not cancelled.  From the POV of\n    something(), the cancellation did not happen.  But its caller is\n    still cancelled, so the yield-from expression still raises\n    CancelledError.  Note: If something() is cancelled by other means\n    this will still cancel shield().\n\n    If you want to completely ignore cancellation (not recommended)\n    you can combine shield() with a try/except clause, as follows:\n\n        try:\n            res = await shield(something())\n        except CancelledError:\n            res = None\n    \"\"\"\n    if loop is not None:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n    inner = ensure_future(arg, loop=loop)\n    if inner.done():\n        # Shortcut.\n        return inner\n    loop = futures._get_loop(inner)\n    outer = loop.create_future()\n\n    def _inner_done_callback(inner):\n        if outer.cancelled():\n            if not inner.cancelled():\n                # Mark inner's result as retrieved.\n                inner.exception()\n            return\n\n        if inner.cancelled():\n            outer.cancel()\n        else:\n            exc = inner.exception()\n            if exc is not None:\n                outer.set_exception(exc)\n            else:\n                outer.set_result(inner.result())\n\n\n    def _outer_done_callback(outer):\n        if not inner.done():\n            inner.remove_done_callback(_inner_done_callback)\n\n    inner.add_done_callback(_inner_done_callback)\n    outer.add_done_callback(_outer_done_callback)\n    return outer\n\n\ndef run_coroutine_threadsafe(coro, loop):\n    \"\"\"Submit a coroutine object to a given event loop.\n\n    Return a concurrent.futures.Future to access the result.\n    \"\"\"\n    if not coroutines.iscoroutine(coro):\n        raise TypeError('A coroutine object is required')\n    future = concurrent.futures.Future()\n\n    def callback():\n        try:\n            futures._chain_future(ensure_future(coro, loop=loop), future)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if future.set_running_or_notify_cancel():\n                future.set_exception(exc)\n            raise\n\n    loop.call_soon_threadsafe(callback)\n    return future\n\n\n# WeakSet containing all alive tasks.\n_all_tasks = weakref.WeakSet()\n\n# Dictionary containing tasks that are currently active in\n# all running event loops.  {EventLoop: Task}\n_current_tasks = {}\n\n\ndef _register_task(task):\n    \"\"\"Register a new task in asyncio as executed by loop.\"\"\"\n    _all_tasks.add(task)\n\n\ndef _enter_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not None:\n        raise RuntimeError(f\"Cannot enter into task {task!r} while another \"\n                           f\"task {current_task!r} is being executed.\")\n    _current_tasks[loop] = task\n\n\ndef _leave_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not task:\n        raise RuntimeError(f\"Leaving task {task!r} does not match \"\n                           f\"the current task {current_task!r}.\")\n    del _current_tasks[loop]\n\n\ndef _unregister_task(task):\n    \"\"\"Unregister a task.\"\"\"\n    _all_tasks.discard(task)\n\n\n_py_register_task = _register_task\n_py_unregister_task = _unregister_task\n_py_enter_task = _enter_task\n_py_leave_task = _leave_task\n\n\ntry:\n    from _asyncio import (_register_task, _unregister_task,\n                          _enter_task, _leave_task,\n                          _all_tasks, _current_tasks)\nexcept ImportError:\n    pass\nelse:\n    _c_register_task = _register_task\n    _c_unregister_task = _unregister_task\n    _c_enter_task = _enter_task\n    _c_leave_task = _leave_task\n", 989], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py": ["from __future__ import annotations\n\nimport array\nimport asyncio\nimport concurrent.futures\nimport math\nimport socket\nimport sys\nimport threading\nfrom asyncio import (\n    AbstractEventLoop,\n    CancelledError,\n    all_tasks,\n    create_task,\n    current_task,\n    get_running_loop,\n    sleep,\n)\nfrom asyncio import run as native_run\nfrom asyncio.base_events import _run_until_complete_cb  # type: ignore[attr-defined]\nfrom collections import OrderedDict, deque\nfrom collections.abc import AsyncIterator, Iterable\nfrom concurrent.futures import Future\nfrom contextlib import suppress\nfrom contextvars import Context, copy_context\nfrom dataclasses import dataclass\nfrom functools import partial, wraps\nfrom inspect import (\n    CORO_RUNNING,\n    CORO_SUSPENDED,\n    getcoroutinestate,\n    iscoroutine,\n)\nfrom io import IOBase\nfrom os import PathLike\nfrom queue import Queue\nfrom signal import Signals\nfrom socket import AddressFamily, SocketKind\nfrom threading import Thread\nfrom types import TracebackType\nfrom typing import (\n    IO,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    Collection,\n    ContextManager,\n    Coroutine,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    TypeVar,\n    cast,\n)\nfrom weakref import WeakKeyDictionary\n\nimport sniffio\n\nfrom .. import CapacityLimiterStatistics, EventStatistics, TaskInfo, abc\nfrom .._core._eventloop import claim_worker_thread\nfrom .._core._exceptions import (\n    BrokenResourceError,\n    BusyResourceError,\n    ClosedResourceError,\n    EndOfStream,\n    WouldBlock,\n)\nfrom .._core._sockets import convert_ipv6_sockaddr\nfrom .._core._streams import create_memory_object_stream\nfrom .._core._synchronization import CapacityLimiter as BaseCapacityLimiter\nfrom .._core._synchronization import Event as BaseEvent\nfrom .._core._synchronization import ResourceGuard\nfrom .._core._tasks import CancelScope as BaseCancelScope\nfrom ..abc import (\n    AsyncBackend,\n    IPSockAddrType,\n    SocketListener,\n    UDPPacketType,\n    UNIXDatagramPacketType,\n)\nfrom ..lowlevel import RunVar\nfrom ..streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nif sys.version_info >= (3, 11):\n    from asyncio import Runner\nelse:\n    import contextvars\n    import enum\n    import signal\n    from asyncio import coroutines, events, exceptions, tasks\n\n    from exceptiongroup import BaseExceptionGroup\n\n    class _State(enum.Enum):\n        CREATED = \"created\"\n        INITIALIZED = \"initialized\"\n        CLOSED = \"closed\"\n\n    class Runner:\n        # Copied from CPython 3.11\n        def __init__(\n            self,\n            *,\n            debug: bool | None = None,\n            loop_factory: Callable[[], AbstractEventLoop] | None = None,\n        ):\n            self._state = _State.CREATED\n            self._debug = debug\n            self._loop_factory = loop_factory\n            self._loop: AbstractEventLoop | None = None\n            self._context = None\n            self._interrupt_count = 0\n            self._set_event_loop = False\n\n        def __enter__(self) -> Runner:\n            self._lazy_init()\n            return self\n\n        def __exit__(\n            self,\n            exc_type: type[BaseException],\n            exc_val: BaseException,\n            exc_tb: TracebackType,\n        ) -> None:\n            self.close()\n\n        def close(self) -> None:\n            \"\"\"Shutdown and close event loop.\"\"\"\n            if self._state is not _State.INITIALIZED:\n                return\n            try:\n                loop = self._loop\n                _cancel_all_tasks(loop)\n                loop.run_until_complete(loop.shutdown_asyncgens())\n                if hasattr(loop, \"shutdown_default_executor\"):\n                    loop.run_until_complete(loop.shutdown_default_executor())\n                else:\n                    loop.run_until_complete(_shutdown_default_executor(loop))\n            finally:\n                if self._set_event_loop:\n                    events.set_event_loop(None)\n                loop.close()\n                self._loop = None\n                self._state = _State.CLOSED\n\n        def get_loop(self) -> AbstractEventLoop:\n            \"\"\"Return embedded event loop.\"\"\"\n            self._lazy_init()\n            return self._loop\n\n        def run(self, coro: Coroutine[T_Retval], *, context=None) -> T_Retval:\n            \"\"\"Run a coroutine inside the embedded event loop.\"\"\"\n            if not coroutines.iscoroutine(coro):\n                raise ValueError(f\"a coroutine was expected, got {coro!r}\")\n\n            if events._get_running_loop() is not None:\n                # fail fast with short traceback\n                raise RuntimeError(\n                    \"Runner.run() cannot be called from a running event loop\"\n                )\n\n            self._lazy_init()\n\n            if context is None:\n                context = self._context\n            task = self._loop.create_task(coro, context=context)\n\n            if (\n                threading.current_thread() is threading.main_thread()\n                and signal.getsignal(signal.SIGINT) is signal.default_int_handler\n            ):\n                sigint_handler = partial(self._on_sigint, main_task=task)\n                try:\n                    signal.signal(signal.SIGINT, sigint_handler)\n                except ValueError:\n                    # `signal.signal` may throw if `threading.main_thread` does\n                    # not support signals (e.g. embedded interpreter with signals\n                    # not registered - see gh-91880)\n                    sigint_handler = None\n            else:\n                sigint_handler = None\n\n            self._interrupt_count = 0\n            try:\n                return self._loop.run_until_complete(task)\n            except exceptions.CancelledError:\n                if self._interrupt_count > 0:\n                    uncancel = getattr(task, \"uncancel\", None)\n                    if uncancel is not None and uncancel() == 0:\n                        raise KeyboardInterrupt()\n                raise  # CancelledError\n            finally:\n                if (\n                    sigint_handler is not None\n                    and signal.getsignal(signal.SIGINT) is sigint_handler\n                ):\n                    signal.signal(signal.SIGINT, signal.default_int_handler)\n\n        def _lazy_init(self) -> None:\n            if self._state is _State.CLOSED:\n                raise RuntimeError(\"Runner is closed\")\n            if self._state is _State.INITIALIZED:\n                return\n            if self._loop_factory is None:\n                self._loop = events.new_event_loop()\n                if not self._set_event_loop:\n                    # Call set_event_loop only once to avoid calling\n                    # attach_loop multiple times on child watchers\n                    events.set_event_loop(self._loop)\n                    self._set_event_loop = True\n            else:\n                self._loop = self._loop_factory()\n            if self._debug is not None:\n                self._loop.set_debug(self._debug)\n            self._context = contextvars.copy_context()\n            self._state = _State.INITIALIZED\n\n        def _on_sigint(self, signum, frame, main_task: asyncio.Task) -> None:\n            self._interrupt_count += 1\n            if self._interrupt_count == 1 and not main_task.done():\n                main_task.cancel()\n                # wakeup loop if it is blocked by select() with long timeout\n                self._loop.call_soon_threadsafe(lambda: None)\n                return\n            raise KeyboardInterrupt()\n\n    def _cancel_all_tasks(loop: AbstractEventLoop) -> None:\n        to_cancel = tasks.all_tasks(loop)\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n\n        loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))\n\n        for task in to_cancel:\n            if task.cancelled():\n                continue\n            if task.exception() is not None:\n                loop.call_exception_handler(\n                    {\n                        \"message\": \"unhandled exception during asyncio.run() shutdown\",\n                        \"exception\": task.exception(),\n                        \"task\": task,\n                    }\n                )\n\n    async def _shutdown_default_executor(loop: AbstractEventLoop) -> None:\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n\n        def _do_shutdown(future: asyncio.futures.Future) -> None:\n            try:\n                loop._default_executor.shutdown(wait=True)  # type: ignore[attr-defined]\n                loop.call_soon_threadsafe(future.set_result, None)\n            except Exception as ex:\n                loop.call_soon_threadsafe(future.set_exception, ex)\n\n        loop._executor_shutdown_called = True\n        if loop._default_executor is None:\n            return\n        future = loop.create_future()\n        thread = threading.Thread(target=_do_shutdown, args=(future,))\n        thread.start()\n        try:\n            await future\n        finally:\n            thread.join()\n\n\nT_Retval = TypeVar(\"T_Retval\")\nT_contra = TypeVar(\"T_contra\", contravariant=True)\n\n_root_task: RunVar[asyncio.Task | None] = RunVar(\"_root_task\")\n\n\ndef find_root_task() -> asyncio.Task:\n    root_task = _root_task.get(None)\n    if root_task is not None and not root_task.done():\n        return root_task\n\n    # Look for a task that has been started via run_until_complete()\n    for task in all_tasks():\n        if task._callbacks and not task.done():\n            callbacks = [cb for cb, context in task._callbacks]\n            for cb in callbacks:\n                if (\n                    cb is _run_until_complete_cb\n                    or getattr(cb, \"__module__\", None) == \"uvloop.loop\"\n                ):\n                    _root_task.set(task)\n                    return task\n\n    # Look up the topmost task in the AnyIO task tree, if possible\n    task = cast(asyncio.Task, current_task())\n    state = _task_states.get(task)\n    if state:\n        cancel_scope = state.cancel_scope\n        while cancel_scope and cancel_scope._parent_scope is not None:\n            cancel_scope = cancel_scope._parent_scope\n\n        if cancel_scope is not None:\n            return cast(asyncio.Task, cancel_scope._host_task)\n\n    return task\n\n\ndef get_callable_name(func: Callable) -> str:\n    module = getattr(func, \"__module__\", None)\n    qualname = getattr(func, \"__qualname__\", None)\n    return \".\".join([x for x in (module, qualname) if x])\n\n\n#\n# Event loop\n#\n\n_run_vars = (\n    WeakKeyDictionary()\n)  # type: WeakKeyDictionary[asyncio.AbstractEventLoop, Any]\n\n\ndef _task_started(task: asyncio.Task) -> bool:\n    \"\"\"Return ``True`` if the task has been started and has not finished.\"\"\"\n    try:\n        return getcoroutinestate(task.get_coro()) in (CORO_RUNNING, CORO_SUSPENDED)\n    except AttributeError:\n        # task coro is async_genenerator_asend https://bugs.python.org/issue37771\n        raise Exception(f\"Cannot determine if task {task} has started or not\") from None\n\n\n#\n# Timeouts and cancellation\n#\n\n\nclass CancelScope(BaseCancelScope):\n    def __new__(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        return object.__new__(cls)\n\n    def __init__(self, deadline: float = math.inf, shield: bool = False):\n        self._deadline = deadline\n        self._shield = shield\n        self._parent_scope: CancelScope | None = None\n        self._cancel_called = False\n        self._cancelled_caught = False\n        self._active = False\n        self._timeout_handle: asyncio.TimerHandle | None = None\n        self._cancel_handle: asyncio.Handle | None = None\n        self._tasks: set[asyncio.Task] = set()\n        self._host_task: asyncio.Task | None = None\n        self._cancel_calls: int = 0\n        self._cancelling: int | None = None\n\n    def __enter__(self) -> CancelScope:\n        if self._active:\n            raise RuntimeError(\n                \"Each CancelScope may only be used for a single 'with' block\"\n            )\n\n        self._host_task = host_task = cast(asyncio.Task, current_task())\n        self._tasks.add(host_task)\n        try:\n            task_state = _task_states[host_task]\n        except KeyError:\n            task_state = TaskState(None, self)\n            _task_states[host_task] = task_state\n        else:\n            self._parent_scope = task_state.cancel_scope\n            task_state.cancel_scope = self\n\n        self._timeout()\n        self._active = True\n        if sys.version_info >= (3, 11):\n            self._cancelling = self._host_task.cancelling()\n\n        # Start cancelling the host task if the scope was cancelled before entering\n        if self._cancel_called:\n            self._deliver_cancellation()\n\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        if not self._active:\n            raise RuntimeError(\"This cancel scope is not active\")\n        if current_task() is not self._host_task:\n            raise RuntimeError(\n                \"Attempted to exit cancel scope in a different task than it was \"\n                \"entered in\"\n            )\n\n        assert self._host_task is not None\n        host_task_state = _task_states.get(self._host_task)\n        if host_task_state is None or host_task_state.cancel_scope is not self:\n            raise RuntimeError(\n                \"Attempted to exit a cancel scope that isn't the current tasks's \"\n                \"current cancel scope\"\n            )\n\n        self._active = False\n        if self._timeout_handle:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        self._tasks.remove(self._host_task)\n\n        host_task_state.cancel_scope = self._parent_scope\n\n        # Restart the cancellation effort in the farthest directly cancelled parent\n        # scope if this one was shielded\n        if self._shield:\n            self._deliver_cancellation_to_parent()\n\n        if isinstance(exc_val, CancelledError) and self._cancel_called:\n            self._cancelled_caught = self._uncancel(exc_val)\n            return self._cancelled_caught\n\n        return None\n\n    def _uncancel(self, cancelled_exc: CancelledError) -> bool:\n        if sys.version_info < (3, 9) or self._host_task is None:\n            self._cancel_calls = 0\n            return True\n\n        # Undo all cancellations done by this scope\n        if self._cancelling is not None:\n            while self._cancel_calls:\n                self._cancel_calls -= 1\n                if self._host_task.uncancel() <= self._cancelling:\n                    return True\n\n        self._cancel_calls = 0\n        return f\"Cancelled by cancel scope {id(self):x}\" in cancelled_exc.args\n\n    def _timeout(self) -> None:\n        if self._deadline != math.inf:\n            loop = get_running_loop()\n            if loop.time() >= self._deadline:\n                self.cancel()\n            else:\n                self._timeout_handle = loop.call_at(self._deadline, self._timeout)\n\n    def _deliver_cancellation(self) -> None:\n        \"\"\"\n        Deliver cancellation to directly contained tasks and nested cancel scopes.\n\n        Schedule another run at the end if we still have tasks eligible for\n        cancellation.\n        \"\"\"\n        should_retry = False\n        current = current_task()\n        for task in self._tasks:\n            if task._must_cancel:  # type: ignore[attr-defined]\n                continue\n\n            # The task is eligible for cancellation if it has started and is not in a\n            # cancel scope shielded from this one\n            cancel_scope = _task_states[task].cancel_scope\n            while cancel_scope is not self:\n                if cancel_scope is None or cancel_scope._shield:\n                    break\n                else:\n                    cancel_scope = cancel_scope._parent_scope\n            else:\n                should_retry = True\n                if task is not current and (\n                    task is self._host_task or _task_started(task)\n                ):\n                    waiter = task._fut_waiter  # type: ignore[attr-defined]\n                    if not isinstance(waiter, asyncio.Future) or not waiter.done():\n                        self._cancel_calls += 1\n                        if sys.version_info >= (3, 9):\n                            task.cancel(f\"Cancelled by cancel scope {id(self):x}\")\n                        else:\n                            task.cancel()\n\n        # Schedule another callback if there are still tasks left\n        if should_retry:\n            self._cancel_handle = get_running_loop().call_soon(\n                self._deliver_cancellation\n            )\n        else:\n            self._cancel_handle = None\n\n    def _deliver_cancellation_to_parent(self) -> None:\n        \"\"\"Start cancellation effort in the farthest directly cancelled parent scope\"\"\"\n        scope = self._parent_scope\n        scope_to_cancel: CancelScope | None = None\n        while scope is not None:\n            if scope._cancel_called and scope._cancel_handle is None:\n                scope_to_cancel = scope\n\n            # No point in looking beyond any shielded scope\n            if scope._shield:\n                break\n\n            scope = scope._parent_scope\n\n        if scope_to_cancel is not None:\n            scope_to_cancel._deliver_cancellation()\n\n    def _parent_cancelled(self) -> bool:\n        # Check whether any parent has been cancelled\n        cancel_scope = self._parent_scope\n        while cancel_scope is not None and not cancel_scope._shield:\n            if cancel_scope._cancel_called:\n                return True\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n        return False\n\n    def cancel(self) -> None:\n        if not self._cancel_called:\n            if self._timeout_handle:\n                self._timeout_handle.cancel()\n                self._timeout_handle = None\n\n            self._cancel_called = True\n            if self._host_task is not None:\n                self._deliver_cancellation()\n\n    @property\n    def deadline(self) -> float:\n        return self._deadline\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        self._deadline = float(value)\n        if self._timeout_handle is not None:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        if self._active and not self._cancel_called:\n            self._timeout()\n\n    @property\n    def cancel_called(self) -> bool:\n        return self._cancel_called\n\n    @property\n    def cancelled_caught(self) -> bool:\n        return self._cancelled_caught\n\n    @property\n    def shield(self) -> bool:\n        return self._shield\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        if self._shield != value:\n            self._shield = value\n            if not value:\n                self._deliver_cancellation_to_parent()\n\n\n#\n# Task states\n#\n\n\nclass TaskState:\n    \"\"\"\n    Encapsulates auxiliary task information that cannot be added to the Task instance\n    itself because there are no guarantees about its implementation.\n    \"\"\"\n\n    __slots__ = \"parent_id\", \"cancel_scope\"\n\n    def __init__(self, parent_id: int | None, cancel_scope: CancelScope | None):\n        self.parent_id = parent_id\n        self.cancel_scope = cancel_scope\n\n\n_task_states = WeakKeyDictionary()  # type: WeakKeyDictionary[asyncio.Task, TaskState]\n\n\n#\n# Task groups\n#\n\n\nclass _AsyncioTaskStatus(abc.TaskStatus):\n    def __init__(self, future: asyncio.Future, parent_id: int):\n        self._future = future\n        self._parent_id = parent_id\n\n    def started(self, value: T_contra | None = None) -> None:\n        try:\n            self._future.set_result(value)\n        except asyncio.InvalidStateError:\n            raise RuntimeError(\n                \"called 'started' twice on the same task status\"\n            ) from None\n\n        task = cast(asyncio.Task, current_task())\n        _task_states[task].parent_id = self._parent_id\n\n\ndef collapse_exception_group(excgroup: BaseExceptionGroup) -> BaseException:\n    exceptions = list(excgroup.exceptions)\n    modified = False\n    for i, exc in enumerate(exceptions):\n        if isinstance(exc, BaseExceptionGroup):\n            new_exc = collapse_exception_group(exc)\n            if new_exc is not exc:\n                modified = True\n                exceptions[i] = new_exc\n\n    if len(exceptions) == 1:\n        return exceptions[0]\n    elif modified:\n        return excgroup.derive(exceptions)\n    else:\n        return excgroup\n\n\nclass TaskGroup(abc.TaskGroup):\n    def __init__(self) -> None:\n        self.cancel_scope: CancelScope = CancelScope()\n        self._active = False\n        self._exceptions: list[BaseException] = []\n\n    async def __aenter__(self) -> TaskGroup:\n        self.cancel_scope.__enter__()\n        self._active = True\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        ignore_exception = self.cancel_scope.__exit__(exc_type, exc_val, exc_tb)\n        if exc_val is not None:\n            self.cancel_scope.cancel()\n            if not isinstance(exc_val, CancelledError):\n                self._exceptions.append(exc_val)\n\n        cancelled_exc_while_waiting_tasks: CancelledError | None = None\n        waited_for_tasks_to_finish = bool(self.cancel_scope._tasks)\n        while self.cancel_scope._tasks:\n            try:\n                await asyncio.wait(self.cancel_scope._tasks)\n            except CancelledError as exc:\n                # This task was cancelled natively; reraise the CancelledError later\n                # unless this task was already interrupted by another exception\n                self.cancel_scope.cancel()\n                if cancelled_exc_while_waiting_tasks is None:\n                    cancelled_exc_while_waiting_tasks = exc\n\n        self._active = False\n        if self._exceptions:\n            raise BaseExceptionGroup(\n                \"unhandled errors in a TaskGroup\", self._exceptions\n            )\n\n        # Raise the CancelledError received while waiting for child tasks to exit,\n        # unless the context manager itself was previously exited with another\n        # exception, or if any of the  child tasks raised an exception other than\n        # CancelledError\n        if cancelled_exc_while_waiting_tasks:\n            if exc_val is None or ignore_exception:\n                raise cancelled_exc_while_waiting_tasks\n\n        # Yield control to the event loop here to ensure that there is at least one\n        # yield point within __aexit__() (trio does the same)\n        if not waited_for_tasks_to_finish:\n            await AsyncIOBackend.checkpoint()\n\n        return ignore_exception\n\n    def _spawn(\n        self,\n        func: Callable[..., Awaitable[Any]],\n        args: tuple,\n        name: object,\n        task_status_future: asyncio.Future | None = None,\n    ) -> asyncio.Task:\n        def task_done(_task: asyncio.Task) -> None:\n            assert _task in self.cancel_scope._tasks\n            self.cancel_scope._tasks.remove(_task)\n            del _task_states[_task]\n\n            try:\n                exc = _task.exception()\n            except CancelledError as e:\n                while isinstance(e.__context__, CancelledError):\n                    e = e.__context__\n\n                exc = e\n\n            if exc is not None:\n                if task_status_future is None or task_status_future.done():\n                    if not isinstance(exc, CancelledError):\n                        self._exceptions.append(exc)\n\n                    self.cancel_scope.cancel()\n                else:\n                    task_status_future.set_exception(exc)\n            elif task_status_future is not None and not task_status_future.done():\n                task_status_future.set_exception(\n                    RuntimeError(\"Child exited without calling task_status.started()\")\n                )\n\n        if not self._active:\n            raise RuntimeError(\n                \"This task group is not active; no new tasks can be started.\"\n            )\n\n        kwargs = {}\n        if task_status_future:\n            parent_id = id(current_task())\n            kwargs[\"task_status\"] = _AsyncioTaskStatus(\n                task_status_future, id(self.cancel_scope._host_task)\n            )\n        else:\n            parent_id = id(self.cancel_scope._host_task)\n\n        coro = func(*args, **kwargs)\n        if not iscoroutine(coro):\n            prefix = f\"{func.__module__}.\" if hasattr(func, \"__module__\") else \"\"\n            raise TypeError(\n                f\"Expected {prefix}{func.__qualname__}() to return a coroutine, but \"\n                f\"the return value ({coro!r}) is not a coroutine object\"\n            )\n\n        name = get_callable_name(func) if name is None else str(name)\n        task = create_task(coro, name=name)\n        task.add_done_callback(task_done)\n\n        # Make the spawned task inherit the task group's cancel scope\n        _task_states[task] = TaskState(\n            parent_id=parent_id, cancel_scope=self.cancel_scope\n        )\n        self.cancel_scope._tasks.add(task)\n        return task\n\n    def start_soon(\n        self, func: Callable[..., Awaitable[Any]], *args: object, name: object = None\n    ) -> None:\n        self._spawn(func, args, name)\n\n    async def start(\n        self, func: Callable[..., Awaitable[Any]], *args: object, name: object = None\n    ) -> None:\n        future: asyncio.Future = asyncio.Future()\n        task = self._spawn(func, args, name, future)\n\n        # If the task raises an exception after sending a start value without a switch\n        # point between, the task group is cancelled and this method never proceeds to\n        # process the completed future. That's why we have to have a shielded cancel\n        # scope here.\n        try:\n            return await future\n        except CancelledError:\n            # Cancel the task and wait for it to exit before returning\n            task.cancel()\n            with CancelScope(shield=True), suppress(CancelledError):\n                await task\n\n            raise\n\n\n#\n# Threads\n#\n\n_Retval_Queue_Type = Tuple[Optional[T_Retval], Optional[BaseException]]\n\n\nclass WorkerThread(Thread):\n    MAX_IDLE_TIME = 10  # seconds\n\n    def __init__(\n        self,\n        root_task: asyncio.Task,\n        workers: set[WorkerThread],\n        idle_workers: deque[WorkerThread],\n    ):\n        super().__init__(name=\"AnyIO worker thread\")\n        self.root_task = root_task\n        self.workers = workers\n        self.idle_workers = idle_workers\n        self.loop = root_task._loop\n        self.queue: Queue[\n            tuple[Context, Callable, tuple, asyncio.Future] | None\n        ] = Queue(2)\n        self.idle_since = AsyncIOBackend.current_time()\n        self.stopping = False\n\n    def _report_result(\n        self, future: asyncio.Future, result: Any, exc: BaseException | None\n    ) -> None:\n        self.idle_since = AsyncIOBackend.current_time()\n        if not self.stopping:\n            self.idle_workers.append(self)\n\n        if not future.cancelled():\n            if exc is not None:\n                if isinstance(exc, StopIteration):\n                    new_exc = RuntimeError(\"coroutine raised StopIteration\")\n                    new_exc.__cause__ = exc\n                    exc = new_exc\n\n                future.set_exception(exc)\n            else:\n                future.set_result(result)\n\n    def run(self) -> None:\n        with claim_worker_thread(AsyncIOBackend, self.loop):\n            while True:\n                item = self.queue.get()\n                if item is None:\n                    # Shutdown command received\n                    return\n\n                context, func, args, future = item\n                if not future.cancelled():\n                    result = None\n                    exception: BaseException | None = None\n                    try:\n                        result = context.run(func, *args)\n                    except BaseException as exc:\n                        exception = exc\n\n                    if not self.loop.is_closed():\n                        self.loop.call_soon_threadsafe(\n                            self._report_result, future, result, exception\n                        )\n\n                self.queue.task_done()\n\n    def stop(self, f: asyncio.Task | None = None) -> None:\n        self.stopping = True\n        self.queue.put_nowait(None)\n        self.workers.discard(self)\n        try:\n            self.idle_workers.remove(self)\n        except ValueError:\n            pass\n\n\n_threadpool_idle_workers: RunVar[deque[WorkerThread]] = RunVar(\n    \"_threadpool_idle_workers\"\n)\n_threadpool_workers: RunVar[set[WorkerThread]] = RunVar(\"_threadpool_workers\")\n\n\nclass BlockingPortal(abc.BlockingPortal):\n    def __new__(cls) -> BlockingPortal:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._loop = get_running_loop()\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable,\n        args: tuple[Any, ...],\n        kwargs: dict[str, Any],\n        name: object,\n        future: Future,\n    ) -> None:\n        AsyncIOBackend.run_sync_from_thread(\n            partial(self._task_group.start_soon, name=name),\n            (self._call_func, func, args, kwargs, future),\n            self._loop,\n        )\n\n\n#\n# Subprocesses\n#\n\n\n@dataclass(eq=False)\nclass StreamReaderWrapper(abc.ByteReceiveStream):\n    _stream: asyncio.StreamReader\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        data = await self._stream.read(max_bytes)\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def aclose(self) -> None:\n        self._stream.feed_eof()\n\n\n@dataclass(eq=False)\nclass StreamWriterWrapper(abc.ByteSendStream):\n    _stream: asyncio.StreamWriter\n\n    async def send(self, item: bytes) -> None:\n        self._stream.write(item)\n        await self._stream.drain()\n\n    async def aclose(self) -> None:\n        self._stream.close()\n\n\n@dataclass(eq=False)\nclass Process(abc.Process):\n    _process: asyncio.subprocess.Process\n    _stdin: StreamWriterWrapper | None\n    _stdout: StreamReaderWrapper | None\n    _stderr: StreamReaderWrapper | None\n\n    async def aclose(self) -> None:\n        if self._stdin:\n            await self._stdin.aclose()\n        if self._stdout:\n            await self._stdout.aclose()\n        if self._stderr:\n            await self._stderr.aclose()\n\n        await self.wait()\n\n    async def wait(self) -> int:\n        return await self._process.wait()\n\n    def terminate(self) -> None:\n        self._process.terminate()\n\n    def kill(self) -> None:\n        self._process.kill()\n\n    def send_signal(self, signal: int) -> None:\n        self._process.send_signal(signal)\n\n    @property\n    def pid(self) -> int:\n        return self._process.pid\n\n    @property\n    def returncode(self) -> int | None:\n        return self._process.returncode\n\n    @property\n    def stdin(self) -> abc.ByteSendStream | None:\n        return self._stdin\n\n    @property\n    def stdout(self) -> abc.ByteReceiveStream | None:\n        return self._stdout\n\n    @property\n    def stderr(self) -> abc.ByteReceiveStream | None:\n        return self._stderr\n\n\ndef _forcibly_shutdown_process_pool_on_exit(\n    workers: set[Process], _task: object\n) -> None:\n    \"\"\"\n    Forcibly shuts down worker processes belonging to this event loop.\"\"\"\n    child_watcher: asyncio.AbstractChildWatcher | None = None\n    if sys.version_info < (3, 12):\n        try:\n            child_watcher = asyncio.get_event_loop_policy().get_child_watcher()\n        except NotImplementedError:\n            pass\n\n    # Close as much as possible (w/o async/await) to avoid warnings\n    for process in workers:\n        if process.returncode is None:\n            continue\n\n        process._stdin._stream._transport.close()  # type: ignore[union-attr]\n        process._stdout._stream._transport.close()  # type: ignore[union-attr]\n        process._stderr._stream._transport.close()  # type: ignore[union-attr]\n        process.kill()\n        if child_watcher:\n            child_watcher.remove_child_handler(process.pid)\n\n\nasync def _shutdown_process_pool_on_exit(workers: set[abc.Process]) -> None:\n    \"\"\"\n    Shuts down worker processes belonging to this event loop.\n\n    NOTE: this only works when the event loop was started using asyncio.run() or\n    anyio.run().\n\n    \"\"\"\n    process: abc.Process\n    try:\n        await sleep(math.inf)\n    except asyncio.CancelledError:\n        for process in workers:\n            if process.returncode is None:\n                process.kill()\n\n        for process in workers:\n            await process.aclose()\n\n\n#\n# Sockets and networking\n#\n\n\nclass StreamProtocol(asyncio.Protocol):\n    read_queue: deque[bytes]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Exception | None = None\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque()\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n        cast(asyncio.Transport, transport).set_write_buffer_limits(0)\n\n    def connection_lost(self, exc: Exception | None) -> None:\n        if exc:\n            self.exception = BrokenResourceError()\n            self.exception.__cause__ = exc\n\n        self.read_event.set()\n        self.write_event.set()\n\n    def data_received(self, data: bytes) -> None:\n        self.read_queue.append(data)\n        self.read_event.set()\n\n    def eof_received(self) -> bool | None:\n        self.read_event.set()\n        return True\n\n    def pause_writing(self) -> None:\n        self.write_event = asyncio.Event()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass DatagramProtocol(asyncio.DatagramProtocol):\n    read_queue: deque[tuple[bytes, IPSockAddrType]]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Exception | None = None\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque(maxlen=100)  # arbitrary value\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n\n    def connection_lost(self, exc: Exception | None) -> None:\n        self.read_event.set()\n        self.write_event.set()\n\n    def datagram_received(self, data: bytes, addr: IPSockAddrType) -> None:\n        addr = convert_ipv6_sockaddr(addr)\n        self.read_queue.append((data, addr))\n        self.read_event.set()\n\n    def error_received(self, exc: Exception) -> None:\n        self.exception = exc\n\n    def pause_writing(self) -> None:\n        self.write_event.clear()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass SocketStream(abc.SocketStream):\n    def __init__(self, transport: asyncio.Transport, protocol: StreamProtocol):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        with self._receive_guard:\n            await AsyncIOBackend.checkpoint()\n\n            if (\n                not self._protocol.read_event.is_set()\n                and not self._transport.is_closing()\n            ):\n                self._transport.resume_reading()\n                await self._protocol.read_event.wait()\n                self._transport.pause_reading()\n\n            try:\n                chunk = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                elif self._protocol.exception:\n                    raise self._protocol.exception from None\n                else:\n                    raise EndOfStream from None\n\n            if len(chunk) > max_bytes:\n                # Split the oversized chunk\n                chunk, leftover = chunk[:max_bytes], chunk[max_bytes:]\n                self._protocol.read_queue.appendleft(leftover)\n\n            # If the read queue is empty, clear the flag so that the next call will\n            # block until data is available\n            if not self._protocol.read_queue:\n                self._protocol.read_event.clear()\n\n        return chunk\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await AsyncIOBackend.checkpoint()\n\n            if self._closed:\n                raise ClosedResourceError\n            elif self._protocol.exception is not None:\n                raise self._protocol.exception\n\n            try:\n                self._transport.write(item)\n            except RuntimeError as exc:\n                if self._transport.is_closing():\n                    raise BrokenResourceError from exc\n                else:\n                    raise\n\n            await self._protocol.write_event.wait()\n\n    async def send_eof(self) -> None:\n        try:\n            self._transport.write_eof()\n        except OSError:\n            pass\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            try:\n                self._transport.write_eof()\n            except OSError:\n                pass\n\n            self._transport.close()\n            await sleep(0)\n            self._transport.abort()\n\n\nclass _RawSocketMixin:\n    _receive_future: asyncio.Future | None = None\n    _send_future: asyncio.Future | None = None\n    _closing = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    def _wait_until_readable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._receive_future\n            loop.remove_reader(self.__raw_socket)\n\n        f = self._receive_future = asyncio.Future()\n        loop.add_reader(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    def _wait_until_writable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._send_future\n            loop.remove_writer(self.__raw_socket)\n\n        f = self._send_future = asyncio.Future()\n        loop.add_writer(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    async def aclose(self) -> None:\n        if not self._closing:\n            self._closing = True\n            if self.__raw_socket.fileno() != -1:\n                self.__raw_socket.close()\n\n            if self._receive_future:\n                self._receive_future.set_result(None)\n            if self._send_future:\n                self._send_future.set_result(None)\n\n\nclass UNIXSocketStream(_RawSocketMixin, abc.UNIXSocketStream):\n    async def send_eof(self) -> None:\n        with self._send_guard:\n            self._raw_socket.shutdown(socket.SHUT_WR)\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self._raw_socket.recv(max_bytes)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not data:\n                        raise EndOfStream\n\n                    return data\n\n    async def send(self, item: bytes) -> None:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            view = memoryview(item)\n            while view:\n                try:\n                    bytes_sent = self._raw_socket.send(view)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    view = view[bytes_sent:]\n\n    async def receive_fds(self, msglen: int, maxfds: int) -> tuple[bytes, list[int]]:\n        if not isinstance(msglen, int) or msglen < 0:\n            raise ValueError(\"msglen must be a non-negative integer\")\n        if not isinstance(maxfds, int) or maxfds < 1:\n            raise ValueError(\"maxfds must be a positive integer\")\n\n        loop = get_running_loop()\n        fds = array.array(\"i\")\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    message, ancdata, flags, addr = self._raw_socket.recvmsg(\n                        msglen, socket.CMSG_LEN(maxfds * fds.itemsize)\n                    )\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not message and not ancdata:\n                        raise EndOfStream\n\n                    break\n\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level != socket.SOL_SOCKET or cmsg_type != socket.SCM_RIGHTS:\n                raise RuntimeError(\n                    f\"Received unexpected ancillary data; message = {message!r}, \"\n                    f\"cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}\"\n                )\n\n            fds.frombytes(cmsg_data[: len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return message, list(fds)\n\n    async def send_fds(self, message: bytes, fds: Collection[int | IOBase]) -> None:\n        if not message:\n            raise ValueError(\"message must not be empty\")\n        if not fds:\n            raise ValueError(\"fds must not be empty\")\n\n        loop = get_running_loop()\n        filenos: list[int] = []\n        for fd in fds:\n            if isinstance(fd, int):\n                filenos.append(fd)\n            elif isinstance(fd, IOBase):\n                filenos.append(fd.fileno())\n\n        fdarray = array.array(\"i\", filenos)\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    # The ignore can be removed after mypy picks up\n                    # https://github.com/python/typeshed/pull/5545\n                    self._raw_socket.sendmsg(\n                        [message], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, fdarray)]\n                    )\n                    break\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n\nclass TCPSocketListener(abc.SocketListener):\n    _accept_scope: CancelScope | None = None\n    _closed = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = cast(asyncio.BaseEventLoop, get_running_loop())\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    async def accept(self) -> abc.SocketStream:\n        if self._closed:\n            raise ClosedResourceError\n\n        with self._accept_guard:\n            await AsyncIOBackend.checkpoint()\n            with CancelScope() as self._accept_scope:\n                try:\n                    client_sock, _addr = await self._loop.sock_accept(self._raw_socket)\n                except asyncio.CancelledError:\n                    # Workaround for https://bugs.python.org/issue41317\n                    try:\n                        self._loop.remove_reader(self._raw_socket)\n                    except (ValueError, NotImplementedError):\n                        pass\n\n                    if self._closed:\n                        raise ClosedResourceError from None\n\n                    raise\n                finally:\n                    self._accept_scope = None\n\n        client_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        transport, protocol = await self._loop.connect_accepted_socket(\n            StreamProtocol, client_sock\n        )\n        return SocketStream(transport, protocol)\n\n    async def aclose(self) -> None:\n        if self._closed:\n            return\n\n        self._closed = True\n        if self._accept_scope:\n            # Workaround for https://bugs.python.org/issue41317\n            try:\n                self._loop.remove_reader(self._raw_socket)\n            except (ValueError, NotImplementedError):\n                pass\n\n            self._accept_scope.cancel()\n            await sleep(0)\n\n        self._raw_socket.close()\n\n\nclass UNIXSocketListener(abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = get_running_loop()\n        self._accept_guard = ResourceGuard(\"accepting connections from\")\n        self._closed = False\n\n    async def accept(self) -> abc.SocketStream:\n        await AsyncIOBackend.checkpoint()\n        with self._accept_guard:\n            while True:\n                try:\n                    client_sock, _ = self.__raw_socket.accept()\n                    client_sock.setblocking(False)\n                    return UNIXSocketStream(client_sock)\n                except BlockingIOError:\n                    f: asyncio.Future = asyncio.Future()\n                    self._loop.add_reader(self.__raw_socket, f.set_result, None)\n                    f.add_done_callback(\n                        lambda _: self._loop.remove_reader(self.__raw_socket)\n                    )\n                    await f\n                except OSError as exc:\n                    if self._closed:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n    async def aclose(self) -> None:\n        self._closed = True\n        self.__raw_socket.close()\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n\nclass UDPSocket(abc.UDPSocket):\n    def __init__(\n        self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol\n    ):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> tuple[bytes, IPSockAddrType]:\n        with self._receive_guard:\n            await AsyncIOBackend.checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                return self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n    async def send(self, item: UDPPacketType) -> None:\n        with self._send_guard:\n            await AsyncIOBackend.checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(*item)\n\n\nclass ConnectedUDPSocket(abc.ConnectedUDPSocket):\n    def __init__(\n        self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol\n    ):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard(\"reading from\")\n        self._send_guard = ResourceGuard(\"writing to\")\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info(\"socket\")\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            await AsyncIOBackend.checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                packet = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n            return packet[0]\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await AsyncIOBackend.checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(item)\n\n\nclass UNIXDatagramSocket(_RawSocketMixin, abc.UNIXDatagramSocket):\n    async def receive(self) -> UNIXDatagramPacketType:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self._raw_socket.recvfrom(65536)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return data\n\n    async def send(self, item: UNIXDatagramPacketType) -> None:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    self._raw_socket.sendto(*item)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return\n\n\nclass ConnectedUNIXDatagramSocket(_RawSocketMixin, abc.ConnectedUNIXDatagramSocket):\n    async def receive(self) -> bytes:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self._raw_socket.recv(65536)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return data\n\n    async def send(self, item: bytes) -> None:\n        loop = get_running_loop()\n        await AsyncIOBackend.checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    self._raw_socket.send(item)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    return\n\n\n_read_events: RunVar[dict[Any, asyncio.Event]] = RunVar(\"read_events\")\n_write_events: RunVar[dict[Any, asyncio.Event]] = RunVar(\"write_events\")\n\n\n#\n# Synchronization\n#\n\n\nclass Event(BaseEvent):\n    def __new__(cls) -> Event:\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        self._event = asyncio.Event()\n\n    def set(self) -> None:\n        self._event.set()\n\n    def is_set(self) -> bool:\n        return self._event.is_set()\n\n    async def wait(self) -> None:\n        if self.is_set():\n            await AsyncIOBackend.checkpoint()\n        else:\n            await self._event.wait()\n\n    def statistics(self) -> EventStatistics:\n        return EventStatistics(len(self._event._waiters))  # type: ignore[attr-defined]\n\n\nclass CapacityLimiter(BaseCapacityLimiter):\n    _total_tokens: float = 0\n\n    def __new__(cls, total_tokens: float) -> CapacityLimiter:\n        return object.__new__(cls)\n\n    def __init__(self, total_tokens: float):\n        self._borrowers: set[Any] = set()\n        self._wait_queue: OrderedDict[Any, asyncio.Event] = OrderedDict()\n        self.total_tokens = total_tokens\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self.release()\n\n    @property\n    def total_tokens(self) -> float:\n        return self._total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        if not isinstance(value, int) and not math.isinf(value):\n            raise TypeError(\"total_tokens must be an int or math.inf\")\n        if value < 1:\n            raise ValueError(\"total_tokens must be >= 1\")\n\n        old_value = self._total_tokens\n        self._total_tokens = value\n        events = []\n        for event in self._wait_queue.values():\n            if value <= old_value:\n                break\n\n            if not event.is_set():\n                events.append(event)\n                old_value += 1\n\n        for event in events:\n            event.set()\n\n    @property\n    def borrowed_tokens(self) -> int:\n        return len(self._borrowers)\n\n    @property\n    def available_tokens(self) -> float:\n        return self._total_tokens - len(self._borrowers)\n\n    def acquire_nowait(self) -> None:\n        self.acquire_on_behalf_of_nowait(current_task())\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> None:\n        if borrower in self._borrowers:\n            raise RuntimeError(\n                \"this borrower is already holding one of this CapacityLimiter's \"\n                \"tokens\"\n            )\n\n        if self._wait_queue or len(self._borrowers) >= self._total_tokens:\n            raise WouldBlock\n\n        self._borrowers.add(borrower)\n\n    async def acquire(self) -> None:\n        return await self.acquire_on_behalf_of(current_task())\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await AsyncIOBackend.checkpoint_if_cancelled()\n        try:\n            self.acquire_on_behalf_of_nowait(borrower)\n        except WouldBlock:\n            event = asyncio.Event()\n            self._wait_queue[borrower] = event\n            try:\n                await event.wait()\n            except BaseException:\n                self._wait_queue.pop(borrower, None)\n                raise\n\n            self._borrowers.add(borrower)\n        else:\n            try:\n                await AsyncIOBackend.cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def release(self) -> None:\n        self.release_on_behalf_of(current_task())\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        try:\n            self._borrowers.remove(borrower)\n        except KeyError:\n            raise RuntimeError(\n                \"this borrower isn't holding any of this CapacityLimiter's \" \"tokens\"\n            ) from None\n\n        # Notify the next task in line if this limiter has free capacity now\n        if self._wait_queue and len(self._borrowers) < self._total_tokens:\n            event = self._wait_queue.popitem(last=False)[1]\n            event.set()\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        return CapacityLimiterStatistics(\n            self.borrowed_tokens,\n            self.total_tokens,\n            tuple(self._borrowers),\n            len(self._wait_queue),\n        )\n\n\n_default_thread_limiter: RunVar[CapacityLimiter] = RunVar(\"_default_thread_limiter\")\n\n\n#\n# Operating system signals\n#\n\n\nclass _SignalReceiver:\n    def __init__(self, signals: tuple[Signals, ...]):\n        self._signals = signals\n        self._loop = get_running_loop()\n        self._signal_queue: deque[Signals] = deque()\n        self._future: asyncio.Future = asyncio.Future()\n        self._handled_signals: set[Signals] = set()\n\n    def _deliver(self, signum: Signals) -> None:\n        self._signal_queue.append(signum)\n        if not self._future.done():\n            self._future.set_result(None)\n\n    def __enter__(self) -> _SignalReceiver:\n        for sig in set(self._signals):\n            self._loop.add_signal_handler(sig, self._deliver, sig)\n            self._handled_signals.add(sig)\n\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        for sig in self._handled_signals:\n            self._loop.remove_signal_handler(sig)\n        return None\n\n    def __aiter__(self) -> _SignalReceiver:\n        return self\n\n    async def __anext__(self) -> Signals:\n        await AsyncIOBackend.checkpoint()\n        if not self._signal_queue:\n            self._future = asyncio.Future()\n            await self._future\n\n        return self._signal_queue.popleft()\n\n\n#\n# Testing and debugging\n#\n\n\ndef _create_task_info(task: asyncio.Task) -> TaskInfo:\n    task_state = _task_states.get(task)\n    if task_state is None:\n        parent_id = None\n    else:\n        parent_id = task_state.parent_id\n\n    return TaskInfo(id(task), parent_id, task.get_name(), task.get_coro())\n\n\nclass TestRunner(abc.TestRunner):\n    _send_stream: MemoryObjectSendStream[tuple[Awaitable[Any], asyncio.Future[Any]]]\n\n    def __init__(\n        self,\n        *,\n        debug: bool | None = None,\n        use_uvloop: bool = False,\n        loop_factory: Callable[[], AbstractEventLoop] | None = None,\n    ) -> None:\n        if use_uvloop and loop_factory is None:\n            import uvloop\n\n            loop_factory = uvloop.new_event_loop\n\n        self._runner = Runner(debug=debug, loop_factory=loop_factory)\n        self._exceptions: list[BaseException] = []\n        self._runner_task: asyncio.Task | None = None\n\n    def __enter__(self) -> TestRunner:\n        self._runner.__enter__()\n        self.get_loop().set_exception_handler(self._exception_handler)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        self._runner.__exit__(exc_type, exc_val, exc_tb)\n\n    def get_loop(self) -> AbstractEventLoop:\n        return self._runner.get_loop()\n\n    def _exception_handler(\n        self, loop: asyncio.AbstractEventLoop, context: dict[str, Any]\n    ) -> None:\n        if isinstance(context.get(\"exception\"), Exception):\n            self._exceptions.append(context[\"exception\"])\n        else:\n            loop.default_exception_handler(context)\n\n    def _raise_async_exceptions(self) -> None:\n        # Re-raise any exceptions raised in asynchronous callbacks\n        if self._exceptions:\n            exceptions, self._exceptions = self._exceptions, []\n            if len(exceptions) == 1:\n                raise exceptions[0]\n            elif exceptions:\n                raise BaseExceptionGroup(\n                    \"Multiple exceptions occurred in asynchronous callbacks\", exceptions\n                )\n\n    @staticmethod\n    async def _run_tests_and_fixtures(\n        receive_stream: MemoryObjectReceiveStream[\n            tuple[Awaitable[T_Retval], asyncio.Future[T_Retval]]\n        ],\n    ) -> None:\n        with receive_stream:\n            async for coro, future in receive_stream:\n                try:\n                    retval = await coro\n                except BaseException as exc:\n                    if not future.cancelled():\n                        future.set_exception(exc)\n                else:\n                    if not future.cancelled():\n                        future.set_result(retval)\n\n    async def _call_in_runner_task(\n        self, func: Callable[..., Awaitable[T_Retval]], *args: object, **kwargs: object\n    ) -> T_Retval:\n        if not self._runner_task:\n            self._send_stream, receive_stream = create_memory_object_stream[\n                Tuple[Awaitable[Any], asyncio.Future]\n            ](1)\n            self._runner_task = self.get_loop().create_task(\n                self._run_tests_and_fixtures(receive_stream)\n            )\n\n        coro = func(*args, **kwargs)\n        future: asyncio.Future[T_Retval] = self.get_loop().create_future()\n        self._send_stream.send_nowait((coro, future))\n        return await future\n\n    def run_asyncgen_fixture(\n        self,\n        fixture_func: Callable[..., AsyncGenerator[T_Retval, Any]],\n        kwargs: dict[str, Any],\n    ) -> Iterable[T_Retval]:\n        asyncgen = fixture_func(**kwargs)\n        fixturevalue: T_Retval = self.get_loop().run_until_complete(\n            self._call_in_runner_task(asyncgen.asend, None)\n        )\n        self._raise_async_exceptions()\n\n        yield fixturevalue\n\n        try:\n            self.get_loop().run_until_complete(\n                self._call_in_runner_task(asyncgen.asend, None)\n            )\n        except StopAsyncIteration:\n            self._raise_async_exceptions()\n        else:\n            self.get_loop().run_until_complete(asyncgen.aclose())\n            raise RuntimeError(\"Async generator fixture did not stop\")\n\n    def run_fixture(\n        self,\n        fixture_func: Callable[..., Coroutine[Any, Any, T_Retval]],\n        kwargs: dict[str, Any],\n    ) -> T_Retval:\n        retval = self.get_loop().run_until_complete(\n            self._call_in_runner_task(fixture_func, **kwargs)\n        )\n        self._raise_async_exceptions()\n        return retval\n\n    def run_test(\n        self, test_func: Callable[..., Coroutine[Any, Any, Any]], kwargs: dict[str, Any]\n    ) -> None:\n        try:\n            self.get_loop().run_until_complete(\n                self._call_in_runner_task(test_func, **kwargs)\n            )\n        except Exception as exc:\n            self._exceptions.append(exc)\n\n        self._raise_async_exceptions()\n\n\nclass AsyncIOBackend(AsyncBackend):\n    @classmethod\n    def run(\n        cls,\n        func: Callable[..., Awaitable[T_Retval]],\n        args: tuple,\n        kwargs: dict[str, Any],\n        options: dict[str, Any],\n    ) -> T_Retval:\n        @wraps(func)\n        async def wrapper() -> T_Retval:\n            task = cast(asyncio.Task, current_task())\n            task.set_name(get_callable_name(func))\n            _task_states[task] = TaskState(None, None)\n\n            try:\n                return await func(*args)\n            finally:\n                del _task_states[task]\n\n        debug = options.get(\"debug\", False)\n        options.get(\"loop_factory\", None)\n        options.get(\"use_uvloop\", False)\n        return native_run(wrapper(), debug=debug)\n\n    @classmethod\n    def current_token(cls) -> object:\n        return get_running_loop()\n\n    @classmethod\n    def current_time(cls) -> float:\n        return get_running_loop().time()\n\n    @classmethod\n    def cancelled_exception_class(cls) -> type[BaseException]:\n        return CancelledError\n\n    @classmethod\n    async def checkpoint(cls) -> None:\n        await sleep(0)\n\n    @classmethod\n    async def checkpoint_if_cancelled(cls) -> None:\n        task = current_task()\n        if task is None:\n            return\n\n        try:\n            cancel_scope = _task_states[task].cancel_scope\n        except KeyError:\n            return\n\n        while cancel_scope:\n            if cancel_scope.cancel_called:\n                await sleep(0)\n            elif cancel_scope.shield:\n                break\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n    @classmethod\n    async def cancel_shielded_checkpoint(cls) -> None:\n        with CancelScope(shield=True):\n            await sleep(0)\n\n    @classmethod\n    async def sleep(cls, delay: float) -> None:\n        await sleep(delay)\n\n    @classmethod\n    def create_cancel_scope(\n        cls, *, deadline: float = math.inf, shield: bool = False\n    ) -> CancelScope:\n        return CancelScope(deadline=deadline, shield=shield)\n\n    @classmethod\n    def current_effective_deadline(cls) -> float:\n        try:\n            cancel_scope = _task_states[\n                current_task()  # type: ignore[index]\n            ].cancel_scope\n        except KeyError:\n            return math.inf\n\n        deadline = math.inf\n        while cancel_scope:\n            deadline = min(deadline, cancel_scope.deadline)\n            if cancel_scope._cancel_called:\n                deadline = -math.inf\n                break\n            elif cancel_scope.shield:\n                break\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n        return deadline\n\n    @classmethod\n    def create_task_group(cls) -> abc.TaskGroup:\n        return TaskGroup()\n\n    @classmethod\n    def create_event(cls) -> abc.Event:\n        return Event()\n\n    @classmethod\n    def create_capacity_limiter(cls, total_tokens: float) -> abc.CapacityLimiter:\n        return CapacityLimiter(total_tokens)\n\n    @classmethod\n    async def run_sync_in_worker_thread(\n        cls,\n        func: Callable[..., T_Retval],\n        args: tuple[Any, ...],\n        cancellable: bool = False,\n        limiter: abc.CapacityLimiter | None = None,\n    ) -> T_Retval:\n        await cls.checkpoint()\n\n        # If this is the first run in this event loop thread, set up the necessary\n        # variables\n        try:\n            idle_workers = _threadpool_idle_workers.get()\n            workers = _threadpool_workers.get()\n        except LookupError:\n            idle_workers = deque()\n            workers = set()\n            _threadpool_idle_workers.set(idle_workers)\n            _threadpool_workers.set(workers)\n\n        async with (limiter or cls.current_default_thread_limiter()):\n            with CancelScope(shield=not cancellable):\n                future: asyncio.Future = asyncio.Future()\n                root_task = find_root_task()\n                if not idle_workers:\n                    worker = WorkerThread(root_task, workers, idle_workers)\n                    worker.start()\n                    workers.add(worker)\n                    root_task.add_done_callback(worker.stop)\n                else:\n                    worker = idle_workers.pop()\n\n                    # Prune any other workers that have been idle for MAX_IDLE_TIME\n                    # seconds or longer\n                    now = cls.current_time()\n                    while idle_workers:\n                        if (\n                            now - idle_workers[0].idle_since\n                            < WorkerThread.MAX_IDLE_TIME\n                        ):\n                            break\n\n                        expired_worker = idle_workers.popleft()\n                        expired_worker.root_task.remove_done_callback(\n                            expired_worker.stop\n                        )\n                        expired_worker.stop()\n\n                context = copy_context()\n                context.run(sniffio.current_async_library_cvar.set, None)\n                worker.queue.put_nowait((context, func, args, future))\n                return await future\n\n    @classmethod\n    def run_async_from_thread(\n        cls,\n        func: Callable[..., Awaitable[T_Retval]],\n        args: tuple[Any, ...],\n        token: object,\n    ) -> T_Retval:\n        loop = cast(AbstractEventLoop, token)\n        context = copy_context()\n        context.run(sniffio.current_async_library_cvar.set, \"asyncio\")\n        f: concurrent.futures.Future[T_Retval] = context.run(\n            asyncio.run_coroutine_threadsafe, func(*args), loop\n        )\n        return f.result()\n\n    @classmethod\n    def run_sync_from_thread(\n        cls, func: Callable[..., T_Retval], args: tuple[Any, ...], token: object\n    ) -> T_Retval:\n        @wraps(func)\n        def wrapper() -> None:\n            try:\n                sniffio.current_async_library_cvar.set(\"asyncio\")\n                f.set_result(func(*args))\n            except BaseException as exc:\n                f.set_exception(exc)\n                if not isinstance(exc, Exception):\n                    raise\n\n        f: concurrent.futures.Future[T_Retval] = Future()\n        loop = cast(AbstractEventLoop, token)\n        loop.call_soon_threadsafe(wrapper)\n        return f.result()\n\n    @classmethod\n    def create_blocking_portal(cls) -> abc.BlockingPortal:\n        return BlockingPortal()\n\n    @classmethod\n    async def open_process(\n        cls,\n        command: str | bytes | Sequence[str | bytes],\n        *,\n        shell: bool,\n        stdin: int | IO[Any] | None,\n        stdout: int | IO[Any] | None,\n        stderr: int | IO[Any] | None,\n        cwd: str | bytes | PathLike | None = None,\n        env: Mapping[str, str] | None = None,\n        start_new_session: bool = False,\n    ) -> Process:\n        await cls.checkpoint()\n        if shell:\n            process = await asyncio.create_subprocess_shell(\n                cast(\"str | bytes\", command),\n                stdin=stdin,\n                stdout=stdout,\n                stderr=stderr,\n                cwd=cwd,\n                env=env,\n                start_new_session=start_new_session,\n            )\n        else:\n            process = await asyncio.create_subprocess_exec(\n                *command,\n                stdin=stdin,\n                stdout=stdout,\n                stderr=stderr,\n                cwd=cwd,\n                env=env,\n                start_new_session=start_new_session,\n            )\n\n        stdin_stream = StreamWriterWrapper(process.stdin) if process.stdin else None\n        stdout_stream = StreamReaderWrapper(process.stdout) if process.stdout else None\n        stderr_stream = StreamReaderWrapper(process.stderr) if process.stderr else None\n        return Process(process, stdin_stream, stdout_stream, stderr_stream)\n\n    @classmethod\n    def setup_process_pool_exit_at_shutdown(cls, workers: set[abc.Process]) -> None:\n        create_task(\n            _shutdown_process_pool_on_exit(workers),\n            name=\"AnyIO process pool shutdown task\",\n        )\n        find_root_task().add_done_callback(\n            partial(_forcibly_shutdown_process_pool_on_exit, workers)\n        )\n\n    @classmethod\n    async def connect_tcp(\n        cls, host: str, port: int, local_address: IPSockAddrType | None = None\n    ) -> abc.SocketStream:\n        transport, protocol = cast(\n            Tuple[asyncio.Transport, StreamProtocol],\n            await get_running_loop().create_connection(\n                StreamProtocol, host, port, local_addr=local_address\n            ),\n        )\n        transport.pause_reading()\n        return SocketStream(transport, protocol)\n\n    @classmethod\n    async def connect_unix(cls, path: str) -> abc.UNIXSocketStream:\n        await cls.checkpoint()\n        loop = get_running_loop()\n        raw_socket = socket.socket(socket.AF_UNIX)\n        raw_socket.setblocking(False)\n        while True:\n            try:\n                raw_socket.connect(path)\n            except BlockingIOError:\n                f: asyncio.Future = asyncio.Future()\n                loop.add_writer(raw_socket, f.set_result, None)\n                f.add_done_callback(lambda _: loop.remove_writer(raw_socket))\n                await f\n            except BaseException:\n                raw_socket.close()\n                raise\n            else:\n                return UNIXSocketStream(raw_socket)\n\n    @classmethod\n    def create_tcp_listener(cls, sock: socket.socket) -> SocketListener:\n        return TCPSocketListener(sock)\n\n    @classmethod\n    def create_unix_listener(cls, sock: socket.socket) -> SocketListener:\n        return UNIXSocketListener(sock)\n\n    @classmethod\n    async def create_udp_socket(\n        cls,\n        family: AddressFamily,\n        local_address: IPSockAddrType | None,\n        remote_address: IPSockAddrType | None,\n        reuse_port: bool,\n    ) -> UDPSocket | ConnectedUDPSocket:\n        transport, protocol = await get_running_loop().create_datagram_endpoint(\n            DatagramProtocol,\n            local_addr=local_address,\n            remote_addr=remote_address,\n            family=family,\n            reuse_port=reuse_port,\n        )\n        if protocol.exception:\n            transport.close()\n            raise protocol.exception\n\n        if not remote_address:\n            return UDPSocket(transport, protocol)\n        else:\n            return ConnectedUDPSocket(transport, protocol)\n\n    @classmethod\n    async def create_unix_datagram_socket(  # type: ignore[override]\n        cls, raw_socket: socket.socket, remote_path: str | None\n    ) -> abc.UNIXDatagramSocket | abc.ConnectedUNIXDatagramSocket:\n        await cls.checkpoint()\n        loop = get_running_loop()\n\n        if remote_path:\n            while True:\n                try:\n                    raw_socket.connect(remote_path)\n                except BlockingIOError:\n                    f: asyncio.Future = asyncio.Future()\n                    loop.add_writer(raw_socket, f.set_result, None)\n                    f.add_done_callback(lambda _: loop.remove_writer(raw_socket))\n                    await f\n                except BaseException:\n                    raw_socket.close()\n                    raise\n                else:\n                    return ConnectedUNIXDatagramSocket(raw_socket)\n        else:\n            return UNIXDatagramSocket(raw_socket)\n\n    @classmethod\n    async def getaddrinfo(\n        cls,\n        host: bytes | str | None,\n        port: str | int | None,\n        *,\n        family: int | AddressFamily = 0,\n        type: int | SocketKind = 0,\n        proto: int = 0,\n        flags: int = 0,\n    ) -> list[\n        tuple[\n            AddressFamily,\n            SocketKind,\n            int,\n            str,\n            tuple[str, int] | tuple[str, int, int, int],\n        ]\n    ]:\n        return await get_running_loop().getaddrinfo(\n            host, port, family=family, type=type, proto=proto, flags=flags\n        )\n\n    @classmethod\n    async def getnameinfo(\n        cls, sockaddr: IPSockAddrType, flags: int = 0\n    ) -> tuple[str, str]:\n        return await get_running_loop().getnameinfo(sockaddr, flags)\n\n    @classmethod\n    async def wait_socket_readable(cls, sock: socket.socket) -> None:\n        await cls.checkpoint()\n        try:\n            read_events = _read_events.get()\n        except LookupError:\n            read_events = {}\n            _read_events.set(read_events)\n\n        if read_events.get(sock):\n            raise BusyResourceError(\"reading from\") from None\n\n        loop = get_running_loop()\n        event = read_events[sock] = asyncio.Event()\n        loop.add_reader(sock, event.set)\n        try:\n            await event.wait()\n        finally:\n            if read_events.pop(sock, None) is not None:\n                loop.remove_reader(sock)\n                readable = True\n            else:\n                readable = False\n\n        if not readable:\n            raise ClosedResourceError\n\n    @classmethod\n    async def wait_socket_writable(cls, sock: socket.socket) -> None:\n        await cls.checkpoint()\n        try:\n            write_events = _write_events.get()\n        except LookupError:\n            write_events = {}\n            _write_events.set(write_events)\n\n        if write_events.get(sock):\n            raise BusyResourceError(\"writing to\") from None\n\n        loop = get_running_loop()\n        event = write_events[sock] = asyncio.Event()\n        loop.add_writer(sock.fileno(), event.set)\n        try:\n            await event.wait()\n        finally:\n            if write_events.pop(sock, None) is not None:\n                loop.remove_writer(sock)\n                writable = True\n            else:\n                writable = False\n\n        if not writable:\n            raise ClosedResourceError\n\n    @classmethod\n    def current_default_thread_limiter(cls) -> CapacityLimiter:\n        try:\n            return _default_thread_limiter.get()\n        except LookupError:\n            limiter = CapacityLimiter(40)\n            _default_thread_limiter.set(limiter)\n            return limiter\n\n    @classmethod\n    def open_signal_receiver(\n        cls, *signals: Signals\n    ) -> ContextManager[AsyncIterator[Signals]]:\n        return _SignalReceiver(signals)\n\n    @classmethod\n    def get_current_task(cls) -> TaskInfo:\n        return _create_task_info(current_task())  # type: ignore[arg-type]\n\n    @classmethod\n    def get_running_tasks(cls) -> list[TaskInfo]:\n        return [_create_task_info(task) for task in all_tasks() if not task.done()]\n\n    @classmethod\n    async def wait_all_tasks_blocked(cls) -> None:\n        await cls.checkpoint()\n        this_task = current_task()\n        while True:\n            for task in all_tasks():\n                if task is this_task:\n                    continue\n\n                waiter = task._fut_waiter  # type: ignore[attr-defined]\n                if waiter is None or waiter.done():\n                    await sleep(0.1)\n                    break\n            else:\n                return\n\n    @classmethod\n    def create_test_runner(cls, options: dict[str, Any]) -> TestRunner:\n        return TestRunner(**options)\n\n\nbackend_class = AsyncIOBackend\n", 2412], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/to_thread.py": ["from __future__ import annotations\n\nfrom collections.abc import Callable\nfrom typing import TypeVar\n\nfrom ._core._eventloop import get_async_backend\nfrom .abc import CapacityLimiter\n\nT_Retval = TypeVar(\"T_Retval\")\n\n\nasync def run_sync(\n    func: Callable[..., T_Retval],\n    *args: object,\n    cancellable: bool = False,\n    limiter: CapacityLimiter | None = None,\n) -> T_Retval:\n    \"\"\"\n    Call the given function with the given arguments in a worker thread.\n\n    If the ``cancellable`` option is enabled and the task waiting for its completion is\n    cancelled, the thread will still run its course but its return value (or any raised\n    exception) will be ignored.\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :param cancellable: ``True`` to allow cancellation of the operation\n    :param limiter: capacity limiter to use to limit the total amount of threads running\n        (if omitted, the default limiter is used)\n    :return: an awaitable that yields the return value of the function.\n\n    \"\"\"\n    return await get_async_backend().run_sync_in_worker_thread(\n        func, args, cancellable=cancellable, limiter=limiter\n    )\n\n\ndef current_default_thread_limiter() -> CapacityLimiter:\n    \"\"\"\n    Return the capacity limiter that is used by default to limit the number of\n    concurrent threads.\n\n    :return: a capacity limiter object\n\n    \"\"\"\n    return get_async_backend().current_default_thread_limiter()\n", 46], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/concurrency.py": ["import functools\nimport sys\nimport typing\nimport warnings\n\nimport anyio\n\nif sys.version_info >= (3, 10):  # pragma: no cover\n    from typing import ParamSpec\nelse:  # pragma: no cover\n    from typing_extensions import ParamSpec\n\n\nT = typing.TypeVar(\"T\")\nP = ParamSpec(\"P\")\n\n\nasync def run_until_first_complete(*args: typing.Tuple[typing.Callable, dict]) -> None:\n    warnings.warn(\n        \"run_until_first_complete is deprecated \"\n        \"and will be removed in a future version.\",\n        DeprecationWarning,\n    )\n\n    async with anyio.create_task_group() as task_group:\n\n        async def run(func: typing.Callable[[], typing.Coroutine]) -> None:\n            await func()\n            task_group.cancel_scope.cancel()\n\n        for func, kwargs in args:\n            task_group.start_soon(run, functools.partial(func, **kwargs))\n\n\nasync def run_in_threadpool(\n    func: typing.Callable[P, T], *args: P.args, **kwargs: P.kwargs\n) -> T:\n    if kwargs:  # pragma: no cover\n        # run_sync doesn't accept 'kwargs', so bind them in here\n        func = functools.partial(func, **kwargs)\n    return await anyio.to_thread.run_sync(func, *args)\n\n\nclass _StopIteration(Exception):\n    pass\n\n\ndef _next(iterator: typing.Iterator[T]) -> T:\n    # We can't raise `StopIteration` from within the threadpool iterator\n    # and catch it outside that context, so we coerce them into a different\n    # exception type.\n    try:\n        return next(iterator)\n    except StopIteration:\n        raise _StopIteration\n\n\nasync def iterate_in_threadpool(\n    iterator: typing.Iterator[T],\n) -> typing.AsyncIterator[T]:\n    while True:\n        try:\n            yield await anyio.to_thread.run_sync(_next, iterator)\n        except _StopIteration:\n            break\n", 65], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/middleware/asyncexitstack.py": ["from typing import Optional\n\nfrom fastapi.concurrency import AsyncExitStack\nfrom starlette.types import ASGIApp, Receive, Scope, Send\n\n\nclass AsyncExitStackMiddleware:\n    def __init__(self, app: ASGIApp, context_name: str = \"fastapi_astack\") -> None:\n        self.app = app\n        self.context_name = context_name\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        dependency_exception: Optional[Exception] = None\n        async with AsyncExitStack() as stack:\n            scope[self.context_name] = stack\n            try:\n                await self.app(scope, receive, send)\n            except Exception as e:\n                dependency_exception = e\n                raise e\n        if dependency_exception:\n            # This exception was possibly handled by the dependency but it should\n            # still bubble up so that the ServerErrorMiddleware can return a 500\n            # or the ExceptionMiddleware can catch and handle any other exceptions\n            raise dependency_exception\n", 25], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py": ["import typing\n\nfrom starlette._utils import is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.exceptions import HTTPException, WebSocketException\nfrom starlette.requests import Request\nfrom starlette.responses import PlainTextResponse, Response\nfrom starlette.types import ASGIApp, Message, Receive, Scope, Send\nfrom starlette.websockets import WebSocket\n\n\nclass ExceptionMiddleware:\n    def __init__(\n        self,\n        app: ASGIApp,\n        handlers: typing.Optional[\n            typing.Mapping[typing.Any, typing.Callable[[Request, Exception], Response]]\n        ] = None,\n        debug: bool = False,\n    ) -> None:\n        self.app = app\n        self.debug = debug  # TODO: We ought to handle 404 cases if debug is set.\n        self._status_handlers: typing.Dict[int, typing.Callable] = {}\n        self._exception_handlers: typing.Dict[\n            typing.Type[Exception], typing.Callable\n        ] = {\n            HTTPException: self.http_exception,\n            WebSocketException: self.websocket_exception,\n        }\n        if handlers is not None:\n            for key, value in handlers.items():\n                self.add_exception_handler(key, value)\n\n    def add_exception_handler(\n        self,\n        exc_class_or_status_code: typing.Union[int, typing.Type[Exception]],\n        handler: typing.Callable[[Request, Exception], Response],\n    ) -> None:\n        if isinstance(exc_class_or_status_code, int):\n            self._status_handlers[exc_class_or_status_code] = handler\n        else:\n            assert issubclass(exc_class_or_status_code, Exception)\n            self._exception_handlers[exc_class_or_status_code] = handler\n\n    def _lookup_exception_handler(\n        self, exc: Exception\n    ) -> typing.Optional[typing.Callable]:\n        for cls in type(exc).__mro__:\n            if cls in self._exception_handlers:\n                return self._exception_handlers[cls]\n        return None\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] not in (\"http\", \"websocket\"):\n            await self.app(scope, receive, send)\n            return\n\n        response_started = False\n\n        async def sender(message: Message) -> None:\n            nonlocal response_started\n\n            if message[\"type\"] == \"http.response.start\":\n                response_started = True\n            await send(message)\n\n        try:\n            await self.app(scope, receive, sender)\n        except Exception as exc:\n            handler = None\n\n            if isinstance(exc, HTTPException):\n                handler = self._status_handlers.get(exc.status_code)\n\n            if handler is None:\n                handler = self._lookup_exception_handler(exc)\n\n            if handler is None:\n                raise exc\n\n            if response_started:\n                msg = \"Caught handled exception, but response already started.\"\n                raise RuntimeError(msg) from exc\n\n            if scope[\"type\"] == \"http\":\n                request = Request(scope, receive=receive)\n                if is_async_callable(handler):\n                    response = await handler(request, exc)\n                else:\n                    response = await run_in_threadpool(handler, request, exc)\n                await response(scope, receive, sender)\n            elif scope[\"type\"] == \"websocket\":\n                websocket = WebSocket(scope, receive=receive, send=send)\n                if is_async_callable(handler):\n                    await handler(websocket, exc)\n                else:\n                    await run_in_threadpool(handler, websocket, exc)\n\n    def http_exception(self, request: Request, exc: HTTPException) -> Response:\n        if exc.status_code in {204, 304}:\n            return Response(status_code=exc.status_code, headers=exc.headers)\n        return PlainTextResponse(\n            exc.detail, status_code=exc.status_code, headers=exc.headers\n        )\n\n    async def websocket_exception(\n        self, websocket: WebSocket, exc: WebSocketException\n    ) -> None:\n        await websocket.close(code=exc.code, reason=exc.reason)\n", 109], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py": ["\"\"\"Base implementation of event loop.\n\nThe event loop can be broken up into a multiplexer (the part\nresponsible for notifying us of I/O events) and the event loop proper,\nwhich wraps a multiplexer with functionality for scheduling callbacks,\nimmediately or at a given time in the future.\n\nWhenever a public API takes a callback, subsequent positional\narguments will be passed to the callback if/when it is called.  This\navoids the proliferation of trivial lambdas implementing closures.\nKeyword arguments for the callback are not supported; this is a\nconscious design decision, leaving the door open for keyword arguments\nto modify the meaning of the API call itself.\n\"\"\"\n\nimport collections\nimport collections.abc\nimport concurrent.futures\nimport functools\nimport heapq\nimport itertools\nimport os\nimport socket\nimport stat\nimport subprocess\nimport threading\nimport time\nimport traceback\nimport sys\nimport warnings\nimport weakref\n\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import staggered\nfrom . import tasks\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\n__all__ = 'BaseEventLoop','Server',\n\n\n# Minimum number of _scheduled timer handles before cleanup of\n# cancelled handles is performed.\n_MIN_SCHEDULED_TIMER_HANDLES = 100\n\n# Minimum fraction of _scheduled timer handles that are cancelled\n# before cleanup of cancelled handles is performed.\n_MIN_CANCELLED_TIMER_HANDLES_FRACTION = 0.5\n\n\n_HAS_IPv6 = hasattr(socket, 'AF_INET6')\n\n# Maximum timeout passed to select to avoid OS limitations\nMAXIMUM_SELECT_TIMEOUT = 24 * 3600\n\n# Used for deprecation and removal of `loop.create_datagram_endpoint()`'s\n# *reuse_address* parameter\n_unset = object()\n\n\ndef _format_handle(handle):\n    cb = handle._callback\n    if isinstance(getattr(cb, '__self__', None), tasks.Task):\n        # format the task\n        return repr(cb.__self__)\n    else:\n        return str(handle)\n\n\ndef _format_pipe(fd):\n    if fd == subprocess.PIPE:\n        return '<pipe>'\n    elif fd == subprocess.STDOUT:\n        return '<stdout>'\n    else:\n        return repr(fd)\n\n\ndef _set_reuseport(sock):\n    if not hasattr(socket, 'SO_REUSEPORT'):\n        raise ValueError('reuse_port not supported by socket module')\n    else:\n        try:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n        except OSError:\n            raise ValueError('reuse_port not supported by socket module, '\n                             'SO_REUSEPORT defined but not implemented.')\n\n\ndef _ipaddr_info(host, port, family, type, proto, flowinfo=0, scopeid=0):\n    # Try to skip getaddrinfo if \"host\" is already an IP. Users might have\n    # handled name resolution in their own code and pass in resolved IPs.\n    if not hasattr(socket, 'inet_pton'):\n        return\n\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or \\\n            host is None:\n        return None\n\n    if type == socket.SOCK_STREAM:\n        proto = socket.IPPROTO_TCP\n    elif type == socket.SOCK_DGRAM:\n        proto = socket.IPPROTO_UDP\n    else:\n        return None\n\n    if port is None:\n        port = 0\n    elif isinstance(port, bytes) and port == b'':\n        port = 0\n    elif isinstance(port, str) and port == '':\n        port = 0\n    else:\n        # If port's a service name like \"http\", don't skip getaddrinfo.\n        try:\n            port = int(port)\n        except (TypeError, ValueError):\n            return None\n\n    if family == socket.AF_UNSPEC:\n        afs = [socket.AF_INET]\n        if _HAS_IPv6:\n            afs.append(socket.AF_INET6)\n    else:\n        afs = [family]\n\n    if isinstance(host, bytes):\n        host = host.decode('idna')\n    if '%' in host:\n        # Linux's inet_pton doesn't accept an IPv6 zone index after host,\n        # like '::1%lo0'.\n        return None\n\n    for af in afs:\n        try:\n            socket.inet_pton(af, host)\n            # The host has already been resolved.\n            if _HAS_IPv6 and af == socket.AF_INET6:\n                return af, type, proto, '', (host, port, flowinfo, scopeid)\n            else:\n                return af, type, proto, '', (host, port)\n        except OSError:\n            pass\n\n    # \"host\" is not an IP address.\n    return None\n\n\ndef _interleave_addrinfos(addrinfos, first_address_family_count=1):\n    \"\"\"Interleave list of addrinfo tuples by family.\"\"\"\n    # Group addresses by family\n    addrinfos_by_family = collections.OrderedDict()\n    for addr in addrinfos:\n        family = addr[0]\n        if family not in addrinfos_by_family:\n            addrinfos_by_family[family] = []\n        addrinfos_by_family[family].append(addr)\n    addrinfos_lists = list(addrinfos_by_family.values())\n\n    reordered = []\n    if first_address_family_count > 1:\n        reordered.extend(addrinfos_lists[0][:first_address_family_count - 1])\n        del addrinfos_lists[0][:first_address_family_count - 1]\n    reordered.extend(\n        a for a in itertools.chain.from_iterable(\n            itertools.zip_longest(*addrinfos_lists)\n        ) if a is not None)\n    return reordered\n\n\ndef _run_until_complete_cb(fut):\n    if not fut.cancelled():\n        exc = fut.exception()\n        if isinstance(exc, (SystemExit, KeyboardInterrupt)):\n            # Issue #22429: run_forever() already finished, no need to\n            # stop it.\n            return\n    futures._get_loop(fut).stop()\n\n\nif hasattr(socket, 'TCP_NODELAY'):\n    def _set_nodelay(sock):\n        if (sock.family in {socket.AF_INET, socket.AF_INET6} and\n                sock.type == socket.SOCK_STREAM and\n                sock.proto == socket.IPPROTO_TCP):\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\nelse:\n    def _set_nodelay(sock):\n        pass\n\n\ndef _check_ssl_socket(sock):\n    if ssl is not None and isinstance(sock, ssl.SSLSocket):\n        raise TypeError(\"Socket cannot be of type SSLSocket\")\n\n\nclass _SendfileFallbackProtocol(protocols.Protocol):\n    def __init__(self, transp):\n        if not isinstance(transp, transports._FlowControlMixin):\n            raise TypeError(\"transport should be _FlowControlMixin instance\")\n        self._transport = transp\n        self._proto = transp.get_protocol()\n        self._should_resume_reading = transp.is_reading()\n        self._should_resume_writing = transp._protocol_paused\n        transp.pause_reading()\n        transp.set_protocol(self)\n        if self._should_resume_writing:\n            self._write_ready_fut = self._transport._loop.create_future()\n        else:\n            self._write_ready_fut = None\n\n    async def drain(self):\n        if self._transport.is_closing():\n            raise ConnectionError(\"Connection closed by peer\")\n        fut = self._write_ready_fut\n        if fut is None:\n            return\n        await fut\n\n    def connection_made(self, transport):\n        raise RuntimeError(\"Invalid state: \"\n                           \"connection should have been established already.\")\n\n    def connection_lost(self, exc):\n        if self._write_ready_fut is not None:\n            # Never happens if peer disconnects after sending the whole content\n            # Thus disconnection is always an exception from user perspective\n            if exc is None:\n                self._write_ready_fut.set_exception(\n                    ConnectionError(\"Connection is closed by peer\"))\n            else:\n                self._write_ready_fut.set_exception(exc)\n        self._proto.connection_lost(exc)\n\n    def pause_writing(self):\n        if self._write_ready_fut is not None:\n            return\n        self._write_ready_fut = self._transport._loop.create_future()\n\n    def resume_writing(self):\n        if self._write_ready_fut is None:\n            return\n        self._write_ready_fut.set_result(False)\n        self._write_ready_fut = None\n\n    def data_received(self, data):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    def eof_received(self):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    async def restore(self):\n        self._transport.set_protocol(self._proto)\n        if self._should_resume_reading:\n            self._transport.resume_reading()\n        if self._write_ready_fut is not None:\n            # Cancel the future.\n            # Basically it has no effect because protocol is switched back,\n            # no code should wait for it anymore.\n            self._write_ready_fut.cancel()\n        if self._should_resume_writing:\n            self._proto.resume_writing()\n\n\nclass Server(events.AbstractServer):\n\n    def __init__(self, loop, sockets, protocol_factory, ssl_context, backlog,\n                 ssl_handshake_timeout):\n        self._loop = loop\n        self._sockets = sockets\n        self._active_count = 0\n        self._waiters = []\n        self._protocol_factory = protocol_factory\n        self._backlog = backlog\n        self._ssl_context = ssl_context\n        self._ssl_handshake_timeout = ssl_handshake_timeout\n        self._serving = False\n        self._serving_forever_fut = None\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__} sockets={self.sockets!r}>'\n\n    def _attach(self):\n        assert self._sockets is not None\n        self._active_count += 1\n\n    def _detach(self):\n        assert self._active_count > 0\n        self._active_count -= 1\n        if self._active_count == 0 and self._sockets is None:\n            self._wakeup()\n\n    def _wakeup(self):\n        waiters = self._waiters\n        self._waiters = None\n        for waiter in waiters:\n            if not waiter.done():\n                waiter.set_result(waiter)\n\n    def _start_serving(self):\n        if self._serving:\n            return\n        self._serving = True\n        for sock in self._sockets:\n            sock.listen(self._backlog)\n            self._loop._start_serving(\n                self._protocol_factory, sock, self._ssl_context,\n                self, self._backlog, self._ssl_handshake_timeout)\n\n    def get_loop(self):\n        return self._loop\n\n    def is_serving(self):\n        return self._serving\n\n    @property\n    def sockets(self):\n        if self._sockets is None:\n            return ()\n        return tuple(trsock.TransportSocket(s) for s in self._sockets)\n\n    def close(self):\n        sockets = self._sockets\n        if sockets is None:\n            return\n        self._sockets = None\n\n        for sock in sockets:\n            self._loop._stop_serving(sock)\n\n        self._serving = False\n\n        if (self._serving_forever_fut is not None and\n                not self._serving_forever_fut.done()):\n            self._serving_forever_fut.cancel()\n            self._serving_forever_fut = None\n\n        if self._active_count == 0:\n            self._wakeup()\n\n    async def start_serving(self):\n        self._start_serving()\n        # Skip one loop iteration so that all 'loop.add_reader'\n        # go through.\n        await tasks.sleep(0)\n\n    async def serve_forever(self):\n        if self._serving_forever_fut is not None:\n            raise RuntimeError(\n                f'server {self!r} is already being awaited on serve_forever()')\n        if self._sockets is None:\n            raise RuntimeError(f'server {self!r} is closed')\n\n        self._start_serving()\n        self._serving_forever_fut = self._loop.create_future()\n\n        try:\n            await self._serving_forever_fut\n        except exceptions.CancelledError:\n            try:\n                self.close()\n                await self.wait_closed()\n            finally:\n                raise\n        finally:\n            self._serving_forever_fut = None\n\n    async def wait_closed(self):\n        if self._sockets is None or self._waiters is None:\n            return\n        waiter = self._loop.create_future()\n        self._waiters.append(waiter)\n        await waiter\n\n\nclass BaseEventLoop(events.AbstractEventLoop):\n\n    def __init__(self):\n        self._timer_cancelled_count = 0\n        self._closed = False\n        self._stopping = False\n        self._ready = collections.deque()\n        self._scheduled = []\n        self._default_executor = None\n        self._internal_fds = 0\n        # Identifier of the thread running the event loop, or None if the\n        # event loop is not running\n        self._thread_id = None\n        self._clock_resolution = time.get_clock_info('monotonic').resolution\n        self._exception_handler = None\n        self.set_debug(coroutines._is_debug_mode())\n        # In debug mode, if the execution of a callback or a step of a task\n        # exceed this duration in seconds, the slow callback/task is logged.\n        self.slow_callback_duration = 0.1\n        self._current_handle = None\n        self._task_factory = None\n        self._coroutine_origin_tracking_enabled = False\n        self._coroutine_origin_tracking_saved_depth = None\n\n        # A weak set of all asynchronous generators that are\n        # being iterated by the loop.\n        self._asyncgens = weakref.WeakSet()\n        # Set to True when `loop.shutdown_asyncgens` is called.\n        self._asyncgens_shutdown_called = False\n        # Set to True when `loop.shutdown_default_executor` is called.\n        self._executor_shutdown_called = False\n\n    def __repr__(self):\n        return (\n            f'<{self.__class__.__name__} running={self.is_running()} '\n            f'closed={self.is_closed()} debug={self.get_debug()}>'\n        )\n\n    def create_future(self):\n        \"\"\"Create a Future object attached to the loop.\"\"\"\n        return futures.Future(loop=self)\n\n    def create_task(self, coro, *, name=None):\n        \"\"\"Schedule a coroutine object.\n\n        Return a task object.\n        \"\"\"\n        self._check_closed()\n        if self._task_factory is None:\n            task = tasks.Task(coro, loop=self, name=name)\n            if task._source_traceback:\n                del task._source_traceback[-1]\n        else:\n            task = self._task_factory(self, coro)\n            tasks._set_task_name(task, name)\n\n        return task\n\n    def set_task_factory(self, factory):\n        \"\"\"Set a task factory that will be used by loop.create_task().\n\n        If factory is None the default task factory will be set.\n\n        If factory is a callable, it should have a signature matching\n        '(loop, coro)', where 'loop' will be a reference to the active\n        event loop, 'coro' will be a coroutine object.  The callable\n        must return a Future.\n        \"\"\"\n        if factory is not None and not callable(factory):\n            raise TypeError('task factory must be a callable or None')\n        self._task_factory = factory\n\n    def get_task_factory(self):\n        \"\"\"Return a task factory, or None if the default one is in use.\"\"\"\n        return self._task_factory\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        \"\"\"Create socket transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=None,\n            call_connection_made=True):\n        \"\"\"Create SSL transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        \"\"\"Create datagram transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        \"\"\"Create read pipe transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        \"\"\"Create write pipe transport.\"\"\"\n        raise NotImplementedError\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        \"\"\"Create subprocess transport.\"\"\"\n        raise NotImplementedError\n\n    def _write_to_self(self):\n        \"\"\"Write a byte to self-pipe, to wake up the event loop.\n\n        This may be called from a different thread.\n\n        The subclass is responsible for implementing the self-pipe.\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_events(self, event_list):\n        \"\"\"Process selector events.\"\"\"\n        raise NotImplementedError\n\n    def _check_closed(self):\n        if self._closed:\n            raise RuntimeError('Event loop is closed')\n\n    def _check_default_executor(self):\n        if self._executor_shutdown_called:\n            raise RuntimeError('Executor shutdown has been called')\n\n    def _asyncgen_finalizer_hook(self, agen):\n        self._asyncgens.discard(agen)\n        if not self.is_closed():\n            self.call_soon_threadsafe(self.create_task, agen.aclose())\n\n    def _asyncgen_firstiter_hook(self, agen):\n        if self._asyncgens_shutdown_called:\n            warnings.warn(\n                f\"asynchronous generator {agen!r} was scheduled after \"\n                f\"loop.shutdown_asyncgens() call\",\n                ResourceWarning, source=self)\n\n        self._asyncgens.add(agen)\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        self._asyncgens_shutdown_called = True\n\n        if not len(self._asyncgens):\n            # If Python version is <3.6 or we don't have any asynchronous\n            # generators alive.\n            return\n\n        closing_agens = list(self._asyncgens)\n        self._asyncgens.clear()\n\n        results = await tasks._gather(\n            *[ag.aclose() for ag in closing_agens],\n            return_exceptions=True,\n            loop=self)\n\n        for result, agen in zip(results, closing_agens):\n            if isinstance(result, Exception):\n                self.call_exception_handler({\n                    'message': f'an error occurred during closing of '\n                               f'asynchronous generator {agen!r}',\n                    'exception': result,\n                    'asyncgen': agen\n                })\n\n    async def shutdown_default_executor(self):\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n        self._executor_shutdown_called = True\n        if self._default_executor is None:\n            return\n        future = self.create_future()\n        thread = threading.Thread(target=self._do_shutdown, args=(future,))\n        thread.start()\n        try:\n            await future\n        finally:\n            thread.join()\n\n    def _do_shutdown(self, future):\n        try:\n            self._default_executor.shutdown(wait=True)\n            self.call_soon_threadsafe(future.set_result, None)\n        except Exception as ex:\n            self.call_soon_threadsafe(future.set_exception, ex)\n\n    def _check_running(self):\n        if self.is_running():\n            raise RuntimeError('This event loop is already running')\n        if events._get_running_loop() is not None:\n            raise RuntimeError(\n                'Cannot run the event loop while another loop is running')\n\n    def run_forever(self):\n        \"\"\"Run until stop() is called.\"\"\"\n        self._check_closed()\n        self._check_running()\n        self._set_coroutine_origin_tracking(self._debug)\n        self._thread_id = threading.get_ident()\n\n        old_agen_hooks = sys.get_asyncgen_hooks()\n        sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n                               finalizer=self._asyncgen_finalizer_hook)\n        try:\n            events._set_running_loop(self)\n            while True:\n                self._run_once()\n                if self._stopping:\n                    break\n        finally:\n            self._stopping = False\n            self._thread_id = None\n            events._set_running_loop(None)\n            self._set_coroutine_origin_tracking(False)\n            sys.set_asyncgen_hooks(*old_agen_hooks)\n\n    def run_until_complete(self, future):\n        \"\"\"Run until the Future is done.\n\n        If the argument is a coroutine, it is wrapped in a Task.\n\n        WARNING: It would be disastrous to call run_until_complete()\n        with the same coroutine twice -- it would wrap it in two\n        different Tasks and that can't be good.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        self._check_closed()\n        self._check_running()\n\n        new_task = not futures.isfuture(future)\n        future = tasks.ensure_future(future, loop=self)\n        if new_task:\n            # An exception is raised if the future didn't complete, so there\n            # is no need to log the \"destroy pending task\" message\n            future._log_destroy_pending = False\n\n        future.add_done_callback(_run_until_complete_cb)\n        try:\n            self.run_forever()\n        except:\n            if new_task and future.done() and not future.cancelled():\n                # The coroutine raised a BaseException. Consume the exception\n                # to not log a warning, the caller doesn't have access to the\n                # local task.\n                future.exception()\n            raise\n        finally:\n            future.remove_done_callback(_run_until_complete_cb)\n        if not future.done():\n            raise RuntimeError('Event loop stopped before Future completed.')\n\n        return future.result()\n\n    def stop(self):\n        \"\"\"Stop running the event loop.\n\n        Every callback already scheduled will still run.  This simply informs\n        run_forever to stop looping after a complete iteration.\n        \"\"\"\n        self._stopping = True\n\n    def close(self):\n        \"\"\"Close the event loop.\n\n        This clears the queues and shuts down the executor,\n        but does not wait for the executor to finish.\n\n        The event loop must not be running.\n        \"\"\"\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self._closed:\n            return\n        if self._debug:\n            logger.debug(\"Close %r\", self)\n        self._closed = True\n        self._ready.clear()\n        self._scheduled.clear()\n        self._executor_shutdown_called = True\n        executor = self._default_executor\n        if executor is not None:\n            self._default_executor = None\n            executor.shutdown(wait=False)\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        return self._closed\n\n    def __del__(self, _warn=warnings.warn):\n        if not self.is_closed():\n            _warn(f\"unclosed event loop {self!r}\", ResourceWarning, source=self)\n            if not self.is_running():\n                self.close()\n\n    def is_running(self):\n        \"\"\"Returns True if the event loop is running.\"\"\"\n        return (self._thread_id is not None)\n\n    def time(self):\n        \"\"\"Return the time according to the event loop's clock.\n\n        This is a float expressed in seconds since an epoch, but the\n        epoch, precision, accuracy and drift are unspecified and may\n        differ per event loop.\n        \"\"\"\n        return time.monotonic()\n\n    def call_later(self, delay, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called at a given time.\n\n        Return a Handle: an opaque object with a cancel() method that\n        can be used to cancel the call.\n\n        The delay can be an int or float, expressed in seconds.  It is\n        always relative to the current time.\n\n        Each callback will be called exactly once.  If two callbacks\n        are scheduled for exactly the same time, it undefined which\n        will be called first.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        timer = self.call_at(self.time() + delay, callback, *args,\n                             context=context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        return timer\n\n    def call_at(self, when, callback, *args, context=None):\n        \"\"\"Like call_later(), but uses an absolute time.\n\n        Absolute time corresponds to the event loop's time() method.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_at')\n        timer = events.TimerHandle(when, callback, args, self, context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        heapq.heappush(self._scheduled, timer)\n        timer._scheduled = True\n        return timer\n\n    def call_soon(self, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called as soon as possible.\n\n        This operates as a FIFO queue: callbacks are called in the\n        order in which they are registered.  Each callback will be\n        called exactly once.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_soon')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        return handle\n\n    def _check_callback(self, callback, method):\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\n                f\"coroutines cannot be used with {method}()\")\n        if not callable(callback):\n            raise TypeError(\n                f'a callable object was expected by {method}(), '\n                f'got {callback!r}')\n\n    def _call_soon(self, callback, args, context):\n        handle = events.Handle(callback, args, self, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._ready.append(handle)\n        return handle\n\n    def _check_thread(self):\n        \"\"\"Check that the current thread is the thread running the event loop.\n\n        Non-thread-safe methods of this class make this assumption and will\n        likely behave incorrectly when the assumption is violated.\n\n        Should only be called when (self._debug == True).  The caller is\n        responsible for checking this condition for performance reasons.\n        \"\"\"\n        if self._thread_id is None:\n            return\n        thread_id = threading.get_ident()\n        if thread_id != self._thread_id:\n            raise RuntimeError(\n                \"Non-thread-safe operation invoked on an event loop other \"\n                \"than the current one\")\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        \"\"\"Like call_soon(), but thread-safe.\"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_callback(callback, 'call_soon_threadsafe')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._write_to_self()\n        return handle\n\n    def run_in_executor(self, executor, func, *args):\n        self._check_closed()\n        if self._debug:\n            self._check_callback(func, 'run_in_executor')\n        if executor is None:\n            executor = self._default_executor\n            # Only check when the default executor is being used\n            self._check_default_executor()\n            if executor is None:\n                executor = concurrent.futures.ThreadPoolExecutor(\n                    thread_name_prefix='asyncio'\n                )\n                self._default_executor = executor\n        return futures.wrap_future(\n            executor.submit(func, *args), loop=self)\n\n    def set_default_executor(self, executor):\n        if not isinstance(executor, concurrent.futures.ThreadPoolExecutor):\n            warnings.warn(\n                'Using the default executor that is not an instance of '\n                'ThreadPoolExecutor is deprecated and will be prohibited '\n                'in Python 3.9',\n                DeprecationWarning, 2)\n        self._default_executor = executor\n\n    def _getaddrinfo_debug(self, host, port, family, type, proto, flags):\n        msg = [f\"{host}:{port!r}\"]\n        if family:\n            msg.append(f'family={family!r}')\n        if type:\n            msg.append(f'type={type!r}')\n        if proto:\n            msg.append(f'proto={proto!r}')\n        if flags:\n            msg.append(f'flags={flags!r}')\n        msg = ', '.join(msg)\n        logger.debug('Get address info %s', msg)\n\n        t0 = self.time()\n        addrinfo = socket.getaddrinfo(host, port, family, type, proto, flags)\n        dt = self.time() - t0\n\n        msg = f'Getting address info {msg} took {dt * 1e3:.3f}ms: {addrinfo!r}'\n        if dt >= self.slow_callback_duration:\n            logger.info(msg)\n        else:\n            logger.debug(msg)\n        return addrinfo\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        if self._debug:\n            getaddr_func = self._getaddrinfo_debug\n        else:\n            getaddr_func = socket.getaddrinfo\n\n        return await self.run_in_executor(\n            None, getaddr_func, host, port, family, type, proto, flags)\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        return await self.run_in_executor(\n            None, socket.getnameinfo, sockaddr, flags)\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=True):\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        _check_ssl_socket(sock)\n        self._check_sendfile_params(sock, file, offset, count)\n        try:\n            return await self._sock_sendfile_native(sock, file,\n                                                    offset, count)\n        except exceptions.SendfileNotAvailableError as exc:\n            if not fallback:\n                raise\n        return await self._sock_sendfile_fallback(sock, file,\n                                                  offset, count)\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        # NB: sendfile syscall is not supported for SSL sockets and\n        # non-mmap files even if sendfile is supported by OS\n        raise exceptions.SendfileNotAvailableError(\n            f\"syscall sendfile is not available for socket {sock!r} \"\n            f\"and file {file!r} combination\")\n\n    async def _sock_sendfile_fallback(self, sock, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = (\n            min(count, constants.SENDFILE_FALLBACK_READBUFFER_SIZE)\n            if count else constants.SENDFILE_FALLBACK_READBUFFER_SIZE\n        )\n        buf = bytearray(blocksize)\n        total_sent = 0\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    break  # EOF\n                await self.sock_sendall(sock, view[:read])\n                total_sent += read\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, sock, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not sock.type == socket.SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n        if not isinstance(offset, int):\n            raise TypeError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n        if offset < 0:\n            raise ValueError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n\n    async def _connect_sock(self, exceptions, addr_info, local_addr_infos=None):\n        \"\"\"Create, bind and connect one socket.\"\"\"\n        my_exceptions = []\n        exceptions.append(my_exceptions)\n        family, type_, proto, _, address = addr_info\n        sock = None\n        try:\n            sock = socket.socket(family=family, type=type_, proto=proto)\n            sock.setblocking(False)\n            if local_addr_infos is not None:\n                for _, _, _, _, laddr in local_addr_infos:\n                    try:\n                        sock.bind(laddr)\n                        break\n                    except OSError as exc:\n                        msg = (\n                            f'error while attempting to bind on '\n                            f'address {laddr!r}: '\n                            f'{exc.strerror.lower()}'\n                        )\n                        exc = OSError(exc.errno, msg)\n                        my_exceptions.append(exc)\n                else:  # all bind attempts failed\n                    raise my_exceptions.pop()\n            await self.sock_connect(sock, address)\n            return sock\n        except OSError as exc:\n            my_exceptions.append(exc)\n            if sock is not None:\n                sock.close()\n            raise\n        except:\n            if sock is not None:\n                sock.close()\n            raise\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0,\n            proto=0, flags=0, sock=None,\n            local_addr=None, server_hostname=None,\n            ssl_handshake_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        \"\"\"Connect to a TCP server.\n\n        Create a streaming transport connection to a given Internet host and\n        port: socket family AF_INET or socket.AF_INET6 depending on host (or\n        family if specified), socket type SOCK_STREAM. protocol_factory must be\n        a callable returning a protocol instance.\n\n        This method is a coroutine which will try to establish the connection\n        in the background.  When successful, the coroutine returns a\n        (transport, protocol) pair.\n        \"\"\"\n        if server_hostname is not None and not ssl:\n            raise ValueError('server_hostname is only meaningful with ssl')\n\n        if server_hostname is None and ssl:\n            # Use host as default for server_hostname.  It is an error\n            # if host is empty or not set, e.g. when an\n            # already-connected socket was passed or when only a port\n            # is given.  To avoid this error, you can pass\n            # server_hostname='' -- this will bypass the hostname\n            # check.  (This also means that if host is a numeric\n            # IP/IPv6 address, we will attempt to verify that exact\n            # address; this will probably fail, but it is possible to\n            # create a certificate for a specific IP address, so we\n            # don't judge it here.)\n            if not host:\n                raise ValueError('You must set server_hostname '\n                                 'when using ssl without a host')\n            server_hostname = host\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if happy_eyeballs_delay is not None and interleave is None:\n            # If using happy eyeballs, default to interleave addresses by family\n            interleave = 1\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            infos = await self._ensure_resolved(\n                (host, port), family=family,\n                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)\n            if not infos:\n                raise OSError('getaddrinfo() returned empty list')\n\n            if local_addr is not None:\n                laddr_infos = await self._ensure_resolved(\n                    local_addr, family=family,\n                    type=socket.SOCK_STREAM, proto=proto,\n                    flags=flags, loop=self)\n                if not laddr_infos:\n                    raise OSError('getaddrinfo() returned empty list')\n            else:\n                laddr_infos = None\n\n            if interleave:\n                infos = _interleave_addrinfos(infos, interleave)\n\n            exceptions = []\n            if happy_eyeballs_delay is None:\n                # not using happy eyeballs\n                for addrinfo in infos:\n                    try:\n                        sock = await self._connect_sock(\n                            exceptions, addrinfo, laddr_infos)\n                        break\n                    except OSError:\n                        continue\n            else:  # using happy eyeballs\n                sock, _, _ = await staggered.staggered_race(\n                    (functools.partial(self._connect_sock,\n                                       exceptions, addrinfo, laddr_infos)\n                     for addrinfo in infos),\n                    happy_eyeballs_delay, loop=self)\n\n            if sock is None:\n                exceptions = [exc for sub in exceptions for exc in sub]\n                if len(exceptions) == 1:\n                    raise exceptions[0]\n                else:\n                    # If they all have the same str(), raise one.\n                    model = str(exceptions[0])\n                    if all(str(exc) == model for exc in exceptions):\n                        raise exceptions[0]\n                    # Raise a combined exception so the user can see all\n                    # the various error messages.\n                    raise OSError('Multiple exceptions: {}'.format(\n                        ', '.join(str(exc) for exc in exceptions)))\n\n        else:\n            if sock is None:\n                raise ValueError(\n                    'host and port was not specified and no sock specified')\n            if sock.type != socket.SOCK_STREAM:\n                # We allow AF_INET, AF_INET6, AF_UNIX as long as they\n                # are SOCK_STREAM.\n                # We support passing AF_UNIX sockets even though we have\n                # a dedicated API for that: create_unix_connection.\n                # Disallowing AF_UNIX in this method, breaks backwards\n                # compatibility.\n                raise ValueError(\n                    f'A Stream Socket was expected, got {sock!r}')\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r connected to %s:%r: (%r, %r)\",\n                         sock, host, port, transport, protocol)\n        return transport, protocol\n\n    async def _create_connection_transport(\n            self, sock, protocol_factory, ssl,\n            server_hostname, server_side=False,\n            ssl_handshake_timeout=None):\n\n        sock.setblocking(False)\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        if ssl:\n            sslcontext = None if isinstance(ssl, bool) else ssl\n            transport = self._make_ssl_transport(\n                sock, protocol, sslcontext, waiter,\n                server_side=server_side, server_hostname=server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout)\n        else:\n            transport = self._make_socket_transport(sock, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file to transport.\n\n        Return the total number of bytes which were sent.\n\n        The method uses high-performance os.sendfile if available.\n\n        file must be a regular file object opened in binary mode.\n\n        offset tells from where to start reading the file. If specified,\n        count is the total number of bytes to transmit as opposed to\n        sending the file until EOF is reached. File position is updated on\n        return or also in case of error in which case file.tell()\n        can be used to figure out the number of bytes\n        which were sent.\n\n        fallback set to True makes asyncio to manually read and send\n        the file when the platform does not support the sendfile syscall\n        (e.g. Windows or SSL socket on Unix).\n\n        Raise SendfileNotAvailableError if the system does not support\n        sendfile syscall and fallback is False.\n        \"\"\"\n        if transport.is_closing():\n            raise RuntimeError(\"Transport is closing\")\n        mode = getattr(transport, '_sendfile_compatible',\n                       constants._SendfileMode.UNSUPPORTED)\n        if mode is constants._SendfileMode.UNSUPPORTED:\n            raise RuntimeError(\n                f\"sendfile is not supported for transport {transport!r}\")\n        if mode is constants._SendfileMode.TRY_NATIVE:\n            try:\n                return await self._sendfile_native(transport, file,\n                                                   offset, count)\n            except exceptions.SendfileNotAvailableError as exc:\n                if not fallback:\n                    raise\n\n        if not fallback:\n            raise RuntimeError(\n                f\"fallback is disabled and native sendfile is not \"\n                f\"supported for transport {transport!r}\")\n\n        return await self._sendfile_fallback(transport, file,\n                                             offset, count)\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        raise exceptions.SendfileNotAvailableError(\n            \"sendfile syscall is not supported\")\n\n    async def _sendfile_fallback(self, transp, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 16384) if count else 16384\n        buf = bytearray(blocksize)\n        total_sent = 0\n        proto = _SendfileFallbackProtocol(transp)\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        return total_sent\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    return total_sent  # EOF\n                await proto.drain()\n                transp.write(view[:read])\n                total_sent += read\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n            await proto.restore()\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        if ssl is None:\n            raise RuntimeError('Python ssl module is not available')\n\n        if not isinstance(sslcontext, ssl.SSLContext):\n            raise TypeError(\n                f'sslcontext is expected to be an instance of ssl.SSLContext, '\n                f'got {sslcontext!r}')\n\n        if not getattr(transport, '_start_tls_compatible', False):\n            raise TypeError(\n                f'transport {transport!r} is not supported by start_tls()')\n\n        waiter = self.create_future()\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            call_connection_made=False)\n\n        # Pause early so that \"ssl_protocol.data_received()\" doesn't\n        # have a chance to get called before \"ssl_protocol.connection_made()\".\n        transport.pause_reading()\n\n        transport.set_protocol(ssl_protocol)\n        conmade_cb = self.call_soon(ssl_protocol.connection_made, transport)\n        resume_cb = self.call_soon(transport.resume_reading)\n\n        try:\n            await waiter\n        except BaseException:\n            transport.close()\n            conmade_cb.cancel()\n            resume_cb.cancel()\n            raise\n\n        return ssl_protocol._app_transport\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=_unset, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"Create datagram connection.\"\"\"\n        if sock is not None:\n            if sock.type != socket.SOCK_DGRAM:\n                raise ValueError(\n                    f'A UDP Socket was expected, got {sock!r}')\n            if (local_addr or remote_addr or\n                    family or proto or flags or\n                    reuse_port or allow_broadcast):\n                # show the problematic kwargs in exception msg\n                opts = dict(local_addr=local_addr, remote_addr=remote_addr,\n                            family=family, proto=proto, flags=flags,\n                            reuse_address=reuse_address, reuse_port=reuse_port,\n                            allow_broadcast=allow_broadcast)\n                problems = ', '.join(f'{k}={v}' for k, v in opts.items() if v)\n                raise ValueError(\n                    f'socket modifier keyword arguments can not be used '\n                    f'when sock is specified. ({problems})')\n            sock.setblocking(False)\n            r_addr = None\n        else:\n            if not (local_addr or remote_addr):\n                if family == 0:\n                    raise ValueError('unexpected address family')\n                addr_pairs_info = (((family, proto), (None, None)),)\n            elif hasattr(socket, 'AF_UNIX') and family == socket.AF_UNIX:\n                for addr in (local_addr, remote_addr):\n                    if addr is not None and not isinstance(addr, str):\n                        raise TypeError('string is expected')\n\n                if local_addr and local_addr[0] not in (0, '\\x00'):\n                    try:\n                        if stat.S_ISSOCK(os.stat(local_addr).st_mode):\n                            os.remove(local_addr)\n                    except FileNotFoundError:\n                        pass\n                    except OSError as err:\n                        # Directory may have permissions only to create socket.\n                        logger.error('Unable to check or remove stale UNIX '\n                                     'socket %r: %r',\n                                     local_addr, err)\n\n                addr_pairs_info = (((family, proto),\n                                    (local_addr, remote_addr)), )\n            else:\n                # join address by (family, protocol)\n                addr_infos = {}  # Using order preserving dict\n                for idx, addr in ((0, local_addr), (1, remote_addr)):\n                    if addr is not None:\n                        if not (isinstance(addr, tuple) and len(addr) == 2):\n                            raise TypeError('2-tuple is expected')\n\n                        infos = await self._ensure_resolved(\n                            addr, family=family, type=socket.SOCK_DGRAM,\n                            proto=proto, flags=flags, loop=self)\n                        if not infos:\n                            raise OSError('getaddrinfo() returned empty list')\n\n                        for fam, _, pro, _, address in infos:\n                            key = (fam, pro)\n                            if key not in addr_infos:\n                                addr_infos[key] = [None, None]\n                            addr_infos[key][idx] = address\n\n                # each addr has to have info for each (family, proto) pair\n                addr_pairs_info = [\n                    (key, addr_pair) for key, addr_pair in addr_infos.items()\n                    if not ((local_addr and addr_pair[0] is None) or\n                            (remote_addr and addr_pair[1] is None))]\n\n                if not addr_pairs_info:\n                    raise ValueError('can not get address information')\n\n            exceptions = []\n\n            # bpo-37228\n            if reuse_address is not _unset:\n                if reuse_address:\n                    raise ValueError(\"Passing `reuse_address=True` is no \"\n                                     \"longer supported, as the usage of \"\n                                     \"SO_REUSEPORT in UDP poses a significant \"\n                                     \"security concern.\")\n                else:\n                    warnings.warn(\"The *reuse_address* parameter has been \"\n                                  \"deprecated as of 3.5.10 and is scheduled \"\n                                  \"for removal in 3.11.\", DeprecationWarning,\n                                  stacklevel=2)\n\n            for ((family, proto),\n                 (local_address, remote_address)) in addr_pairs_info:\n                sock = None\n                r_addr = None\n                try:\n                    sock = socket.socket(\n                        family=family, type=socket.SOCK_DGRAM, proto=proto)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    if allow_broadcast:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n                    sock.setblocking(False)\n\n                    if local_addr:\n                        sock.bind(local_address)\n                    if remote_addr:\n                        if not allow_broadcast:\n                            await self.sock_connect(sock, remote_address)\n                        r_addr = remote_address\n                except OSError as exc:\n                    if sock is not None:\n                        sock.close()\n                    exceptions.append(exc)\n                except:\n                    if sock is not None:\n                        sock.close()\n                    raise\n                else:\n                    break\n            else:\n                raise exceptions[0]\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_datagram_transport(\n            sock, protocol, r_addr, waiter)\n        if self._debug:\n            if local_addr:\n                logger.info(\"Datagram endpoint local_addr=%r remote_addr=%r \"\n                            \"created: (%r, %r)\",\n                            local_addr, remote_addr, transport, protocol)\n            else:\n                logger.debug(\"Datagram endpoint remote_addr=%r created: \"\n                             \"(%r, %r)\",\n                             remote_addr, transport, protocol)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def _ensure_resolved(self, address, *,\n                               family=0, type=socket.SOCK_STREAM,\n                               proto=0, flags=0, loop):\n        host, port = address[:2]\n        info = _ipaddr_info(host, port, family, type, proto, *address[2:])\n        if info is not None:\n            # \"host\" is already a resolved IP.\n            return [info]\n        else:\n            return await loop.getaddrinfo(host, port, family=family, type=type,\n                                          proto=proto, flags=flags)\n\n    async def _create_server_getaddrinfo(self, host, port, family, flags):\n        infos = await self._ensure_resolved((host, port), family=family,\n                                            type=socket.SOCK_STREAM,\n                                            flags=flags, loop=self)\n        if not infos:\n            raise OSError(f'getaddrinfo({host!r}) returned empty list')\n        return infos\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *,\n            family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE,\n            sock=None,\n            backlog=100,\n            ssl=None,\n            reuse_address=None,\n            reuse_port=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"Create a TCP server.\n\n        The host parameter can be a string, in that case the TCP server is\n        bound to host and port.\n\n        The host parameter can also be a sequence of strings and in that case\n        the TCP server is bound to all hosts of the sequence. If a host\n        appears multiple times (possibly indirectly e.g. when hostnames\n        resolve to the same IP address), the server is only bound once to that\n        host.\n\n        Return a Server object which can be used to stop the service.\n\n        This method is a coroutine.\n        \"\"\"\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            if reuse_address is None:\n                reuse_address = os.name == 'posix' and sys.platform != 'cygwin'\n            sockets = []\n            if host == '':\n                hosts = [None]\n            elif (isinstance(host, str) or\n                  not isinstance(host, collections.abc.Iterable)):\n                hosts = [host]\n            else:\n                hosts = host\n\n            fs = [self._create_server_getaddrinfo(host, port, family=family,\n                                                  flags=flags)\n                  for host in hosts]\n            infos = await tasks._gather(*fs, loop=self)\n            infos = set(itertools.chain.from_iterable(infos))\n\n            completed = False\n            try:\n                for res in infos:\n                    af, socktype, proto, canonname, sa = res\n                    try:\n                        sock = socket.socket(af, socktype, proto)\n                    except socket.error:\n                        # Assume it's a bad family/type/protocol combination.\n                        if self._debug:\n                            logger.warning('create_server() failed to create '\n                                           'socket.socket(%r, %r, %r)',\n                                           af, socktype, proto, exc_info=True)\n                        continue\n                    sockets.append(sock)\n                    if reuse_address:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    # Disable IPv4/IPv6 dual stack support (enabled by\n                    # default on Linux) which makes a single socket\n                    # listen on both address families.\n                    if (_HAS_IPv6 and\n                            af == socket.AF_INET6 and\n                            hasattr(socket, 'IPPROTO_IPV6')):\n                        sock.setsockopt(socket.IPPROTO_IPV6,\n                                        socket.IPV6_V6ONLY,\n                                        True)\n                    try:\n                        sock.bind(sa)\n                    except OSError as err:\n                        raise OSError(err.errno, 'error while attempting '\n                                      'to bind on address %r: %s'\n                                      % (sa, err.strerror.lower())) from None\n                completed = True\n            finally:\n                if not completed:\n                    for sock in sockets:\n                        sock.close()\n        else:\n            if sock is None:\n                raise ValueError('Neither host/port nor sock were specified')\n            if sock.type != socket.SOCK_STREAM:\n                raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n            sockets = [sock]\n\n        for sock in sockets:\n            sock.setblocking(False)\n\n        server = Server(self, sockets, protocol_factory,\n                        ssl, backlog, ssl_handshake_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0)\n\n        if self._debug:\n            logger.info(\"%r is serving\", server)\n        return server\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None):\n        \"\"\"Handle an accepted connection.\n\n        This is used by servers that accept connections outside of\n        asyncio but that use asyncio to handle connections.\n\n        This method is a coroutine.  When completed, the coroutine\n        returns a (transport, protocol) pair.\n        \"\"\"\n        if sock.type != socket.SOCK_STREAM:\n            raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, '', server_side=True,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r handled: (%r, %r)\", sock, transport, protocol)\n        return transport, protocol\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_read_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Read pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_write_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Write pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    def _log_subprocess(self, msg, stdin, stdout, stderr):\n        info = [msg]\n        if stdin is not None:\n            info.append(f'stdin={_format_pipe(stdin)}')\n        if stdout is not None and stderr == subprocess.STDOUT:\n            info.append(f'stdout=stderr={_format_pipe(stdout)}')\n        else:\n            if stdout is not None:\n                info.append(f'stdout={_format_pipe(stdout)}')\n            if stderr is not None:\n                info.append(f'stderr={_format_pipe(stderr)}')\n        logger.debug(' '.join(info))\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               universal_newlines=False,\n                               shell=True, bufsize=0,\n                               encoding=None, errors=None, text=None,\n                               **kwargs):\n        if not isinstance(cmd, (bytes, str)):\n            raise ValueError(\"cmd must be a string\")\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if not shell:\n            raise ValueError(\"shell must be True\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = 'run shell command %r' % cmd\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, cmd, True, stdin, stdout, stderr, bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    async def subprocess_exec(self, protocol_factory, program, *args,\n                              stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE, universal_newlines=False,\n                              shell=False, bufsize=0,\n                              encoding=None, errors=None, text=None,\n                              **kwargs):\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if shell:\n            raise ValueError(\"shell must be False\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        popen_args = (program,) + args\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = f'execute program {program!r}'\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, popen_args, False, stdin, stdout, stderr,\n            bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    def get_exception_handler(self):\n        \"\"\"Return an exception handler, or None if the default one is in use.\n        \"\"\"\n        return self._exception_handler\n\n    def set_exception_handler(self, handler):\n        \"\"\"Set handler as the new event loop exception handler.\n\n        If handler is None, the default exception handler will\n        be set.\n\n        If handler is a callable object, it should have a\n        signature matching '(loop, context)', where 'loop'\n        will be a reference to the active event loop, 'context'\n        will be a dict object (see `call_exception_handler()`\n        documentation for details about context).\n        \"\"\"\n        if handler is not None and not callable(handler):\n            raise TypeError(f'A callable object or None is expected, '\n                            f'got {handler!r}')\n        self._exception_handler = handler\n\n    def default_exception_handler(self, context):\n        \"\"\"Default exception handler.\n\n        This is called when an exception occurs and no exception\n        handler is set, and can be called by a custom exception\n        handler that wants to defer to the default behavior.\n\n        This default handler logs the error message and other\n        context-dependent information.  In debug mode, a truncated\n        stack trace is also appended showing where the given object\n        (e.g. a handle or future or task) was created, if any.\n\n        The context parameter has the same meaning as in\n        `call_exception_handler()`.\n        \"\"\"\n        message = context.get('message')\n        if not message:\n            message = 'Unhandled exception in event loop'\n\n        exception = context.get('exception')\n        if exception is not None:\n            exc_info = (type(exception), exception, exception.__traceback__)\n        else:\n            exc_info = False\n\n        if ('source_traceback' not in context and\n                self._current_handle is not None and\n                self._current_handle._source_traceback):\n            context['handle_traceback'] = \\\n                self._current_handle._source_traceback\n\n        log_lines = [message]\n        for key in sorted(context):\n            if key in {'message', 'exception'}:\n                continue\n            value = context[key]\n            if key == 'source_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Object created at (most recent call last):\\n'\n                value += tb.rstrip()\n            elif key == 'handle_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Handle created at (most recent call last):\\n'\n                value += tb.rstrip()\n            else:\n                value = repr(value)\n            log_lines.append(f'{key}: {value}')\n\n        logger.error('\\n'.join(log_lines), exc_info=exc_info)\n\n    def call_exception_handler(self, context):\n        \"\"\"Call the current event loop's exception handler.\n\n        The context argument is a dict containing the following keys:\n\n        - 'message': Error message;\n        - 'exception' (optional): Exception object;\n        - 'future' (optional): Future instance;\n        - 'task' (optional): Task instance;\n        - 'handle' (optional): Handle instance;\n        - 'protocol' (optional): Protocol instance;\n        - 'transport' (optional): Transport instance;\n        - 'socket' (optional): Socket instance;\n        - 'asyncgen' (optional): Asynchronous generator that caused\n                                 the exception.\n\n        New keys maybe introduced in the future.\n\n        Note: do not overload this method in an event loop subclass.\n        For custom exception handling, use the\n        `set_exception_handler()` method.\n        \"\"\"\n        if self._exception_handler is None:\n            try:\n                self.default_exception_handler(context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                # Second protection layer for unexpected errors\n                # in the default implementation, as well as for subclassed\n                # event loops with overloaded \"default_exception_handler\".\n                logger.error('Exception in default exception handler',\n                             exc_info=True)\n        else:\n            try:\n                self._exception_handler(self, context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                # Exception in the user set custom exception handler.\n                try:\n                    # Let's try default handler.\n                    self.default_exception_handler({\n                        'message': 'Unhandled error in exception handler',\n                        'exception': exc,\n                        'context': context,\n                    })\n                except (SystemExit, KeyboardInterrupt):\n                    raise\n                except BaseException:\n                    # Guard 'default_exception_handler' in case it is\n                    # overloaded.\n                    logger.error('Exception in default exception handler '\n                                 'while handling an unexpected error '\n                                 'in custom exception handler',\n                                 exc_info=True)\n\n    def _add_callback(self, handle):\n        \"\"\"Add a Handle to _scheduled (TimerHandle) or _ready.\"\"\"\n        assert isinstance(handle, events.Handle), 'A Handle is required here'\n        if handle._cancelled:\n            return\n        assert not isinstance(handle, events.TimerHandle)\n        self._ready.append(handle)\n\n    def _add_callback_signalsafe(self, handle):\n        \"\"\"Like _add_callback() but called from a signal handler.\"\"\"\n        self._add_callback(handle)\n        self._write_to_self()\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        if handle._scheduled:\n            self._timer_cancelled_count += 1\n\n    def _run_once(self):\n        \"\"\"Run one full iteration of the event loop.\n\n        This calls all currently ready callbacks, polls for I/O,\n        schedules the resulting callbacks, and finally schedules\n        'call_later' callbacks.\n        \"\"\"\n\n        sched_count = len(self._scheduled)\n        if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and\n            self._timer_cancelled_count / sched_count >\n                _MIN_CANCELLED_TIMER_HANDLES_FRACTION):\n            # Remove delayed calls that were cancelled if their number\n            # is too high\n            new_scheduled = []\n            for handle in self._scheduled:\n                if handle._cancelled:\n                    handle._scheduled = False\n                else:\n                    new_scheduled.append(handle)\n\n            heapq.heapify(new_scheduled)\n            self._scheduled = new_scheduled\n            self._timer_cancelled_count = 0\n        else:\n            # Remove delayed calls that were cancelled from head of queue.\n            while self._scheduled and self._scheduled[0]._cancelled:\n                self._timer_cancelled_count -= 1\n                handle = heapq.heappop(self._scheduled)\n                handle._scheduled = False\n\n        timeout = None\n        if self._ready or self._stopping:\n            timeout = 0\n        elif self._scheduled:\n            # Compute the desired timeout.\n            when = self._scheduled[0]._when\n            timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT)\n\n        event_list = self._selector.select(timeout)\n        self._process_events(event_list)\n\n        # Handle 'later' callbacks that are ready.\n        end_time = self.time() + self._clock_resolution\n        while self._scheduled:\n            handle = self._scheduled[0]\n            if handle._when >= end_time:\n                break\n            handle = heapq.heappop(self._scheduled)\n            handle._scheduled = False\n            self._ready.append(handle)\n\n        # This is the only place where callbacks are actually *called*.\n        # All other places just add them to ready.\n        # Note: We run all currently scheduled callbacks, but not any\n        # callbacks scheduled by callbacks run this time around --\n        # they will be run the next time (after another I/O poll).\n        # Use an idiom that is thread-safe without using locks.\n        ntodo = len(self._ready)\n        for i in range(ntodo):\n            handle = self._ready.popleft()\n            if handle._cancelled:\n                continue\n            if self._debug:\n                try:\n                    self._current_handle = handle\n                    t0 = self.time()\n                    handle._run()\n                    dt = self.time() - t0\n                    if dt >= self.slow_callback_duration:\n                        logger.warning('Executing %s took %.3f seconds',\n                                       _format_handle(handle), dt)\n                finally:\n                    self._current_handle = None\n            else:\n                handle._run()\n        handle = None  # Needed to break cycles when an exception occurs.\n\n    def _set_coroutine_origin_tracking(self, enabled):\n        if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):\n            return\n\n        if enabled:\n            self._coroutine_origin_tracking_saved_depth = (\n                sys.get_coroutine_origin_tracking_depth())\n            sys.set_coroutine_origin_tracking_depth(\n                constants.DEBUG_STACK_DEPTH)\n        else:\n            sys.set_coroutine_origin_tracking_depth(\n                self._coroutine_origin_tracking_saved_depth)\n\n        self._coroutine_origin_tracking_enabled = enabled\n\n    def get_debug(self):\n        return self._debug\n\n    def set_debug(self, enabled):\n        self._debug = enabled\n\n        if self.is_running():\n            self.call_soon_threadsafe(self._set_coroutine_origin_tracking, enabled)\n", 1930], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py": ["\"\"\"Event loop and event loop policy.\"\"\"\n\n__all__ = (\n    'AbstractEventLoopPolicy',\n    'AbstractEventLoop', 'AbstractServer',\n    'Handle', 'TimerHandle',\n    'get_event_loop_policy', 'set_event_loop_policy',\n    'get_event_loop', 'set_event_loop', 'new_event_loop',\n    'get_child_watcher', 'set_child_watcher',\n    '_set_running_loop', 'get_running_loop',\n    '_get_running_loop',\n)\n\nimport contextvars\nimport os\nimport socket\nimport subprocess\nimport sys\nimport threading\n\nfrom . import format_helpers\n\n\nclass Handle:\n    \"\"\"Object returned by callback registration methods.\"\"\"\n\n    __slots__ = ('_callback', '_args', '_cancelled', '_loop',\n                 '_source_traceback', '_repr', '__weakref__',\n                 '_context')\n\n    def __init__(self, callback, args, loop, context=None):\n        if context is None:\n            context = contextvars.copy_context()\n        self._context = context\n        self._loop = loop\n        self._callback = callback\n        self._args = args\n        self._cancelled = False\n        self._repr = None\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n        else:\n            self._source_traceback = None\n\n    def _repr_info(self):\n        info = [self.__class__.__name__]\n        if self._cancelled:\n            info.append('cancelled')\n        if self._callback is not None:\n            info.append(format_helpers._format_callback_source(\n                self._callback, self._args))\n        if self._source_traceback:\n            frame = self._source_traceback[-1]\n            info.append(f'created at {frame[0]}:{frame[1]}')\n        return info\n\n    def __repr__(self):\n        if self._repr is not None:\n            return self._repr\n        info = self._repr_info()\n        return '<{}>'.format(' '.join(info))\n\n    def cancel(self):\n        if not self._cancelled:\n            self._cancelled = True\n            if self._loop.get_debug():\n                # Keep a representation in debug mode to keep callback and\n                # parameters. For example, to log the warning\n                # \"Executing <Handle...> took 2.5 second\"\n                self._repr = repr(self)\n            self._callback = None\n            self._args = None\n\n    def cancelled(self):\n        return self._cancelled\n\n    def _run(self):\n        try:\n            self._context.run(self._callback, *self._args)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            cb = format_helpers._format_callback_source(\n                self._callback, self._args)\n            msg = f'Exception in callback {cb}'\n            context = {\n                'message': msg,\n                'exception': exc,\n                'handle': self,\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        self = None  # Needed to break cycles when an exception occurs.\n\n\nclass TimerHandle(Handle):\n    \"\"\"Object returned by timed callback registration methods.\"\"\"\n\n    __slots__ = ['_scheduled', '_when']\n\n    def __init__(self, when, callback, args, loop, context=None):\n        assert when is not None\n        super().__init__(callback, args, loop, context)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        self._when = when\n        self._scheduled = False\n\n    def _repr_info(self):\n        info = super()._repr_info()\n        pos = 2 if self._cancelled else 1\n        info.insert(pos, f'when={self._when}')\n        return info\n\n    def __hash__(self):\n        return hash(self._when)\n\n    def __lt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when\n        return NotImplemented\n\n    def __le__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __gt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when\n        return NotImplemented\n\n    def __ge__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __eq__(self, other):\n        if isinstance(other, TimerHandle):\n            return (self._when == other._when and\n                    self._callback == other._callback and\n                    self._args == other._args and\n                    self._cancelled == other._cancelled)\n        return NotImplemented\n\n    def cancel(self):\n        if not self._cancelled:\n            self._loop._timer_handle_cancelled(self)\n        super().cancel()\n\n    def when(self):\n        \"\"\"Return a scheduled callback time.\n\n        The time is an absolute timestamp, using the same time\n        reference as loop.time().\n        \"\"\"\n        return self._when\n\n\nclass AbstractServer:\n    \"\"\"Abstract server returned by create_server().\"\"\"\n\n    def close(self):\n        \"\"\"Stop serving.  This leaves existing connections open.\"\"\"\n        raise NotImplementedError\n\n    def get_loop(self):\n        \"\"\"Get the event loop the Server object is attached to.\"\"\"\n        raise NotImplementedError\n\n    def is_serving(self):\n        \"\"\"Return True if the server is accepting connections.\"\"\"\n        raise NotImplementedError\n\n    async def start_serving(self):\n        \"\"\"Start accepting connections.\n\n        This method is idempotent, so it can be called when\n        the server is already being serving.\n        \"\"\"\n        raise NotImplementedError\n\n    async def serve_forever(self):\n        \"\"\"Start accepting connections until the coroutine is cancelled.\n\n        The server is closed when the coroutine is cancelled.\n        \"\"\"\n        raise NotImplementedError\n\n    async def wait_closed(self):\n        \"\"\"Coroutine to wait until service is closed.\"\"\"\n        raise NotImplementedError\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc):\n        self.close()\n        await self.wait_closed()\n\n\nclass AbstractEventLoop:\n    \"\"\"Abstract event loop.\"\"\"\n\n    # Running and stopping the event loop.\n\n    def run_forever(self):\n        \"\"\"Run the event loop until stop() is called.\"\"\"\n        raise NotImplementedError\n\n    def run_until_complete(self, future):\n        \"\"\"Run the event loop until a Future is done.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        raise NotImplementedError\n\n    def stop(self):\n        \"\"\"Stop the event loop as soon as reasonable.\n\n        Exactly how soon that is may depend on the implementation, but\n        no more I/O callbacks should be scheduled.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_running(self):\n        \"\"\"Return whether the event loop is currently running.\"\"\"\n        raise NotImplementedError\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the loop.\n\n        The loop should not be running.\n\n        This is idempotent and irreversible.\n\n        No other methods should be called after this one.\n        \"\"\"\n        raise NotImplementedError\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        raise NotImplementedError\n\n    async def shutdown_default_executor(self):\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n        raise NotImplementedError\n\n    # Methods scheduling callbacks.  All these return Handles.\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        raise NotImplementedError\n\n    def call_soon(self, callback, *args, context=None):\n        return self.call_later(0, callback, *args, context=context)\n\n    def call_later(self, delay, callback, *args, context=None):\n        raise NotImplementedError\n\n    def call_at(self, when, callback, *args, context=None):\n        raise NotImplementedError\n\n    def time(self):\n        raise NotImplementedError\n\n    def create_future(self):\n        raise NotImplementedError\n\n    # Method scheduling a coroutine object: create a task.\n\n    def create_task(self, coro, *, name=None):\n        raise NotImplementedError\n\n    # Methods for interacting with threads.\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        raise NotImplementedError\n\n    def run_in_executor(self, executor, func, *args):\n        raise NotImplementedError\n\n    def set_default_executor(self, executor):\n        raise NotImplementedError\n\n    # Network I/O methods returning Futures.\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        raise NotImplementedError\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        raise NotImplementedError\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0, proto=0,\n            flags=0, sock=None, local_addr=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        raise NotImplementedError\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *, family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE, sock=None, backlog=100,\n            ssl=None, reuse_address=None, reuse_port=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a TCP server bound to host and port.\n\n        The return value is a Server object which can be used to stop\n        the service.\n\n        If host is an empty string or None all interfaces are assumed\n        and a list of multiple sockets will be returned (most likely\n        one for IPv4 and another one for IPv6). The host parameter can also be\n        a sequence (e.g. list) of hosts to bind to.\n\n        family can be set to either AF_INET or AF_INET6 to force the\n        socket to use IPv4 or IPv6. If not set it will be determined\n        from host (defaults to AF_UNSPEC).\n\n        flags is a bitmask for getaddrinfo().\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL handshake before aborting the\n        connection. Default is 60s.\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file through a transport.\n\n        Return an amount of sent bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade a transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None):\n        raise NotImplementedError\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a UNIX Domain Socket server.\n\n        The return value is a Server object, which can be used to stop\n        the service.\n\n        path is a str, representing a file system path to bind the\n        server socket to.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for the SSL handshake to complete (defaults to 60s).\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=None, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"A coroutine which creates a datagram endpoint.\n\n        This method will try to establish the endpoint in the background.\n        When successful, the coroutine returns a (transport, protocol) pair.\n\n        protocol_factory must be a callable returning a protocol instance.\n\n        socket family AF_INET, socket.AF_INET6 or socket.AF_UNIX depending on\n        host (or family if specified), socket type SOCK_DGRAM.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified it will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows and some UNIX's. If the\n        :py:data:`~socket.SO_REUSEPORT` constant is not defined then this\n        capability is unsupported.\n\n        allow_broadcast tells the kernel to allow this endpoint to send\n        messages to the broadcast address.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n        \"\"\"\n        raise NotImplementedError\n\n    # Pipes and subprocesses.\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        \"\"\"Register read pipe in event loop. Set the pipe to non-blocking mode.\n\n        protocol_factory should instantiate object with Protocol interface.\n        pipe is a file-like object.\n        Return pair (transport, protocol), where transport supports the\n        ReadTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        \"\"\"Register write pipe in event loop.\n\n        protocol_factory should instantiate object with BaseProtocol interface.\n        Pipe is file-like object already switched to nonblocking.\n        Return pair (transport, protocol), where transport support\n        WriteTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               **kwargs):\n        raise NotImplementedError\n\n    async def subprocess_exec(self, protocol_factory, *args,\n                              stdin=subprocess.PIPE,\n                              stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE,\n                              **kwargs):\n        raise NotImplementedError\n\n    # Ready-based callback registration methods.\n    # The add_*() methods return None.\n    # The remove_*() methods return True if something was removed,\n    # False if there was nothing to delete.\n\n    def add_reader(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_reader(self, fd):\n        raise NotImplementedError\n\n    def add_writer(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_writer(self, fd):\n        raise NotImplementedError\n\n    # Completion based I/O methods returning Futures.\n\n    async def sock_recv(self, sock, nbytes):\n        raise NotImplementedError\n\n    async def sock_recv_into(self, sock, buf):\n        raise NotImplementedError\n\n    async def sock_sendall(self, sock, data):\n        raise NotImplementedError\n\n    async def sock_connect(self, sock, address):\n        raise NotImplementedError\n\n    async def sock_accept(self, sock):\n        raise NotImplementedError\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=None):\n        raise NotImplementedError\n\n    # Signal handling.\n\n    def add_signal_handler(self, sig, callback, *args):\n        raise NotImplementedError\n\n    def remove_signal_handler(self, sig):\n        raise NotImplementedError\n\n    # Task factory.\n\n    def set_task_factory(self, factory):\n        raise NotImplementedError\n\n    def get_task_factory(self):\n        raise NotImplementedError\n\n    # Error handlers.\n\n    def get_exception_handler(self):\n        raise NotImplementedError\n\n    def set_exception_handler(self, handler):\n        raise NotImplementedError\n\n    def default_exception_handler(self, context):\n        raise NotImplementedError\n\n    def call_exception_handler(self, context):\n        raise NotImplementedError\n\n    # Debug flag management.\n\n    def get_debug(self):\n        raise NotImplementedError\n\n    def set_debug(self, enabled):\n        raise NotImplementedError\n\n\nclass AbstractEventLoopPolicy:\n    \"\"\"Abstract policy for accessing the event loop.\"\"\"\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an event loop object implementing the BaseEventLoop interface,\n        or raises an exception in case no event loop has been set for the\n        current context and the current policy does not specify to create one.\n\n        It should never return None.\"\"\"\n        raise NotImplementedError\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop for the current context to loop.\"\"\"\n        raise NotImplementedError\n\n    def new_event_loop(self):\n        \"\"\"Create and return a new event loop object according to this\n        policy's rules. If there's need to set this loop as the event loop for\n        the current context, set_event_loop must be called explicitly.\"\"\"\n        raise NotImplementedError\n\n    # Child processes handling (Unix only).\n\n    def get_child_watcher(self):\n        \"Get the watcher for child processes.\"\n        raise NotImplementedError\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n        raise NotImplementedError\n\n\nclass BaseDefaultEventLoopPolicy(AbstractEventLoopPolicy):\n    \"\"\"Default policy implementation for accessing the event loop.\n\n    In this policy, each thread has its own event loop.  However, we\n    only automatically create an event loop by default for the main\n    thread; other threads by default have no event loop.\n\n    Other policies may have different rules (e.g. a single global\n    event loop, or automatically creating an event loop per thread, or\n    using some other notion of context to which an event loop is\n    associated).\n    \"\"\"\n\n    _loop_factory = None\n\n    class _Local(threading.local):\n        _loop = None\n        _set_called = False\n\n    def __init__(self):\n        self._local = self._Local()\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an instance of EventLoop or raises an exception.\n        \"\"\"\n        if (self._local._loop is None and\n                not self._local._set_called and\n                threading.current_thread() is threading.main_thread()):\n            self.set_event_loop(self.new_event_loop())\n\n        if self._local._loop is None:\n            raise RuntimeError('There is no current event loop in thread %r.'\n                               % threading.current_thread().name)\n\n        return self._local._loop\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\"\"\"\n        self._local._set_called = True\n        assert loop is None or isinstance(loop, AbstractEventLoop)\n        self._local._loop = loop\n\n    def new_event_loop(self):\n        \"\"\"Create a new event loop.\n\n        You must call set_event_loop() to make this the current event\n        loop.\n        \"\"\"\n        return self._loop_factory()\n\n\n# Event loop policy.  The policy itself is always global, even if the\n# policy's rules say that there is an event loop per thread (or other\n# notion of context).  The default policy is installed by the first\n# call to get_event_loop_policy().\n_event_loop_policy = None\n\n# Lock for protecting the on-the-fly creation of the event loop policy.\n_lock = threading.Lock()\n\n\n# A TLS for the running event loop, used by _get_running_loop.\nclass _RunningLoop(threading.local):\n    loop_pid = (None, None)\n\n\n_running_loop = _RunningLoop()\n\n\ndef get_running_loop():\n    \"\"\"Return the running event loop.  Raise a RuntimeError if there is none.\n\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    loop = _get_running_loop()\n    if loop is None:\n        raise RuntimeError('no running event loop')\n    return loop\n\n\ndef _get_running_loop():\n    \"\"\"Return the running event loop or None.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    running_loop, pid = _running_loop.loop_pid\n    if running_loop is not None and pid == os.getpid():\n        return running_loop\n\n\ndef _set_running_loop(loop):\n    \"\"\"Set the running event loop.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    _running_loop.loop_pid = (loop, os.getpid())\n\n\ndef _init_event_loop_policy():\n    global _event_loop_policy\n    with _lock:\n        if _event_loop_policy is None:  # pragma: no branch\n            from . import DefaultEventLoopPolicy\n            _event_loop_policy = DefaultEventLoopPolicy()\n\n\ndef get_event_loop_policy():\n    \"\"\"Get the current event loop policy.\"\"\"\n    if _event_loop_policy is None:\n        _init_event_loop_policy()\n    return _event_loop_policy\n\n\ndef set_event_loop_policy(policy):\n    \"\"\"Set the current event loop policy.\n\n    If policy is None, the default policy is restored.\"\"\"\n    global _event_loop_policy\n    assert policy is None or isinstance(policy, AbstractEventLoopPolicy)\n    _event_loop_policy = policy\n\n\ndef get_event_loop():\n    \"\"\"Return an asyncio event loop.\n\n    When called from a coroutine or a callback (e.g. scheduled with call_soon\n    or similar API), this function will always return the running event loop.\n\n    If there is no running event loop set, the function will return\n    the result of `get_event_loop_policy().get_event_loop()` call.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    current_loop = _get_running_loop()\n    if current_loop is not None:\n        return current_loop\n    return get_event_loop_policy().get_event_loop()\n\n\ndef set_event_loop(loop):\n    \"\"\"Equivalent to calling get_event_loop_policy().set_event_loop(loop).\"\"\"\n    get_event_loop_policy().set_event_loop(loop)\n\n\ndef new_event_loop():\n    \"\"\"Equivalent to calling get_event_loop_policy().new_event_loop().\"\"\"\n    return get_event_loop_policy().new_event_loop()\n\n\ndef get_child_watcher():\n    \"\"\"Equivalent to calling get_event_loop_policy().get_child_watcher().\"\"\"\n    return get_event_loop_policy().get_child_watcher()\n\n\ndef set_child_watcher(watcher):\n    \"\"\"Equivalent to calling\n    get_event_loop_policy().set_child_watcher(watcher).\"\"\"\n    return get_event_loop_policy().set_child_watcher(watcher)\n\n\n# Alias pure-Python implementations for testing purposes.\n_py__get_running_loop = _get_running_loop\n_py__set_running_loop = _set_running_loop\n_py_get_running_loop = get_running_loop\n_py_get_event_loop = get_event_loop\n\n\ntry:\n    # get_event_loop() is one of the most frequently called\n    # functions in asyncio.  Pure Python implementation is\n    # about 4 times slower than C-accelerated.\n    from _asyncio import (_get_running_loop, _set_running_loop,\n                          get_running_loop, get_event_loop)\nexcept ImportError:\n    pass\nelse:\n    # Alias C implementations for testing purposes.\n    _c__get_running_loop = _get_running_loop\n    _c__set_running_loop = _set_running_loop\n    _c_get_running_loop = get_running_loop\n    _c_get_event_loop = get_event_loop\n", 795], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/unix_events.py": ["\"\"\"Selector event loop for Unix with signal handling.\"\"\"\n\nimport errno\nimport io\nimport itertools\nimport os\nimport selectors\nimport signal\nimport socket\nimport stat\nimport subprocess\nimport sys\nimport threading\nimport warnings\n\nfrom . import base_events\nfrom . import base_subprocess\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import selector_events\nfrom . import tasks\nfrom . import transports\nfrom .log import logger\n\n\n__all__ = (\n    'SelectorEventLoop',\n    'AbstractChildWatcher', 'SafeChildWatcher',\n    'FastChildWatcher', 'PidfdChildWatcher',\n    'MultiLoopChildWatcher', 'ThreadedChildWatcher',\n    'DefaultEventLoopPolicy',\n)\n\n\nif sys.platform == 'win32':  # pragma: no cover\n    raise ImportError('Signals are not really supported on Windows')\n\n\ndef _sighandler_noop(signum, frame):\n    \"\"\"Dummy signal handler.\"\"\"\n    pass\n\n\nclass _UnixSelectorEventLoop(selector_events.BaseSelectorEventLoop):\n    \"\"\"Unix event loop.\n\n    Adds signal handling and UNIX Domain Socket support to SelectorEventLoop.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__(selector)\n        self._signal_handlers = {}\n\n    def close(self):\n        super().close()\n        if not sys.is_finalizing():\n            for sig in list(self._signal_handlers):\n                self.remove_signal_handler(sig)\n        else:\n            if self._signal_handlers:\n                warnings.warn(f\"Closing the loop {self!r} \"\n                              f\"on interpreter shutdown \"\n                              f\"stage, skipping signal handlers removal\",\n                              ResourceWarning,\n                              source=self)\n                self._signal_handlers.clear()\n\n    def _process_self_data(self, data):\n        for signum in data:\n            if not signum:\n                # ignore null bytes written by _write_to_self()\n                continue\n            self._handle_signal(signum)\n\n    def add_signal_handler(self, sig, callback, *args):\n        \"\"\"Add a handler for a signal.  UNIX only.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\"coroutines cannot be used \"\n                            \"with add_signal_handler()\")\n        self._check_signal(sig)\n        self._check_closed()\n        try:\n            # set_wakeup_fd() raises ValueError if this is not the\n            # main thread.  By calling it early we ensure that an\n            # event loop running in another thread cannot add a signal\n            # handler.\n            signal.set_wakeup_fd(self._csock.fileno())\n        except (ValueError, OSError) as exc:\n            raise RuntimeError(str(exc))\n\n        handle = events.Handle(callback, args, self, None)\n        self._signal_handlers[sig] = handle\n\n        try:\n            # Register a dummy signal handler to ask Python to write the signal\n            # number in the wakeup file descriptor. _process_self_data() will\n            # read signal numbers from this file descriptor to handle signals.\n            signal.signal(sig, _sighandler_noop)\n\n            # Set SA_RESTART to limit EINTR occurrences.\n            signal.siginterrupt(sig, False)\n        except OSError as exc:\n            del self._signal_handlers[sig]\n            if not self._signal_handlers:\n                try:\n                    signal.set_wakeup_fd(-1)\n                except (ValueError, OSError) as nexc:\n                    logger.info('set_wakeup_fd(-1) failed: %s', nexc)\n\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n    def _handle_signal(self, sig):\n        \"\"\"Internal helper that is the actual signal handler.\"\"\"\n        handle = self._signal_handlers.get(sig)\n        if handle is None:\n            return  # Assume it's some race condition.\n        if handle._cancelled:\n            self.remove_signal_handler(sig)  # Remove it properly.\n        else:\n            self._add_callback_signalsafe(handle)\n\n    def remove_signal_handler(self, sig):\n        \"\"\"Remove a handler for a signal.  UNIX only.\n\n        Return True if a signal handler was removed, False if not.\n        \"\"\"\n        self._check_signal(sig)\n        try:\n            del self._signal_handlers[sig]\n        except KeyError:\n            return False\n\n        if sig == signal.SIGINT:\n            handler = signal.default_int_handler\n        else:\n            handler = signal.SIG_DFL\n\n        try:\n            signal.signal(sig, handler)\n        except OSError as exc:\n            if exc.errno == errno.EINVAL:\n                raise RuntimeError(f'sig {sig} cannot be caught')\n            else:\n                raise\n\n        if not self._signal_handlers:\n            try:\n                signal.set_wakeup_fd(-1)\n            except (ValueError, OSError) as exc:\n                logger.info('set_wakeup_fd(-1) failed: %s', exc)\n\n        return True\n\n    def _check_signal(self, sig):\n        \"\"\"Internal helper to validate a signal.\n\n        Raise ValueError if the signal number is invalid or uncatchable.\n        Raise RuntimeError if there is a problem setting up the handler.\n        \"\"\"\n        if not isinstance(sig, int):\n            raise TypeError(f'sig must be an int, not {sig!r}')\n\n        if sig not in signal.valid_signals():\n            raise ValueError(f'invalid signal number {sig}')\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        return _UnixReadPipeTransport(self, pipe, protocol, waiter, extra)\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        return _UnixWritePipeTransport(self, pipe, protocol, waiter, extra)\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        with events.get_child_watcher() as watcher:\n            if not watcher.is_active():\n                # Check early.\n                # Raising exception before process creation\n                # prevents subprocess execution if the watcher\n                # is not ready to handle it.\n                raise RuntimeError(\"asyncio.get_child_watcher() is not activated, \"\n                                   \"subprocess support is not installed.\")\n            waiter = self.create_future()\n            transp = _UnixSubprocessTransport(self, protocol, args, shell,\n                                              stdin, stdout, stderr, bufsize,\n                                              waiter=waiter, extra=extra,\n                                              **kwargs)\n\n            watcher.add_child_handler(transp.get_pid(),\n                                      self._child_watcher_callback, transp)\n            try:\n                await waiter\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                transp.close()\n                await transp._wait()\n                raise\n\n        return transp\n\n    def _child_watcher_callback(self, pid, returncode, transp):\n        self.call_soon_threadsafe(transp._process_exited, returncode)\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None):\n        assert server_hostname is None or isinstance(server_hostname, str)\n        if ssl:\n            if server_hostname is None:\n                raise ValueError(\n                    'you have to pass server_hostname when using ssl')\n        else:\n            if server_hostname is not None:\n                raise ValueError('server_hostname is only meaningful with ssl')\n            if ssl_handshake_timeout is not None:\n                raise ValueError(\n                    'ssl_handshake_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM, 0)\n            try:\n                sock.setblocking(False)\n                await self.sock_connect(sock, path)\n            except:\n                sock.close()\n                raise\n\n        else:\n            if sock is None:\n                raise ValueError('no path and sock were specified')\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n            sock.setblocking(False)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        return transport, protocol\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if path is not None:\n            if sock is not None:\n                raise ValueError(\n                    'path and sock can not be specified at the same time')\n\n            path = os.fspath(path)\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n\n            # Check for abstract socket. `str` and `bytes` paths are supported.\n            if path[0] not in (0, '\\x00'):\n                try:\n                    if stat.S_ISSOCK(os.stat(path).st_mode):\n                        os.remove(path)\n                except FileNotFoundError:\n                    pass\n                except OSError as err:\n                    # Directory may have permissions only to create socket.\n                    logger.error('Unable to check or remove stale UNIX socket '\n                                 '%r: %r', path, err)\n\n            try:\n                sock.bind(path)\n            except OSError as exc:\n                sock.close()\n                if exc.errno == errno.EADDRINUSE:\n                    # Let's improve the error message by adding\n                    # with what exact address it occurs.\n                    msg = f'Address {path!r} is already in use'\n                    raise OSError(errno.EADDRINUSE, msg) from None\n                else:\n                    raise\n            except:\n                sock.close()\n                raise\n        else:\n            if sock is None:\n                raise ValueError(\n                    'path was not specified, and no sock specified')\n\n            if (sock.family != socket.AF_UNIX or\n                    sock.type != socket.SOCK_STREAM):\n                raise ValueError(\n                    f'A UNIX Domain Stream Socket was expected, got {sock!r}')\n\n        sock.setblocking(False)\n        server = base_events.Server(self, [sock], protocol_factory,\n                                    ssl, backlog, ssl_handshake_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0)\n\n        return server\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        try:\n            os.sendfile\n        except AttributeError:\n            raise exceptions.SendfileNotAvailableError(\n                \"os.sendfile() is not available\")\n        try:\n            fileno = file.fileno()\n        except (AttributeError, io.UnsupportedOperation) as err:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        try:\n            fsize = os.fstat(fileno).st_size\n        except OSError:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        blocksize = count if count else fsize\n        if not blocksize:\n            return 0  # empty file\n\n        fut = self.create_future()\n        self._sock_sendfile_native_impl(fut, None, sock, fileno,\n                                        offset, count, blocksize, 0)\n        return await fut\n\n    def _sock_sendfile_native_impl(self, fut, registered_fd, sock, fileno,\n                                   offset, count, blocksize, total_sent):\n        fd = sock.fileno()\n        if registered_fd is not None:\n            # Remove the callback early.  It should be rare that the\n            # selector says the fd is ready but the call still returns\n            # EAGAIN, and I am willing to take a hit in that case in\n            # order to simplify the common case.\n            self.remove_writer(registered_fd)\n        if fut.cancelled():\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            return\n        if count:\n            blocksize = count - total_sent\n            if blocksize <= 0:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n                return\n\n        try:\n            sent = os.sendfile(fd, fileno, offset, blocksize)\n        except (BlockingIOError, InterruptedError):\n            if registered_fd is None:\n                self._sock_add_cancellation_callback(fut, sock)\n            self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                            fd, sock, fileno,\n                            offset, count, blocksize, total_sent)\n        except OSError as exc:\n            if (registered_fd is not None and\n                    exc.errno == errno.ENOTCONN and\n                    type(exc) is not ConnectionError):\n                # If we have an ENOTCONN and this isn't a first call to\n                # sendfile(), i.e. the connection was closed in the middle\n                # of the operation, normalize the error to ConnectionError\n                # to make it consistent across all Posix systems.\n                new_exc = ConnectionError(\n                    \"socket is not connected\", errno.ENOTCONN)\n                new_exc.__cause__ = exc\n                exc = new_exc\n            if total_sent == 0:\n                # We can get here for different reasons, the main\n                # one being 'file' is not a regular mmap(2)-like\n                # file, in which case we'll fall back on using\n                # plain send().\n                err = exceptions.SendfileNotAvailableError(\n                    \"os.sendfile call failed\")\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(err)\n            else:\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_exception(exc)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n            fut.set_exception(exc)\n        else:\n            if sent == 0:\n                # EOF\n                self._sock_sendfile_update_filepos(fileno, offset, total_sent)\n                fut.set_result(total_sent)\n            else:\n                offset += sent\n                total_sent += sent\n                if registered_fd is None:\n                    self._sock_add_cancellation_callback(fut, sock)\n                self.add_writer(fd, self._sock_sendfile_native_impl, fut,\n                                fd, sock, fileno,\n                                offset, count, blocksize, total_sent)\n\n    def _sock_sendfile_update_filepos(self, fileno, offset, total_sent):\n        if total_sent > 0:\n            os.lseek(fileno, offset, os.SEEK_SET)\n\n    def _sock_add_cancellation_callback(self, fut, sock):\n        def cb(fut):\n            if fut.cancelled():\n                fd = sock.fileno()\n                if fd != -1:\n                    self.remove_writer(fd)\n        fut.add_done_callback(cb)\n\n\nclass _UnixReadPipeTransport(transports.ReadTransport):\n\n    max_size = 256 * 1024  # max bytes we read in one event loop iteration\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra)\n        self._extra['pipe'] = pipe\n        self._loop = loop\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._closing = False\n        self._paused = False\n\n        mode = os.fstat(self._fileno).st_mode\n        if not (stat.S_ISFIFO(mode) or\n                stat.S_ISSOCK(mode) or\n                stat.S_ISCHR(mode)):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is for pipes/sockets only.\")\n\n        os.set_blocking(self._fileno, False)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._loop._add_reader,\n                             self._fileno, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_READ)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def _read_ready(self):\n        try:\n            data = os.read(self._fileno, self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._fatal_error(exc, 'Fatal read error on pipe transport')\n        else:\n            if data:\n                self._protocol.data_received(data)\n            else:\n                if self._loop.get_debug():\n                    logger.info(\"%r was closed by peer\", self)\n                self._closing = True\n                self._loop._remove_reader(self._fileno)\n                self._loop.call_soon(self._protocol.eof_received)\n                self._loop.call_soon(self._call_connection_lost, None)\n\n    def pause_reading(self):\n        if self._closing or self._paused:\n            return\n        self._paused = True\n        self._loop._remove_reader(self._fileno)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._loop._add_reader(self._fileno, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if not self._closing:\n            self._close(None)\n\n    def __del__(self, _warn=warnings.warn):\n        if self._pipe is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._pipe.close()\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if (isinstance(exc, OSError) and exc.errno == errno.EIO):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc):\n        self._closing = True\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixWritePipeTransport(transports._FlowControlMixin,\n                              transports.WriteTransport):\n\n    def __init__(self, loop, pipe, protocol, waiter=None, extra=None):\n        super().__init__(extra, loop)\n        self._extra['pipe'] = pipe\n        self._pipe = pipe\n        self._fileno = pipe.fileno()\n        self._protocol = protocol\n        self._buffer = bytearray()\n        self._conn_lost = 0\n        self._closing = False  # Set when close() or write_eof() called.\n\n        mode = os.fstat(self._fileno).st_mode\n        is_char = stat.S_ISCHR(mode)\n        is_fifo = stat.S_ISFIFO(mode)\n        is_socket = stat.S_ISSOCK(mode)\n        if not (is_char or is_fifo or is_socket):\n            self._pipe = None\n            self._fileno = None\n            self._protocol = None\n            raise ValueError(\"Pipe transport is only for \"\n                             \"pipes, sockets and character devices\")\n\n        os.set_blocking(self._fileno, False)\n        self._loop.call_soon(self._protocol.connection_made, self)\n\n        # On AIX, the reader trick (to be notified when the read end of the\n        # socket is closed) only works for sockets. On other platforms it\n        # works for pipes and sockets. (Exception: OS X 10.4?  Issue #19294.)\n        if is_socket or (is_fifo and not sys.platform.startswith(\"aix\")):\n            # only start reading when connection_made() has been called\n            self._loop.call_soon(self._loop._add_reader,\n                                 self._fileno, self._read_ready)\n\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._pipe is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._fileno}')\n        selector = getattr(self._loop, '_selector', None)\n        if self._pipe is not None and selector is not None:\n            polling = selector_events._test_selector_event(\n                selector, self._fileno, selectors.EVENT_WRITE)\n            if polling:\n                info.append('polling')\n            else:\n                info.append('idle')\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'bufsize={bufsize}')\n        elif self._pipe is not None:\n            info.append('open')\n        else:\n            info.append('closed')\n        return '<{}>'.format(' '.join(info))\n\n    def get_write_buffer_size(self):\n        return len(self._buffer)\n\n    def _read_ready(self):\n        # Pipe was closed by peer.\n        if self._loop.get_debug():\n            logger.info(\"%r was closed by peer\", self)\n        if self._buffer:\n            self._close(BrokenPipeError())\n        else:\n            self._close()\n\n    def write(self, data):\n        assert isinstance(data, (bytes, bytearray, memoryview)), repr(data)\n        if isinstance(data, bytearray):\n            data = memoryview(data)\n        if not data:\n            return\n\n        if self._conn_lost or self._closing:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('pipe closed by peer or '\n                               'os.write(pipe, data) raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                n = os.write(self._fileno, data)\n            except (BlockingIOError, InterruptedError):\n                n = 0\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._conn_lost += 1\n                self._fatal_error(exc, 'Fatal write error on pipe transport')\n                return\n            if n == len(data):\n                return\n            elif n > 0:\n                data = memoryview(data)[n:]\n            self._loop._add_writer(self._fileno, self._write_ready)\n\n        self._buffer += data\n        self._maybe_pause_protocol()\n\n    def _write_ready(self):\n        assert self._buffer, 'Data should not be empty'\n\n        try:\n            n = os.write(self._fileno, self._buffer)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._buffer.clear()\n            self._conn_lost += 1\n            # Remove writer here, _fatal_error() doesn't it\n            # because _buffer is empty.\n            self._loop._remove_writer(self._fileno)\n            self._fatal_error(exc, 'Fatal write error on pipe transport')\n        else:\n            if n == len(self._buffer):\n                self._buffer.clear()\n                self._loop._remove_writer(self._fileno)\n                self._maybe_resume_protocol()  # May append to buffer.\n                if self._closing:\n                    self._loop._remove_reader(self._fileno)\n                    self._call_connection_lost(None)\n                return\n            elif n > 0:\n                del self._buffer[:n]\n\n    def can_write_eof(self):\n        return True\n\n    def write_eof(self):\n        if self._closing:\n            return\n        assert self._pipe\n        self._closing = True\n        if not self._buffer:\n            self._loop._remove_reader(self._fileno)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if self._pipe is not None and not self._closing:\n            # write_eof is all what we needed to close the write pipe\n            self.write_eof()\n\n    def __del__(self, _warn=warnings.warn):\n        if self._pipe is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._pipe.close()\n\n    def abort(self):\n        self._close(None)\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        # should be called by exception handler only\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._close(exc)\n\n    def _close(self, exc=None):\n        self._closing = True\n        if self._buffer:\n            self._loop._remove_writer(self._fileno)\n        self._buffer.clear()\n        self._loop._remove_reader(self._fileno)\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            self._pipe.close()\n            self._pipe = None\n            self._protocol = None\n            self._loop = None\n\n\nclass _UnixSubprocessTransport(base_subprocess.BaseSubprocessTransport):\n\n    def _start(self, args, shell, stdin, stdout, stderr, bufsize, **kwargs):\n        stdin_w = None\n        if stdin == subprocess.PIPE:\n            # Use a socket pair for stdin, since not all platforms\n            # support selecting read events on the write end of a\n            # socket (which we use in order to detect closing of the\n            # other end).  Notably this is needed on AIX, and works\n            # just fine on other platforms.\n            stdin, stdin_w = socket.socketpair()\n        try:\n            self._proc = subprocess.Popen(\n                args, shell=shell, stdin=stdin, stdout=stdout, stderr=stderr,\n                universal_newlines=False, bufsize=bufsize, **kwargs)\n            if stdin_w is not None:\n                stdin.close()\n                self._proc.stdin = open(stdin_w.detach(), 'wb', buffering=bufsize)\n                stdin_w = None\n        finally:\n            if stdin_w is not None:\n                stdin.close()\n                stdin_w.close()\n\n\nclass AbstractChildWatcher:\n    \"\"\"Abstract base class for monitoring child processes.\n\n    Objects derived from this class monitor a collection of subprocesses and\n    report their termination or interruption by a signal.\n\n    New callbacks are registered with .add_child_handler(). Starting a new\n    process must be done within a 'with' block to allow the watcher to suspend\n    its activity until the new process if fully registered (this is needed to\n    prevent a race condition in some implementations).\n\n    Example:\n        with watcher:\n            proc = subprocess.Popen(\"sleep 1\")\n            watcher.add_child_handler(proc.pid, callback)\n\n    Notes:\n        Implementations of this class must be thread-safe.\n\n        Since child watcher objects may catch the SIGCHLD signal and call\n        waitpid(-1), there should be only one active object per process.\n    \"\"\"\n\n    def add_child_handler(self, pid, callback, *args):\n        \"\"\"Register a new child handler.\n\n        Arrange for callback(pid, returncode, *args) to be called when\n        process 'pid' terminates. Specifying another callback for the same\n        process replaces the previous handler.\n\n        Note: callback() must be thread-safe.\n        \"\"\"\n        raise NotImplementedError()\n\n    def remove_child_handler(self, pid):\n        \"\"\"Removes the handler for process 'pid'.\n\n        The function returns True if the handler was successfully removed,\n        False if there was nothing to remove.\"\"\"\n\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        \"\"\"Attach the watcher to an event loop.\n\n        If the watcher was previously attached to an event loop, then it is\n        first detached before attaching to the new loop.\n\n        Note: loop may be None.\n        \"\"\"\n        raise NotImplementedError()\n\n    def close(self):\n        \"\"\"Close the watcher.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        raise NotImplementedError()\n\n    def is_active(self):\n        \"\"\"Return ``True`` if the watcher is active and is used by the event loop.\n\n        Return True if the watcher is installed and ready to handle process exit\n        notifications.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def __enter__(self):\n        \"\"\"Enter the watcher's context and allow starting new processes\n\n        This function must return self\"\"\"\n        raise NotImplementedError()\n\n    def __exit__(self, a, b, c):\n        \"\"\"Exit the watcher's context\"\"\"\n        raise NotImplementedError()\n\n\nclass PidfdChildWatcher(AbstractChildWatcher):\n    \"\"\"Child watcher implementation using Linux's pid file descriptors.\n\n    This child watcher polls process file descriptors (pidfds) to await child\n    process termination. In some respects, PidfdChildWatcher is a \"Goldilocks\"\n    child watcher implementation. It doesn't require signals or threads, doesn't\n    interfere with any processes launched outside the event loop, and scales\n    linearly with the number of subprocesses launched by the event loop. The\n    main disadvantage is that pidfds are specific to Linux, and only work on\n    recent (5.3+) kernels.\n    \"\"\"\n\n    def __init__(self):\n        self._loop = None\n        self._callbacks = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        pass\n\n    def is_active(self):\n        return self._loop is not None and self._loop.is_running()\n\n    def close(self):\n        self.attach_loop(None)\n\n    def attach_loop(self, loop):\n        if self._loop is not None and loop is None and self._callbacks:\n            warnings.warn(\n                'A loop is being detached '\n                'from a child watcher with pending handlers',\n                RuntimeWarning)\n        for pidfd, _, _ in self._callbacks.values():\n            self._loop._remove_reader(pidfd)\n            os.close(pidfd)\n        self._callbacks.clear()\n        self._loop = loop\n\n    def add_child_handler(self, pid, callback, *args):\n        existing = self._callbacks.get(pid)\n        if existing is not None:\n            self._callbacks[pid] = existing[0], callback, args\n        else:\n            pidfd = os.pidfd_open(pid)\n            self._loop._add_reader(pidfd, self._do_wait, pid)\n            self._callbacks[pid] = pidfd, callback, args\n\n    def _do_wait(self, pid):\n        pidfd, callback, args = self._callbacks.pop(pid)\n        self._loop._remove_reader(pidfd)\n        try:\n            _, status = os.waitpid(pid, 0)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            returncode = 255\n            logger.warning(\n                \"child process pid %d exit status already read: \"\n                \" will report returncode 255\",\n                pid)\n        else:\n            returncode = _compute_returncode(status)\n\n        os.close(pidfd)\n        callback(pid, returncode, *args)\n\n    def remove_child_handler(self, pid):\n        try:\n            pidfd, _, _ = self._callbacks.pop(pid)\n        except KeyError:\n            return False\n        self._loop._remove_reader(pidfd)\n        os.close(pidfd)\n        return True\n\n\ndef _compute_returncode(status):\n    if os.WIFSIGNALED(status):\n        # The child process died because of a signal.\n        return -os.WTERMSIG(status)\n    elif os.WIFEXITED(status):\n        # The child process exited (e.g sys.exit()).\n        return os.WEXITSTATUS(status)\n    else:\n        # The child exited, but we don't understand its status.\n        # This shouldn't happen, but if it does, let's just\n        # return that status; perhaps that helps debug it.\n        return status\n\n\nclass BaseChildWatcher(AbstractChildWatcher):\n\n    def __init__(self):\n        self._loop = None\n        self._callbacks = {}\n\n    def close(self):\n        self.attach_loop(None)\n\n    def is_active(self):\n        return self._loop is not None and self._loop.is_running()\n\n    def _do_waitpid(self, expected_pid):\n        raise NotImplementedError()\n\n    def _do_waitpid_all(self):\n        raise NotImplementedError()\n\n    def attach_loop(self, loop):\n        assert loop is None or isinstance(loop, events.AbstractEventLoop)\n\n        if self._loop is not None and loop is None and self._callbacks:\n            warnings.warn(\n                'A loop is being detached '\n                'from a child watcher with pending handlers',\n                RuntimeWarning)\n\n        if self._loop is not None:\n            self._loop.remove_signal_handler(signal.SIGCHLD)\n\n        self._loop = loop\n        if loop is not None:\n            loop.add_signal_handler(signal.SIGCHLD, self._sig_chld)\n\n            # Prevent a race condition in case a child terminated\n            # during the switch.\n            self._do_waitpid_all()\n\n    def _sig_chld(self):\n        try:\n            self._do_waitpid_all()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            # self._loop should always be available here\n            # as '_sig_chld' is added as a signal handler\n            # in 'attach_loop'\n            self._loop.call_exception_handler({\n                'message': 'Unknown exception in SIGCHLD handler',\n                'exception': exc,\n            })\n\n\nclass SafeChildWatcher(BaseChildWatcher):\n    \"\"\"'Safe' child watcher implementation.\n\n    This implementation avoids disrupting other code spawning processes by\n    polling explicitly each process in the SIGCHLD handler instead of calling\n    os.waitpid(-1).\n\n    This is a safe solution but it has a significant overhead when handling a\n    big number of children (O(n) each time SIGCHLD is raised)\n    \"\"\"\n\n    def close(self):\n        self._callbacks.clear()\n        super().close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, a, b, c):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        self._callbacks[pid] = (callback, args)\n\n        # Prevent a race condition in case the child is already terminated.\n        self._do_waitpid(pid)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n\n        for pid in list(self._callbacks):\n            self._do_waitpid(pid)\n\n    def _do_waitpid(self, expected_pid):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, os.WNOHANG)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n        else:\n            if pid == 0:\n                # The child process is still alive.\n                return\n\n            returncode = _compute_returncode(status)\n            if self._loop.get_debug():\n                logger.debug('process %s exited with returncode %s',\n                             expected_pid, returncode)\n\n        try:\n            callback, args = self._callbacks.pop(pid)\n        except KeyError:  # pragma: no cover\n            # May happen if .remove_child_handler() is called\n            # after os.waitpid() returns.\n            if self._loop.get_debug():\n                logger.warning(\"Child watcher got an unexpected pid: %r\",\n                               pid, exc_info=True)\n        else:\n            callback(pid, returncode, *args)\n\n\nclass FastChildWatcher(BaseChildWatcher):\n    \"\"\"'Fast' child watcher implementation.\n\n    This implementation reaps every terminated processes by calling\n    os.waitpid(-1) directly, possibly breaking other code spawning processes\n    and waiting for their termination.\n\n    There is no noticeable overhead when handling a big number of children\n    (O(1) each time a child terminates).\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._lock = threading.Lock()\n        self._zombies = {}\n        self._forks = 0\n\n    def close(self):\n        self._callbacks.clear()\n        self._zombies.clear()\n        super().close()\n\n    def __enter__(self):\n        with self._lock:\n            self._forks += 1\n\n            return self\n\n    def __exit__(self, a, b, c):\n        with self._lock:\n            self._forks -= 1\n\n            if self._forks or not self._zombies:\n                return\n\n            collateral_victims = str(self._zombies)\n            self._zombies.clear()\n\n        logger.warning(\n            \"Caught subprocesses termination from unknown pids: %s\",\n            collateral_victims)\n\n    def add_child_handler(self, pid, callback, *args):\n        assert self._forks, \"Must use the context manager\"\n\n        with self._lock:\n            try:\n                returncode = self._zombies.pop(pid)\n            except KeyError:\n                # The child is running.\n                self._callbacks[pid] = callback, args\n                return\n\n        # The child is dead already. We can fire the callback.\n        callback(pid, returncode, *args)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def _do_waitpid_all(self):\n        # Because of signal coalescing, we must keep calling waitpid() as\n        # long as we're able to reap a child.\n        while True:\n            try:\n                pid, status = os.waitpid(-1, os.WNOHANG)\n            except ChildProcessError:\n                # No more child processes exist.\n                return\n            else:\n                if pid == 0:\n                    # A child process is still alive.\n                    return\n\n                returncode = _compute_returncode(status)\n\n            with self._lock:\n                try:\n                    callback, args = self._callbacks.pop(pid)\n                except KeyError:\n                    # unknown child\n                    if self._forks:\n                        # It may not be registered yet.\n                        self._zombies[pid] = returncode\n                        if self._loop.get_debug():\n                            logger.debug('unknown process %s exited '\n                                         'with returncode %s',\n                                         pid, returncode)\n                        continue\n                    callback = None\n                else:\n                    if self._loop.get_debug():\n                        logger.debug('process %s exited with returncode %s',\n                                     pid, returncode)\n\n            if callback is None:\n                logger.warning(\n                    \"Caught subprocess termination from unknown pid: \"\n                    \"%d -> %d\", pid, returncode)\n            else:\n                callback(pid, returncode, *args)\n\n\nclass MultiLoopChildWatcher(AbstractChildWatcher):\n    \"\"\"A watcher that doesn't require running loop in the main thread.\n\n    This implementation registers a SIGCHLD signal handler on\n    instantiation (which may conflict with other code that\n    install own handler for this signal).\n\n    The solution is safe but it has a significant overhead when\n    handling a big number of processes (*O(n)* each time a\n    SIGCHLD is received).\n    \"\"\"\n\n    # Implementation note:\n    # The class keeps compatibility with AbstractChildWatcher ABC\n    # To achieve this it has empty attach_loop() method\n    # and doesn't accept explicit loop argument\n    # for add_child_handler()/remove_child_handler()\n    # but retrieves the current loop by get_running_loop()\n\n    def __init__(self):\n        self._callbacks = {}\n        self._saved_sighandler = None\n\n    def is_active(self):\n        return self._saved_sighandler is not None\n\n    def close(self):\n        self._callbacks.clear()\n        if self._saved_sighandler is None:\n            return\n\n        handler = signal.getsignal(signal.SIGCHLD)\n        if handler != self._sig_chld:\n            logger.warning(\"SIGCHLD handler was changed by outside code\")\n        else:\n            signal.signal(signal.SIGCHLD, self._saved_sighandler)\n        self._saved_sighandler = None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def add_child_handler(self, pid, callback, *args):\n        loop = events.get_running_loop()\n        self._callbacks[pid] = (loop, callback, args)\n\n        # Prevent a race condition in case the child is already terminated.\n        self._do_waitpid(pid)\n\n    def remove_child_handler(self, pid):\n        try:\n            del self._callbacks[pid]\n            return True\n        except KeyError:\n            return False\n\n    def attach_loop(self, loop):\n        # Don't save the loop but initialize itself if called first time\n        # The reason to do it here is that attach_loop() is called from\n        # unix policy only for the main thread.\n        # Main thread is required for subscription on SIGCHLD signal\n        if self._saved_sighandler is not None:\n            return\n\n        self._saved_sighandler = signal.signal(signal.SIGCHLD, self._sig_chld)\n        if self._saved_sighandler is None:\n            logger.warning(\"Previous SIGCHLD handler was set by non-Python code, \"\n                           \"restore to default handler on watcher close.\")\n            self._saved_sighandler = signal.SIG_DFL\n\n        # Set SA_RESTART to limit EINTR occurrences.\n        signal.siginterrupt(signal.SIGCHLD, False)\n\n    def _do_waitpid_all(self):\n        for pid in list(self._callbacks):\n            self._do_waitpid(pid)\n\n    def _do_waitpid(self, expected_pid):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, os.WNOHANG)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n            debug_log = False\n        else:\n            if pid == 0:\n                # The child process is still alive.\n                return\n\n            returncode = _compute_returncode(status)\n            debug_log = True\n        try:\n            loop, callback, args = self._callbacks.pop(pid)\n        except KeyError:  # pragma: no cover\n            # May happen if .remove_child_handler() is called\n            # after os.waitpid() returns.\n            logger.warning(\"Child watcher got an unexpected pid: %r\",\n                           pid, exc_info=True)\n        else:\n            if loop.is_closed():\n                logger.warning(\"Loop %r that handles pid %r is closed\", loop, pid)\n            else:\n                if debug_log and loop.get_debug():\n                    logger.debug('process %s exited with returncode %s',\n                                 expected_pid, returncode)\n                loop.call_soon_threadsafe(callback, pid, returncode, *args)\n\n    def _sig_chld(self, signum, frame):\n        try:\n            self._do_waitpid_all()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException:\n            logger.warning('Unknown exception in SIGCHLD handler', exc_info=True)\n\n\nclass ThreadedChildWatcher(AbstractChildWatcher):\n    \"\"\"Threaded child watcher implementation.\n\n    The watcher uses a thread per process\n    for waiting for the process finish.\n\n    It doesn't require subscription on POSIX signal\n    but a thread creation is not free.\n\n    The watcher has O(1) complexity, its performance doesn't depend\n    on amount of spawn processes.\n    \"\"\"\n\n    def __init__(self):\n        self._pid_counter = itertools.count(0)\n        self._threads = {}\n\n    def is_active(self):\n        return True\n\n    def close(self):\n        self._join_threads()\n\n    def _join_threads(self):\n        \"\"\"Internal: Join all non-daemon threads\"\"\"\n        threads = [thread for thread in list(self._threads.values())\n                   if thread.is_alive() and not thread.daemon]\n        for thread in threads:\n            thread.join()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def __del__(self, _warn=warnings.warn):\n        threads = [thread for thread in list(self._threads.values())\n                   if thread.is_alive()]\n        if threads:\n            _warn(f\"{self.__class__} has registered but not finished child processes\",\n                  ResourceWarning,\n                  source=self)\n\n    def add_child_handler(self, pid, callback, *args):\n        loop = events.get_running_loop()\n        thread = threading.Thread(target=self._do_waitpid,\n                                  name=f\"waitpid-{next(self._pid_counter)}\",\n                                  args=(loop, pid, callback, args),\n                                  daemon=True)\n        self._threads[pid] = thread\n        thread.start()\n\n    def remove_child_handler(self, pid):\n        # asyncio never calls remove_child_handler() !!!\n        # The method is no-op but is implemented because\n        # abstract base classes require it.\n        return True\n\n    def attach_loop(self, loop):\n        pass\n\n    def _do_waitpid(self, loop, expected_pid, callback, args):\n        assert expected_pid > 0\n\n        try:\n            pid, status = os.waitpid(expected_pid, 0)\n        except ChildProcessError:\n            # The child process is already reaped\n            # (may happen if waitpid() is called elsewhere).\n            pid = expected_pid\n            returncode = 255\n            logger.warning(\n                \"Unknown child process pid %d, will report returncode 255\",\n                pid)\n        else:\n            returncode = _compute_returncode(status)\n            if loop.get_debug():\n                logger.debug('process %s exited with returncode %s',\n                             expected_pid, returncode)\n\n        if loop.is_closed():\n            logger.warning(\"Loop %r that handles pid %r is closed\", loop, pid)\n        else:\n            loop.call_soon_threadsafe(callback, pid, returncode, *args)\n\n        self._threads.pop(expected_pid)\n\n\nclass _UnixDefaultEventLoopPolicy(events.BaseDefaultEventLoopPolicy):\n    \"\"\"UNIX event loop policy with a watcher for child processes.\"\"\"\n    _loop_factory = _UnixSelectorEventLoop\n\n    def __init__(self):\n        super().__init__()\n        self._watcher = None\n\n    def _init_watcher(self):\n        with events._lock:\n            if self._watcher is None:  # pragma: no branch\n                self._watcher = ThreadedChildWatcher()\n                if threading.current_thread() is threading.main_thread():\n                    self._watcher.attach_loop(self._local._loop)\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\n\n        As a side effect, if a child watcher was set before, then calling\n        .set_event_loop() from the main thread will call .attach_loop(loop) on\n        the child watcher.\n        \"\"\"\n\n        super().set_event_loop(loop)\n\n        if (self._watcher is not None and\n                threading.current_thread() is threading.main_thread()):\n            self._watcher.attach_loop(loop)\n\n    def get_child_watcher(self):\n        \"\"\"Get the watcher for child processes.\n\n        If not yet set, a ThreadedChildWatcher object is automatically created.\n        \"\"\"\n        if self._watcher is None:\n            self._init_watcher()\n\n        return self._watcher\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n\n        assert watcher is None or isinstance(watcher, AbstractChildWatcher)\n\n        if self._watcher is not None:\n            self._watcher.close()\n\n        self._watcher = watcher\n\n\nSelectorEventLoop = _UnixSelectorEventLoop\nDefaultEventLoopPolicy = _UnixDefaultEventLoopPolicy\n", 1470], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py": ["\"\"\"Event loop using a selector and related classes.\n\nA selector is a \"notify-when-ready\" multiplexer.  For a subclass which\nalso includes support for signal handling, see the unix_events sub-module.\n\"\"\"\n\n__all__ = 'BaseSelectorEventLoop',\n\nimport collections\nimport errno\nimport functools\nimport selectors\nimport socket\nimport warnings\nimport weakref\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import base_events\nfrom . import constants\nfrom . import events\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\ndef _test_selector_event(selector, fd, event):\n    # Test if the selector is monitoring 'event' events\n    # for the file descriptor 'fd'.\n    try:\n        key = selector.get_key(fd)\n    except KeyError:\n        return False\n    else:\n        return bool(key.events & event)\n\n\nclass BaseSelectorEventLoop(base_events.BaseEventLoop):\n    \"\"\"Selector event loop.\n\n    See events.EventLoop for API specification.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__()\n\n        if selector is None:\n            selector = selectors.DefaultSelector()\n        logger.debug('Using selector: %s', selector.__class__.__name__)\n        self._selector = selector\n        self._make_self_pipe()\n        self._transports = weakref.WeakValueDictionary()\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        return _SelectorSocketTransport(self, sock, protocol, waiter,\n                                        extra, server)\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        ssl_protocol = sslproto.SSLProtocol(\n                self, protocol, sslcontext, waiter,\n                server_side, server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout)\n        _SelectorSocketTransport(self, rawsock, ssl_protocol,\n                                 extra=extra, server=server)\n        return ssl_protocol._app_transport\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        return _SelectorDatagramTransport(self, sock, protocol,\n                                          address, waiter, extra)\n\n    def close(self):\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self.is_closed():\n            return\n        self._close_self_pipe()\n        super().close()\n        if self._selector is not None:\n            self._selector.close()\n            self._selector = None\n\n    def _close_self_pipe(self):\n        self._remove_reader(self._ssock.fileno())\n        self._ssock.close()\n        self._ssock = None\n        self._csock.close()\n        self._csock = None\n        self._internal_fds -= 1\n\n    def _make_self_pipe(self):\n        # A self-socket, really. :-)\n        self._ssock, self._csock = socket.socketpair()\n        self._ssock.setblocking(False)\n        self._csock.setblocking(False)\n        self._internal_fds += 1\n        self._add_reader(self._ssock.fileno(), self._read_from_self)\n\n    def _process_self_data(self, data):\n        pass\n\n    def _read_from_self(self):\n        while True:\n            try:\n                data = self._ssock.recv(4096)\n                if not data:\n                    break\n                self._process_self_data(data)\n            except InterruptedError:\n                continue\n            except BlockingIOError:\n                break\n\n    def _write_to_self(self):\n        # This may be called from a different thread, possibly after\n        # _close_self_pipe() has been called or even while it is\n        # running.  Guard for self._csock being None or closed.  When\n        # a socket is closed, send() raises OSError (with errno set to\n        # EBADF, but let's not rely on the exact error code).\n        csock = self._csock\n        if csock is None:\n            return\n\n        try:\n            csock.send(b'\\0')\n        except OSError:\n            if self._debug:\n                logger.debug(\"Fail to write a null byte into the \"\n                             \"self-pipe socket\",\n                             exc_info=True)\n\n    def _start_serving(self, protocol_factory, sock,\n                       sslcontext=None, server=None, backlog=100,\n                       ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        self._add_reader(sock.fileno(), self._accept_connection,\n                         protocol_factory, sock, sslcontext, server, backlog,\n                         ssl_handshake_timeout)\n\n    def _accept_connection(\n            self, protocol_factory, sock,\n            sslcontext=None, server=None, backlog=100,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        # This method is only called once for each event loop tick where the\n        # listening socket has triggered an EVENT_READ. There may be multiple\n        # connections waiting for an .accept() so it is called in a loop.\n        # See https://bugs.python.org/issue27906 for more details.\n        for _ in range(backlog):\n            try:\n                conn, addr = sock.accept()\n                if self._debug:\n                    logger.debug(\"%r got a new connection from %r: %r\",\n                                 server, addr, conn)\n                conn.setblocking(False)\n            except (BlockingIOError, InterruptedError, ConnectionAbortedError):\n                # Early exit because the socket accept buffer is empty.\n                return None\n            except OSError as exc:\n                # There's nowhere to send the error, so just log it.\n                if exc.errno in (errno.EMFILE, errno.ENFILE,\n                                 errno.ENOBUFS, errno.ENOMEM):\n                    # Some platforms (e.g. Linux keep reporting the FD as\n                    # ready, so we remove the read handler temporarily.\n                    # We'll try again in a while.\n                    self.call_exception_handler({\n                        'message': 'socket.accept() out of system resource',\n                        'exception': exc,\n                        'socket': trsock.TransportSocket(sock),\n                    })\n                    self._remove_reader(sock.fileno())\n                    self.call_later(constants.ACCEPT_RETRY_DELAY,\n                                    self._start_serving,\n                                    protocol_factory, sock, sslcontext, server,\n                                    backlog, ssl_handshake_timeout)\n                else:\n                    raise  # The event loop will catch, log and ignore it.\n            else:\n                extra = {'peername': addr}\n                accept = self._accept_connection2(\n                    protocol_factory, conn, extra, sslcontext, server,\n                    ssl_handshake_timeout)\n                self.create_task(accept)\n\n    async def _accept_connection2(\n            self, protocol_factory, conn, extra,\n            sslcontext=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT):\n        protocol = None\n        transport = None\n        try:\n            protocol = protocol_factory()\n            waiter = self.create_future()\n            if sslcontext:\n                transport = self._make_ssl_transport(\n                    conn, protocol, sslcontext, waiter=waiter,\n                    server_side=True, extra=extra, server=server,\n                    ssl_handshake_timeout=ssl_handshake_timeout)\n            else:\n                transport = self._make_socket_transport(\n                    conn, protocol, waiter=waiter, extra=extra,\n                    server=server)\n\n            try:\n                await waiter\n            except BaseException:\n                transport.close()\n                raise\n                # It's now up to the protocol to handle the connection.\n\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if self._debug:\n                context = {\n                    'message':\n                        'Error on transport creation for incoming connection',\n                    'exception': exc,\n                }\n                if protocol is not None:\n                    context['protocol'] = protocol\n                if transport is not None:\n                    context['transport'] = transport\n                self.call_exception_handler(context)\n\n    def _ensure_fd_no_transport(self, fd):\n        fileno = fd\n        if not isinstance(fileno, int):\n            try:\n                fileno = int(fileno.fileno())\n            except (AttributeError, TypeError, ValueError):\n                # This code matches selectors._fileobj_to_fd function.\n                raise ValueError(f\"Invalid file object: {fd!r}\") from None\n        try:\n            transport = self._transports[fileno]\n        except KeyError:\n            pass\n        else:\n            if not transport.is_closing():\n                raise RuntimeError(\n                    f'File descriptor {fd!r} is used by transport '\n                    f'{transport!r}')\n\n    def _add_reader(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            self._selector.register(fd, selectors.EVENT_READ,\n                                    (handle, None))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_READ,\n                                  (handle, writer))\n            if reader is not None:\n                reader.cancel()\n        return handle\n\n    def _remove_reader(self, fd):\n        if self.is_closed():\n            return False\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            return False\n        else:\n            mask, (reader, writer) = key.events, key.data\n            mask &= ~selectors.EVENT_READ\n            if not mask:\n                self._selector.unregister(fd)\n            else:\n                self._selector.modify(fd, mask, (None, writer))\n\n            if reader is not None:\n                reader.cancel()\n                return True\n            else:\n                return False\n\n    def _add_writer(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            self._selector.register(fd, selectors.EVENT_WRITE,\n                                    (None, handle))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_WRITE,\n                                  (reader, handle))\n            if writer is not None:\n                writer.cancel()\n        return handle\n\n    def _remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        if self.is_closed():\n            return False\n        try:\n            key = self._selector.get_key(fd)\n        except KeyError:\n            return False\n        else:\n            mask, (reader, writer) = key.events, key.data\n            # Remove both writer and connector.\n            mask &= ~selectors.EVENT_WRITE\n            if not mask:\n                self._selector.unregister(fd)\n            else:\n                self._selector.modify(fd, mask, (reader, None))\n\n            if writer is not None:\n                writer.cancel()\n                return True\n            else:\n                return False\n\n    def add_reader(self, fd, callback, *args):\n        \"\"\"Add a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        self._add_reader(fd, callback, *args)\n\n    def remove_reader(self, fd):\n        \"\"\"Remove a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_reader(fd)\n\n    def add_writer(self, fd, callback, *args):\n        \"\"\"Add a writer callback..\"\"\"\n        self._ensure_fd_no_transport(fd)\n        self._add_writer(fd, callback, *args)\n\n    def remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_writer(fd)\n\n    async def sock_recv(self, sock, n):\n        \"\"\"Receive data from the socket.\n\n        The return value is a bytes object representing the data received.\n        The maximum amount of data to be received at once is specified by\n        nbytes.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recv, fut, sock, n)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_read_done(self, fd, fut, handle=None):\n        if handle is None or not handle.cancelled():\n            self.remove_reader(fd)\n\n    def _sock_recv(self, fut, sock, n):\n        # _sock_recv() can add itself as an I/O callback if the operation can't\n        # be done immediately. Don't use it directly, call sock_recv().\n        if fut.done():\n            return\n        try:\n            data = sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(data)\n\n    async def sock_recv_into(self, sock, buf):\n        \"\"\"Receive data from the socket.\n\n        The received data is written into *buf* (a writable buffer).\n        The return value is the number of bytes written.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recv_into, fut, sock, buf)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recv_into(self, fut, sock, buf):\n        # _sock_recv_into() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recv_into().\n        if fut.done():\n            return\n        try:\n            nbytes = sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(nbytes)\n\n    async def sock_sendall(self, sock, data):\n        \"\"\"Send data to the socket.\n\n        The socket must be connected to a remote socket. This method continues\n        to send data from data until either all data has been sent or an\n        error occurs. None is returned on success. On error, an exception is\n        raised, and there is no way to determine how much data, if any, was\n        successfully processed by the receiving end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            n = sock.send(data)\n        except (BlockingIOError, InterruptedError):\n            n = 0\n\n        if n == len(data):\n            # all data sent\n            return\n\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        # use a trick with a list in closure to store a mutable state\n        handle = self._add_writer(fd, self._sock_sendall, fut, sock,\n                                  memoryview(data), [n])\n        fut.add_done_callback(\n            functools.partial(self._sock_write_done, fd, handle=handle))\n        return await fut\n\n    def _sock_sendall(self, fut, sock, view, pos):\n        if fut.done():\n            # Future cancellation can be scheduled on previous loop iteration\n            return\n        start = pos[0]\n        try:\n            n = sock.send(view[start:])\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n            return\n\n        start += n\n\n        if start == len(view):\n            fut.set_result(None)\n        else:\n            pos[0] = start\n\n    async def sock_connect(self, sock, address):\n        \"\"\"Connect to a remote socket at address.\n\n        This method is a coroutine.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n\n        if sock.family == socket.AF_INET or (\n                base_events._HAS_IPv6 and sock.family == socket.AF_INET6):\n            resolved = await self._ensure_resolved(\n                address, family=sock.family, type=sock.type, proto=sock.proto,\n                loop=self,\n            )\n            _, _, _, _, address = resolved[0]\n\n        fut = self.create_future()\n        self._sock_connect(fut, sock, address)\n        return await fut\n\n    def _sock_connect(self, fut, sock, address):\n        fd = sock.fileno()\n        try:\n            sock.connect(address)\n        except (BlockingIOError, InterruptedError):\n            # Issue #23618: When the C function connect() fails with EINTR, the\n            # connection runs in background. We have to wait until the socket\n            # becomes writable to be notified when the connection succeed or\n            # fails.\n            self._ensure_fd_no_transport(fd)\n            handle = self._add_writer(\n                fd, self._sock_connect_cb, fut, sock, address)\n            fut.add_done_callback(\n                functools.partial(self._sock_write_done, fd, handle=handle))\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n\n    def _sock_write_done(self, fd, fut, handle=None):\n        if handle is None or not handle.cancelled():\n            self.remove_writer(fd)\n\n    def _sock_connect_cb(self, fut, sock, address):\n        if fut.done():\n            return\n\n        try:\n            err = sock.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)\n            if err != 0:\n                # Jump to any except clause below.\n                raise OSError(err, f'Connect call failed {address}')\n        except (BlockingIOError, InterruptedError):\n            # socket is still registered, the callback will be retried later\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n\n    async def sock_accept(self, sock):\n        \"\"\"Accept a connection.\n\n        The socket must be bound to an address and listening for connections.\n        The return value is a pair (conn, address) where conn is a new socket\n        object usable to send and receive data on the connection, and address\n        is the address bound to the socket on the other end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        fut = self.create_future()\n        self._sock_accept(fut, sock)\n        return await fut\n\n    def _sock_accept(self, fut, sock):\n        fd = sock.fileno()\n        try:\n            conn, address = sock.accept()\n            conn.setblocking(False)\n        except (BlockingIOError, InterruptedError):\n            self._ensure_fd_no_transport(fd)\n            handle = self._add_reader(fd, self._sock_accept, fut, sock)\n            fut.add_done_callback(\n                functools.partial(self._sock_read_done, fd, handle=handle))\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result((conn, address))\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        del self._transports[transp._sock_fd]\n        resume_reading = transp.is_reading()\n        transp.pause_reading()\n        await transp._make_empty_waiter()\n        try:\n            return await self.sock_sendfile(transp._sock, file, offset, count,\n                                            fallback=False)\n        finally:\n            transp._reset_empty_waiter()\n            if resume_reading:\n                transp.resume_reading()\n            self._transports[transp._sock_fd] = transp\n\n    def _process_events(self, event_list):\n        for key, mask in event_list:\n            fileobj, (reader, writer) = key.fileobj, key.data\n            if mask & selectors.EVENT_READ and reader is not None:\n                if reader._cancelled:\n                    self._remove_reader(fileobj)\n                else:\n                    self._add_callback(reader)\n            if mask & selectors.EVENT_WRITE and writer is not None:\n                if writer._cancelled:\n                    self._remove_writer(fileobj)\n                else:\n                    self._add_callback(writer)\n\n    def _stop_serving(self, sock):\n        self._remove_reader(sock.fileno())\n        sock.close()\n\n\nclass _SelectorTransport(transports._FlowControlMixin,\n                         transports.Transport):\n\n    max_size = 256 * 1024  # Buffer size passed to recv().\n\n    _buffer_factory = bytearray  # Constructs initial value for self._buffer.\n\n    # Attribute used in the destructor: it must be set even if the constructor\n    # is not called (see _SelectorSslTransport which may start by raising an\n    # exception)\n    _sock = None\n\n    def __init__(self, loop, sock, protocol, extra=None, server=None):\n        super().__init__(extra, loop)\n        self._extra['socket'] = trsock.TransportSocket(sock)\n        try:\n            self._extra['sockname'] = sock.getsockname()\n        except OSError:\n            self._extra['sockname'] = None\n        if 'peername' not in self._extra:\n            try:\n                self._extra['peername'] = sock.getpeername()\n            except socket.error:\n                self._extra['peername'] = None\n        self._sock = sock\n        self._sock_fd = sock.fileno()\n\n        self._protocol_connected = False\n        self.set_protocol(protocol)\n\n        self._server = server\n        self._buffer = self._buffer_factory()\n        self._conn_lost = 0  # Set when call to connection_lost scheduled.\n        self._closing = False  # Set when close() called.\n        if self._server is not None:\n            self._server._attach()\n        loop._transports[self._sock_fd] = self\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._sock is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._sock_fd}')\n        # test if the transport was closed\n        if self._loop is not None and not self._loop.is_closed():\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd, selectors.EVENT_READ)\n            if polling:\n                info.append('read=polling')\n            else:\n                info.append('read=idle')\n\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd,\n                                           selectors.EVENT_WRITE)\n            if polling:\n                state = 'polling'\n            else:\n                state = 'idle'\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'write=<{state}, bufsize={bufsize}>')\n        return '<{}>'.format(' '.join(info))\n\n    def abort(self):\n        self._force_close(None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n        self._protocol_connected = True\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if self._closing:\n            return\n        self._closing = True\n        self._loop._remove_reader(self._sock_fd)\n        if not self._buffer:\n            self._conn_lost += 1\n            self._loop._remove_writer(self._sock_fd)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def __del__(self, _warn=warnings.warn):\n        if self._sock is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._sock.close()\n\n    def _fatal_error(self, exc, message='Fatal error on transport'):\n        # Should be called from exception handler only.\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._force_close(exc)\n\n    def _force_close(self, exc):\n        if self._conn_lost:\n            return\n        if self._buffer:\n            self._buffer.clear()\n            self._loop._remove_writer(self._sock_fd)\n        if not self._closing:\n            self._closing = True\n            self._loop._remove_reader(self._sock_fd)\n        self._conn_lost += 1\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            if self._protocol_connected:\n                self._protocol.connection_lost(exc)\n        finally:\n            self._sock.close()\n            self._sock = None\n            self._protocol = None\n            self._loop = None\n            server = self._server\n            if server is not None:\n                server._detach()\n                self._server = None\n\n    def get_write_buffer_size(self):\n        return len(self._buffer)\n\n    def _add_reader(self, fd, callback, *args):\n        if self._closing:\n            return\n\n        self._loop._add_reader(fd, callback, *args)\n\n\nclass _SelectorSocketTransport(_SelectorTransport):\n\n    _start_tls_compatible = True\n    _sendfile_compatible = constants._SendfileMode.TRY_NATIVE\n\n    def __init__(self, loop, sock, protocol, waiter=None,\n                 extra=None, server=None):\n\n        self._read_ready_cb = None\n        super().__init__(loop, sock, protocol, extra, server)\n        self._eof = False\n        self._paused = False\n        self._empty_waiter = None\n\n        # Disable the Nagle algorithm -- small writes will be\n        # sent without waiting for the TCP ACK.  This generally\n        # decreases the latency (in some cases significantly.)\n        base_events._set_nodelay(self._sock)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def set_protocol(self, protocol):\n        if isinstance(protocol, protocols.BufferedProtocol):\n            self._read_ready_cb = self._read_ready__get_buffer\n        else:\n            self._read_ready_cb = self._read_ready__data_received\n\n        super().set_protocol(protocol)\n\n    def is_reading(self):\n        return not self._paused and not self._closing\n\n    def pause_reading(self):\n        if self._closing or self._paused:\n            return\n        self._paused = True\n        self._loop._remove_reader(self._sock_fd)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._add_reader(self._sock_fd, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def _read_ready(self):\n        self._read_ready_cb()\n\n    def _read_ready__get_buffer(self):\n        if self._conn_lost:\n            return\n\n        try:\n            buf = self._protocol.get_buffer(-1)\n            if not len(buf):\n                raise RuntimeError('get_buffer() returned an empty buffer')\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.get_buffer() call failed.')\n            return\n\n        try:\n            nbytes = self._sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not nbytes:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.buffer_updated(nbytes)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.buffer_updated() call failed.')\n\n    def _read_ready__data_received(self):\n        if self._conn_lost:\n            return\n        try:\n            data = self._sock.recv(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not data:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.data_received(data)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.data_received() call failed.')\n\n    def _read_ready__on_eof(self):\n        if self._loop.get_debug():\n            logger.debug(\"%r received EOF\", self)\n\n        try:\n            keep_open = self._protocol.eof_received()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.eof_received() call failed.')\n            return\n\n        if keep_open:\n            # We're keeping the connection open so the\n            # protocol can write more, but we still can't\n            # receive more, so remove the reader callback.\n            self._loop._remove_reader(self._sock_fd)\n        else:\n            self.close()\n\n    def write(self, data):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if self._eof:\n            raise RuntimeError('Cannot call write() after write_eof()')\n        if self._empty_waiter is not None:\n            raise RuntimeError('unable to write; sendfile is in progress')\n        if not data:\n            return\n\n        if self._conn_lost:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Optimization: try to send now.\n            try:\n                n = self._sock.send(data)\n            except (BlockingIOError, InterruptedError):\n                pass\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(exc, 'Fatal write error on socket transport')\n                return\n            else:\n                data = data[n:]\n                if not data:\n                    return\n            # Not all was written; register write handler.\n            self._loop._add_writer(self._sock_fd, self._write_ready)\n\n        # Add it to the buffer.\n        self._buffer.extend(data)\n        self._maybe_pause_protocol()\n\n    def _write_ready(self):\n        assert self._buffer, 'Data should not be empty'\n\n        if self._conn_lost:\n            return\n        try:\n            n = self._sock.send(self._buffer)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._loop._remove_writer(self._sock_fd)\n            self._buffer.clear()\n            self._fatal_error(exc, 'Fatal write error on socket transport')\n            if self._empty_waiter is not None:\n                self._empty_waiter.set_exception(exc)\n        else:\n            if n:\n                del self._buffer[:n]\n            self._maybe_resume_protocol()  # May append to buffer.\n            if not self._buffer:\n                self._loop._remove_writer(self._sock_fd)\n                if self._empty_waiter is not None:\n                    self._empty_waiter.set_result(None)\n                if self._closing:\n                    self._call_connection_lost(None)\n                elif self._eof:\n                    self._sock.shutdown(socket.SHUT_WR)\n\n    def write_eof(self):\n        if self._closing or self._eof:\n            return\n        self._eof = True\n        if not self._buffer:\n            self._sock.shutdown(socket.SHUT_WR)\n\n    def can_write_eof(self):\n        return True\n\n    def _call_connection_lost(self, exc):\n        super()._call_connection_lost(exc)\n        if self._empty_waiter is not None:\n            self._empty_waiter.set_exception(\n                ConnectionError(\"Connection is closed by peer\"))\n\n    def _make_empty_waiter(self):\n        if self._empty_waiter is not None:\n            raise RuntimeError(\"Empty waiter is already set\")\n        self._empty_waiter = self._loop.create_future()\n        if not self._buffer:\n            self._empty_waiter.set_result(None)\n        return self._empty_waiter\n\n    def _reset_empty_waiter(self):\n        self._empty_waiter = None\n\n\nclass _SelectorDatagramTransport(_SelectorTransport):\n\n    _buffer_factory = collections.deque\n\n    def __init__(self, loop, sock, protocol, address=None,\n                 waiter=None, extra=None):\n        super().__init__(loop, sock, protocol, extra)\n        self._address = address\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def get_write_buffer_size(self):\n        return sum(len(data) for data, _ in self._buffer)\n\n    def _read_ready(self):\n        if self._conn_lost:\n            return\n        try:\n            data, addr = self._sock.recvfrom(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._protocol.error_received(exc)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on datagram transport')\n        else:\n            self._protocol.datagram_received(data, addr)\n\n    def sendto(self, data, addr=None):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if not data:\n            return\n\n        if self._address:\n            if addr not in (None, self._address):\n                raise ValueError(\n                    f'Invalid address: must be None or {self._address}')\n            addr = self._address\n\n        if self._conn_lost and self._address:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n                return\n            except (BlockingIOError, InterruptedError):\n                self._loop._add_writer(self._sock_fd, self._sendto_ready)\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        # Ensure that what we buffer is immutable.\n        self._buffer.append((bytes(data), addr))\n        self._maybe_pause_protocol()\n\n    def _sendto_ready(self):\n        while self._buffer:\n            data, addr = self._buffer.popleft()\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n            except (BlockingIOError, InterruptedError):\n                self._buffer.appendleft((data, addr))  # Try again later.\n                break\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        self._maybe_resume_protocol()  # May append to buffer.\n        if not self._buffer:\n            self._loop._remove_writer(self._sock_fd)\n            if self._closing:\n                self._call_connection_lost(None)\n", 1097], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py": ["\"\"\"Selectors module.\n\nThis module allows high-level and efficient I/O multiplexing, built upon the\n`select` module primitives.\n\"\"\"\n\n\nfrom abc import ABCMeta, abstractmethod\nfrom collections import namedtuple\nfrom collections.abc import Mapping\nimport math\nimport select\nimport sys\n\n\n# generic events, that must be mapped to implementation-specific ones\nEVENT_READ = (1 << 0)\nEVENT_WRITE = (1 << 1)\n\n\ndef _fileobj_to_fd(fileobj):\n    \"\"\"Return a file descriptor from a file object.\n\n    Parameters:\n    fileobj -- file object or file descriptor\n\n    Returns:\n    corresponding file descriptor\n\n    Raises:\n    ValueError if the object is invalid\n    \"\"\"\n    if isinstance(fileobj, int):\n        fd = fileobj\n    else:\n        try:\n            fd = int(fileobj.fileno())\n        except (AttributeError, TypeError, ValueError):\n            raise ValueError(\"Invalid file object: \"\n                             \"{!r}\".format(fileobj)) from None\n    if fd < 0:\n        raise ValueError(\"Invalid file descriptor: {}\".format(fd))\n    return fd\n\n\nSelectorKey = namedtuple('SelectorKey', ['fileobj', 'fd', 'events', 'data'])\n\nSelectorKey.__doc__ = \"\"\"SelectorKey(fileobj, fd, events, data)\n\n    Object used to associate a file object to its backing\n    file descriptor, selected event mask, and attached data.\n\"\"\"\nif sys.version_info >= (3, 5):\n    SelectorKey.fileobj.__doc__ = 'File object registered.'\n    SelectorKey.fd.__doc__ = 'Underlying file descriptor.'\n    SelectorKey.events.__doc__ = 'Events that must be waited for on this file object.'\n    SelectorKey.data.__doc__ = ('''Optional opaque data associated to this file object.\n    For example, this could be used to store a per-client session ID.''')\n\n\nclass _SelectorMapping(Mapping):\n    \"\"\"Mapping of file objects to selector keys.\"\"\"\n\n    def __init__(self, selector):\n        self._selector = selector\n\n    def __len__(self):\n        return len(self._selector._fd_to_key)\n\n    def __getitem__(self, fileobj):\n        try:\n            fd = self._selector._fileobj_lookup(fileobj)\n            return self._selector._fd_to_key[fd]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    def __iter__(self):\n        return iter(self._selector._fd_to_key)\n\n\nclass BaseSelector(metaclass=ABCMeta):\n    \"\"\"Selector abstract base class.\n\n    A selector supports registering file objects to be monitored for specific\n    I/O events.\n\n    A file object is a file descriptor or any object with a `fileno()` method.\n    An arbitrary object can be attached to the file object, which can be used\n    for example to store context information, a callback, etc.\n\n    A selector can use various implementations (select(), poll(), epoll()...)\n    depending on the platform. The default `Selector` class uses the most\n    efficient implementation on the current platform.\n    \"\"\"\n\n    @abstractmethod\n    def register(self, fileobj, events, data=None):\n        \"\"\"Register a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        ValueError if events is invalid\n        KeyError if fileobj is already registered\n        OSError if fileobj is closed or otherwise is unacceptable to\n                the underlying system call (if a system call is made)\n\n        Note:\n        OSError may or may not be raised\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def unregister(self, fileobj):\n        \"\"\"Unregister a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        KeyError if fileobj is not registered\n\n        Note:\n        If fileobj is registered but has since been closed this does\n        *not* raise OSError (even if the wrapped syscall does)\n        \"\"\"\n        raise NotImplementedError\n\n    def modify(self, fileobj, events, data=None):\n        \"\"\"Change a registered file object monitored events or attached data.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        Anything that unregister() or register() raises\n        \"\"\"\n        self.unregister(fileobj)\n        return self.register(fileobj, events, data)\n\n    @abstractmethod\n    def select(self, timeout=None):\n        \"\"\"Perform the actual selection, until some monitored file objects are\n        ready or a timeout expires.\n\n        Parameters:\n        timeout -- if timeout > 0, this specifies the maximum wait time, in\n                   seconds\n                   if timeout <= 0, the select() call won't block, and will\n                   report the currently ready file objects\n                   if timeout is None, select() will block until a monitored\n                   file object becomes ready\n\n        Returns:\n        list of (key, events) for ready file objects\n        `events` is a bitwise mask of EVENT_READ|EVENT_WRITE\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the selector.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        pass\n\n    def get_key(self, fileobj):\n        \"\"\"Return the key associated to a registered file object.\n\n        Returns:\n        SelectorKey for this file object\n        \"\"\"\n        mapping = self.get_map()\n        if mapping is None:\n            raise RuntimeError('Selector is closed')\n        try:\n            return mapping[fileobj]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    @abstractmethod\n    def get_map(self):\n        \"\"\"Return a mapping of file objects to selector keys.\"\"\"\n        raise NotImplementedError\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n\nclass _BaseSelectorImpl(BaseSelector):\n    \"\"\"Base selector implementation.\"\"\"\n\n    def __init__(self):\n        # this maps file descriptors to keys\n        self._fd_to_key = {}\n        # read-only mapping returned by get_map()\n        self._map = _SelectorMapping(self)\n\n    def _fileobj_lookup(self, fileobj):\n        \"\"\"Return a file descriptor from a file object.\n\n        This wraps _fileobj_to_fd() to do an exhaustive search in case\n        the object is invalid but we still have it in our map.  This\n        is used by unregister() so we can unregister an object that\n        was previously registered even if it is closed.  It is also\n        used by _SelectorMapping.\n        \"\"\"\n        try:\n            return _fileobj_to_fd(fileobj)\n        except ValueError:\n            # Do an exhaustive search.\n            for key in self._fd_to_key.values():\n                if key.fileobj is fileobj:\n                    return key.fd\n            # Raise ValueError after all.\n            raise\n\n    def register(self, fileobj, events, data=None):\n        if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):\n            raise ValueError(\"Invalid events: {!r}\".format(events))\n\n        key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)\n\n        if key.fd in self._fd_to_key:\n            raise KeyError(\"{!r} (FD {}) is already registered\"\n                           .format(fileobj, key.fd))\n\n        self._fd_to_key[key.fd] = key\n        return key\n\n    def unregister(self, fileobj):\n        try:\n            key = self._fd_to_key.pop(self._fileobj_lookup(fileobj))\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        if events != key.events:\n            self.unregister(fileobj)\n            key = self.register(fileobj, events, data)\n        elif data != key.data:\n            # Use a shortcut to update the data.\n            key = key._replace(data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def close(self):\n        self._fd_to_key.clear()\n        self._map = None\n\n    def get_map(self):\n        return self._map\n\n    def _key_from_fd(self, fd):\n        \"\"\"Return the key associated to a given file descriptor.\n\n        Parameters:\n        fd -- file descriptor\n\n        Returns:\n        corresponding key, or None if not found\n        \"\"\"\n        try:\n            return self._fd_to_key[fd]\n        except KeyError:\n            return None\n\n\nclass SelectSelector(_BaseSelectorImpl):\n    \"\"\"Select-based selector.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._readers = set()\n        self._writers = set()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        if events & EVENT_READ:\n            self._readers.add(key.fd)\n        if events & EVENT_WRITE:\n            self._writers.add(key.fd)\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        self._readers.discard(key.fd)\n        self._writers.discard(key.fd)\n        return key\n\n    if sys.platform == 'win32':\n        def _select(self, r, w, _, timeout=None):\n            r, w, x = select.select(r, w, w, timeout)\n            return r, w + x, []\n    else:\n        _select = select.select\n\n    def select(self, timeout=None):\n        timeout = None if timeout is None else max(timeout, 0)\n        ready = []\n        try:\n            r, w, _ = self._select(self._readers, self._writers, [], timeout)\n        except InterruptedError:\n            return ready\n        r = set(r)\n        w = set(w)\n        for fd in r | w:\n            events = 0\n            if fd in r:\n                events |= EVENT_READ\n            if fd in w:\n                events |= EVENT_WRITE\n\n            key = self._key_from_fd(fd)\n            if key:\n                ready.append((key, events & key.events))\n        return ready\n\n\nclass _PollLikeSelector(_BaseSelectorImpl):\n    \"\"\"Base class shared between poll, epoll and devpoll selectors.\"\"\"\n    _selector_cls = None\n    _EVENT_READ = None\n    _EVENT_WRITE = None\n\n    def __init__(self):\n        super().__init__()\n        self._selector = self._selector_cls()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        poller_events = 0\n        if events & EVENT_READ:\n            poller_events |= self._EVENT_READ\n        if events & EVENT_WRITE:\n            poller_events |= self._EVENT_WRITE\n        try:\n            self._selector.register(key.fd, poller_events)\n        except:\n            super().unregister(fileobj)\n            raise\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        try:\n            self._selector.unregister(key.fd)\n        except OSError:\n            # This can happen if the FD was closed since it\n            # was registered.\n            pass\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(f\"{fileobj!r} is not registered\") from None\n\n        changed = False\n        if events != key.events:\n            selector_events = 0\n            if events & EVENT_READ:\n                selector_events |= self._EVENT_READ\n            if events & EVENT_WRITE:\n                selector_events |= self._EVENT_WRITE\n            try:\n                self._selector.modify(key.fd, selector_events)\n            except:\n                super().unregister(fileobj)\n                raise\n            changed = True\n        if data != key.data:\n            changed = True\n\n        if changed:\n            key = key._replace(events=events, data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def select(self, timeout=None):\n        # This is shared between poll() and epoll().\n        # epoll() has a different signature and handling of timeout parameter.\n        if timeout is None:\n            timeout = None\n        elif timeout <= 0:\n            timeout = 0\n        else:\n            # poll() has a resolution of 1 millisecond, round away from\n            # zero to wait *at least* timeout seconds.\n            timeout = math.ceil(timeout * 1e3)\n        ready = []\n        try:\n            fd_event_list = self._selector.poll(timeout)\n        except InterruptedError:\n            return ready\n        for fd, event in fd_event_list:\n            events = 0\n            if event & ~self._EVENT_READ:\n                events |= EVENT_WRITE\n            if event & ~self._EVENT_WRITE:\n                events |= EVENT_READ\n\n            key = self._key_from_fd(fd)\n            if key:\n                ready.append((key, events & key.events))\n        return ready\n\n\nif hasattr(select, 'poll'):\n\n    class PollSelector(_PollLikeSelector):\n        \"\"\"Poll-based selector.\"\"\"\n        _selector_cls = select.poll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n\nif hasattr(select, 'epoll'):\n\n    class EpollSelector(_PollLikeSelector):\n        \"\"\"Epoll-based selector.\"\"\"\n        _selector_cls = select.epoll\n        _EVENT_READ = select.EPOLLIN\n        _EVENT_WRITE = select.EPOLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def select(self, timeout=None):\n            if timeout is None:\n                timeout = -1\n            elif timeout <= 0:\n                timeout = 0\n            else:\n                # epoll_wait() has a resolution of 1 millisecond, round away\n                # from zero to wait *at least* timeout seconds.\n                timeout = math.ceil(timeout * 1e3) * 1e-3\n\n            # epoll_wait() expects `maxevents` to be greater than zero;\n            # we want to make sure that `select()` can be called when no\n            # FD is registered.\n            max_ev = max(len(self._fd_to_key), 1)\n\n            ready = []\n            try:\n                fd_event_list = self._selector.poll(timeout, max_ev)\n            except InterruptedError:\n                return ready\n            for fd, event in fd_event_list:\n                events = 0\n                if event & ~select.EPOLLIN:\n                    events |= EVENT_WRITE\n                if event & ~select.EPOLLOUT:\n                    events |= EVENT_READ\n\n                key = self._key_from_fd(fd)\n                if key:\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'devpoll'):\n\n    class DevpollSelector(_PollLikeSelector):\n        \"\"\"Solaris /dev/poll selector.\"\"\"\n        _selector_cls = select.devpoll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'kqueue'):\n\n    class KqueueSelector(_BaseSelectorImpl):\n        \"\"\"Kqueue-based selector.\"\"\"\n\n        def __init__(self):\n            super().__init__()\n            self._selector = select.kqueue()\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def register(self, fileobj, events, data=None):\n            key = super().register(fileobj, events, data)\n            try:\n                if events & EVENT_READ:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n                if events & EVENT_WRITE:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n            except:\n                super().unregister(fileobj)\n                raise\n            return key\n\n        def unregister(self, fileobj):\n            key = super().unregister(fileobj)\n            if key.events & EVENT_READ:\n                kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                    select.KQ_EV_DELETE)\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # This can happen if the FD was closed since it\n                    # was registered.\n                    pass\n            if key.events & EVENT_WRITE:\n                kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                    select.KQ_EV_DELETE)\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # See comment above.\n                    pass\n            return key\n\n        def select(self, timeout=None):\n            timeout = None if timeout is None else max(timeout, 0)\n            # If max_ev is 0, kqueue will ignore the timeout. For consistent\n            # behavior with the other selector classes, we prevent that here\n            # (using max). See https://bugs.python.org/issue29255\n            max_ev = max(len(self._fd_to_key), 1)\n            ready = []\n            try:\n                kev_list = self._selector.control(None, max_ev, timeout)\n            except InterruptedError:\n                return ready\n            for kev in kev_list:\n                fd = kev.ident\n                flag = kev.filter\n                events = 0\n                if flag == select.KQ_FILTER_READ:\n                    events |= EVENT_READ\n                if flag == select.KQ_FILTER_WRITE:\n                    events |= EVENT_WRITE\n\n                key = self._key_from_fd(fd)\n                if key:\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\ndef _can_use(method):\n    \"\"\"Check if we can use the selector depending upon the\n    operating system. \"\"\"\n    # Implementation based upon https://github.com/sethmlarson/selectors2/blob/master/selectors2.py\n    selector = getattr(select, method, None)\n    if selector is None:\n        # select module does not implement method\n        return False\n    # check if the OS and Kernel actually support the method. Call may fail with\n    # OSError: [Errno 38] Function not implemented\n    try:\n        selector_obj = selector()\n        if method == 'poll':\n            # check that poll actually works\n            selector_obj.poll(0)\n        else:\n            # close epoll, kqueue, and devpoll fd\n            selector_obj.close()\n        return True\n    except OSError:\n        return False\n\n\n# Choose the best implementation, roughly:\n#    epoll|kqueue|devpoll > poll > select.\n# select() also can't accept a FD > FD_SETSIZE (usually around 1024)\nif _can_use('kqueue'):\n    DefaultSelector = KqueueSelector\nelif _can_use('epoll'):\n    DefaultSelector = EpollSelector\nelif _can_use('devpoll'):\n    DefaultSelector = DevpollSelector\nelif _can_use('poll'):\n    DefaultSelector = PollSelector\nelse:\n    DefaultSelector = SelectSelector\n", 619], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py": ["from __future__ import annotations\n\nimport enum\nfrom dataclasses import dataclass\nfrom typing import Any, Generic, Literal, TypeVar, overload\nfrom weakref import WeakKeyDictionary\n\nfrom ._core._eventloop import get_async_backend\n\nT = TypeVar(\"T\")\nD = TypeVar(\"D\")\n\n\nasync def checkpoint() -> None:\n    \"\"\"\n    Check for cancellation and allow the scheduler to switch to another task.\n\n    Equivalent to (but more efficient than)::\n\n        await checkpoint_if_cancelled()\n        await cancel_shielded_checkpoint()\n\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_async_backend().checkpoint()\n\n\nasync def checkpoint_if_cancelled() -> None:\n    \"\"\"\n    Enter a checkpoint if the enclosing cancel scope has been cancelled.\n\n    This does not allow the scheduler to switch to a different task.\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_async_backend().checkpoint_if_cancelled()\n\n\nasync def cancel_shielded_checkpoint() -> None:\n    \"\"\"\n    Allow the scheduler to switch to another task but without checking for cancellation.\n\n    Equivalent to (but potentially more efficient than)::\n\n        with CancelScope(shield=True):\n            await checkpoint()\n\n\n    .. versionadded:: 3.0\n\n    \"\"\"\n    await get_async_backend().cancel_shielded_checkpoint()\n\n\ndef current_token() -> object:\n    \"\"\"\n    Return a backend specific token object that can be used to get back to the event\n    loop.\n\n    \"\"\"\n    return get_async_backend().current_token()\n\n\n_run_vars: WeakKeyDictionary[Any, dict[str, Any]] = WeakKeyDictionary()\n_token_wrappers: dict[Any, _TokenWrapper] = {}\n\n\n@dataclass(frozen=True)\nclass _TokenWrapper:\n    __slots__ = \"_token\", \"__weakref__\"\n    _token: object\n\n\nclass _NoValueSet(enum.Enum):\n    NO_VALUE_SET = enum.auto()\n\n\nclass RunvarToken(Generic[T]):\n    __slots__ = \"_var\", \"_value\", \"_redeemed\"\n\n    def __init__(self, var: RunVar[T], value: T | Literal[_NoValueSet.NO_VALUE_SET]):\n        self._var = var\n        self._value: T | Literal[_NoValueSet.NO_VALUE_SET] = value\n        self._redeemed = False\n\n\nclass RunVar(Generic[T]):\n    \"\"\"\n    Like a :class:`~contextvars.ContextVar`, except scoped to the running event loop.\n    \"\"\"\n\n    __slots__ = \"_name\", \"_default\"\n\n    NO_VALUE_SET: Literal[_NoValueSet.NO_VALUE_SET] = _NoValueSet.NO_VALUE_SET\n\n    _token_wrappers: set[_TokenWrapper] = set()\n\n    def __init__(\n        self, name: str, default: T | Literal[_NoValueSet.NO_VALUE_SET] = NO_VALUE_SET\n    ):\n        self._name = name\n        self._default = default\n\n    @property\n    def _current_vars(self) -> dict[str, T]:\n        token = current_token()\n        try:\n            return _run_vars[token]\n        except KeyError:\n            run_vars = _run_vars[token] = {}\n            return run_vars\n\n    @overload\n    def get(self, default: D) -> T | D:\n        ...\n\n    @overload\n    def get(self) -> T:\n        ...\n\n    def get(\n        self, default: D | Literal[_NoValueSet.NO_VALUE_SET] = NO_VALUE_SET\n    ) -> T | D:\n        try:\n            return self._current_vars[self._name]\n        except KeyError:\n            if default is not RunVar.NO_VALUE_SET:\n                return default\n            elif self._default is not RunVar.NO_VALUE_SET:\n                return self._default\n\n        raise LookupError(\n            f'Run variable \"{self._name}\" has no value and no default set'\n        )\n\n    def set(self, value: T) -> RunvarToken[T]:\n        current_vars = self._current_vars\n        token = RunvarToken(self, current_vars.get(self._name, RunVar.NO_VALUE_SET))\n        current_vars[self._name] = value\n        return token\n\n    def reset(self, token: RunvarToken[T]) -> None:\n        if token._var is not self:\n            raise ValueError(\"This token does not belong to this RunVar\")\n\n        if token._redeemed:\n            raise ValueError(\"This token has already been used\")\n\n        if token._value is _NoValueSet.NO_VALUE_SET:\n            try:\n                del self._current_vars[self._name]\n            except KeyError:\n                pass\n        else:\n            self._current_vars[self._name] = token._value\n\n        token._redeemed = True\n\n    def __repr__(self) -> str:\n        return f\"<RunVar name={self._name!r}>\"\n", 163], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py": ["\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttps://www.python.org/dev/peps/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType,\n     _remove_dead_weakref)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nimport _collections_abc  # Import after _weakref to avoid circular import.\nimport sys\nimport itertools\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\",\n           \"WeakSet\", \"WeakMethod\", \"finalize\"]\n\n\n_collections_abc.Set.register(WeakSet)\n_collections_abc.MutableSet.register(WeakSet)\n\nclass WeakMethod(ref):\n    \"\"\"\n    A custom `weakref.ref` subclass which simulates a weak reference to\n    a bound method, working around the lifetime problem of bound methods.\n    \"\"\"\n\n    __slots__ = \"_func_ref\", \"_meth_type\", \"_alive\", \"__weakref__\"\n\n    def __new__(cls, meth, callback=None):\n        try:\n            obj = meth.__self__\n            func = meth.__func__\n        except AttributeError:\n            raise TypeError(\"argument should be a bound method, not {}\"\n                            .format(type(meth))) from None\n        def _cb(arg):\n            # The self-weakref trick is needed to avoid creating a reference\n            # cycle.\n            self = self_wr()\n            if self._alive:\n                self._alive = False\n                if callback is not None:\n                    callback(self)\n        self = ref.__new__(cls, obj, _cb)\n        self._func_ref = ref(func, _cb)\n        self._meth_type = type(meth)\n        self._alive = True\n        self_wr = ref(self)\n        return self\n\n    def __call__(self):\n        obj = super().__call__()\n        func = self._func_ref()\n        if obj is None or func is None:\n            return None\n        return self._meth_type(func, obj)\n\n    def __eq__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is other\n            return ref.__eq__(self, other) and self._func_ref == other._func_ref\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is not other\n            return ref.__ne__(self, other) or self._func_ref != other._func_ref\n        return NotImplemented\n\n    __hash__ = ref.__hash__\n\n\nclass WeakValueDictionary(_collections_abc.MutableMapping):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, other=(), /, **kw):\n        def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Atomic removal is necessary since this function\n                    # can be called asynchronously by the GC\n                    _atomic_removal(self.data, wr.key)\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n        self.update(other, **kw)\n\n    def _commit_removals(self, _atomic_removal=_remove_dead_weakref):\n        pop = self._pending_removals.pop\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n            _atomic_removal(d, key)\n\n    def __getitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        o = self.data[key]()\n        if o is None:\n            raise KeyError(key)\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __len__(self):\n        if self._pending_removals:\n            self._commit_removals()\n        return len(self.data)\n\n    def __contains__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def copy(self):\n        if self._pending_removals:\n            self._commit_removals()\n        new = WeakValueDictionary()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        if self._pending_removals:\n            self._commit_removals()\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                v = wr()\n                if v is not None:\n                    yield k, v\n\n    def keys(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                if wr() is not None:\n                    yield k\n\n    __iter__ = keys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            yield from self.data.values()\n\n    def values(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for wr in self.data.values():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            else:\n                raise KeyError(key)\n        else:\n            return o\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n\n    def update(self, other=None, /, **kwargs):\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if other is not None:\n            if not hasattr(other, \"items\"):\n                other = dict(other)\n            for key, o in other.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        for key, o in kwargs.items():\n            d[key] = KeyedRef(o, self._remove, key)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        return list(self.data.values())\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def __or__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.copy()\n            c.update(other)\n            return c\n        return NotImplemented\n\n    def __ror__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.__class__()\n            c.update(other)\n            c.update(self)\n            return c\n        return NotImplemented\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super().__init__(ob, callback)\n\n\nclass WeakKeyDictionary(_collections_abc.MutableMapping):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    try:\n                        del self.data[k]\n                    except KeyError:\n                        pass\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        self._dirty_len = False\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        pop = self._pending_removals.pop\n        d = self.data\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n\n            try:\n                del d[key]\n            except KeyError:\n                pass\n\n    def _scrub_removals(self):\n        d = self.data\n        self._pending_removals = [k for k in self._pending_removals if k in d]\n        self._dirty_len = False\n\n    def __delitem__(self, key):\n        self._dirty_len = True\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __len__(self):\n        if self._dirty_len and self._pending_removals:\n            # self._pending_removals may still contain keys which were\n            # explicitly removed, we have to scrub them (see issue #21173).\n            self._scrub_removals()\n        return len(self.data) - len(self._pending_removals)\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def items(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def keys(self):\n        with _IterationGuard(self):\n            for wr in self.data:\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = keys\n\n    def values(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                if wr() is not None:\n                    yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return list(self.data)\n\n    def popitem(self):\n        self._dirty_len = True\n        while True:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        self._dirty_len = True\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, /, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def __or__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.copy()\n            c.update(other)\n            return c\n        return NotImplemented\n\n    def __ror__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.__class__()\n            c.update(other)\n            c.update(self)\n            return c\n        return NotImplemented\n\n\nclass finalize:\n    \"\"\"Class for finalization of weakrefable objects\n\n    finalize(obj, func, *args, **kwargs) returns a callable finalizer\n    object which will be called when obj is garbage collected. The\n    first time the finalizer is called it evaluates func(*arg, **kwargs)\n    and returns the result. After this the finalizer is dead, and\n    calling it just returns None.\n\n    When the program exits any remaining finalizers for which the\n    atexit attribute is true will be run in reverse order of creation.\n    By default atexit is true.\n    \"\"\"\n\n    # Finalizer objects don't have any state of their own.  They are\n    # just used as keys to lookup _Info objects in the registry.  This\n    # ensures that they cannot be part of a ref-cycle.\n\n    __slots__ = ()\n    _registry = {}\n    _shutdown = False\n    _index_iter = itertools.count()\n    _dirty = False\n    _registered_with_atexit = False\n\n    class _Info:\n        __slots__ = (\"weakref\", \"func\", \"args\", \"kwargs\", \"atexit\", \"index\")\n\n    def __init__(self, obj, func, /, *args, **kwargs):\n        if not self._registered_with_atexit:\n            # We may register the exit function more than once because\n            # of a thread race, but that is harmless\n            import atexit\n            atexit.register(self._exitfunc)\n            finalize._registered_with_atexit = True\n        info = self._Info()\n        info.weakref = ref(obj, self)\n        info.func = func\n        info.args = args\n        info.kwargs = kwargs or None\n        info.atexit = True\n        info.index = next(self._index_iter)\n        self._registry[self] = info\n        finalize._dirty = True\n\n    def __call__(self, _=None):\n        \"\"\"If alive then mark as dead and return func(*args, **kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.pop(self, None)\n        if info and not self._shutdown:\n            return info.func(*info.args, **(info.kwargs or {}))\n\n    def detach(self):\n        \"\"\"If alive then mark as dead and return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None and self._registry.pop(self, None):\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    def peek(self):\n        \"\"\"If alive then return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None:\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    @property\n    def alive(self):\n        \"\"\"Whether finalizer is alive\"\"\"\n        return self in self._registry\n\n    @property\n    def atexit(self):\n        \"\"\"Whether finalizer should be called at exit\"\"\"\n        info = self._registry.get(self)\n        return bool(info) and info.atexit\n\n    @atexit.setter\n    def atexit(self, value):\n        info = self._registry.get(self)\n        if info:\n            info.atexit = bool(value)\n\n    def __repr__(self):\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is None:\n            return '<%s object at %#x; dead>' % (type(self).__name__, id(self))\n        else:\n            return '<%s object at %#x; for %r at %#x>' % \\\n                (type(self).__name__, id(self), type(obj).__name__, id(obj))\n\n    @classmethod\n    def _select_for_exit(cls):\n        # Return live finalizers marked for exit, oldest first\n        L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]\n        L.sort(key=lambda item:item[1].index)\n        return [f for (f,i) in L]\n\n    @classmethod\n    def _exitfunc(cls):\n        # At shutdown invoke finalizers for which atexit is true.\n        # This is called once all other non-daemonic threads have been\n        # joined.\n        reenable_gc = False\n        try:\n            if cls._registry:\n                import gc\n                if gc.isenabled():\n                    reenable_gc = True\n                    gc.disable()\n                pending = None\n                while True:\n                    if pending is None or finalize._dirty:\n                        pending = cls._select_for_exit()\n                        finalize._dirty = False\n                    if not pending:\n                        break\n                    f = pending.pop()\n                    try:\n                        # gc is disabled, so (assuming no daemonic\n                        # threads) the following is the only line in\n                        # this function which might trigger creation\n                        # of a new finalizer\n                        f()\n                    except Exception:\n                        sys.excepthook(*sys.exc_info())\n                    assert f not in cls._registry\n        finally:\n            # prevent any more finalizers from executing during shutdown\n            finalize._shutdown = True\n            if reenable_gc:\n                gc.enable()\n", 675], "/Users/shopbox/projects/profyle/profyle/infrastructure/middleware/fastapi.py": ["from typing import Optional\nfrom starlette.types import ASGIApp, Scope, Receive, Send\n\nfrom profyle.application.profyle import profyle\nfrom profyle.infrastructure.sqlite3.get_connection import get_connection\nfrom profyle.infrastructure.sqlite3.repository import SQLiteTraceRepository\n\n\nclass ProfyleMiddleware:\n    def __init__(\n        self,\n        app: ASGIApp,\n        enabled: bool = True,\n        pattern: Optional[str] = None\n    ):\n        self.app = app\n        self.enabled = enabled\n        self.pattern = pattern\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if self.enabled and scope['type'] == 'http':\n            db = get_connection()\n            sqlite_repo = SQLiteTraceRepository(db)\n            with profyle(\n                name=scope['raw_path'].decode(\"utf-8\"),\n                pattern=self.pattern,\n                repo=sqlite_repo\n            ):\n                await self.app(scope, receive, send)\n            return\n        await self.app(scope, receive, send)\n", 31], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/errors.py": ["import html\nimport inspect\nimport traceback\nimport typing\n\nfrom starlette._utils import is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.requests import Request\nfrom starlette.responses import HTMLResponse, PlainTextResponse, Response\nfrom starlette.types import ASGIApp, Message, Receive, Scope, Send\n\nSTYLES = \"\"\"\np {\n    color: #211c1c;\n}\n.traceback-container {\n    border: 1px solid #038BB8;\n}\n.traceback-title {\n    background-color: #038BB8;\n    color: lemonchiffon;\n    padding: 12px;\n    font-size: 20px;\n    margin-top: 0px;\n}\n.frame-line {\n    padding-left: 10px;\n    font-family: monospace;\n}\n.frame-filename {\n    font-family: monospace;\n}\n.center-line {\n    background-color: #038BB8;\n    color: #f9f6e1;\n    padding: 5px 0px 5px 5px;\n}\n.lineno {\n    margin-right: 5px;\n}\n.frame-title {\n    font-weight: unset;\n    padding: 10px 10px 10px 10px;\n    background-color: #E4F4FD;\n    margin-right: 10px;\n    color: #191f21;\n    font-size: 17px;\n    border: 1px solid #c7dce8;\n}\n.collapse-btn {\n    float: right;\n    padding: 0px 5px 1px 5px;\n    border: solid 1px #96aebb;\n    cursor: pointer;\n}\n.collapsed {\n  display: none;\n}\n.source-code {\n  font-family: courier;\n  font-size: small;\n  padding-bottom: 10px;\n}\n\"\"\"\n\nJS = \"\"\"\n<script type=\"text/javascript\">\n    function collapse(element){\n        const frameId = element.getAttribute(\"data-frame-id\");\n        const frame = document.getElementById(frameId);\n\n        if (frame.classList.contains(\"collapsed\")){\n            element.innerHTML = \"&#8210;\";\n            frame.classList.remove(\"collapsed\");\n        } else {\n            element.innerHTML = \"+\";\n            frame.classList.add(\"collapsed\");\n        }\n    }\n</script>\n\"\"\"\n\nTEMPLATE = \"\"\"\n<html>\n    <head>\n        <style type='text/css'>\n            {styles}\n        </style>\n        <title>Starlette Debugger</title>\n    </head>\n    <body>\n        <h1>500 Server Error</h1>\n        <h2>{error}</h2>\n        <div class=\"traceback-container\">\n            <p class=\"traceback-title\">Traceback</p>\n            <div>{exc_html}</div>\n        </div>\n        {js}\n    </body>\n</html>\n\"\"\"\n\nFRAME_TEMPLATE = \"\"\"\n<div>\n    <p class=\"frame-title\">File <span class=\"frame-filename\">{frame_filename}</span>,\n    line <i>{frame_lineno}</i>,\n    in <b>{frame_name}</b>\n    <span class=\"collapse-btn\" data-frame-id=\"{frame_filename}-{frame_lineno}\" onclick=\"collapse(this)\">{collapse_button}</span>\n    </p>\n    <div id=\"{frame_filename}-{frame_lineno}\" class=\"source-code {collapsed}\">{code_context}</div>\n</div>\n\"\"\"  # noqa: E501\n\nLINE = \"\"\"\n<p><span class=\"frame-line\">\n<span class=\"lineno\">{lineno}.</span> {line}</span></p>\n\"\"\"\n\nCENTER_LINE = \"\"\"\n<p class=\"center-line\"><span class=\"frame-line center-line\">\n<span class=\"lineno\">{lineno}.</span> {line}</span></p>\n\"\"\"\n\n\nclass ServerErrorMiddleware:\n    \"\"\"\n    Handles returning 500 responses when a server error occurs.\n\n    If 'debug' is set, then traceback responses will be returned,\n    otherwise the designated 'handler' will be called.\n\n    This middleware class should generally be used to wrap *everything*\n    else up, so that unhandled exceptions anywhere in the stack\n    always result in an appropriate 500 response.\n    \"\"\"\n\n    def __init__(\n        self,\n        app: ASGIApp,\n        handler: typing.Optional[typing.Callable] = None,\n        debug: bool = False,\n    ) -> None:\n        self.app = app\n        self.handler = handler\n        self.debug = debug\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] != \"http\":\n            await self.app(scope, receive, send)\n            return\n\n        response_started = False\n\n        async def _send(message: Message) -> None:\n            nonlocal response_started, send\n\n            if message[\"type\"] == \"http.response.start\":\n                response_started = True\n            await send(message)\n\n        try:\n            await self.app(scope, receive, _send)\n        except Exception as exc:\n            request = Request(scope)\n            if self.debug:\n                # In debug mode, return traceback responses.\n                response = self.debug_response(request, exc)\n            elif self.handler is None:\n                # Use our default 500 error handler.\n                response = self.error_response(request, exc)\n            else:\n                # Use an installed 500 error handler.\n                if is_async_callable(self.handler):\n                    response = await self.handler(request, exc)\n                else:\n                    response = await run_in_threadpool(self.handler, request, exc)\n\n            if not response_started:\n                await response(scope, receive, send)\n\n            # We always continue to raise the exception.\n            # This allows servers to log the error, or allows test clients\n            # to optionally raise the error within the test case.\n            raise exc\n\n    def format_line(\n        self, index: int, line: str, frame_lineno: int, frame_index: int\n    ) -> str:\n        values = {\n            # HTML escape - line could contain < or >\n            \"line\": html.escape(line).replace(\" \", \"&nbsp\"),\n            \"lineno\": (frame_lineno - frame_index) + index,\n        }\n\n        if index != frame_index:\n            return LINE.format(**values)\n        return CENTER_LINE.format(**values)\n\n    def generate_frame_html(self, frame: inspect.FrameInfo, is_collapsed: bool) -> str:\n        code_context = \"\".join(\n            self.format_line(\n                index, line, frame.lineno, frame.index  # type: ignore[arg-type]\n            )\n            for index, line in enumerate(frame.code_context or [])\n        )\n\n        values = {\n            # HTML escape - filename could contain < or >, especially if it's a virtual\n            # file e.g. <stdin> in the REPL\n            \"frame_filename\": html.escape(frame.filename),\n            \"frame_lineno\": frame.lineno,\n            # HTML escape - if you try very hard it's possible to name a function with <\n            # or >\n            \"frame_name\": html.escape(frame.function),\n            \"code_context\": code_context,\n            \"collapsed\": \"collapsed\" if is_collapsed else \"\",\n            \"collapse_button\": \"+\" if is_collapsed else \"&#8210;\",\n        }\n        return FRAME_TEMPLATE.format(**values)\n\n    def generate_html(self, exc: Exception, limit: int = 7) -> str:\n        traceback_obj = traceback.TracebackException.from_exception(\n            exc, capture_locals=True\n        )\n\n        exc_html = \"\"\n        is_collapsed = False\n        exc_traceback = exc.__traceback__\n        if exc_traceback is not None:\n            frames = inspect.getinnerframes(exc_traceback, limit)\n            for frame in reversed(frames):\n                exc_html += self.generate_frame_html(frame, is_collapsed)\n                is_collapsed = True\n\n        # escape error class and text\n        error = (\n            f\"{html.escape(traceback_obj.exc_type.__name__)}: \"\n            f\"{html.escape(str(traceback_obj))}\"\n        )\n\n        return TEMPLATE.format(styles=STYLES, js=JS, error=error, exc_html=exc_html)\n\n    def generate_plain_text(self, exc: Exception) -> str:\n        return \"\".join(traceback.format_exception(type(exc), exc, exc.__traceback__))\n\n    def debug_response(self, request: Request, exc: Exception) -> Response:\n        accept = request.headers.get(\"accept\", \"\")\n\n        if \"text/html\" in accept:\n            content = self.generate_html(exc)\n            return HTMLResponse(content, status_code=500)\n        content = self.generate_plain_text(exc)\n        return PlainTextResponse(content, status_code=500)\n\n    def error_response(self, request: Request, exc: Exception) -> Response:\n        return PlainTextResponse(\"Internal Server Error\", status_code=500)\n", 256], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/applications.py": ["import typing\nimport warnings\n\nfrom starlette.datastructures import State, URLPath\nfrom starlette.middleware import Middleware\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.middleware.errors import ServerErrorMiddleware\nfrom starlette.middleware.exceptions import ExceptionMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom starlette.routing import BaseRoute, Router\nfrom starlette.types import ASGIApp, Lifespan, Receive, Scope, Send\n\nAppType = typing.TypeVar(\"AppType\", bound=\"Starlette\")\n\n\nclass Starlette:\n    \"\"\"\n    Creates an application instance.\n\n    **Parameters:**\n\n    * **debug** - Boolean indicating if debug tracebacks should be returned on errors.\n    * **routes** - A list of routes to serve incoming HTTP and WebSocket requests.\n    * **middleware** - A list of middleware to run for every request. A starlette\n    application will always automatically include two middleware classes.\n    `ServerErrorMiddleware` is added as the very outermost middleware, to handle\n    any uncaught errors occurring anywhere in the entire stack.\n    `ExceptionMiddleware` is added as the very innermost middleware, to deal\n    with handled exception cases occurring in the routing or endpoints.\n    * **exception_handlers** - A mapping of either integer status codes,\n    or exception class types onto callables which handle the exceptions.\n    Exception handler callables should be of the form\n    `handler(request, exc) -> response` and may be be either standard functions, or\n    async functions.\n    * **on_startup** - A list of callables to run on application startup.\n    Startup handler callables do not take any arguments, and may be be either\n    standard functions, or async functions.\n    * **on_shutdown** - A list of callables to run on application shutdown.\n    Shutdown handler callables do not take any arguments, and may be be either\n    standard functions, or async functions.\n    * **lifespan** - A lifespan context function, which can be used to perform\n    startup and shutdown tasks. This is a newer style that replaces the\n    `on_startup` and `on_shutdown` handlers. Use one or the other, not both.\n    \"\"\"\n\n    def __init__(\n        self: \"AppType\",\n        debug: bool = False,\n        routes: typing.Optional[typing.Sequence[BaseRoute]] = None,\n        middleware: typing.Optional[typing.Sequence[Middleware]] = None,\n        exception_handlers: typing.Optional[\n            typing.Mapping[\n                typing.Any,\n                typing.Callable[\n                    [Request, Exception],\n                    typing.Union[Response, typing.Awaitable[Response]],\n                ],\n            ]\n        ] = None,\n        on_startup: typing.Optional[typing.Sequence[typing.Callable]] = None,\n        on_shutdown: typing.Optional[typing.Sequence[typing.Callable]] = None,\n        lifespan: typing.Optional[Lifespan[\"AppType\"]] = None,\n    ) -> None:\n        # The lifespan context function is a newer style that replaces\n        # on_startup / on_shutdown handlers. Use one or the other, not both.\n        assert lifespan is None or (\n            on_startup is None and on_shutdown is None\n        ), \"Use either 'lifespan' or 'on_startup'/'on_shutdown', not both.\"\n\n        self.debug = debug\n        self.state = State()\n        self.router = Router(\n            routes, on_startup=on_startup, on_shutdown=on_shutdown, lifespan=lifespan\n        )\n        self.exception_handlers = (\n            {} if exception_handlers is None else dict(exception_handlers)\n        )\n        self.user_middleware = [] if middleware is None else list(middleware)\n        self.middleware_stack: typing.Optional[ASGIApp] = None\n\n    def build_middleware_stack(self) -> ASGIApp:\n        debug = self.debug\n        error_handler = None\n        exception_handlers: typing.Dict[\n            typing.Any, typing.Callable[[Request, Exception], Response]\n        ] = {}\n\n        for key, value in self.exception_handlers.items():\n            if key in (500, Exception):\n                error_handler = value\n            else:\n                exception_handlers[key] = value\n\n        middleware = (\n            [Middleware(ServerErrorMiddleware, handler=error_handler, debug=debug)]\n            + self.user_middleware\n            + [\n                Middleware(\n                    ExceptionMiddleware, handlers=exception_handlers, debug=debug\n                )\n            ]\n        )\n\n        app = self.router\n        for cls, options in reversed(middleware):\n            app = cls(app=app, **options)\n        return app\n\n    @property\n    def routes(self) -> typing.List[BaseRoute]:\n        return self.router.routes\n\n    # TODO: Make `__name` a positional-only argument when we drop Python 3.7 support.\n    def url_path_for(self, __name: str, **path_params: typing.Any) -> URLPath:\n        return self.router.url_path_for(__name, **path_params)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        scope[\"app\"] = self\n        if self.middleware_stack is None:\n            self.middleware_stack = self.build_middleware_stack()\n        await self.middleware_stack(scope, receive, send)\n\n    def on_event(self, event_type: str) -> typing.Callable:  # pragma: nocover\n        return self.router.on_event(event_type)\n\n    def mount(\n        self, path: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:  # pragma: nocover\n        self.router.mount(path, app=app, name=name)\n\n    def host(\n        self, host: str, app: ASGIApp, name: typing.Optional[str] = None\n    ) -> None:  # pragma: no cover\n        self.router.host(host, app=app, name=name)\n\n    def add_middleware(self, middleware_class: type, **options: typing.Any) -> None:\n        if self.middleware_stack is not None:  # pragma: no cover\n            raise RuntimeError(\"Cannot add middleware after an application has started\")\n        self.user_middleware.insert(0, Middleware(middleware_class, **options))\n\n    def add_exception_handler(\n        self,\n        exc_class_or_status_code: typing.Union[int, typing.Type[Exception]],\n        handler: typing.Callable,\n    ) -> None:  # pragma: no cover\n        self.exception_handlers[exc_class_or_status_code] = handler\n\n    def add_event_handler(\n        self, event_type: str, func: typing.Callable\n    ) -> None:  # pragma: no cover\n        self.router.add_event_handler(event_type, func)\n\n    def add_route(\n        self,\n        path: str,\n        route: typing.Callable,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> None:  # pragma: no cover\n        self.router.add_route(\n            path, route, methods=methods, name=name, include_in_schema=include_in_schema\n        )\n\n    def add_websocket_route(\n        self, path: str, route: typing.Callable, name: typing.Optional[str] = None\n    ) -> None:  # pragma: no cover\n        self.router.add_websocket_route(path, route, name=name)\n\n    def exception_handler(\n        self, exc_class_or_status_code: typing.Union[int, typing.Type[Exception]]\n    ) -> typing.Callable:\n        warnings.warn(\n            \"The `exception_handler` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/exceptions/ for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_exception_handler(exc_class_or_status_code, func)\n            return func\n\n        return decorator\n\n    def route(\n        self,\n        path: str,\n        methods: typing.Optional[typing.List[str]] = None,\n        name: typing.Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [Route(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `route` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/routing/ for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.router.add_route(\n                path,\n                func,\n                methods=methods,\n                name=name,\n                include_in_schema=include_in_schema,\n            )\n            return func\n\n        return decorator\n\n    def websocket_route(\n        self, path: str, name: typing.Optional[str] = None\n    ) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [WebSocketRoute(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `websocket_route` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/routing/#websocket-routing for the recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.router.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def middleware(self, middleware_type: str) -> typing.Callable:\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> middleware = [Middleware(...), ...]\n        >>> app = Starlette(middleware=middleware)\n        \"\"\"\n        warnings.warn(\n            \"The `middleware` decorator is deprecated, and will be removed in version 1.0.0. \"  # noqa: E501\n            \"Refer to https://www.starlette.io/middleware/#using-middleware for recommended approach.\",  # noqa: E501\n            DeprecationWarning,\n        )\n        assert (\n            middleware_type == \"http\"\n        ), 'Currently only middleware(\"http\") is supported.'\n\n        def decorator(func: typing.Callable) -> typing.Callable:\n            self.add_middleware(BaseHTTPMiddleware, dispatch=func)\n            return func\n\n        return decorator\n", 261], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/applications.py": ["from enum import Enum\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n)\n\nfrom fastapi import routing\nfrom fastapi.datastructures import Default, DefaultPlaceholder\nfrom fastapi.exception_handlers import (\n    http_exception_handler,\n    request_validation_exception_handler,\n    websocket_request_validation_exception_handler,\n)\nfrom fastapi.exceptions import RequestValidationError, WebSocketRequestValidationError\nfrom fastapi.logger import logger\nfrom fastapi.middleware.asyncexitstack import AsyncExitStackMiddleware\nfrom fastapi.openapi.docs import (\n    get_redoc_html,\n    get_swagger_ui_html,\n    get_swagger_ui_oauth2_redirect_html,\n)\nfrom fastapi.openapi.utils import get_openapi\nfrom fastapi.params import Depends\nfrom fastapi.types import DecoratedCallable, IncEx\nfrom fastapi.utils import generate_unique_id\nfrom starlette.applications import Starlette\nfrom starlette.datastructures import State\nfrom starlette.exceptions import HTTPException\nfrom starlette.middleware import Middleware\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.middleware.errors import ServerErrorMiddleware\nfrom starlette.middleware.exceptions import ExceptionMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import HTMLResponse, JSONResponse, Response\nfrom starlette.routing import BaseRoute\nfrom starlette.types import ASGIApp, Lifespan, Receive, Scope, Send\n\nAppType = TypeVar(\"AppType\", bound=\"FastAPI\")\n\n\nclass FastAPI(Starlette):\n    def __init__(\n        self: AppType,\n        *,\n        debug: bool = False,\n        routes: Optional[List[BaseRoute]] = None,\n        title: str = \"FastAPI\",\n        summary: Optional[str] = None,\n        description: str = \"\",\n        version: str = \"0.1.0\",\n        openapi_url: Optional[str] = \"/openapi.json\",\n        openapi_tags: Optional[List[Dict[str, Any]]] = None,\n        servers: Optional[List[Dict[str, Union[str, Any]]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        default_response_class: Type[Response] = Default(JSONResponse),\n        redirect_slashes: bool = True,\n        docs_url: Optional[str] = \"/docs\",\n        redoc_url: Optional[str] = \"/redoc\",\n        swagger_ui_oauth2_redirect_url: Optional[str] = \"/docs/oauth2-redirect\",\n        swagger_ui_init_oauth: Optional[Dict[str, Any]] = None,\n        middleware: Optional[Sequence[Middleware]] = None,\n        exception_handlers: Optional[\n            Dict[\n                Union[int, Type[Exception]],\n                Callable[[Request, Any], Coroutine[Any, Any, Response]],\n            ]\n        ] = None,\n        on_startup: Optional[Sequence[Callable[[], Any]]] = None,\n        on_shutdown: Optional[Sequence[Callable[[], Any]]] = None,\n        lifespan: Optional[Lifespan[AppType]] = None,\n        terms_of_service: Optional[str] = None,\n        contact: Optional[Dict[str, Union[str, Any]]] = None,\n        license_info: Optional[Dict[str, Union[str, Any]]] = None,\n        openapi_prefix: str = \"\",\n        root_path: str = \"\",\n        root_path_in_servers: bool = True,\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        webhooks: Optional[routing.APIRouter] = None,\n        deprecated: Optional[bool] = None,\n        include_in_schema: bool = True,\n        swagger_ui_parameters: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n        separate_input_output_schemas: bool = True,\n        **extra: Any,\n    ) -> None:\n        self.debug = debug\n        self.title = title\n        self.summary = summary\n        self.description = description\n        self.version = version\n        self.terms_of_service = terms_of_service\n        self.contact = contact\n        self.license_info = license_info\n        self.openapi_url = openapi_url\n        self.openapi_tags = openapi_tags\n        self.root_path_in_servers = root_path_in_servers\n        self.docs_url = docs_url\n        self.redoc_url = redoc_url\n        self.swagger_ui_oauth2_redirect_url = swagger_ui_oauth2_redirect_url\n        self.swagger_ui_init_oauth = swagger_ui_init_oauth\n        self.swagger_ui_parameters = swagger_ui_parameters\n        self.servers = servers or []\n        self.separate_input_output_schemas = separate_input_output_schemas\n        self.extra = extra\n        self.openapi_version = \"3.1.0\"\n        self.openapi_schema: Optional[Dict[str, Any]] = None\n        if self.openapi_url:\n            assert self.title, \"A title must be provided for OpenAPI, e.g.: 'My API'\"\n            assert self.version, \"A version must be provided for OpenAPI, e.g.: '2.1.0'\"\n        # TODO: remove when discarding the openapi_prefix parameter\n        if openapi_prefix:\n            logger.warning(\n                '\"openapi_prefix\" has been deprecated in favor of \"root_path\", which '\n                \"follows more closely the ASGI standard, is simpler, and more \"\n                \"automatic. Check the docs at \"\n                \"https://fastapi.tiangolo.com/advanced/sub-applications/\"\n            )\n        self.webhooks = webhooks or routing.APIRouter()\n        self.root_path = root_path or openapi_prefix\n        self.state: State = State()\n        self.dependency_overrides: Dict[Callable[..., Any], Callable[..., Any]] = {}\n        self.router: routing.APIRouter = routing.APIRouter(\n            routes=routes,\n            redirect_slashes=redirect_slashes,\n            dependency_overrides_provider=self,\n            on_startup=on_startup,\n            on_shutdown=on_shutdown,\n            lifespan=lifespan,\n            default_response_class=default_response_class,\n            dependencies=dependencies,\n            callbacks=callbacks,\n            deprecated=deprecated,\n            include_in_schema=include_in_schema,\n            responses=responses,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n        self.exception_handlers: Dict[\n            Any, Callable[[Request, Any], Union[Response, Awaitable[Response]]]\n        ] = ({} if exception_handlers is None else dict(exception_handlers))\n        self.exception_handlers.setdefault(HTTPException, http_exception_handler)\n        self.exception_handlers.setdefault(\n            RequestValidationError, request_validation_exception_handler\n        )\n        self.exception_handlers.setdefault(\n            WebSocketRequestValidationError,\n            # Starlette still has incorrect type specification for the handlers\n            websocket_request_validation_exception_handler,  # type: ignore\n        )\n\n        self.user_middleware: List[Middleware] = (\n            [] if middleware is None else list(middleware)\n        )\n        self.middleware_stack: Union[ASGIApp, None] = None\n        self.setup()\n\n    def build_middleware_stack(self) -> ASGIApp:\n        # Duplicate/override from Starlette to add AsyncExitStackMiddleware\n        # inside of ExceptionMiddleware, inside of custom user middlewares\n        debug = self.debug\n        error_handler = None\n        exception_handlers = {}\n\n        for key, value in self.exception_handlers.items():\n            if key in (500, Exception):\n                error_handler = value\n            else:\n                exception_handlers[key] = value\n\n        middleware = (\n            [Middleware(ServerErrorMiddleware, handler=error_handler, debug=debug)]\n            + self.user_middleware\n            + [\n                Middleware(\n                    ExceptionMiddleware, handlers=exception_handlers, debug=debug\n                ),\n                # Add FastAPI-specific AsyncExitStackMiddleware for dependencies with\n                # contextvars.\n                # This needs to happen after user middlewares because those create a\n                # new contextvars context copy by using a new AnyIO task group.\n                # The initial part of dependencies with yield is executed in the\n                # FastAPI code, inside all the middlewares, but the teardown part\n                # (after yield) is executed in the AsyncExitStack in this middleware,\n                # if the AsyncExitStack lived outside of the custom middlewares and\n                # contextvars were set in a dependency with yield in that internal\n                # contextvars context, the values would not be available in the\n                # outside context of the AsyncExitStack.\n                # By putting the middleware and the AsyncExitStack here, inside all\n                # user middlewares, the code before and after yield in dependencies\n                # with yield is executed in the same contextvars context, so all values\n                # set in contextvars before yield is still available after yield as\n                # would be expected.\n                # Additionally, by having this AsyncExitStack here, after the\n                # ExceptionMiddleware, now dependencies can catch handled exceptions,\n                # e.g. HTTPException, to customize the teardown code (e.g. DB session\n                # rollback).\n                Middleware(AsyncExitStackMiddleware),\n            ]\n        )\n\n        app = self.router\n        for cls, options in reversed(middleware):\n            app = cls(app=app, **options)\n        return app\n\n    def openapi(self) -> Dict[str, Any]:\n        if not self.openapi_schema:\n            self.openapi_schema = get_openapi(\n                title=self.title,\n                version=self.version,\n                openapi_version=self.openapi_version,\n                summary=self.summary,\n                description=self.description,\n                terms_of_service=self.terms_of_service,\n                contact=self.contact,\n                license_info=self.license_info,\n                routes=self.routes,\n                webhooks=self.webhooks.routes,\n                tags=self.openapi_tags,\n                servers=self.servers,\n                separate_input_output_schemas=self.separate_input_output_schemas,\n            )\n        return self.openapi_schema\n\n    def setup(self) -> None:\n        if self.openapi_url:\n            urls = (server_data.get(\"url\") for server_data in self.servers)\n            server_urls = {url for url in urls if url}\n\n            async def openapi(req: Request) -> JSONResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                if root_path not in server_urls:\n                    if root_path and self.root_path_in_servers:\n                        self.servers.insert(0, {\"url\": root_path})\n                        server_urls.add(root_path)\n                return JSONResponse(self.openapi())\n\n            self.add_route(self.openapi_url, openapi, include_in_schema=False)\n        if self.openapi_url and self.docs_url:\n\n            async def swagger_ui_html(req: Request) -> HTMLResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                openapi_url = root_path + self.openapi_url\n                oauth2_redirect_url = self.swagger_ui_oauth2_redirect_url\n                if oauth2_redirect_url:\n                    oauth2_redirect_url = root_path + oauth2_redirect_url\n                return get_swagger_ui_html(\n                    openapi_url=openapi_url,\n                    title=self.title + \" - Swagger UI\",\n                    oauth2_redirect_url=oauth2_redirect_url,\n                    init_oauth=self.swagger_ui_init_oauth,\n                    swagger_ui_parameters=self.swagger_ui_parameters,\n                )\n\n            self.add_route(self.docs_url, swagger_ui_html, include_in_schema=False)\n\n            if self.swagger_ui_oauth2_redirect_url:\n\n                async def swagger_ui_redirect(req: Request) -> HTMLResponse:\n                    return get_swagger_ui_oauth2_redirect_html()\n\n                self.add_route(\n                    self.swagger_ui_oauth2_redirect_url,\n                    swagger_ui_redirect,\n                    include_in_schema=False,\n                )\n        if self.openapi_url and self.redoc_url:\n\n            async def redoc_html(req: Request) -> HTMLResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                openapi_url = root_path + self.openapi_url\n                return get_redoc_html(\n                    openapi_url=openapi_url, title=self.title + \" - ReDoc\"\n                )\n\n            self.add_route(self.redoc_url, redoc_html, include_in_schema=False)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if self.root_path:\n            scope[\"root_path\"] = self.root_path\n        await super().__call__(scope, receive, send)\n\n    def add_api_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Coroutine[Any, Any, Response]],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        name: Optional[str] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        self.router.add_api_route(\n            path,\n            endpoint=endpoint,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=methods,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def api_route(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.router.add_api_route(\n                path,\n                func,\n                response_model=response_model,\n                status_code=status_code,\n                tags=tags,\n                dependencies=dependencies,\n                summary=summary,\n                description=description,\n                response_description=response_description,\n                responses=responses,\n                deprecated=deprecated,\n                methods=methods,\n                operation_id=operation_id,\n                response_model_include=response_model_include,\n                response_model_exclude=response_model_exclude,\n                response_model_by_alias=response_model_by_alias,\n                response_model_exclude_unset=response_model_exclude_unset,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                include_in_schema=include_in_schema,\n                response_class=response_class,\n                name=name,\n                openapi_extra=openapi_extra,\n                generate_unique_id_function=generate_unique_id_function,\n            )\n            return func\n\n        return decorator\n\n    def add_api_websocket_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        name: Optional[str] = None,\n        *,\n        dependencies: Optional[Sequence[Depends]] = None,\n    ) -> None:\n        self.router.add_api_websocket_route(\n            path,\n            endpoint,\n            name=name,\n            dependencies=dependencies,\n        )\n\n    def websocket(\n        self,\n        path: str,\n        name: Optional[str] = None,\n        *,\n        dependencies: Optional[Sequence[Depends]] = None,\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_websocket_route(\n                path,\n                func,\n                name=name,\n                dependencies=dependencies,\n            )\n            return func\n\n        return decorator\n\n    def include_router(\n        self,\n        router: routing.APIRouter,\n        *,\n        prefix: str = \"\",\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        include_in_schema: bool = True,\n        default_response_class: Type[Response] = Default(JSONResponse),\n        callbacks: Optional[List[BaseRoute]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        self.router.include_router(\n            router,\n            prefix=prefix,\n            tags=tags,\n            dependencies=dependencies,\n            responses=responses,\n            deprecated=deprecated,\n            include_in_schema=include_in_schema,\n            default_response_class=default_response_class,\n            callbacks=callbacks,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def get(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.get(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def put(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.put(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def post(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.post(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def delete(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.delete(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def options(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.options(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def head(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.head(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def patch(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.patch(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def trace(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.trace(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def websocket_route(\n        self, path: str, name: Union[str, None] = None\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.router.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def on_event(\n        self, event_type: str\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        return self.router.on_event(event_type)\n\n    def middleware(\n        self, middleware_type: str\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_middleware(BaseHTTPMiddleware, dispatch=func)\n            return func\n\n        return decorator\n\n    def exception_handler(\n        self, exc_class_or_status_code: Union[int, Type[Exception]]\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_exception_handler(exc_class_or_status_code, func)\n            return func\n\n        return decorator\n", 945], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/from_thread.py": ["from __future__ import annotations\n\nimport threading\nfrom collections.abc import Awaitable, Callable, Generator\nfrom concurrent.futures import FIRST_COMPLETED, Future, ThreadPoolExecutor, wait\nfrom contextlib import AbstractContextManager, contextmanager\nfrom inspect import isawaitable\nfrom types import TracebackType\nfrom typing import (\n    Any,\n    AsyncContextManager,\n    ContextManager,\n    Generic,\n    Iterable,\n    TypeVar,\n    cast,\n    overload,\n)\n\nfrom ._core import _eventloop\nfrom ._core._eventloop import get_async_backend, get_cancelled_exc_class, threadlocals\nfrom ._core._synchronization import Event\nfrom ._core._tasks import CancelScope, create_task_group\nfrom .abc._tasks import TaskStatus\n\nT_Retval = TypeVar(\"T_Retval\")\nT_co = TypeVar(\"T_co\")\n\n\ndef run(func: Callable[..., Awaitable[T_Retval]], *args: object) -> T_Retval:\n    \"\"\"\n    Call a coroutine function from a worker thread.\n\n    :param func: a coroutine function\n    :param args: positional arguments for the callable\n    :return: the return value of the coroutine function\n\n    \"\"\"\n    try:\n        async_backend = threadlocals.current_async_backend\n        token = threadlocals.current_token\n    except AttributeError:\n        raise RuntimeError(\"This function can only be run from an AnyIO worker thread\")\n\n    return async_backend.run_async_from_thread(func, args, token=token)\n\n\ndef run_sync(func: Callable[..., T_Retval], *args: object) -> T_Retval:\n    \"\"\"\n    Call a function in the event loop thread from a worker thread.\n\n    :param func: a callable\n    :param args: positional arguments for the callable\n    :return: the return value of the callable\n\n    \"\"\"\n    try:\n        async_backend = threadlocals.current_async_backend\n        token = threadlocals.current_token\n    except AttributeError:\n        raise RuntimeError(\"This function can only be run from an AnyIO worker thread\")\n\n    return async_backend.run_sync_from_thread(func, args, token=token)\n\n\nclass _BlockingAsyncContextManager(Generic[T_co], AbstractContextManager):\n    _enter_future: Future\n    _exit_future: Future\n    _exit_event: Event\n    _exit_exc_info: tuple[\n        type[BaseException] | None, BaseException | None, TracebackType | None\n    ] = (None, None, None)\n\n    def __init__(self, async_cm: AsyncContextManager[T_co], portal: BlockingPortal):\n        self._async_cm = async_cm\n        self._portal = portal\n\n    async def run_async_cm(self) -> bool | None:\n        try:\n            self._exit_event = Event()\n            value = await self._async_cm.__aenter__()\n        except BaseException as exc:\n            self._enter_future.set_exception(exc)\n            raise\n        else:\n            self._enter_future.set_result(value)\n\n        try:\n            # Wait for the sync context manager to exit.\n            # This next statement can raise `get_cancelled_exc_class()` if\n            # something went wrong in a task group in this async context\n            # manager.\n            await self._exit_event.wait()\n        finally:\n            # In case of cancellation, it could be that we end up here before\n            # `_BlockingAsyncContextManager.__exit__` is called, and an\n            # `_exit_exc_info` has been set.\n            result = await self._async_cm.__aexit__(*self._exit_exc_info)\n            return result\n\n    def __enter__(self) -> T_co:\n        self._enter_future = Future()\n        self._exit_future = self._portal.start_task_soon(self.run_async_cm)\n        cm = self._enter_future.result()\n        return cast(T_co, cm)\n\n    def __exit__(\n        self,\n        __exc_type: type[BaseException] | None,\n        __exc_value: BaseException | None,\n        __traceback: TracebackType | None,\n    ) -> bool | None:\n        self._exit_exc_info = __exc_type, __exc_value, __traceback\n        self._portal.call(self._exit_event.set)\n        return self._exit_future.result()\n\n\nclass _BlockingPortalTaskStatus(TaskStatus):\n    def __init__(self, future: Future):\n        self._future = future\n\n    def started(self, value: object = None) -> None:\n        self._future.set_result(value)\n\n\nclass BlockingPortal:\n    \"\"\"An object that lets external threads run code in an asynchronous event loop.\"\"\"\n\n    def __new__(cls) -> BlockingPortal:\n        return get_async_backend().create_blocking_portal()\n\n    def __init__(self) -> None:\n        self._event_loop_thread_id: int | None = threading.get_ident()\n        self._stop_event = Event()\n        self._task_group = create_task_group()\n        self._cancelled_exc_class = get_cancelled_exc_class()\n\n    async def __aenter__(self) -> BlockingPortal:\n        await self._task_group.__aenter__()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        await self.stop()\n        return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n\n    def _check_running(self) -> None:\n        if self._event_loop_thread_id is None:\n            raise RuntimeError(\"This portal is not running\")\n        if self._event_loop_thread_id == threading.get_ident():\n            raise RuntimeError(\n                \"This method cannot be called from the event loop thread\"\n            )\n\n    async def sleep_until_stopped(self) -> None:\n        \"\"\"Sleep until :meth:`stop` is called.\"\"\"\n        await self._stop_event.wait()\n\n    async def stop(self, cancel_remaining: bool = False) -> None:\n        \"\"\"\n        Signal the portal to shut down.\n\n        This marks the portal as no longer accepting new calls and exits from\n        :meth:`sleep_until_stopped`.\n\n        :param cancel_remaining: ``True`` to cancel all the remaining tasks, ``False``\n            to let them finish before returning\n\n        \"\"\"\n        self._event_loop_thread_id = None\n        self._stop_event.set()\n        if cancel_remaining:\n            self._task_group.cancel_scope.cancel()\n\n    async def _call_func(\n        self, func: Callable, args: tuple, kwargs: dict[str, Any], future: Future\n    ) -> None:\n        def callback(f: Future) -> None:\n            if f.cancelled() and self._event_loop_thread_id not in (\n                None,\n                threading.get_ident(),\n            ):\n                self.call(scope.cancel)\n\n        try:\n            retval = func(*args, **kwargs)\n            if isawaitable(retval):\n                with CancelScope() as scope:\n                    if future.cancelled():\n                        scope.cancel()\n                    else:\n                        future.add_done_callback(callback)\n\n                    retval = await retval\n        except self._cancelled_exc_class:\n            future.cancel()\n            future.set_running_or_notify_cancel()\n        except BaseException as exc:\n            if not future.cancelled():\n                future.set_exception(exc)\n\n            # Let base exceptions fall through\n            if not isinstance(exc, Exception):\n                raise\n        else:\n            if not future.cancelled():\n                future.set_result(retval)\n        finally:\n            scope = None  # type: ignore[assignment]\n\n    def _spawn_task_from_thread(\n        self,\n        func: Callable,\n        args: tuple[Any, ...],\n        kwargs: dict[str, Any],\n        name: object,\n        future: Future,\n    ) -> None:\n        \"\"\"\n        Spawn a new task using the given callable.\n\n        Implementors must ensure that the future is resolved when the task finishes.\n\n        :param func: a callable\n        :param args: positional arguments to be passed to the callable\n        :param kwargs: keyword arguments to be passed to the callable\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :param future: a future that will resolve to the return value of the callable,\n            or the exception raised during its execution\n\n        \"\"\"\n        raise NotImplementedError\n\n    @overload\n    def call(self, func: Callable[..., Awaitable[T_Retval]], *args: object) -> T_Retval:\n        ...\n\n    @overload\n    def call(self, func: Callable[..., T_Retval], *args: object) -> T_Retval:\n        ...\n\n    def call(\n        self,\n        func: Callable[..., Awaitable[T_Retval] | T_Retval],\n        *args: object,\n    ) -> T_Retval:\n        \"\"\"\n        Call the given function in the event loop thread.\n\n        If the callable returns a coroutine object, it is awaited on.\n\n        :param func: any callable\n        :raises RuntimeError: if the portal is not running or if this method is called\n            from within the event loop thread\n\n        \"\"\"\n        return cast(T_Retval, self.start_task_soon(func, *args).result())\n\n    @overload\n    def start_task_soon(\n        self,\n        func: Callable[..., Awaitable[T_Retval]],\n        *args: object,\n        name: object = None,\n    ) -> Future[T_Retval]:\n        ...\n\n    @overload\n    def start_task_soon(\n        self, func: Callable[..., T_Retval], *args: object, name: object = None\n    ) -> Future[T_Retval]:\n        ...\n\n    def start_task_soon(\n        self,\n        func: Callable[..., Awaitable[T_Retval] | T_Retval],\n        *args: object,\n        name: object = None,\n    ) -> Future[T_Retval]:\n        \"\"\"\n        Start a task in the portal's task group.\n\n        The task will be run inside a cancel scope which can be cancelled by cancelling\n        the returned future.\n\n        :param func: the target function\n        :param args: positional arguments passed to ``func``\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :return: a future that resolves with the return value of the callable if the\n            task completes successfully, or with the exception raised in the task\n        :raises RuntimeError: if the portal is not running or if this method is called\n            from within the event loop thread\n        :rtype: concurrent.futures.Future[T_Retval]\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        self._check_running()\n        f: Future = Future()\n        self._spawn_task_from_thread(func, args, {}, name, f)\n        return f\n\n    def start_task(\n        self,\n        func: Callable[..., Awaitable[Any]],\n        *args: object,\n        name: object = None,\n    ) -> tuple[Future[Any], Any]:\n        \"\"\"\n        Start a task in the portal's task group and wait until it signals for readiness.\n\n        This method works the same way as :meth:`.abc.TaskGroup.start`.\n\n        :param func: the target function\n        :param args: positional arguments passed to ``func``\n        :param name: name of the task (will be coerced to a string if not ``None``)\n        :return: a tuple of (future, task_status_value) where the ``task_status_value``\n            is the value passed to ``task_status.started()`` from within the target\n            function\n        :rtype: tuple[concurrent.futures.Future[Any], Any]\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n\n        def task_done(future: Future) -> None:\n            if not task_status_future.done():\n                if future.cancelled():\n                    task_status_future.cancel()\n                elif future.exception():\n                    task_status_future.set_exception(future.exception())\n                else:\n                    exc = RuntimeError(\n                        \"Task exited without calling task_status.started()\"\n                    )\n                    task_status_future.set_exception(exc)\n\n        self._check_running()\n        task_status_future: Future = Future()\n        task_status = _BlockingPortalTaskStatus(task_status_future)\n        f: Future = Future()\n        f.add_done_callback(task_done)\n        self._spawn_task_from_thread(func, args, {\"task_status\": task_status}, name, f)\n        return f, task_status_future.result()\n\n    def wrap_async_context_manager(\n        self, cm: AsyncContextManager[T_co]\n    ) -> ContextManager[T_co]:\n        \"\"\"\n        Wrap an async context manager as a synchronous context manager via this portal.\n\n        Spawns a task that will call both ``__aenter__()`` and ``__aexit__()``, stopping\n        in the middle until the synchronous context manager exits.\n\n        :param cm: an asynchronous context manager\n        :return: a synchronous context manager\n\n        .. versionadded:: 2.1\n\n        \"\"\"\n        return _BlockingAsyncContextManager(cm, self)\n\n\n@contextmanager\ndef start_blocking_portal(\n    backend: str = \"asyncio\", backend_options: dict[str, Any] | None = None\n) -> Generator[BlockingPortal, Any, None]:\n    \"\"\"\n    Start a new event loop in a new thread and run a blocking portal in its main task.\n\n    The parameters are the same as for :func:`~anyio.run`.\n\n    :param backend: name of the backend\n    :param backend_options: backend options\n    :return: a context manager that yields a blocking portal\n\n    .. versionchanged:: 3.0\n        Usage as a context manager is now required.\n\n    \"\"\"\n\n    async def run_portal() -> None:\n        async with BlockingPortal() as portal_:\n            if future.set_running_or_notify_cancel():\n                future.set_result(portal_)\n                await portal_.sleep_until_stopped()\n\n    future: Future[BlockingPortal] = Future()\n    with ThreadPoolExecutor(1) as executor:\n        run_future = executor.submit(\n            _eventloop.run,\n            run_portal,  # type: ignore[arg-type]\n            backend=backend,\n            backend_options=backend_options,\n        )\n        try:\n            wait(\n                cast(Iterable[Future], [run_future, future]),\n                return_when=FIRST_COMPLETED,\n            )\n        except BaseException:\n            future.cancel()\n            run_future.cancel()\n            raise\n\n        if future.done():\n            portal = future.result()\n            cancel_remaining_tasks = False\n            try:\n                yield portal\n            except BaseException:\n                cancel_remaining_tasks = True\n                raise\n            finally:\n                try:\n                    portal.call(portal.stop, cancel_remaining_tasks)\n                except RuntimeError:\n                    pass\n\n        run_future.result()\n", 424], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py": ["# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\nfrom types import GenericAlias\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard:\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet:\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        pop = self._pending_removals.pop\n        discard = self.data.discard\n        while True:\n            try:\n                item = pop()\n            except IndexError:\n                return\n            discard(item)\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),),\n                getattr(self, '__dict__', None))\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet') from None\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(map(ref, other))\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(map(ref, other))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(map(ref, other))\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n\n    def __repr__(self):\n        return repr(self.data)\n\n    __class_getitem__ = classmethod(GenericAlias)\n", 206], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py": ["\"\"\"A Future class similar to the one in PEP 3148.\"\"\"\n\n__all__ = (\n    'Future', 'wrap_future', 'isfuture',\n)\n\nimport concurrent.futures\nimport contextvars\nimport logging\nimport sys\nfrom types import GenericAlias\n\nfrom . import base_futures\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\n\n\nisfuture = base_futures.isfuture\n\n\n_PENDING = base_futures._PENDING\n_CANCELLED = base_futures._CANCELLED\n_FINISHED = base_futures._FINISHED\n\n\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\n\n\nclass Future:\n    \"\"\"This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - This class is not thread-safe.\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.\n\n    (In Python 3.4 or later we may be able to unify the implementations.)\n    \"\"\"\n\n    # Class variables serving as defaults for instance variables.\n    _state = _PENDING\n    _result = None\n    _exception = None\n    _loop = None\n    _source_traceback = None\n    _cancel_message = None\n    # A saved CancelledError for later chaining as an exception context.\n    _cancelled_exc = None\n\n    # This field is used for a dual purpose:\n    # - Its presence is a marker to declare that a class implements\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\n    #   The value must also be not-None, to enable a subclass to declare\n    #   that it is not compatible by setting this to None.\n    # - It is set by __iter__() below so that Task._step() can tell\n    #   the difference between\n    #   `await Future()` or`yield from Future()` (correct) vs.\n    #   `yield Future()` (incorrect).\n    _asyncio_future_blocking = False\n\n    __log_traceback = False\n\n    def __init__(self, *, loop=None):\n        \"\"\"Initialize the future.\n\n        The optional event_loop argument allows explicitly setting the event\n        loop object used by the future. If it's not provided, the future uses\n        the default event loop.\n        \"\"\"\n        if loop is None:\n            self._loop = events.get_event_loop()\n        else:\n            self._loop = loop\n        self._callbacks = []\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    _repr_info = base_futures._future_repr_info\n\n    def __repr__(self):\n        return '<{} {}>'.format(self.__class__.__name__,\n                                ' '.join(self._repr_info()))\n\n    def __del__(self):\n        if not self.__log_traceback:\n            # set_exception() was not called, or result() or exception()\n            # has consumed the exception\n            return\n        exc = self._exception\n        context = {\n            'message':\n                f'{self.__class__.__name__} exception was never retrieved',\n            'exception': exc,\n            'future': self,\n        }\n        if self._source_traceback:\n            context['source_traceback'] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    @property\n    def _log_traceback(self):\n        return self.__log_traceback\n\n    @_log_traceback.setter\n    def _log_traceback(self, val):\n        if bool(val):\n            raise ValueError('_log_traceback can only be set to False')\n        self.__log_traceback = False\n\n    def get_loop(self):\n        \"\"\"Return the event loop the Future is bound to.\"\"\"\n        loop = self._loop\n        if loop is None:\n            raise RuntimeError(\"Future object is not initialized.\")\n        return loop\n\n    def _make_cancelled_error(self):\n        \"\"\"Create the CancelledError to raise if the Future is cancelled.\n\n        This should only be called once when handling a cancellation since\n        it erases the saved context exception value.\n        \"\"\"\n        if self._cancel_message is None:\n            exc = exceptions.CancelledError()\n        else:\n            exc = exceptions.CancelledError(self._cancel_message)\n        exc.__context__ = self._cancelled_exc\n        # Remove the reference since we don't need this anymore.\n        self._cancelled_exc = None\n        return exc\n\n    def cancel(self, msg=None):\n        \"\"\"Cancel the future and schedule callbacks.\n\n        If the future is already done or cancelled, return False.  Otherwise,\n        change the future's state to cancelled, schedule the callbacks and\n        return True.\n        \"\"\"\n        self.__log_traceback = False\n        if self._state != _PENDING:\n            return False\n        self._state = _CANCELLED\n        self._cancel_message = msg\n        self.__schedule_callbacks()\n        return True\n\n    def __schedule_callbacks(self):\n        \"\"\"Internal: Ask the event loop to call all callbacks.\n\n        The callbacks are scheduled to be called as soon as possible. Also\n        clears the callback list.\n        \"\"\"\n        callbacks = self._callbacks[:]\n        if not callbacks:\n            return\n\n        self._callbacks[:] = []\n        for callback, ctx in callbacks:\n            self._loop.call_soon(callback, self, context=ctx)\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        return self._state == _CANCELLED\n\n    # Don't implement running(); see http://bugs.python.org/issue18699\n\n    def done(self):\n        \"\"\"Return True if the future is done.\n\n        Done means either that a result / exception are available, or that the\n        future was cancelled.\n        \"\"\"\n        return self._state != _PENDING\n\n    def result(self):\n        \"\"\"Return the result this future represents.\n\n        If the future has been cancelled, raises CancelledError.  If the\n        future's result isn't yet available, raises InvalidStateError.  If\n        the future is done and has an exception set, this exception is raised.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Result is not ready.')\n        self.__log_traceback = False\n        if self._exception is not None:\n            raise self._exception\n        return self._result\n\n    def exception(self):\n        \"\"\"Return the exception that was set on this future.\n\n        The exception (or None if no exception was set) is returned only if\n        the future is done.  If the future has been cancelled, raises\n        CancelledError.  If the future isn't done yet, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Exception is not set.')\n        self.__log_traceback = False\n        return self._exception\n\n    def add_done_callback(self, fn, *, context=None):\n        \"\"\"Add a callback to be run when the future becomes done.\n\n        The callback is called with a single argument - the future object. If\n        the future is already done when this is called, the callback is\n        scheduled with call_soon.\n        \"\"\"\n        if self._state != _PENDING:\n            self._loop.call_soon(fn, self, context=context)\n        else:\n            if context is None:\n                context = contextvars.copy_context()\n            self._callbacks.append((fn, context))\n\n    # New method not in PEP 3148.\n\n    def remove_done_callback(self, fn):\n        \"\"\"Remove all instances of a callback from the \"call when done\" list.\n\n        Returns the number of callbacks removed.\n        \"\"\"\n        filtered_callbacks = [(f, ctx)\n                              for (f, ctx) in self._callbacks\n                              if f != fn]\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\n        if removed_count:\n            self._callbacks[:] = filtered_callbacks\n        return removed_count\n\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\n\n    def set_result(self, result):\n        \"\"\"Mark the future done and set its result.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        self._result = result\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Mark the future done and set an exception.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        if isinstance(exception, type):\n            exception = exception()\n        if type(exception) is StopIteration:\n            raise TypeError(\"StopIteration interacts badly with generators \"\n                            \"and cannot be raised into a Future\")\n        self._exception = exception\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n        self.__log_traceback = True\n\n    def __await__(self):\n        if not self.done():\n            self._asyncio_future_blocking = True\n            yield self  # This tells Task to wait for completion.\n        if not self.done():\n            raise RuntimeError(\"await wasn't used with future\")\n        return self.result()  # May raise too.\n\n    __iter__ = __await__  # make compatible with 'yield from'.\n\n\n# Needed for testing purposes.\n_PyFuture = Future\n\n\ndef _get_loop(fut):\n    # Tries to call Future.get_loop() if it's available.\n    # Otherwise fallbacks to using the old '_loop' property.\n    try:\n        get_loop = fut.get_loop\n    except AttributeError:\n        pass\n    else:\n        return get_loop()\n    return fut._loop\n\n\ndef _set_result_unless_cancelled(fut, result):\n    \"\"\"Helper setting the result only if the future was not cancelled.\"\"\"\n    if fut.cancelled():\n        return\n    fut.set_result(result)\n\n\ndef _convert_future_exc(exc):\n    exc_class = type(exc)\n    if exc_class is concurrent.futures.CancelledError:\n        return exceptions.CancelledError(*exc.args)\n    elif exc_class is concurrent.futures.TimeoutError:\n        return exceptions.TimeoutError(*exc.args)\n    elif exc_class is concurrent.futures.InvalidStateError:\n        return exceptions.InvalidStateError(*exc.args)\n    else:\n        return exc\n\n\ndef _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(_convert_future_exc(exception))\n    else:\n        result = source.result()\n        concurrent.set_result(result)\n\n\ndef _copy_future_state(source, dest):\n    \"\"\"Internal helper to copy state from another Future.\n\n    The other Future may be a concurrent.futures.Future.\n    \"\"\"\n    assert source.done()\n    if dest.cancelled():\n        return\n    assert not dest.done()\n    if source.cancelled():\n        dest.cancel()\n    else:\n        exception = source.exception()\n        if exception is not None:\n            dest.set_exception(_convert_future_exc(exception))\n        else:\n            result = source.result()\n            dest.set_result(result)\n\n\ndef _chain_future(source, destination):\n    \"\"\"Chain two futures so that when one completes, so does the other.\n\n    The result (or exception) of source will be copied to destination.\n    If destination is cancelled, source gets cancelled too.\n    Compatible with both asyncio.Future and concurrent.futures.Future.\n    \"\"\"\n    if not isfuture(source) and not isinstance(source,\n                                               concurrent.futures.Future):\n        raise TypeError('A future is required for source argument')\n    if not isfuture(destination) and not isinstance(destination,\n                                                    concurrent.futures.Future):\n        raise TypeError('A future is required for destination argument')\n    source_loop = _get_loop(source) if isfuture(source) else None\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\n\n    def _set_state(future, other):\n        if isfuture(future):\n            _copy_future_state(other, future)\n        else:\n            _set_concurrent_future_state(future, other)\n\n    def _call_check_cancel(destination):\n        if destination.cancelled():\n            if source_loop is None or source_loop is dest_loop:\n                source.cancel()\n            else:\n                source_loop.call_soon_threadsafe(source.cancel)\n\n    def _call_set_state(source):\n        if (destination.cancelled() and\n                dest_loop is not None and dest_loop.is_closed()):\n            return\n        if dest_loop is None or dest_loop is source_loop:\n            _set_state(destination, source)\n        else:\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\n\n    destination.add_done_callback(_call_check_cancel)\n    source.add_done_callback(_call_set_state)\n\n\ndef wrap_future(future, *, loop=None):\n    \"\"\"Wrap concurrent.futures.Future object.\"\"\"\n    if isfuture(future):\n        return future\n    assert isinstance(future, concurrent.futures.Future), \\\n        f'concurrent.futures.Future is expected, got {future!r}'\n    if loop is None:\n        loop = events.get_event_loop()\n    new_future = loop.create_future()\n    _chain_future(future, new_future)\n    return new_future\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CFuture is needed for tests.\n    Future = _CFuture = _asyncio.Future\n", 423], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py": ["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\nimport functools\n\nfrom time import monotonic as _time\nfrom _weakrefset import WeakSet\nfrom itertools import islice as _islice, count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size',\n           'excepthook', 'ExceptHookArgs']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_new_thread = _thread.start_new_thread\n_allocate_lock = _thread.allocate_lock\n_set_sentinel = _thread._set_sentinel\nget_ident = _thread.get_ident\ntry:\n    get_native_id = _thread.get_native_id\n    _HAVE_THREAD_NATIVE_ID = True\n    __all__.append('get_native_id')\nexcept AttributeError:\n    _HAVE_THREAD_NATIVE_ID = False\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\n# Synchronization classes\n\nLock = _allocate_lock\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def _at_fork_reinit(self):\n        self._block._at_fork_reinit()\n        self._owner = None\n        self._count = 0\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self._waiters = _deque()\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n        self._waiters.clear()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(False):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        waiters = self._waiters\n        while waiters and n > 0:\n            waiter = waiters[0]\n            try:\n                waiter.release()\n            except RuntimeError:\n                # gh-92530: The previous call of notify() released the lock,\n                # but was interrupted before removing it from the queue.\n                # It can happen if a signal handler raises an exception,\n                # like CTRL+C which raises KeyboardInterrupt.\n                pass\n            else:\n                n -= 1\n            try:\n                waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    notifyAll = notify_all\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        Semaphore.__init__(self, value)\n        self._initial_value = value\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            if self._value + n > self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def _at_fork_reinit(self):\n        # Private method called by Thread._reset_internal_locks()\n        self._cond._at_fork_reinit()\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    isSet = is_set\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0  # 0 filling, 1 draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count().__next__\n_counter() # Consume 0 so first non-main thread has id 1.\ndef _newname(template=\"Thread-%d\"):\n    return template % _counter()\n\n# Active thread administration.\n#\n# bpo-44422: Use a reentrant lock to allow reentrant calls to functions like\n# threading.enumerate().\n_active_limbo_lock = RLock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n\n# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()\n# to wait until all Python thread states get deleted:\n# see Thread._set_tstate_lock().\n_shutdown_locks_lock = _allocate_lock()\n_shutdown_locks = set()\n\ndef _maintain_shutdown_locks():\n    \"\"\"\n    Drop any shutdown locks that don't correspond to running threads anymore.\n\n    Calling this from time to time avoids an ever-growing _shutdown_locks\n    set when Thread objects are not joined explicitly. See bpo-37788.\n\n    This must be called with _shutdown_locks_lock acquired.\n    \"\"\"\n    # If a lock was released, the corresponding thread has exited\n    to_remove = [lock for lock in _shutdown_locks if not lock.locked()]\n    _shutdown_locks.difference_update(to_remove)\n\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is the argument tuple for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        self._target = target\n        self._name = str(name or _newname())\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        if _HAVE_THREAD_NATIVE_ID:\n            self._native_id = None\n        self._tstate_lock = None\n        self._started = Event()\n        self._is_stopped = False\n        self._initialized = True\n        # Copy of sys.stderr used by self._invoke_excepthook()\n        self._stderr = _sys.stderr\n        self._invoke_excepthook = _make_invoke_excepthook()\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _reset_internal_locks(self, is_alive):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        self._started._at_fork_reinit()\n        if is_alive:\n            # bpo-42350: If the fork happens when the thread is already stopped\n            # (ex: after threading._shutdown() has been called), _tstate_lock\n            # is None. Do nothing in this case.\n            if self._tstate_lock is not None:\n                self._tstate_lock._at_fork_reinit()\n                self._tstate_lock.acquire()\n        else:\n            # The thread isn't alive after fork: it doesn't have a tstate\n            # anymore.\n            self._is_stopped = True\n            self._tstate_lock = None\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        self.is_alive() # easy way to get ._is_stopped set when appropriate\n        if self._is_stopped:\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self._bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    if _HAVE_THREAD_NATIVE_ID:\n        def _set_native_id(self):\n            self._native_id = get_native_id()\n\n    def _set_tstate_lock(self):\n        \"\"\"\n        Set a lock object which will be released by the interpreter when\n        the underlying thread state (see pystate.h) gets deleted.\n        \"\"\"\n        self._tstate_lock = _set_sentinel()\n        self._tstate_lock.acquire()\n\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _maintain_shutdown_locks()\n                _shutdown_locks.add(self._tstate_lock)\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self._set_tstate_lock()\n            if _HAVE_THREAD_NATIVE_ID:\n                self._set_native_id()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except:\n                self._invoke_excepthook(self)\n        finally:\n            with _active_limbo_lock:\n                try:\n                    # We don't call self._delete() because it also\n                    # grabs _active_limbo_lock.\n                    del _active[get_ident()]\n                except:\n                    pass\n\n    def _stop(self):\n        # After calling ._stop(), .is_alive() returns False and .join() returns\n        # immediately.  ._tstate_lock must be released before calling ._stop().\n        #\n        # Normal case:  C code at the end of the thread's life\n        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and\n        # that's detected by our ._wait_for_tstate_lock(), called by .join()\n        # and .is_alive().  Any number of threads _may_ call ._stop()\n        # simultaneously (for example, if multiple threads are blocked in\n        # .join() calls), and they're not serialized.  That's harmless -\n        # they'll just make redundant rebindings of ._is_stopped and\n        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the\n        # \"assert self._is_stopped\" in ._wait_for_tstate_lock() always works\n        # (the assert is executed only if ._tstate_lock is None).\n        #\n        # Special case:  _main_thread releases ._tstate_lock via this\n        # module's _shutdown() function.\n        lock = self._tstate_lock\n        if lock is not None:\n            assert not lock.locked()\n        self._is_stopped = True\n        self._tstate_lock = None\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                # Remove our lock and other released locks from _shutdown_locks\n                _maintain_shutdown_locks()\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if timeout is None:\n            self._wait_for_tstate_lock()\n        else:\n            # the behavior of a negative timeout isn't documented, but\n            # historically .join(timeout=x) for x<0 has acted as if timeout=0\n            self._wait_for_tstate_lock(timeout=max(timeout, 0))\n\n    def _wait_for_tstate_lock(self, block=True, timeout=-1):\n        # Issue #18808: wait for the thread state to be gone.\n        # At the end of the thread's life, after all knowledge of the thread\n        # is removed from C data structures, C code releases our _tstate_lock.\n        # This method passes its arguments to _tstate_lock.acquire().\n        # If the lock is acquired, the C code is done, and self._stop() is\n        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.\n        lock = self._tstate_lock\n        if lock is None:\n            # already determined that the C code is done\n            assert self._is_stopped\n            return\n\n        try:\n            if lock.acquire(block, timeout):\n                lock.release()\n                self._stop()\n        except:\n            if lock.locked():\n                # bpo-45274: lock.acquire() acquired the lock, but the function\n                # was interrupted with an exception before reaching the\n                # lock.release(). It can happen if a signal handler raises an\n                # exception, like CTRL+C which raises KeyboardInterrupt.\n                lock.release()\n                self._stop()\n            raise\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    if _HAVE_THREAD_NATIVE_ID:\n        @property\n        def native_id(self):\n            \"\"\"Native integral thread ID of this thread, or None if it has not been started.\n\n            This is a non-negative integer. See the get_native_id() function.\n            This represents the Thread ID as reported by the kernel.\n\n            \"\"\"\n            assert self._initialized, \"Thread.__init__() not called\"\n            return self._native_id\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. See also the module function\n        enumerate().\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        if self._is_stopped or not self._started.is_set():\n            return False\n        self._wait_for_tstate_lock(False)\n        return not self._is_stopped\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        self.daemon = daemonic\n\n    def getName(self):\n        return self.name\n\n    def setName(self, name):\n        self.name = name\n\n\ntry:\n    from _thread import (_excepthook as excepthook,\n                         _ExceptHookArgs as ExceptHookArgs)\nexcept ImportError:\n    # Simple Python implementation if _thread._excepthook() is not available\n    from traceback import print_exception as _print_exception\n    from collections import namedtuple\n\n    _ExceptHookArgs = namedtuple(\n        'ExceptHookArgs',\n        'exc_type exc_value exc_traceback thread')\n\n    def ExceptHookArgs(args):\n        return _ExceptHookArgs(*args)\n\n    def excepthook(args, /):\n        \"\"\"\n        Handle uncaught Thread.run() exception.\n        \"\"\"\n        if args.exc_type == SystemExit:\n            # silently ignore SystemExit\n            return\n\n        if _sys is not None and _sys.stderr is not None:\n            stderr = _sys.stderr\n        elif args.thread is not None:\n            stderr = args.thread._stderr\n            if stderr is None:\n                # do nothing if sys.stderr is None and sys.stderr was None\n                # when the thread was created\n                return\n        else:\n            # do nothing if sys.stderr is None and args.thread is None\n            return\n\n        if args.thread is not None:\n            name = args.thread.name\n        else:\n            name = get_ident()\n        print(f\"Exception in thread {name}:\",\n              file=stderr, flush=True)\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\n                         file=stderr)\n        stderr.flush()\n\n\ndef _make_invoke_excepthook():\n    # Create a local namespace to ensure that variables remain alive\n    # when _invoke_excepthook() is called, even if it is called late during\n    # Python shutdown. It is mostly needed for daemon threads.\n\n    old_excepthook = excepthook\n    old_sys_excepthook = _sys.excepthook\n    if old_excepthook is None:\n        raise RuntimeError(\"threading.excepthook is None\")\n    if old_sys_excepthook is None:\n        raise RuntimeError(\"sys.excepthook is None\")\n\n    sys_exc_info = _sys.exc_info\n    local_print = print\n    local_sys = _sys\n\n    def invoke_excepthook(thread):\n        global excepthook\n        try:\n            hook = excepthook\n            if hook is None:\n                hook = old_excepthook\n\n            args = ExceptHookArgs([*sys_exc_info(), thread])\n\n            hook(args)\n        except Exception as exc:\n            exc.__suppress_context__ = True\n            del exc\n\n            if local_sys is not None and local_sys.stderr is not None:\n                stderr = local_sys.stderr\n            else:\n                stderr = thread._stderr\n\n            local_print(\"Exception in threading.excepthook:\",\n                        file=stderr, flush=True)\n\n            if local_sys is not None and local_sys.excepthook is not None:\n                sys_excepthook = local_sys.excepthook\n            else:\n                sys_excepthook = old_sys_excepthook\n\n            sys_excepthook(*sys_exc_info())\n        finally:\n            # Break reference cycle (exception stored in a variable)\n            args = None\n\n    return invoke_excepthook\n\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._set_tstate_lock()\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"), daemon=True)\n\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n    def _stop(self):\n        pass\n\n    def is_alive(self):\n        assert not self._is_stopped and self._started.is_set()\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ncurrentThread = current_thread\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\nactiveCount = active_count\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\n\n_threading_atexits = []\n_SHUTTING_DOWN = False\n\ndef _register_atexit(func, *arg, **kwargs):\n    \"\"\"CPython internal: register *func* to be called before joining threads.\n\n    The registered *func* is called with its arguments just before all\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\n    purpose to `atexit.register()`, but its functions are called prior to\n    threading shutdown instead of interpreter shutdown.\n\n    For similarity to atexit, the registered functions are called in reverse.\n    \"\"\"\n    if _SHUTTING_DOWN:\n        raise RuntimeError(\"can't register atexit after shutdown\")\n\n    call = functools.partial(func, *arg, **kwargs)\n    _threading_atexits.append(call)\n\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure:  other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it.  We can't wait for C code to release\n    # the main thread's tstate_lock - that won't happen until the interpreter\n    # is nearly dead.  So we release it here.  Note that just calling _stop()\n    # isn't enough:  other threads may already be waiting on _tstate_lock.\n    if _main_thread._is_stopped:\n        # _shutdown() was already called\n        return\n\n    global _SHUTTING_DOWN\n    _SHUTTING_DOWN = True\n\n    # Call registered threading atexit functions before threads are joined.\n    # Order is reversed, similar to atexit.\n    for atexit_call in reversed(_threading_atexits):\n        atexit_call()\n\n    # Main thread\n    if _main_thread.ident == get_ident():\n        tlock = _main_thread._tstate_lock\n        # The main thread isn't finished yet, so its thread state lock can't\n        # have been released.\n        assert tlock is not None\n        assert tlock.locked()\n        tlock.release()\n        _main_thread._stop()\n    else:\n        # bpo-1596321: _shutdown() must be called in the main thread.\n        # If the threading module was not imported by the main thread,\n        # _main_thread is the thread which imported the threading module.\n        # In this case, ignore _main_thread, similar behavior than for threads\n        # spawned by C libraries or using _thread.start_new_thread().\n        pass\n\n    # Join all non-deamon threads\n    while True:\n        with _shutdown_locks_lock:\n            locks = list(_shutdown_locks)\n            _shutdown_locks.clear()\n\n        if not locks:\n            break\n\n        for lock in locks:\n            # mimic Thread.join()\n            lock.acquire()\n            lock.release()\n\n        # new threads can be spawned while we were waiting for the other\n        # threads to complete\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    return _main_thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    global _shutdown_locks_lock, _shutdown_locks\n    _active_limbo_lock = RLock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    # reset _shutdown() locks: threads re-register their _tstate_lock below\n    _shutdown_locks_lock = _allocate_lock()\n    _shutdown_locks = set()\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                thread._reset_internal_locks(True)\n                ident = get_ident()\n                thread._ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._reset_internal_locks(False)\n                thread._stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n", 1555], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py": ["'''A multi-producer, multi-consumer queue.'''\n\nimport threading\nimport types\nfrom collections import deque\nfrom heapq import heappush, heappop\nfrom time import monotonic as time\ntry:\n    from _queue import SimpleQueue\nexcept ImportError:\n    SimpleQueue = None\n\n__all__ = ['Empty', 'Full', 'Queue', 'PriorityQueue', 'LifoQueue', 'SimpleQueue']\n\n\ntry:\n    from _queue import Empty\nexcept ImportError:\n    class Empty(Exception):\n        'Exception raised by Queue.get(block=0)/get_nowait().'\n        pass\n\nclass Full(Exception):\n    'Exception raised by Queue.put(block=0)/put_nowait().'\n    pass\n\n\nclass Queue:\n    '''Create a queue object with a given maximum size.\n\n    If maxsize is <= 0, the queue size is infinite.\n    '''\n\n    def __init__(self, maxsize=0):\n        self.maxsize = maxsize\n        self._init(maxsize)\n\n        # mutex must be held whenever the queue is mutating.  All methods\n        # that acquire mutex must release it before returning.  mutex\n        # is shared between the three conditions, so acquiring and\n        # releasing the conditions also acquires and releases mutex.\n        self.mutex = threading.Lock()\n\n        # Notify not_empty whenever an item is added to the queue; a\n        # thread waiting to get is notified then.\n        self.not_empty = threading.Condition(self.mutex)\n\n        # Notify not_full whenever an item is removed from the queue;\n        # a thread waiting to put is notified then.\n        self.not_full = threading.Condition(self.mutex)\n\n        # Notify all_tasks_done whenever the number of unfinished tasks\n        # drops to zero; thread waiting to join() is notified to resume\n        self.all_tasks_done = threading.Condition(self.mutex)\n        self.unfinished_tasks = 0\n\n    def task_done(self):\n        '''Indicate that a formerly enqueued task is complete.\n\n        Used by Queue consumer threads.  For each get() used to fetch a task,\n        a subsequent call to task_done() tells the queue that the processing\n        on the task is complete.\n\n        If a join() is currently blocking, it will resume when all items\n        have been processed (meaning that a task_done() call was received\n        for every item that had been put() into the queue).\n\n        Raises a ValueError if called more times than there were items\n        placed in the queue.\n        '''\n        with self.all_tasks_done:\n            unfinished = self.unfinished_tasks - 1\n            if unfinished <= 0:\n                if unfinished < 0:\n                    raise ValueError('task_done() called too many times')\n                self.all_tasks_done.notify_all()\n            self.unfinished_tasks = unfinished\n\n    def join(self):\n        '''Blocks until all items in the Queue have been gotten and processed.\n\n        The count of unfinished tasks goes up whenever an item is added to the\n        queue. The count goes down whenever a consumer thread calls task_done()\n        to indicate the item was retrieved and all work on it is complete.\n\n        When the count of unfinished tasks drops to zero, join() unblocks.\n        '''\n        with self.all_tasks_done:\n            while self.unfinished_tasks:\n                self.all_tasks_done.wait()\n\n    def qsize(self):\n        '''Return the approximate size of the queue (not reliable!).'''\n        with self.mutex:\n            return self._qsize()\n\n    def empty(self):\n        '''Return True if the queue is empty, False otherwise (not reliable!).\n\n        This method is likely to be removed at some point.  Use qsize() == 0\n        as a direct substitute, but be aware that either approach risks a race\n        condition where a queue can grow before the result of empty() or\n        qsize() can be used.\n\n        To create code that needs to wait for all queued tasks to be\n        completed, the preferred technique is to use the join() method.\n        '''\n        with self.mutex:\n            return not self._qsize()\n\n    def full(self):\n        '''Return True if the queue is full, False otherwise (not reliable!).\n\n        This method is likely to be removed at some point.  Use qsize() >= n\n        as a direct substitute, but be aware that either approach risks a race\n        condition where a queue can shrink before the result of full() or\n        qsize() can be used.\n        '''\n        with self.mutex:\n            return 0 < self.maxsize <= self._qsize()\n\n    def put(self, item, block=True, timeout=None):\n        '''Put an item into the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until a free slot is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Full exception if no free slot was available within that time.\n        Otherwise ('block' is false), put an item on the queue if a free slot\n        is immediately available, else raise the Full exception ('timeout'\n        is ignored in that case).\n        '''\n        with self.not_full:\n            if self.maxsize > 0:\n                if not block:\n                    if self._qsize() >= self.maxsize:\n                        raise Full\n                elif timeout is None:\n                    while self._qsize() >= self.maxsize:\n                        self.not_full.wait()\n                elif timeout < 0:\n                    raise ValueError(\"'timeout' must be a non-negative number\")\n                else:\n                    endtime = time() + timeout\n                    while self._qsize() >= self.maxsize:\n                        remaining = endtime - time()\n                        if remaining <= 0.0:\n                            raise Full\n                        self.not_full.wait(remaining)\n            self._put(item)\n            self.unfinished_tasks += 1\n            self.not_empty.notify()\n\n    def get(self, block=True, timeout=None):\n        '''Remove and return an item from the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until an item is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Empty exception if no item was available within that time.\n        Otherwise ('block' is false), return an item if one is immediately\n        available, else raise the Empty exception ('timeout' is ignored\n        in that case).\n        '''\n        with self.not_empty:\n            if not block:\n                if not self._qsize():\n                    raise Empty\n            elif timeout is None:\n                while not self._qsize():\n                    self.not_empty.wait()\n            elif timeout < 0:\n                raise ValueError(\"'timeout' must be a non-negative number\")\n            else:\n                endtime = time() + timeout\n                while not self._qsize():\n                    remaining = endtime - time()\n                    if remaining <= 0.0:\n                        raise Empty\n                    self.not_empty.wait(remaining)\n            item = self._get()\n            self.not_full.notify()\n            return item\n\n    def put_nowait(self, item):\n        '''Put an item into the queue without blocking.\n\n        Only enqueue the item if a free slot is immediately available.\n        Otherwise raise the Full exception.\n        '''\n        return self.put(item, block=False)\n\n    def get_nowait(self):\n        '''Remove and return an item from the queue without blocking.\n\n        Only get an item if one is immediately available. Otherwise\n        raise the Empty exception.\n        '''\n        return self.get(block=False)\n\n    # Override these methods to implement other queue organizations\n    # (e.g. stack or priority queue).\n    # These will only be called with appropriate locks held\n\n    # Initialize the queue representation\n    def _init(self, maxsize):\n        self.queue = deque()\n\n    def _qsize(self):\n        return len(self.queue)\n\n    # Put a new item in the queue\n    def _put(self, item):\n        self.queue.append(item)\n\n    # Get an item from the queue\n    def _get(self):\n        return self.queue.popleft()\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\n\nclass PriorityQueue(Queue):\n    '''Variant of Queue that retrieves open entries in priority order (lowest first).\n\n    Entries are typically tuples of the form:  (priority number, data).\n    '''\n\n    def _init(self, maxsize):\n        self.queue = []\n\n    def _qsize(self):\n        return len(self.queue)\n\n    def _put(self, item):\n        heappush(self.queue, item)\n\n    def _get(self):\n        return heappop(self.queue)\n\n\nclass LifoQueue(Queue):\n    '''Variant of Queue that retrieves most recently added entries first.'''\n\n    def _init(self, maxsize):\n        self.queue = []\n\n    def _qsize(self):\n        return len(self.queue)\n\n    def _put(self, item):\n        self.queue.append(item)\n\n    def _get(self):\n        return self.queue.pop()\n\n\nclass _PySimpleQueue:\n    '''Simple, unbounded FIFO queue.\n\n    This pure Python implementation is not reentrant.\n    '''\n    # Note: while this pure Python version provides fairness\n    # (by using a threading.Semaphore which is itself fair, being based\n    #  on threading.Condition), fairness is not part of the API contract.\n    # This allows the C version to use a different implementation.\n\n    def __init__(self):\n        self._queue = deque()\n        self._count = threading.Semaphore(0)\n\n    def put(self, item, block=True, timeout=None):\n        '''Put the item on the queue.\n\n        The optional 'block' and 'timeout' arguments are ignored, as this method\n        never blocks.  They are provided for compatibility with the Queue class.\n        '''\n        self._queue.append(item)\n        self._count.release()\n\n    def get(self, block=True, timeout=None):\n        '''Remove and return an item from the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until an item is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Empty exception if no item was available within that time.\n        Otherwise ('block' is false), return an item if one is immediately\n        available, else raise the Empty exception ('timeout' is ignored\n        in that case).\n        '''\n        if timeout is not None and timeout < 0:\n            raise ValueError(\"'timeout' must be a non-negative number\")\n        if not self._count.acquire(block, timeout):\n            raise Empty\n        return self._queue.popleft()\n\n    def put_nowait(self, item):\n        '''Put an item into the queue without blocking.\n\n        This is exactly equivalent to `put(item, block=False)` and is only provided\n        for compatibility with the Queue class.\n        '''\n        return self.put(item, block=False)\n\n    def get_nowait(self):\n        '''Remove and return an item from the queue without blocking.\n\n        Only get an item if one is immediately available. Otherwise\n        raise the Empty exception.\n        '''\n        return self.get(block=False)\n\n    def empty(self):\n        '''Return True if the queue is empty, False otherwise (not reliable!).'''\n        return len(self._queue) == 0\n\n    def qsize(self):\n        '''Return the approximate size of the queue (not reliable!).'''\n        return len(self._queue)\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\n\nif SimpleQueue is None:\n    SimpleQueue = _PySimpleQueue\n", 326], "/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py": ["# License: EPL\nimport os\nimport re\nimport sys\nfrom _pydev_bundle._pydev_saved_modules import threading\nfrom _pydevd_bundle.pydevd_constants import get_global_debugger, IS_WINDOWS, IS_JYTHON, get_current_thread_id, \\\n    sorted_dict_repr, set_global_debugger, DebugInfoHolder\nfrom _pydev_bundle import pydev_log\nfrom contextlib import contextmanager\nfrom _pydevd_bundle import pydevd_constants, pydevd_defaults\nfrom _pydevd_bundle.pydevd_defaults import PydevdCustomization\nimport ast\n\ntry:\n    from pathlib import Path\nexcept ImportError:\n    Path = None\n\n#===============================================================================\n# Things that are dependent on having the pydevd debugger\n#===============================================================================\n\npydev_src_dir = os.path.dirname(os.path.dirname(__file__))\n\n_arg_patch = threading.local()\n\n\n@contextmanager\ndef skip_subprocess_arg_patch():\n    _arg_patch.apply_arg_patching = False\n    try:\n        yield\n    finally:\n        _arg_patch.apply_arg_patching = True\n\n\ndef _get_apply_arg_patching():\n    return getattr(_arg_patch, 'apply_arg_patching', True)\n\n\ndef _get_setup_updated_with_protocol_and_ppid(setup, is_exec=False):\n    if setup is None:\n        setup = {}\n    setup = setup.copy()\n    # Discard anything related to the protocol (we'll set the the protocol based on the one\n    # currently set).\n    setup.pop(pydevd_constants.ARGUMENT_HTTP_JSON_PROTOCOL, None)\n    setup.pop(pydevd_constants.ARGUMENT_JSON_PROTOCOL, None)\n    setup.pop(pydevd_constants.ARGUMENT_QUOTED_LINE_PROTOCOL, None)\n\n    if not is_exec:\n        # i.e.: The ppid for the subprocess is the current pid.\n        # If it's an exec, keep it what it was.\n        setup[pydevd_constants.ARGUMENT_PPID] = os.getpid()\n\n    protocol = pydevd_constants.get_protocol()\n    if protocol == pydevd_constants.HTTP_JSON_PROTOCOL:\n        setup[pydevd_constants.ARGUMENT_HTTP_JSON_PROTOCOL] = True\n\n    elif protocol == pydevd_constants.JSON_PROTOCOL:\n        setup[pydevd_constants.ARGUMENT_JSON_PROTOCOL] = True\n\n    elif protocol == pydevd_constants.QUOTED_LINE_PROTOCOL:\n        setup[pydevd_constants.ARGUMENT_QUOTED_LINE_PROTOCOL] = True\n\n    elif protocol == pydevd_constants.HTTP_PROTOCOL:\n        setup[pydevd_constants.ARGUMENT_HTTP_PROTOCOL] = True\n\n    else:\n        pydev_log.debug('Unexpected protocol: %s', protocol)\n\n    mode = pydevd_defaults.PydevdCustomization.DEBUG_MODE\n    if mode:\n        setup['debug-mode'] = mode\n\n    preimport = pydevd_defaults.PydevdCustomization.PREIMPORT\n    if preimport:\n        setup['preimport'] = preimport\n\n    if DebugInfoHolder.PYDEVD_DEBUG_FILE:\n        setup['log-file'] = DebugInfoHolder.PYDEVD_DEBUG_FILE\n\n    if DebugInfoHolder.DEBUG_TRACE_LEVEL:\n        setup['log-level'] = DebugInfoHolder.DEBUG_TRACE_LEVEL\n\n    return setup\n\n\nclass _LastFutureImportFinder(ast.NodeVisitor):\n\n    def __init__(self):\n        self.last_future_import_found = None\n\n    def visit_ImportFrom(self, node):\n        if node.module == '__future__':\n            self.last_future_import_found = node\n\n\ndef _get_offset_from_line_col(code, line, col):\n    offset = 0\n    for i, line_contents in enumerate(code.splitlines(True)):\n        if i == line:\n            offset += col\n            return offset\n        else:\n            offset += len(line_contents)\n\n    return -1\n\n\ndef _separate_future_imports(code):\n    '''\n    :param code:\n        The code from where we want to get the __future__ imports (note that it's possible that\n        there's no such entry).\n\n    :return tuple(str, str):\n        The return is a tuple(future_import, code).\n\n        If the future import is not available a return such as ('', code) is given, otherwise, the\n        future import will end with a ';' (so that it can be put right before the pydevd attach\n        code).\n    '''\n    try:\n        node = ast.parse(code, '<string>', 'exec')\n        visitor = _LastFutureImportFinder()\n        visitor.visit(node)\n\n        if visitor.last_future_import_found is None:\n            return '', code\n\n        node = visitor.last_future_import_found\n        offset = -1\n        if hasattr(node, 'end_lineno') and hasattr(node, 'end_col_offset'):\n            # Python 3.8 onwards has these (so, use when possible).\n            line, col = node.end_lineno, node.end_col_offset\n            offset = _get_offset_from_line_col(code, line - 1, col)  # ast lines are 1-based, make it 0-based.\n\n        else:\n            # end line/col not available, let's just find the offset and then search\n            # for the alias from there.\n            line, col = node.lineno, node.col_offset\n            offset = _get_offset_from_line_col(code, line - 1, col)  # ast lines are 1-based, make it 0-based.\n            if offset >= 0 and node.names:\n                from_future_import_name = node.names[-1].name\n                i = code.find(from_future_import_name, offset)\n                if i < 0:\n                    offset = -1\n                else:\n                    offset = i + len(from_future_import_name)\n\n        if offset >= 0:\n            for i in range(offset, len(code)):\n                if code[i] in (' ', '\\t', ';', ')', '\\n'):\n                    offset += 1\n                else:\n                    break\n\n            future_import = code[:offset]\n            code_remainder = code[offset:]\n\n            # Now, put '\\n' lines back into the code remainder (we had to search for\n            # `\\n)`, but in case we just got the `\\n`, it should be at the remainder,\n            # not at the future import.\n            while future_import.endswith('\\n'):\n                future_import = future_import[:-1]\n                code_remainder = '\\n' + code_remainder\n\n            if not future_import.endswith(';'):\n                future_import += ';'\n            return future_import, code_remainder\n\n        # This shouldn't happen...\n        pydev_log.info('Unable to find line %s in code:\\n%r', line, code)\n        return '', code\n\n    except:\n        pydev_log.exception('Error getting from __future__ imports from: %r', code)\n        return '', code\n\n\ndef _get_python_c_args(host, port, code, args, setup):\n    setup = _get_setup_updated_with_protocol_and_ppid(setup)\n\n    # i.e.: We want to make the repr sorted so that it works in tests.\n    setup_repr = setup if setup is None else (sorted_dict_repr(setup))\n\n    future_imports = ''\n    if '__future__' in code:\n        # If the code has a __future__ import, we need to be able to strip the __future__\n        # imports from the code and add them to the start of our code snippet.\n        future_imports, code = _separate_future_imports(code)\n\n    return (\"%simport sys; sys.path.insert(0, r'%s'); import pydevd; pydevd.config(%r, %r); \"\n            \"pydevd.settrace(host=%r, port=%s, suspend=False, trace_only_current_thread=False, patch_multiprocessing=True, access_token=%r, client_access_token=%r, __setup_holder__=%s); \"\n            \"%s\"\n            ) % (\n               future_imports,\n               pydev_src_dir,\n               pydevd_constants.get_protocol(),\n               PydevdCustomization.DEBUG_MODE,\n               host,\n               port,\n               setup.get('access-token'),\n               setup.get('client-access-token'),\n               setup_repr,\n               code)\n\n\ndef _get_host_port():\n    import pydevd\n    host, port = pydevd.dispatch()\n    return host, port\n\n\ndef _is_managed_arg(arg):\n    pydevd_py = _get_str_type_compatible(arg, 'pydevd.py')\n    if arg.endswith(pydevd_py):\n        return True\n    return False\n\n\ndef _on_forked_process(setup_tracing=True):\n    pydevd_constants.after_fork()\n    pydev_log.initialize_debug_stream(reinitialize=True)\n\n    if setup_tracing:\n        pydev_log.debug('pydevd on forked process: %s', os.getpid())\n\n    import pydevd\n    pydevd.threadingCurrentThread().__pydevd_main_thread = True\n    pydevd.settrace_forked(setup_tracing=setup_tracing)\n\n\ndef _on_set_trace_for_new_thread(global_debugger):\n    if global_debugger is not None:\n        global_debugger.enable_tracing()\n\n\ndef _get_str_type_compatible(s, args):\n    '''\n    This method converts `args` to byte/unicode based on the `s' type.\n    '''\n    if isinstance(args, (list, tuple)):\n        ret = []\n        for arg in args:\n            if type(s) == type(arg):\n                ret.append(arg)\n            else:\n                if isinstance(s, bytes):\n                    ret.append(arg.encode('utf-8'))\n                else:\n                    ret.append(arg.decode('utf-8'))\n        return ret\n    else:\n        if type(s) == type(args):\n            return args\n        else:\n            if isinstance(s, bytes):\n                return args.encode('utf-8')\n            else:\n                return args.decode('utf-8')\n\n\n#===============================================================================\n# Things related to monkey-patching\n#===============================================================================\ndef is_python(path):\n    single_quote, double_quote = _get_str_type_compatible(path, [\"'\", '\"'])\n\n    if path.endswith(single_quote) or path.endswith(double_quote):\n        path = path[1:len(path) - 1]\n    filename = os.path.basename(path).lower()\n    for name in _get_str_type_compatible(filename, ['python', 'jython', 'pypy']):\n        if filename.find(name) != -1:\n            return True\n\n    return False\n\n\nclass InvalidTypeInArgsException(Exception):\n    pass\n\n\ndef remove_quotes_from_args(args):\n    if sys.platform == \"win32\":\n        new_args = []\n\n        for x in args:\n            if Path is not None and isinstance(x, Path):\n                x = str(x)\n            else:\n                if not isinstance(x, (bytes, str)):\n                    raise InvalidTypeInArgsException(str(type(x)))\n\n            double_quote, two_double_quotes = _get_str_type_compatible(x, ['\"', '\"\"'])\n\n            if x != two_double_quotes:\n                if len(x) > 1 and x.startswith(double_quote) and x.endswith(double_quote):\n                    x = x[1:-1]\n\n            new_args.append(x)\n        return new_args\n    else:\n        new_args = []\n        for x in args:\n            if Path is not None and isinstance(x, Path):\n                x = x.as_posix()\n            else:\n                if not isinstance(x, (bytes, str)):\n                    raise InvalidTypeInArgsException(str(type(x)))\n            new_args.append(x)\n\n        return new_args\n\n\ndef quote_arg_win32(arg):\n    fix_type = lambda x: _get_str_type_compatible(arg, x)\n\n    # See if we need to quote at all - empty strings need quoting, as do strings\n    # with whitespace or quotes in them. Backslashes do not need quoting.\n    if arg and not set(arg).intersection(fix_type(' \"\\t\\n\\v')):\n        return arg\n\n    # Per https://docs.microsoft.com/en-us/windows/desktop/api/shellapi/nf-shellapi-commandlinetoargvw,\n    # the standard way to interpret arguments in double quotes is as follows:\n    #\n    #       2N backslashes followed by a quotation mark produce N backslashes followed by\n    #       begin/end quote. This does not become part of the parsed argument, but toggles\n    #       the \"in quotes\" mode.\n    #\n    #       2N+1 backslashes followed by a quotation mark again produce N backslashes followed\n    #       by a quotation mark literal (\"). This does not toggle the \"in quotes\" mode.\n    #\n    #       N backslashes not followed by a quotation mark simply produce N backslashes.\n    #\n    # This code needs to do the reverse transformation, thus:\n    #\n    #       N backslashes followed by \" produce 2N+1 backslashes followed by \"\n    #\n    #       N backslashes at the end (i.e. where the closing \" goes) produce 2N backslashes.\n    #\n    #       N backslashes in any other position remain as is.\n\n    arg = re.sub(fix_type(r'(\\\\*)\\\"'), fix_type(r'\\1\\1\\\\\"'), arg)\n    arg = re.sub(fix_type(r'(\\\\*)$'), fix_type(r'\\1\\1'), arg)\n    return fix_type('\"') + arg + fix_type('\"')\n\n\ndef quote_args(args):\n    if sys.platform == \"win32\":\n        return list(map(quote_arg_win32, args))\n    else:\n        return args\n\n\ndef patch_args(args, is_exec=False):\n    '''\n    :param list args:\n        Arguments to patch.\n\n    :param bool is_exec:\n        If it's an exec, the current process will be replaced (this means we have\n        to keep the same ppid).\n    '''\n    try:\n        pydev_log.debug(\"Patching args: %s\", args)\n        original_args = args\n        try:\n            unquoted_args = remove_quotes_from_args(args)\n        except InvalidTypeInArgsException as e:\n            pydev_log.info('Unable to monkey-patch subprocess arguments because a type found in the args is invalid: %s', e)\n            return original_args\n\n        # Internally we should reference original_args (if we want to return them) or unquoted_args\n        # to add to the list which will be then quoted in the end.\n        del args\n\n        from pydevd import SetupHolder\n        if not unquoted_args:\n            return original_args\n\n        if not is_python(unquoted_args[0]):\n            pydev_log.debug(\"Process is not python, returning.\")\n            return original_args\n\n        # Note: we create a copy as string to help with analyzing the arguments, but\n        # the final list should have items from the unquoted_args as they were initially.\n        args_as_str = _get_str_type_compatible('', unquoted_args)\n\n        params_with_value_in_separate_arg = (\n            '--check-hash-based-pycs',\n            '--jit'  # pypy option\n        )\n\n        # All short switches may be combined together. The ones below require a value and the\n        # value itself may be embedded in the arg.\n        #\n        # i.e.: Python accepts things as:\n        #\n        # python -OQold -qmtest\n        #\n        # Which is the same as:\n        #\n        # python -O -Q old -q -m test\n        #\n        # or even:\n        #\n        # python -OQold \"-vcimport sys;print(sys)\"\n        #\n        # Which is the same as:\n        #\n        # python -O -Q old -v -c \"import sys;print(sys)\"\n\n        params_with_combinable_arg = set(('W', 'X', 'Q', 'c', 'm'))\n\n        module_name = None\n        before_module_flag = ''\n        module_name_i_start = -1\n        module_name_i_end = -1\n\n        code = None\n        code_i = -1\n        code_i_end = -1\n        code_flag = ''\n\n        filename = None\n        filename_i = -1\n\n        ignore_next = True  # start ignoring the first (the first entry is the python executable)\n        for i, arg_as_str in enumerate(args_as_str):\n            if ignore_next:\n                ignore_next = False\n                continue\n\n            if arg_as_str.startswith('-'):\n                if arg_as_str == '-':\n                    # Contents will be read from the stdin. This is not currently handled.\n                    pydev_log.debug('Unable to fix arguments to attach debugger on subprocess when reading from stdin (\"python ... -\").')\n                    return original_args\n\n                if arg_as_str.startswith(params_with_value_in_separate_arg):\n                    if arg_as_str in params_with_value_in_separate_arg:\n                        ignore_next = True\n                    continue\n\n                break_out = False\n                for j, c in enumerate(arg_as_str):\n\n                    # i.e.: Python supports -X faulthandler as well as -Xfaulthandler\n                    # (in one case we have to ignore the next and in the other we don't\n                    # have to ignore it).\n                    if c in params_with_combinable_arg:\n                        remainder = arg_as_str[j + 1:]\n                        if not remainder:\n                            ignore_next = True\n\n                        if c == 'm':\n                            # i.e.: Something as\n                            # python -qm test\n                            # python -m test\n                            # python -qmtest\n                            before_module_flag = arg_as_str[:j]  # before_module_flag would then be \"-q\"\n                            if before_module_flag == '-':\n                                before_module_flag = ''\n                            module_name_i_start = i\n                            if not remainder:\n                                module_name = unquoted_args[i + 1]\n                                module_name_i_end = i + 1\n                            else:\n                                # i.e.: python -qmtest should provide 'test' as the module_name\n                                module_name = unquoted_args[i][j + 1:]\n                                module_name_i_end = module_name_i_start\n                            break_out = True\n                            break\n\n                        elif c == 'c':\n                            # i.e.: Something as\n                            # python -qc \"import sys\"\n                            # python -c \"import sys\"\n                            # python \"-qcimport sys\"\n                            code_flag = arg_as_str[:j + 1]  # code_flag would then be \"-qc\"\n\n                            if not remainder:\n                                # arg_as_str is something as \"-qc\", \"import sys\"\n                                code = unquoted_args[i + 1]\n                                code_i_end = i + 2\n                            else:\n                                # if arg_as_str is something as \"-qcimport sys\"\n                                code = remainder  # code would be \"import sys\"\n                                code_i_end = i + 1\n                            code_i = i\n                            break_out = True\n                            break\n\n                        else:\n                            break\n\n                if break_out:\n                    break\n\n            else:\n                # It doesn't start with '-' and we didn't ignore this entry:\n                # this means that this is the file to be executed.\n                filename = unquoted_args[i]\n\n                # Note that the filename is not validated here.\n                # There are cases where even a .exe is valid (xonsh.exe):\n                # https://github.com/microsoft/debugpy/issues/945\n                # So, we should support whatever runpy.run_path\n                # supports in this case.\n\n                filename_i = i\n\n                if _is_managed_arg(filename):  # no need to add pydevd twice\n                    pydev_log.debug('Skipped monkey-patching as pydevd.py is in args already.')\n                    return original_args\n\n                break\n        else:\n            # We didn't find the filename (something is unexpected).\n            pydev_log.debug('Unable to fix arguments to attach debugger on subprocess (filename not found).')\n            return original_args\n\n        if code_i != -1:\n            host, port = _get_host_port()\n\n            if port is not None:\n                new_args = []\n                new_args.extend(unquoted_args[:code_i])\n                new_args.append(code_flag)\n                new_args.append(_get_python_c_args(host, port, code, unquoted_args, SetupHolder.setup))\n                new_args.extend(unquoted_args[code_i_end:])\n\n                return quote_args(new_args)\n\n        first_non_vm_index = max(filename_i, module_name_i_start)\n        if first_non_vm_index == -1:\n            pydev_log.debug('Unable to fix arguments to attach debugger on subprocess (could not resolve filename nor module name).')\n            return original_args\n\n        # Original args should be something as:\n        # ['X:\\\\pysrc\\\\pydevd.py', '--multiprocess', '--print-in-debugger-startup',\n        #  '--vm_type', 'python', '--client', '127.0.0.1', '--port', '56352', '--file', 'x:\\\\snippet1.py']\n        from _pydevd_bundle.pydevd_command_line_handling import setup_to_argv\n        new_args = []\n        new_args.extend(unquoted_args[:first_non_vm_index])\n        if before_module_flag:\n            new_args.append(before_module_flag)\n\n        add_module_at = len(new_args) + 1\n\n        new_args.extend(setup_to_argv(\n            _get_setup_updated_with_protocol_and_ppid(SetupHolder.setup, is_exec=is_exec),\n            skip_names=set(('module', 'cmd-line'))\n        ))\n        new_args.append('--file')\n\n        if module_name is not None:\n            assert module_name_i_start != -1\n            assert module_name_i_end != -1\n            # Always after 'pydevd' (i.e.: pydevd \"--module\" --multiprocess ...)\n            new_args.insert(add_module_at, '--module')\n            new_args.append(module_name)\n            new_args.extend(unquoted_args[module_name_i_end + 1:])\n\n        elif filename is not None:\n            assert filename_i != -1\n            new_args.append(filename)\n            new_args.extend(unquoted_args[filename_i + 1:])\n\n        else:\n            raise AssertionError('Internal error (unexpected condition)')\n\n        return quote_args(new_args)\n    except:\n        pydev_log.exception('Error patching args (debugger not attached to subprocess).')\n        return original_args\n\n\ndef str_to_args_windows(args):\n    # See https://docs.microsoft.com/en-us/cpp/c-language/parsing-c-command-line-arguments.\n    #\n    # Implemetation ported from DebugPlugin.parseArgumentsWindows:\n    # https://github.com/eclipse/eclipse.platform.debug/blob/master/org.eclipse.debug.core/core/org/eclipse/debug/core/DebugPlugin.java\n\n    result = []\n\n    DEFAULT = 0\n    ARG = 1\n    IN_DOUBLE_QUOTE = 2\n\n    state = DEFAULT\n    backslashes = 0\n    buf = ''\n\n    args_len = len(args)\n    for i in range(args_len):\n        ch = args[i]\n        if (ch == '\\\\'):\n            backslashes += 1\n            continue\n        elif (backslashes != 0):\n            if ch == '\"':\n                while backslashes >= 2:\n                    backslashes -= 2\n                    buf += '\\\\'\n                if (backslashes == 1):\n                    if (state == DEFAULT):\n                        state = ARG\n\n                    buf += '\"'\n                    backslashes = 0\n                    continue\n                # else fall through to switch\n            else:\n                # false alarm, treat passed backslashes literally...\n                if (state == DEFAULT):\n                    state = ARG\n\n                while backslashes > 0:\n                    backslashes -= 1\n                    buf += '\\\\'\n                # fall through to switch\n        if ch in (' ', '\\t'):\n            if (state == DEFAULT):\n                # skip\n                continue\n            elif (state == ARG):\n                state = DEFAULT\n                result.append(buf)\n                buf = ''\n                continue\n\n        if state in (DEFAULT, ARG):\n            if ch == '\"':\n                state = IN_DOUBLE_QUOTE\n            else:\n                state = ARG\n                buf += ch\n\n        elif state == IN_DOUBLE_QUOTE:\n            if ch == '\"':\n                if (i + 1 < args_len and args[i + 1] == '\"'):\n                    # Undocumented feature in Windows:\n                    # Two consecutive double quotes inside a double-quoted argument are interpreted as\n                    # a single double quote.\n                    buf += '\"'\n                    i += 1\n                else:\n                    state = ARG\n            else:\n                buf += ch\n\n        else:\n            raise RuntimeError('Illegal condition')\n\n    if len(buf) > 0 or state != DEFAULT:\n        result.append(buf)\n\n    return result\n\n\ndef patch_arg_str_win(arg_str):\n    args = str_to_args_windows(arg_str)\n    # Fix https://youtrack.jetbrains.com/issue/PY-9767 (args may be empty)\n    if not args or not is_python(args[0]):\n        return arg_str\n    arg_str = ' '.join(patch_args(args))\n    pydev_log.debug(\"New args: %s\", arg_str)\n    return arg_str\n\n\ndef monkey_patch_module(module, funcname, create_func):\n    if hasattr(module, funcname):\n        original_name = 'original_' + funcname\n        if not hasattr(module, original_name):\n            setattr(module, original_name, getattr(module, funcname))\n            setattr(module, funcname, create_func(original_name))\n\n\ndef monkey_patch_os(funcname, create_func):\n    monkey_patch_module(os, funcname, create_func)\n\n\ndef warn_multiproc():\n    pass  # TODO: Provide logging as messages to the IDE.\n    # pydev_log.error_once(\n    #     \"pydev debugger: New process is launching (breakpoints won't work in the new process).\\n\"\n    #     \"pydev debugger: To debug that process please enable 'Attach to subprocess automatically while debugging?' option in the debugger settings.\\n\")\n    #\n\n\ndef create_warn_multiproc(original_name):\n\n    def new_warn_multiproc(*args, **kwargs):\n        import os\n\n        warn_multiproc()\n\n        return getattr(os, original_name)(*args, **kwargs)\n\n    return new_warn_multiproc\n\n\ndef create_execl(original_name):\n\n    def new_execl(path, *args):\n        \"\"\"\n        os.execl(path, arg0, arg1, ...)\n        os.execle(path, arg0, arg1, ..., env)\n        os.execlp(file, arg0, arg1, ...)\n        os.execlpe(file, arg0, arg1, ..., env)\n        \"\"\"\n        if _get_apply_arg_patching():\n            args = patch_args(args, is_exec=True)\n            send_process_created_message()\n            send_process_about_to_be_replaced()\n\n        return getattr(os, original_name)(path, *args)\n\n    return new_execl\n\n\ndef create_execv(original_name):\n\n    def new_execv(path, args):\n        \"\"\"\n        os.execv(path, args)\n        os.execvp(file, args)\n        \"\"\"\n        if _get_apply_arg_patching():\n            args = patch_args(args, is_exec=True)\n            send_process_created_message()\n            send_process_about_to_be_replaced()\n\n        return getattr(os, original_name)(path, args)\n\n    return new_execv\n\n\ndef create_execve(original_name):\n    \"\"\"\n    os.execve(path, args, env)\n    os.execvpe(file, args, env)\n    \"\"\"\n\n    def new_execve(path, args, env):\n        if _get_apply_arg_patching():\n            args = patch_args(args, is_exec=True)\n            send_process_created_message()\n            send_process_about_to_be_replaced()\n\n        return getattr(os, original_name)(path, args, env)\n\n    return new_execve\n\n\ndef create_spawnl(original_name):\n\n    def new_spawnl(mode, path, *args):\n        \"\"\"\n        os.spawnl(mode, path, arg0, arg1, ...)\n        os.spawnlp(mode, file, arg0, arg1, ...)\n        \"\"\"\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(os, original_name)(mode, path, *args)\n\n    return new_spawnl\n\n\ndef create_spawnv(original_name):\n\n    def new_spawnv(mode, path, args):\n        \"\"\"\n        os.spawnv(mode, path, args)\n        os.spawnvp(mode, file, args)\n        \"\"\"\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(os, original_name)(mode, path, args)\n\n    return new_spawnv\n\n\ndef create_spawnve(original_name):\n    \"\"\"\n    os.spawnve(mode, path, args, env)\n    os.spawnvpe(mode, file, args, env)\n    \"\"\"\n\n    def new_spawnve(mode, path, args, env):\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(os, original_name)(mode, path, args, env)\n\n    return new_spawnve\n\n\ndef create_posix_spawn(original_name):\n    \"\"\"\n    os.posix_spawn(executable, args, env, **kwargs)\n    \"\"\"\n\n    def new_posix_spawn(executable, args, env, **kwargs):\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(os, original_name)(executable, args, env, **kwargs)\n\n    return new_posix_spawn\n\n\ndef create_fork_exec(original_name):\n    \"\"\"\n    _posixsubprocess.fork_exec(args, executable_list, close_fds, ... (13 more))\n    \"\"\"\n\n    def new_fork_exec(args, *other_args):\n        import _posixsubprocess  # @UnresolvedImport\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(_posixsubprocess, original_name)(args, *other_args)\n\n    return new_fork_exec\n\n\ndef create_warn_fork_exec(original_name):\n    \"\"\"\n    _posixsubprocess.fork_exec(args, executable_list, close_fds, ... (13 more))\n    \"\"\"\n\n    def new_warn_fork_exec(*args):\n        try:\n            import _posixsubprocess\n            warn_multiproc()\n            return getattr(_posixsubprocess, original_name)(*args)\n        except:\n            pass\n\n    return new_warn_fork_exec\n\n\ndef create_subprocess_fork_exec(original_name):\n    \"\"\"\n    subprocess._fork_exec(args, executable_list, close_fds, ... (13 more))\n    \"\"\"\n\n    def new_fork_exec(args, *other_args):\n        import subprocess\n        if _get_apply_arg_patching():\n            args = patch_args(args)\n            send_process_created_message()\n\n        return getattr(subprocess, original_name)(args, *other_args)\n\n    return new_fork_exec\n\n\ndef create_subprocess_warn_fork_exec(original_name):\n    \"\"\"\n    subprocess._fork_exec(args, executable_list, close_fds, ... (13 more))\n    \"\"\"\n\n    def new_warn_fork_exec(*args):\n        try:\n            import subprocess\n            warn_multiproc()\n            return getattr(subprocess, original_name)(*args)\n        except:\n            pass\n\n    return new_warn_fork_exec\n\n\ndef create_CreateProcess(original_name):\n    \"\"\"\n    CreateProcess(*args, **kwargs)\n    \"\"\"\n\n    def new_CreateProcess(app_name, cmd_line, *args):\n        try:\n            import _subprocess\n        except ImportError:\n            import _winapi as _subprocess\n\n        if _get_apply_arg_patching():\n            cmd_line = patch_arg_str_win(cmd_line)\n            send_process_created_message()\n\n        return getattr(_subprocess, original_name)(app_name, cmd_line, *args)\n\n    return new_CreateProcess\n\n\ndef create_CreateProcessWarnMultiproc(original_name):\n    \"\"\"\n    CreateProcess(*args, **kwargs)\n    \"\"\"\n\n    def new_CreateProcess(*args):\n        try:\n            import _subprocess\n        except ImportError:\n            import _winapi as _subprocess\n        warn_multiproc()\n        return getattr(_subprocess, original_name)(*args)\n\n    return new_CreateProcess\n\n\ndef create_fork(original_name):\n\n    def new_fork():\n        # A simple fork will result in a new python process\n        is_new_python_process = True\n        frame = sys._getframe()\n\n        apply_arg_patch = _get_apply_arg_patching()\n\n        is_subprocess_fork = False\n        while frame is not None:\n            if frame.f_code.co_name == '_execute_child' and 'subprocess' in frame.f_code.co_filename:\n                is_subprocess_fork = True\n                # If we're actually in subprocess.Popen creating a child, it may\n                # result in something which is not a Python process, (so, we\n                # don't want to connect with it in the forked version).\n                executable = frame.f_locals.get('executable')\n                if executable is not None:\n                    is_new_python_process = False\n                    if is_python(executable):\n                        is_new_python_process = True\n                break\n\n            frame = frame.f_back\n        frame = None  # Just make sure we don't hold on to it.\n\n        protocol = pydevd_constants.get_protocol()\n        debug_mode = PydevdCustomization.DEBUG_MODE\n\n        child_process = getattr(os, original_name)()  # fork\n        if not child_process:\n            if is_new_python_process:\n                PydevdCustomization.DEFAULT_PROTOCOL = protocol\n                PydevdCustomization.DEBUG_MODE = debug_mode\n                _on_forked_process(setup_tracing=apply_arg_patch and not is_subprocess_fork)\n            else:\n                set_global_debugger(None)\n        else:\n            if is_new_python_process:\n                send_process_created_message()\n        return child_process\n\n    return new_fork\n\n\ndef send_process_created_message():\n    py_db = get_global_debugger()\n    if py_db is not None:\n        py_db.send_process_created_message()\n\n\ndef send_process_about_to_be_replaced():\n    py_db = get_global_debugger()\n    if py_db is not None:\n        py_db.send_process_about_to_be_replaced()\n\n\ndef patch_new_process_functions():\n    # os.execl(path, arg0, arg1, ...)\n    # os.execle(path, arg0, arg1, ..., env)\n    # os.execlp(file, arg0, arg1, ...)\n    # os.execlpe(file, arg0, arg1, ..., env)\n    # os.execv(path, args)\n    # os.execve(path, args, env)\n    # os.execvp(file, args)\n    # os.execvpe(file, args, env)\n    monkey_patch_os('execl', create_execl)\n    monkey_patch_os('execle', create_execl)\n    monkey_patch_os('execlp', create_execl)\n    monkey_patch_os('execlpe', create_execl)\n    monkey_patch_os('execv', create_execv)\n    monkey_patch_os('execve', create_execve)\n    monkey_patch_os('execvp', create_execv)\n    monkey_patch_os('execvpe', create_execve)\n\n    # os.spawnl(mode, path, ...)\n    # os.spawnle(mode, path, ..., env)\n    # os.spawnlp(mode, file, ...)\n    # os.spawnlpe(mode, file, ..., env)\n    # os.spawnv(mode, path, args)\n    # os.spawnve(mode, path, args, env)\n    # os.spawnvp(mode, file, args)\n    # os.spawnvpe(mode, file, args, env)\n\n    monkey_patch_os('spawnl', create_spawnl)\n    monkey_patch_os('spawnle', create_spawnl)\n    monkey_patch_os('spawnlp', create_spawnl)\n    monkey_patch_os('spawnlpe', create_spawnl)\n    monkey_patch_os('spawnv', create_spawnv)\n    monkey_patch_os('spawnve', create_spawnve)\n    monkey_patch_os('spawnvp', create_spawnv)\n    monkey_patch_os('spawnvpe', create_spawnve)\n    monkey_patch_os('posix_spawn', create_posix_spawn)\n\n    if not IS_JYTHON:\n        if not IS_WINDOWS:\n            monkey_patch_os('fork', create_fork)\n            try:\n                import _posixsubprocess\n                monkey_patch_module(_posixsubprocess, 'fork_exec', create_fork_exec)\n            except ImportError:\n                pass\n\n            try:\n                import subprocess\n                monkey_patch_module(subprocess, '_fork_exec', create_subprocess_fork_exec)\n            except AttributeError:\n                pass\n        else:\n            # Windows\n            try:\n                import _subprocess\n            except ImportError:\n                import _winapi as _subprocess\n            monkey_patch_module(_subprocess, 'CreateProcess', create_CreateProcess)\n\n\ndef patch_new_process_functions_with_warning():\n    monkey_patch_os('execl', create_warn_multiproc)\n    monkey_patch_os('execle', create_warn_multiproc)\n    monkey_patch_os('execlp', create_warn_multiproc)\n    monkey_patch_os('execlpe', create_warn_multiproc)\n    monkey_patch_os('execv', create_warn_multiproc)\n    monkey_patch_os('execve', create_warn_multiproc)\n    monkey_patch_os('execvp', create_warn_multiproc)\n    monkey_patch_os('execvpe', create_warn_multiproc)\n    monkey_patch_os('spawnl', create_warn_multiproc)\n    monkey_patch_os('spawnle', create_warn_multiproc)\n    monkey_patch_os('spawnlp', create_warn_multiproc)\n    monkey_patch_os('spawnlpe', create_warn_multiproc)\n    monkey_patch_os('spawnv', create_warn_multiproc)\n    monkey_patch_os('spawnve', create_warn_multiproc)\n    monkey_patch_os('spawnvp', create_warn_multiproc)\n    monkey_patch_os('spawnvpe', create_warn_multiproc)\n    monkey_patch_os('posix_spawn', create_warn_multiproc)\n\n    if not IS_JYTHON:\n        if not IS_WINDOWS:\n            monkey_patch_os('fork', create_warn_multiproc)\n            try:\n                import _posixsubprocess\n                monkey_patch_module(_posixsubprocess, 'fork_exec', create_warn_fork_exec)\n            except ImportError:\n                pass\n\n            try:\n                import subprocess\n                monkey_patch_module(subprocess, '_fork_exec', create_subprocess_warn_fork_exec)\n            except AttributeError:\n                pass\n\n        else:\n            # Windows\n            try:\n                import _subprocess\n            except ImportError:\n                import _winapi as _subprocess\n            monkey_patch_module(_subprocess, 'CreateProcess', create_CreateProcessWarnMultiproc)\n\n\nclass _NewThreadStartupWithTrace:\n\n    def __init__(self, original_func, args, kwargs):\n        self.original_func = original_func\n        self.args = args\n        self.kwargs = kwargs\n\n    def __call__(self):\n        # We monkey-patch the thread creation so that this function is called in the new thread. At this point\n        # we notify of its creation and start tracing it.\n        py_db = get_global_debugger()\n\n        thread_id = None\n        if py_db is not None:\n            # Note: if this is a thread from threading.py, we're too early in the boostrap process (because we mocked\n            # the start_new_thread internal machinery and thread._bootstrap has not finished), so, the code below needs\n            # to make sure that we use the current thread bound to the original function and not use\n            # threading.current_thread() unless we're sure it's a dummy thread.\n            t = getattr(self.original_func, '__self__', getattr(self.original_func, 'im_self', None))\n            if not isinstance(t, threading.Thread):\n                # This is not a threading.Thread but a Dummy thread (so, get it as a dummy thread using\n                # currentThread).\n                t = threading.current_thread()\n\n            if not getattr(t, 'is_pydev_daemon_thread', False):\n                thread_id = get_current_thread_id(t)\n                py_db.notify_thread_created(thread_id, t)\n                _on_set_trace_for_new_thread(py_db)\n\n            if getattr(py_db, 'thread_analyser', None) is not None:\n                try:\n                    from _pydevd_bundle.pydevd_concurrency_analyser.pydevd_concurrency_logger import log_new_thread\n                    log_new_thread(py_db, t)\n                except:\n                    sys.stderr.write(\"Failed to detect new thread for visualization\")\n        try:\n            ret = self.original_func(*self.args, **self.kwargs)\n        finally:\n            if thread_id is not None:\n                if py_db is not None:\n                    # At thread shutdown we only have pydevd-related code running (which shouldn't\n                    # be tracked).\n                    py_db.disable_tracing()\n                    py_db.notify_thread_not_alive(thread_id)\n\n        return ret\n\n\nclass _NewThreadStartupWithoutTrace:\n\n    def __init__(self, original_func, args, kwargs):\n        self.original_func = original_func\n        self.args = args\n        self.kwargs = kwargs\n\n    def __call__(self):\n        return self.original_func(*self.args, **self.kwargs)\n\n\n_UseNewThreadStartup = _NewThreadStartupWithTrace\n\n\ndef _get_threading_modules_to_patch():\n    threading_modules_to_patch = []\n\n    try:\n        import thread as _thread\n    except:\n        import _thread\n    threading_modules_to_patch.append(_thread)\n    threading_modules_to_patch.append(threading)\n\n    return threading_modules_to_patch\n\n\nthreading_modules_to_patch = _get_threading_modules_to_patch()\n\n\ndef patch_thread_module(thread_module):\n\n    if getattr(thread_module, '_original_start_new_thread', None) is None:\n        if thread_module is threading:\n            if not hasattr(thread_module, '_start_new_thread'):\n                return  # Jython doesn't have it.\n            _original_start_new_thread = thread_module._original_start_new_thread = thread_module._start_new_thread\n        else:\n            _original_start_new_thread = thread_module._original_start_new_thread = thread_module.start_new_thread\n    else:\n        _original_start_new_thread = thread_module._original_start_new_thread\n\n    class ClassWithPydevStartNewThread:\n\n        def pydev_start_new_thread(self, function, args=(), kwargs={}):\n            '''\n            We need to replace the original thread_module.start_new_thread with this function so that threads started\n            through it and not through the threading module are properly traced.\n            '''\n            return _original_start_new_thread(_UseNewThreadStartup(function, args, kwargs), ())\n\n    # This is a hack for the situation where the thread_module.start_new_thread is declared inside a class, such as the one below\n    # class F(object):\n    #    start_new_thread = thread_module.start_new_thread\n    #\n    #    def start_it(self):\n    #        self.start_new_thread(self.function, args, kwargs)\n    # So, if it's an already bound method, calling self.start_new_thread won't really receive a different 'self' -- it\n    # does work in the default case because in builtins self isn't passed either.\n    pydev_start_new_thread = ClassWithPydevStartNewThread().pydev_start_new_thread\n\n    try:\n        # We need to replace the original thread_module.start_new_thread with this function so that threads started through\n        # it and not through the threading module are properly traced.\n        if thread_module is threading:\n            thread_module._start_new_thread = pydev_start_new_thread\n        else:\n            thread_module.start_new_thread = pydev_start_new_thread\n            thread_module.start_new = pydev_start_new_thread\n    except:\n        pass\n\n\ndef patch_thread_modules():\n    for t in threading_modules_to_patch:\n        patch_thread_module(t)\n\n\ndef undo_patch_thread_modules():\n    for t in threading_modules_to_patch:\n        try:\n            t.start_new_thread = t._original_start_new_thread\n        except:\n            pass\n\n        try:\n            t.start_new = t._original_start_new_thread\n        except:\n            pass\n\n        try:\n            t._start_new_thread = t._original_start_new_thread\n        except:\n            pass\n\n\ndef disable_trace_thread_modules():\n    '''\n    Can be used to temporarily stop tracing threads created with thread.start_new_thread.\n    '''\n    global _UseNewThreadStartup\n    _UseNewThreadStartup = _NewThreadStartupWithoutTrace\n\n\ndef enable_trace_thread_modules():\n    '''\n    Can be used to start tracing threads created with thread.start_new_thread again.\n    '''\n    global _UseNewThreadStartup\n    _UseNewThreadStartup = _NewThreadStartupWithTrace\n\n\ndef get_original_start_new_thread(threading_module):\n    try:\n        return threading_module._original_start_new_thread\n    except:\n        return threading_module.start_new_thread\n", 1246], "/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py": ["from fastapi import APIRouter, FastAPI\nfrom fastapi.testclient import TestClient\nfrom profyle import ProfyleFastApiMiddleware\n\napp = FastAPI()\napp.add_middleware(ProfyleFastApiMiddleware, pattern='*test[?]*')\n\nrouter = APIRouter()\n\n\n@router.post('/test')\ndef run_middleware():\n    return {'message': 'OK'}\n\n\n@router.post('/test1')\ndef run_middleware_1():\n    return {'message': 'OK'}\n\n\napp.include_router(router)\n\nclient = TestClient(app)\n\n\ndef test_fastapi_middleware():\n    client.post('test?demo=true')\n    client.post('test1')\n", 28], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/pydantic/_internal/_model_construction.py": ["\"\"\"Private logic for creating models.\"\"\"\nfrom __future__ import annotations as _annotations\n\nimport typing\nimport warnings\nimport weakref\nfrom abc import ABCMeta\nfrom functools import partial\nfrom types import FunctionType\nfrom typing import Any, Callable, Generic, Mapping\n\nfrom pydantic_core import PydanticUndefined, SchemaSerializer\nfrom typing_extensions import dataclass_transform, deprecated\n\nfrom ..errors import PydanticUndefinedAnnotation, PydanticUserError\nfrom ..fields import Field, FieldInfo, ModelPrivateAttr, PrivateAttr\nfrom ..plugin._schema_validator import create_schema_validator\nfrom ..warnings import PydanticDeprecatedSince20\nfrom ._config import ConfigWrapper\nfrom ._core_utils import collect_invalid_schemas, simplify_schema_references, validate_core_schema\nfrom ._decorators import (\n    ComputedFieldInfo,\n    DecoratorInfos,\n    PydanticDescriptorProxy,\n    get_attribute_from_bases,\n)\nfrom ._discriminated_union import apply_discriminators\nfrom ._fields import collect_model_fields, is_valid_field_name, is_valid_privateattr_name\nfrom ._generate_schema import GenerateSchema\nfrom ._generics import PydanticGenericMetadata, get_model_typevars_map\nfrom ._mock_val_ser import MockValSer, set_model_mocks\nfrom ._schema_generation_shared import CallbackGetCoreSchemaHandler\nfrom ._typing_extra import get_cls_types_namespace, is_classvar, parent_frame_namespace\nfrom ._utils import ClassAttribute, is_valid_identifier\nfrom ._validate_call import ValidateCallWrapper\n\nif typing.TYPE_CHECKING:\n    from inspect import Signature\n\n    from ..main import BaseModel\nelse:\n    # See PyCharm issues https://youtrack.jetbrains.com/issue/PY-21915\n    # and https://youtrack.jetbrains.com/issue/PY-51428\n    DeprecationWarning = PydanticDeprecatedSince20\n\n\nIGNORED_TYPES: tuple[Any, ...] = (\n    FunctionType,\n    property,\n    classmethod,\n    staticmethod,\n    PydanticDescriptorProxy,\n    ComputedFieldInfo,\n    ValidateCallWrapper,\n)\nobject_setattr = object.__setattr__\n\n\nclass _ModelNamespaceDict(dict):\n    \"\"\"A dictionary subclass that intercepts attribute setting on model classes and\n    warns about overriding of decorators.\n    \"\"\"\n\n    def __setitem__(self, k: str, v: object) -> None:\n        existing: Any = self.get(k, None)\n        if existing and v is not existing and isinstance(existing, PydanticDescriptorProxy):\n            warnings.warn(f'`{k}` overrides an existing Pydantic `{existing.decorator_info.decorator_repr}` decorator')\n\n        return super().__setitem__(k, v)\n\n\n@dataclass_transform(kw_only_default=True, field_specifiers=(Field,))\nclass ModelMetaclass(ABCMeta):\n    def __new__(\n        mcs,\n        cls_name: str,\n        bases: tuple[type[Any], ...],\n        namespace: dict[str, Any],\n        __pydantic_generic_metadata__: PydanticGenericMetadata | None = None,\n        __pydantic_reset_parent_namespace__: bool = True,\n        **kwargs: Any,\n    ) -> type:\n        \"\"\"Metaclass for creating Pydantic models.\n\n        Args:\n            cls_name: The name of the class to be created.\n            bases: The base classes of the class to be created.\n            namespace: The attribute dictionary of the class to be created.\n            __pydantic_generic_metadata__: Metadata for generic models.\n            __pydantic_reset_parent_namespace__: Reset parent namespace.\n            **kwargs: Catch-all for any other keyword arguments.\n\n        Returns:\n            The new class created by the metaclass.\n        \"\"\"\n        # Note `ModelMetaclass` refers to `BaseModel`, but is also used to *create* `BaseModel`, so we rely on the fact\n        # that `BaseModel` itself won't have any bases, but any subclass of it will, to determine whether the `__new__`\n        # call we're in the middle of is for the `BaseModel` class.\n        if bases:\n            base_field_names, class_vars, base_private_attributes = mcs._collect_bases_data(bases)\n\n            config_wrapper = ConfigWrapper.for_model(bases, namespace, kwargs)\n            namespace['model_config'] = config_wrapper.config_dict\n            private_attributes = inspect_namespace(\n                namespace, config_wrapper.ignored_types, class_vars, base_field_names\n            )\n            if private_attributes:\n                original_model_post_init = get_model_post_init(namespace, bases)\n                if original_model_post_init is not None:\n                    # if there are private_attributes and a model_post_init function, we handle both\n\n                    def wrapped_model_post_init(self: BaseModel, __context: Any) -> None:\n                        \"\"\"We need to both initialize private attributes and call the user-defined model_post_init\n                        method.\n                        \"\"\"\n                        init_private_attributes(self, __context)\n                        original_model_post_init(self, __context)\n\n                    namespace['model_post_init'] = wrapped_model_post_init\n                else:\n                    namespace['model_post_init'] = init_private_attributes\n\n            namespace['__class_vars__'] = class_vars\n            namespace['__private_attributes__'] = {**base_private_attributes, **private_attributes}\n\n            if config_wrapper.frozen:\n                set_default_hash_func(namespace, bases)\n\n            cls: type[BaseModel] = super().__new__(mcs, cls_name, bases, namespace, **kwargs)  # type: ignore\n\n            from ..main import BaseModel\n\n            cls.__pydantic_custom_init__ = not getattr(cls.__init__, '__pydantic_base_init__', False)\n            cls.__pydantic_post_init__ = None if cls.model_post_init is BaseModel.model_post_init else 'model_post_init'\n\n            cls.__pydantic_decorators__ = DecoratorInfos.build(cls)\n\n            # Use the getattr below to grab the __parameters__ from the `typing.Generic` parent class\n            if __pydantic_generic_metadata__:\n                cls.__pydantic_generic_metadata__ = __pydantic_generic_metadata__\n            else:\n                parent_parameters = getattr(cls, '__pydantic_generic_metadata__', {}).get('parameters', ())\n                parameters = getattr(cls, '__parameters__', None) or parent_parameters\n                if parameters and parent_parameters and not all(x in parameters for x in parent_parameters):\n                    combined_parameters = parent_parameters + tuple(x for x in parameters if x not in parent_parameters)\n                    parameters_str = ', '.join([str(x) for x in combined_parameters])\n                    generic_type_label = f'typing.Generic[{parameters_str}]'\n                    error_message = (\n                        f'All parameters must be present on typing.Generic;'\n                        f' you should inherit from {generic_type_label}.'\n                    )\n                    if Generic not in bases:  # pragma: no cover\n                        # We raise an error here not because it is desirable, but because some cases are mishandled.\n                        # It would be nice to remove this error and still have things behave as expected, it's just\n                        # challenging because we are using a custom `__class_getitem__` to parametrize generic models,\n                        # and not returning a typing._GenericAlias from it.\n                        bases_str = ', '.join([x.__name__ for x in bases] + [generic_type_label])\n                        error_message += (\n                            f' Note: `typing.Generic` must go last: `class {cls.__name__}({bases_str}): ...`)'\n                        )\n                    raise TypeError(error_message)\n\n                cls.__pydantic_generic_metadata__ = {\n                    'origin': None,\n                    'args': (),\n                    'parameters': parameters,\n                }\n\n            cls.__pydantic_complete__ = False  # Ensure this specific class gets completed\n\n            # preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\n            # for attributes not in `new_namespace` (e.g. private attributes)\n            for name, obj in private_attributes.items():\n                obj.__set_name__(cls, name)\n\n            if __pydantic_reset_parent_namespace__:\n                cls.__pydantic_parent_namespace__ = build_lenient_weakvaluedict(parent_frame_namespace())\n            parent_namespace = getattr(cls, '__pydantic_parent_namespace__', None)\n            if isinstance(parent_namespace, dict):\n                parent_namespace = unpack_lenient_weakvaluedict(parent_namespace)\n\n            types_namespace = get_cls_types_namespace(cls, parent_namespace)\n            set_model_fields(cls, bases, config_wrapper, types_namespace)\n            complete_model_class(\n                cls,\n                cls_name,\n                config_wrapper,\n                raise_errors=False,\n                types_namespace=types_namespace,\n            )\n            # using super(cls, cls) on the next line ensures we only call the parent class's __pydantic_init_subclass__\n            # I believe the `type: ignore` is only necessary because mypy doesn't realize that this code branch is\n            # only hit for _proper_ subclasses of BaseModel\n            super(cls, cls).__pydantic_init_subclass__(**kwargs)  # type: ignore[misc]\n            return cls\n        else:\n            # this is the BaseModel class itself being created, no logic required\n            return super().__new__(mcs, cls_name, bases, namespace, **kwargs)\n\n    if not typing.TYPE_CHECKING:\n        # We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access\n\n        def __getattr__(self, item: str) -> Any:\n            \"\"\"This is necessary to keep attribute access working for class attribute access.\"\"\"\n            private_attributes = self.__dict__.get('__private_attributes__')\n            if private_attributes and item in private_attributes:\n                return private_attributes[item]\n            if item == '__pydantic_core_schema__':\n                # This means the class didn't get a schema generated for it, likely because there was an undefined reference\n                maybe_mock_validator = getattr(self, '__pydantic_validator__', None)\n                if isinstance(maybe_mock_validator, MockValSer):\n                    rebuilt_validator = maybe_mock_validator.rebuild()\n                    if rebuilt_validator is not None:\n                        # In this case, a validator was built, and so `__pydantic_core_schema__` should now be set\n                        return getattr(self, '__pydantic_core_schema__')\n            raise AttributeError(item)\n\n    @classmethod\n    def __prepare__(cls, *args: Any, **kwargs: Any) -> Mapping[str, object]:\n        return _ModelNamespaceDict()\n\n    def __instancecheck__(self, instance: Any) -> bool:\n        \"\"\"Avoid calling ABC _abc_subclasscheck unless we're pretty sure.\n\n        See #3829 and python/cpython#92810\n        \"\"\"\n        return hasattr(instance, '__pydantic_validator__') and super().__instancecheck__(instance)\n\n    @staticmethod\n    def _collect_bases_data(bases: tuple[type[Any], ...]) -> tuple[set[str], set[str], dict[str, ModelPrivateAttr]]:\n        from ..main import BaseModel\n\n        field_names: set[str] = set()\n        class_vars: set[str] = set()\n        private_attributes: dict[str, ModelPrivateAttr] = {}\n        for base in bases:\n            if issubclass(base, BaseModel) and base is not BaseModel:\n                # model_fields might not be defined yet in the case of generics, so we use getattr here:\n                field_names.update(getattr(base, 'model_fields', {}).keys())\n                class_vars.update(base.__class_vars__)\n                private_attributes.update(base.__private_attributes__)\n        return field_names, class_vars, private_attributes\n\n    @property\n    @deprecated(\n        'The `__fields__` attribute is deprecated, use `model_fields` instead.', category=PydanticDeprecatedSince20\n    )\n    def __fields__(self) -> dict[str, FieldInfo]:\n        warnings.warn('The `__fields__` attribute is deprecated, use `model_fields` instead.', DeprecationWarning)\n        return self.model_fields  # type: ignore\n\n\ndef init_private_attributes(self: BaseModel, __context: Any) -> None:\n    \"\"\"This function is meant to behave like a BaseModel method to initialise private attributes.\n\n    It takes context as an argument since that's what pydantic-core passes when calling it.\n\n    Args:\n        self: The BaseModel instance.\n        __context: The context.\n    \"\"\"\n    pydantic_private = {}\n    for name, private_attr in self.__private_attributes__.items():\n        default = private_attr.get_default()\n        if default is not PydanticUndefined:\n            pydantic_private[name] = default\n    object_setattr(self, '__pydantic_private__', pydantic_private)\n\n\ndef get_model_post_init(namespace: dict[str, Any], bases: tuple[type[Any], ...]) -> Callable[..., Any] | None:\n    \"\"\"Get the `model_post_init` method from the namespace or the class bases, or `None` if not defined.\"\"\"\n    if 'model_post_init' in namespace:\n        return namespace['model_post_init']\n\n    from ..main import BaseModel\n\n    model_post_init = get_attribute_from_bases(bases, 'model_post_init')\n    if model_post_init is not BaseModel.model_post_init:\n        return model_post_init\n\n\ndef inspect_namespace(  # noqa C901\n    namespace: dict[str, Any],\n    ignored_types: tuple[type[Any], ...],\n    base_class_vars: set[str],\n    base_class_fields: set[str],\n) -> dict[str, ModelPrivateAttr]:\n    \"\"\"Iterate over the namespace and:\n    * gather private attributes\n    * check for items which look like fields but are not (e.g. have no annotation) and warn.\n\n    Args:\n        namespace: The attribute dictionary of the class to be created.\n        ignored_types: A tuple of ignore types.\n        base_class_vars: A set of base class class variables.\n        base_class_fields: A set of base class fields.\n\n    Returns:\n        A dict contains private attributes info.\n\n    Raises:\n        TypeError: If there is a `__root__` field in model.\n        NameError: If private attribute name is invalid.\n        PydanticUserError:\n            - If a field does not have a type annotation.\n            - If a field on base class was overridden by a non-annotated attribute.\n    \"\"\"\n    all_ignored_types = ignored_types + IGNORED_TYPES\n\n    private_attributes: dict[str, ModelPrivateAttr] = {}\n    raw_annotations = namespace.get('__annotations__', {})\n\n    if '__root__' in raw_annotations or '__root__' in namespace:\n        raise TypeError(\"To define root models, use `pydantic.RootModel` rather than a field called '__root__'\")\n\n    ignored_names: set[str] = set()\n    for var_name, value in list(namespace.items()):\n        if var_name == 'model_config':\n            continue\n        elif (\n            isinstance(value, type)\n            and value.__module__ == namespace['__module__']\n            and value.__qualname__.startswith(namespace['__qualname__'])\n        ):\n            # `value` is a nested type defined in this namespace; don't error\n            continue\n        elif isinstance(value, all_ignored_types) or value.__class__.__module__ == 'functools':\n            ignored_names.add(var_name)\n            continue\n        elif isinstance(value, ModelPrivateAttr):\n            if var_name.startswith('__'):\n                raise NameError(\n                    'Private attributes must not use dunder names;'\n                    f' use a single underscore prefix instead of {var_name!r}.'\n                )\n            elif is_valid_field_name(var_name):\n                raise NameError(\n                    'Private attributes must not use valid field names;'\n                    f' use sunder names, e.g. {\"_\" + var_name!r} instead of {var_name!r}.'\n                )\n            private_attributes[var_name] = value\n            del namespace[var_name]\n        elif isinstance(value, FieldInfo) and not is_valid_field_name(var_name):\n            suggested_name = var_name.lstrip('_') or 'my_field'  # don't suggest '' for all-underscore name\n            raise NameError(\n                f'Fields must not use names with leading underscores;'\n                f' e.g., use {suggested_name!r} instead of {var_name!r}.'\n            )\n\n        elif var_name.startswith('__'):\n            continue\n        elif is_valid_privateattr_name(var_name):\n            if var_name not in raw_annotations or not is_classvar(raw_annotations[var_name]):\n                private_attributes[var_name] = PrivateAttr(default=value)\n                del namespace[var_name]\n        elif var_name in base_class_vars:\n            continue\n        elif var_name not in raw_annotations:\n            if var_name in base_class_fields:\n                raise PydanticUserError(\n                    f'Field {var_name!r} defined on a base class was overridden by a non-annotated attribute. '\n                    f'All field definitions, including overrides, require a type annotation.',\n                    code='model-field-overridden',\n                )\n            elif isinstance(value, FieldInfo):\n                raise PydanticUserError(\n                    f'Field {var_name!r} requires a type annotation', code='model-field-missing-annotation'\n                )\n            else:\n                raise PydanticUserError(\n                    f\"A non-annotated attribute was detected: `{var_name} = {value!r}`. All model fields require a \"\n                    f\"type annotation; if `{var_name}` is not meant to be a field, you may be able to resolve this \"\n                    f\"error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\",\n                    code='model-field-missing-annotation',\n                )\n\n    for ann_name, ann_type in raw_annotations.items():\n        if (\n            is_valid_privateattr_name(ann_name)\n            and ann_name not in private_attributes\n            and ann_name not in ignored_names\n            and not is_classvar(ann_type)\n            and ann_type not in all_ignored_types\n            and getattr(ann_type, '__module__', None) != 'functools'\n        ):\n            private_attributes[ann_name] = PrivateAttr()\n\n    return private_attributes\n\n\ndef set_default_hash_func(namespace: dict[str, Any], bases: tuple[type[Any], ...]) -> None:\n    if '__hash__' in namespace:\n        return\n\n    base_hash_func = get_attribute_from_bases(bases, '__hash__')\n    if base_hash_func in {None, object.__hash__}:\n        # If `__hash__` is None _or_ `object.__hash__`, we generate a hash function.\n        # It will be `None` if not overridden from BaseModel, but may be `object.__hash__` if there is another\n        # parent class earlier in the bases which doesn't override `__hash__` (e.g. `typing.Generic`).\n        def hash_func(self: Any) -> int:\n            return hash(self.__class__) + hash(tuple(self.__dict__.values()))\n\n        namespace['__hash__'] = hash_func\n\n\ndef set_model_fields(\n    cls: type[BaseModel], bases: tuple[type[Any], ...], config_wrapper: ConfigWrapper, types_namespace: dict[str, Any]\n) -> None:\n    \"\"\"Collect and set `cls.model_fields` and `cls.__class_vars__`.\n\n    Args:\n        cls: BaseModel or dataclass.\n        bases: Parents of the class, generally `cls.__bases__`.\n        config_wrapper: The config wrapper instance.\n        types_namespace: Optional extra namespace to look for types in.\n    \"\"\"\n    typevars_map = get_model_typevars_map(cls)\n    fields, class_vars = collect_model_fields(cls, bases, config_wrapper, types_namespace, typevars_map=typevars_map)\n\n    cls.model_fields = fields\n    cls.__class_vars__.update(class_vars)\n\n    for k in class_vars:\n        # Class vars should not be private attributes\n        #     We remove them _here_ and not earlier because we rely on inspecting the class to determine its classvars,\n        #     but private attributes are determined by inspecting the namespace _prior_ to class creation.\n        #     In the case that a classvar with a leading-'_' is defined via a ForwardRef (e.g., when using\n        #     `__future__.annotations`), we want to remove the private attribute which was detected _before_ we knew it\n        #     evaluated to a classvar\n\n        value = cls.__private_attributes__.pop(k, None)\n        if value is not None and value.default is not PydanticUndefined:\n            setattr(cls, k, value.default)\n\n\ndef complete_model_class(\n    cls: type[BaseModel],\n    cls_name: str,\n    config_wrapper: ConfigWrapper,\n    *,\n    raise_errors: bool = True,\n    types_namespace: dict[str, Any] | None,\n) -> bool:\n    \"\"\"Finish building a model class.\n\n    This logic must be called after class has been created since validation functions must be bound\n    and `get_type_hints` requires a class object.\n\n    Args:\n        cls: BaseModel or dataclass.\n        cls_name: The model or dataclass name.\n        config_wrapper: The config wrapper instance.\n        raise_errors: Whether to raise errors.\n        types_namespace: Optional extra namespace to look for types in.\n\n    Returns:\n        `True` if the model is successfully completed, else `False`.\n\n    Raises:\n        PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n            and `raise_errors=True`.\n    \"\"\"\n    typevars_map = get_model_typevars_map(cls)\n    gen_schema = GenerateSchema(\n        config_wrapper,\n        types_namespace,\n        typevars_map,\n    )\n\n    handler = CallbackGetCoreSchemaHandler(\n        partial(gen_schema.generate_schema, from_dunder_get_core_schema=False),\n        gen_schema,\n        ref_mode='unpack',\n    )\n\n    if config_wrapper.defer_build:\n        set_model_mocks(cls, cls_name)\n        return False\n\n    try:\n        schema = cls.__get_pydantic_core_schema__(cls, handler)\n    except PydanticUndefinedAnnotation as e:\n        if raise_errors:\n            raise\n        set_model_mocks(cls, cls_name, f'`{e.name}`')\n        return False\n\n    core_config = config_wrapper.core_config(cls)\n\n    schema = gen_schema.collect_definitions(schema)\n\n    schema = apply_discriminators(simplify_schema_references(schema))\n    if collect_invalid_schemas(schema):\n        set_model_mocks(cls, cls_name)\n        return False\n\n    # debug(schema)\n    cls.__pydantic_core_schema__ = schema = validate_core_schema(schema)\n    cls.__pydantic_validator__ = create_schema_validator(schema, core_config, config_wrapper.plugin_settings)\n    cls.__pydantic_serializer__ = SchemaSerializer(schema, core_config)\n    cls.__pydantic_complete__ = True\n\n    # set __signature__ attr only for model class, but not for its instances\n    cls.__signature__ = ClassAttribute(\n        '__signature__', generate_model_signature(cls.__init__, cls.model_fields, config_wrapper)\n    )\n    return True\n\n\ndef generate_model_signature(\n    init: Callable[..., None], fields: dict[str, FieldInfo], config_wrapper: ConfigWrapper\n) -> Signature:\n    \"\"\"Generate signature for model based on its fields.\n\n    Args:\n        init: The class init.\n        fields: The model fields.\n        config_wrapper: The config wrapper instance.\n\n    Returns:\n        The model signature.\n    \"\"\"\n    from inspect import Parameter, Signature, signature\n    from itertools import islice\n\n    present_params = signature(init).parameters.values()\n    merged_params: dict[str, Parameter] = {}\n    var_kw = None\n    use_var_kw = False\n\n    for param in islice(present_params, 1, None):  # skip self arg\n        # inspect does \"clever\" things to show annotations as strings because we have\n        # `from __future__ import annotations` in main, we don't want that\n        if param.annotation == 'Any':\n            param = param.replace(annotation=Any)\n        if param.kind is param.VAR_KEYWORD:\n            var_kw = param\n            continue\n        merged_params[param.name] = param\n\n    if var_kw:  # if custom init has no var_kw, fields which are not declared in it cannot be passed through\n        allow_names = config_wrapper.populate_by_name\n        for field_name, field in fields.items():\n            # when alias is a str it should be used for signature generation\n            if isinstance(field.alias, str):\n                param_name = field.alias\n            else:\n                param_name = field_name\n\n            if field_name in merged_params or param_name in merged_params:\n                continue\n\n            if not is_valid_identifier(param_name):\n                if allow_names and is_valid_identifier(field_name):\n                    param_name = field_name\n                else:\n                    use_var_kw = True\n                    continue\n\n            kwargs = {} if field.is_required() else {'default': field.get_default(call_default_factory=False)}\n            merged_params[param_name] = Parameter(\n                param_name, Parameter.KEYWORD_ONLY, annotation=field.rebuild_annotation(), **kwargs\n            )\n\n    if config_wrapper.extra == 'allow':\n        use_var_kw = True\n\n    if var_kw and use_var_kw:\n        # Make sure the parameter for extra kwargs\n        # does not have the same name as a field\n        default_model_signature = [\n            ('__pydantic_self__', Parameter.POSITIONAL_OR_KEYWORD),\n            ('data', Parameter.VAR_KEYWORD),\n        ]\n        if [(p.name, p.kind) for p in present_params] == default_model_signature:\n            # if this is the standard model signature, use extra_data as the extra args name\n            var_kw_name = 'extra_data'\n        else:\n            # else start from var_kw\n            var_kw_name = var_kw.name\n\n        # generate a name that's definitely unique\n        while var_kw_name in fields:\n            var_kw_name += '_'\n        merged_params[var_kw_name] = var_kw.replace(name=var_kw_name)\n\n    return Signature(parameters=list(merged_params.values()), return_annotation=None)\n\n\nclass _PydanticWeakRef(weakref.ReferenceType):\n    pass\n\n\ndef build_lenient_weakvaluedict(d: dict[str, Any] | None) -> dict[str, Any] | None:\n    \"\"\"Takes an input dictionary, and produces a new value that (invertibly) replaces the values with weakrefs.\n\n    We can't just use a WeakValueDictionary because many types (including int, str, etc.) can't be stored as values\n    in a WeakValueDictionary.\n\n    The `unpack_lenient_weakvaluedict` function can be used to reverse this operation.\n    \"\"\"\n    if d is None:\n        return None\n    result = {}\n    for k, v in d.items():\n        try:\n            proxy = _PydanticWeakRef(v)\n        except TypeError:\n            proxy = v\n        result[k] = proxy\n    return result\n\n\ndef unpack_lenient_weakvaluedict(d: dict[str, Any] | None) -> dict[str, Any] | None:\n    \"\"\"Inverts the transform performed by `build_lenient_weakvaluedict`.\"\"\"\n    if d is None:\n        return None\n\n    result = {}\n    for k, v in d.items():\n        if isinstance(v, _PydanticWeakRef):\n            v = v()\n            if v is not None:\n                result[k] = v\n        else:\n            result[k] = v\n    return result\n", 627], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/dataclasses.py": ["import re\nimport sys\nimport copy\nimport types\nimport inspect\nimport keyword\nimport builtins\nimport functools\nimport _thread\nfrom types import GenericAlias\n\n\n__all__ = ['dataclass',\n           'field',\n           'Field',\n           'FrozenInstanceError',\n           'InitVar',\n           'MISSING',\n\n           # Helper functions.\n           'fields',\n           'asdict',\n           'astuple',\n           'make_dataclass',\n           'replace',\n           'is_dataclass',\n           ]\n\n# Conditions for adding methods.  The boxes indicate what action the\n# dataclass decorator takes.  For all of these tables, when I talk\n# about init=, repr=, eq=, order=, unsafe_hash=, or frozen=, I'm\n# referring to the arguments to the @dataclass decorator.  When\n# checking if a dunder method already exists, I mean check for an\n# entry in the class's __dict__.  I never check to see if an attribute\n# is defined in a base class.\n\n# Key:\n# +=========+=========================================+\n# + Value   | Meaning                                 |\n# +=========+=========================================+\n# | <blank> | No action: no method is added.          |\n# +---------+-----------------------------------------+\n# | add     | Generated method is added.              |\n# +---------+-----------------------------------------+\n# | raise   | TypeError is raised.                    |\n# +---------+-----------------------------------------+\n# | None    | Attribute is set to None.               |\n# +=========+=========================================+\n\n# __init__\n#\n#   +--- init= parameter\n#   |\n#   v     |       |       |\n#         |  no   |  yes  |  <--- class has __init__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __repr__\n#\n#    +--- repr= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __repr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n\n# __setattr__\n# __delattr__\n#\n#    +--- frozen= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __setattr__ or __delattr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because not adding these methods would break the \"frozen-ness\"\n# of the class.\n\n# __eq__\n#\n#    +--- eq= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __eq__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __lt__\n# __le__\n# __gt__\n# __ge__\n#\n#    +--- order= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has any comparison method in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because to allow this case would interfere with using\n# functools.total_ordering.\n\n# __hash__\n\n#    +------------------- unsafe_hash= parameter\n#    |       +----------- eq= parameter\n#    |       |       +--- frozen= parameter\n#    |       |       |\n#    v       v       v    |        |        |\n#                         |   no   |  yes   |  <--- class has explicitly defined __hash__\n# +=======+=======+=======+========+========+\n# | False | False | False |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | False | True  |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | True  | False | None   |        | <-- the default, not hashable\n# +-------+-------+-------+--------+--------+\n# | False | True  | True  | add    |        | Frozen, so hashable, allows override\n# +-------+-------+-------+--------+--------+\n# | True  | False | False | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | False | True  | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | False | add    | raise  | Not frozen, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | True  | add    | raise  | Frozen, so hashable\n# +=======+=======+=======+========+========+\n# For boxes that are blank, __hash__ is untouched and therefore\n# inherited from the base class.  If the base is object, then\n# id-based hashing is used.\n#\n# Note that a class may already have __hash__=None if it specified an\n# __eq__ method in the class body (not one that was created by\n# @dataclass).\n#\n# See _hash_action (below) for a coded version of this table.\n\n\n# Raised when an attempt is made to modify a frozen class.\nclass FrozenInstanceError(AttributeError): pass\n\n# A sentinel object for default values to signal that a default\n# factory will be used.  This is given a nice repr() which will appear\n# in the function signature of dataclasses' constructors.\nclass _HAS_DEFAULT_FACTORY_CLASS:\n    def __repr__(self):\n        return '<factory>'\n_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\n\n# A sentinel object to detect if a parameter is supplied or not.  Use\n# a class to give it a better repr.\nclass _MISSING_TYPE:\n    pass\nMISSING = _MISSING_TYPE()\n\n# Since most per-field metadata will be unused, create an empty\n# read-only proxy that can be shared among all fields.\n_EMPTY_METADATA = types.MappingProxyType({})\n\n# Markers for the various kinds of fields and pseudo-fields.\nclass _FIELD_BASE:\n    def __init__(self, name):\n        self.name = name\n    def __repr__(self):\n        return self.name\n_FIELD = _FIELD_BASE('_FIELD')\n_FIELD_CLASSVAR = _FIELD_BASE('_FIELD_CLASSVAR')\n_FIELD_INITVAR = _FIELD_BASE('_FIELD_INITVAR')\n\n# The name of an attribute on the class where we store the Field\n# objects.  Also used to check if a class is a Data Class.\n_FIELDS = '__dataclass_fields__'\n\n# The name of an attribute on the class that stores the parameters to\n# @dataclass.\n_PARAMS = '__dataclass_params__'\n\n# The name of the function, that if it exists, is called at the end of\n# __init__.\n_POST_INIT_NAME = '__post_init__'\n\n# String regex that string annotations for ClassVar or InitVar must match.\n# Allows \"identifier.identifier[\" or \"identifier[\".\n# https://bugs.python.org/issue33453 for details.\n_MODULE_IDENTIFIER_RE = re.compile(r'^(?:\\s*(\\w+)\\s*\\.)?\\s*(\\w+)')\n\nclass InitVar:\n    __slots__ = ('type', )\n\n    def __init__(self, type):\n        self.type = type\n\n    def __repr__(self):\n        if isinstance(self.type, type) and not isinstance(self.type, GenericAlias):\n            type_name = self.type.__name__\n        else:\n            # typing objects, e.g. List[int]\n            type_name = repr(self.type)\n        return f'dataclasses.InitVar[{type_name}]'\n\n    def __class_getitem__(cls, type):\n        return InitVar(type)\n\n\n# Instances of Field are only ever created from within this module,\n# and only from the field() function, although Field instances are\n# exposed externally as (conceptually) read-only objects.\n#\n# name and type are filled in after the fact, not in __init__.\n# They're not known at the time this class is instantiated, but it's\n# convenient if they're available later.\n#\n# When cls._FIELDS is filled in with a list of Field objects, the name\n# and type fields will have been populated.\nclass Field:\n    __slots__ = ('name',\n                 'type',\n                 'default',\n                 'default_factory',\n                 'repr',\n                 'hash',\n                 'init',\n                 'compare',\n                 'metadata',\n                 '_field_type',  # Private: not to be used by user code.\n                 )\n\n    def __init__(self, default, default_factory, init, repr, hash, compare,\n                 metadata):\n        self.name = None\n        self.type = None\n        self.default = default\n        self.default_factory = default_factory\n        self.init = init\n        self.repr = repr\n        self.hash = hash\n        self.compare = compare\n        self.metadata = (_EMPTY_METADATA\n                         if metadata is None else\n                         types.MappingProxyType(metadata))\n        self._field_type = None\n\n    def __repr__(self):\n        return ('Field('\n                f'name={self.name!r},'\n                f'type={self.type!r},'\n                f'default={self.default!r},'\n                f'default_factory={self.default_factory!r},'\n                f'init={self.init!r},'\n                f'repr={self.repr!r},'\n                f'hash={self.hash!r},'\n                f'compare={self.compare!r},'\n                f'metadata={self.metadata!r},'\n                f'_field_type={self._field_type}'\n                ')')\n\n    # This is used to support the PEP 487 __set_name__ protocol in the\n    # case where we're using a field that contains a descriptor as a\n    # default value.  For details on __set_name__, see\n    # https://www.python.org/dev/peps/pep-0487/#implementation-details.\n    #\n    # Note that in _process_class, this Field object is overwritten\n    # with the default value, so the end result is a descriptor that\n    # had __set_name__ called on it at the right time.\n    def __set_name__(self, owner, name):\n        func = getattr(type(self.default), '__set_name__', None)\n        if func:\n            # There is a __set_name__ method on the descriptor, call\n            # it.\n            func(self.default, owner, name)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass _DataclassParams:\n    __slots__ = ('init',\n                 'repr',\n                 'eq',\n                 'order',\n                 'unsafe_hash',\n                 'frozen',\n                 )\n\n    def __init__(self, init, repr, eq, order, unsafe_hash, frozen):\n        self.init = init\n        self.repr = repr\n        self.eq = eq\n        self.order = order\n        self.unsafe_hash = unsafe_hash\n        self.frozen = frozen\n\n    def __repr__(self):\n        return ('_DataclassParams('\n                f'init={self.init!r},'\n                f'repr={self.repr!r},'\n                f'eq={self.eq!r},'\n                f'order={self.order!r},'\n                f'unsafe_hash={self.unsafe_hash!r},'\n                f'frozen={self.frozen!r}'\n                ')')\n\n\n# This function is used instead of exposing Field creation directly,\n# so that a type checker can be told (via overloads) that this is a\n# function whose type depends on its parameters.\ndef field(*, default=MISSING, default_factory=MISSING, init=True, repr=True,\n          hash=None, compare=True, metadata=None):\n    \"\"\"Return an object to identify dataclass fields.\n\n    default is the default value of the field.  default_factory is a\n    0-argument function called to initialize a field's value.  If init\n    is True, the field will be a parameter to the class's __init__()\n    function.  If repr is True, the field will be included in the\n    object's repr().  If hash is True, the field will be included in\n    the object's hash().  If compare is True, the field will be used\n    in comparison functions.  metadata, if specified, must be a\n    mapping which is stored but not otherwise examined by dataclass.\n\n    It is an error to specify both default and default_factory.\n    \"\"\"\n\n    if default is not MISSING and default_factory is not MISSING:\n        raise ValueError('cannot specify both default and default_factory')\n    return Field(default, default_factory, init, repr, hash, compare,\n                 metadata)\n\n\ndef _tuple_str(obj_name, fields):\n    # Return a string representing each field of obj_name as a tuple\n    # member.  So, if fields is ['x', 'y'] and obj_name is \"self\",\n    # return \"(self.x,self.y)\".\n\n    # Special case for the 0-tuple.\n    if not fields:\n        return '()'\n    # Note the trailing comma, needed if this turns out to be a 1-tuple.\n    return f'({\",\".join([f\"{obj_name}.{f.name}\" for f in fields])},)'\n\n\n# This function's logic is copied from \"recursive_repr\" function in\n# reprlib module to avoid dependency.\ndef _recursive_repr(user_function):\n    # Decorator to make a repr function return \"...\" for a recursive\n    # call.\n    repr_running = set()\n\n    @functools.wraps(user_function)\n    def wrapper(self):\n        key = id(self), _thread.get_ident()\n        if key in repr_running:\n            return '...'\n        repr_running.add(key)\n        try:\n            result = user_function(self)\n        finally:\n            repr_running.discard(key)\n        return result\n    return wrapper\n\n\ndef _create_fn(name, args, body, *, globals=None, locals=None,\n               return_type=MISSING):\n    # Note that we mutate locals when exec() is called.  Caller\n    # beware!  The only callers are internal to this module, so no\n    # worries about external callers.\n    if locals is None:\n        locals = {}\n    if 'BUILTINS' not in locals:\n        locals['BUILTINS'] = builtins\n    return_annotation = ''\n    if return_type is not MISSING:\n        locals['_return_type'] = return_type\n        return_annotation = '->_return_type'\n    args = ','.join(args)\n    body = '\\n'.join(f'  {b}' for b in body)\n\n    # Compute the text of the entire function.\n    txt = f' def {name}({args}){return_annotation}:\\n{body}'\n\n    local_vars = ', '.join(locals.keys())\n    txt = f\"def __create_fn__({local_vars}):\\n{txt}\\n return {name}\"\n\n    ns = {}\n    exec(txt, globals, ns)\n    return ns['__create_fn__'](**locals)\n\n\ndef _field_assign(frozen, name, value, self_name):\n    # If we're a frozen class, then assign to our fields in __init__\n    # via object.__setattr__.  Otherwise, just use a simple\n    # assignment.\n    #\n    # self_name is what \"self\" is called in this function: don't\n    # hard-code \"self\", since that might be a field name.\n    if frozen:\n        return f'BUILTINS.object.__setattr__({self_name},{name!r},{value})'\n    return f'{self_name}.{name}={value}'\n\n\ndef _field_init(f, frozen, globals, self_name):\n    # Return the text of the line in the body of __init__ that will\n    # initialize this field.\n\n    default_name = f'_dflt_{f.name}'\n    if f.default_factory is not MISSING:\n        if f.init:\n            # This field has a default factory.  If a parameter is\n            # given, use it.  If not, call the factory.\n            globals[default_name] = f.default_factory\n            value = (f'{default_name}() '\n                     f'if {f.name} is _HAS_DEFAULT_FACTORY '\n                     f'else {f.name}')\n        else:\n            # This is a field that's not in the __init__ params, but\n            # has a default factory function.  It needs to be\n            # initialized here by calling the factory function,\n            # because there's no other way to initialize it.\n\n            # For a field initialized with a default=defaultvalue, the\n            # class dict just has the default value\n            # (cls.fieldname=defaultvalue).  But that won't work for a\n            # default factory, the factory must be called in __init__\n            # and we must assign that to self.fieldname.  We can't\n            # fall back to the class dict's value, both because it's\n            # not set, and because it might be different per-class\n            # (which, after all, is why we have a factory function!).\n\n            globals[default_name] = f.default_factory\n            value = f'{default_name}()'\n    else:\n        # No default factory.\n        if f.init:\n            if f.default is MISSING:\n                # There's no default, just do an assignment.\n                value = f.name\n            elif f.default is not MISSING:\n                globals[default_name] = f.default\n                value = f.name\n        else:\n            # This field does not need initialization.  Signify that\n            # to the caller by returning None.\n            return None\n\n    # Only test this now, so that we can create variables for the\n    # default.  However, return None to signify that we're not going\n    # to actually do the assignment statement for InitVars.\n    if f._field_type is _FIELD_INITVAR:\n        return None\n\n    # Now, actually generate the field assignment.\n    return _field_assign(frozen, f.name, value, self_name)\n\n\ndef _init_param(f):\n    # Return the __init__ parameter string for this field.  For\n    # example, the equivalent of 'x:int=3' (except instead of 'int',\n    # reference a variable set to int, and instead of '3', reference a\n    # variable set to 3).\n    if f.default is MISSING and f.default_factory is MISSING:\n        # There's no default, and no default_factory, just output the\n        # variable name and type.\n        default = ''\n    elif f.default is not MISSING:\n        # There's a default, this will be the name that's used to look\n        # it up.\n        default = f'=_dflt_{f.name}'\n    elif f.default_factory is not MISSING:\n        # There's a factory function.  Set a marker.\n        default = '=_HAS_DEFAULT_FACTORY'\n    return f'{f.name}:_type_{f.name}{default}'\n\n\ndef _init_fn(fields, frozen, has_post_init, self_name, globals):\n    # fields contains both real fields and InitVar pseudo-fields.\n\n    # Make sure we don't have fields without defaults following fields\n    # with defaults.  This actually would be caught when exec-ing the\n    # function source code, but catching it here gives a better error\n    # message, and future-proofs us in case we build up the function\n    # using ast.\n    seen_default = False\n    for f in fields:\n        # Only consider fields in the __init__ call.\n        if f.init:\n            if not (f.default is MISSING and f.default_factory is MISSING):\n                seen_default = True\n            elif seen_default:\n                raise TypeError(f'non-default argument {f.name!r} '\n                                'follows default argument')\n\n    locals = {f'_type_{f.name}': f.type for f in fields}\n    locals.update({\n        'MISSING': MISSING,\n        '_HAS_DEFAULT_FACTORY': _HAS_DEFAULT_FACTORY,\n    })\n\n    body_lines = []\n    for f in fields:\n        line = _field_init(f, frozen, locals, self_name)\n        # line is None means that this field doesn't require\n        # initialization (it's a pseudo-field).  Just skip it.\n        if line:\n            body_lines.append(line)\n\n    # Does this class have a post-init function?\n    if has_post_init:\n        params_str = ','.join(f.name for f in fields\n                              if f._field_type is _FIELD_INITVAR)\n        body_lines.append(f'{self_name}.{_POST_INIT_NAME}({params_str})')\n\n    # If no body lines, use 'pass'.\n    if not body_lines:\n        body_lines = ['pass']\n\n    return _create_fn('__init__',\n                      [self_name] + [_init_param(f) for f in fields if f.init],\n                      body_lines,\n                      locals=locals,\n                      globals=globals,\n                      return_type=None)\n\n\ndef _repr_fn(fields, globals):\n    fn = _create_fn('__repr__',\n                    ('self',),\n                    ['return self.__class__.__qualname__ + f\"(' +\n                     ', '.join([f\"{f.name}={{self.{f.name}!r}}\"\n                                for f in fields]) +\n                     ')\"'],\n                     globals=globals)\n    return _recursive_repr(fn)\n\n\ndef _frozen_get_del_attr(cls, fields, globals):\n    locals = {'cls': cls,\n              'FrozenInstanceError': FrozenInstanceError}\n    if fields:\n        fields_str = '(' + ','.join(repr(f.name) for f in fields) + ',)'\n    else:\n        # Special case for the zero-length tuple.\n        fields_str = '()'\n    return (_create_fn('__setattr__',\n                      ('self', 'name', 'value'),\n                      (f'if type(self) is cls or name in {fields_str}:',\n                        ' raise FrozenInstanceError(f\"cannot assign to field {name!r}\")',\n                       f'super(cls, self).__setattr__(name, value)'),\n                       locals=locals,\n                       globals=globals),\n            _create_fn('__delattr__',\n                      ('self', 'name'),\n                      (f'if type(self) is cls or name in {fields_str}:',\n                        ' raise FrozenInstanceError(f\"cannot delete field {name!r}\")',\n                       f'super(cls, self).__delattr__(name)'),\n                       locals=locals,\n                       globals=globals),\n            )\n\n\ndef _cmp_fn(name, op, self_tuple, other_tuple, globals):\n    # Create a comparison function.  If the fields in the object are\n    # named 'x' and 'y', then self_tuple is the string\n    # '(self.x,self.y)' and other_tuple is the string\n    # '(other.x,other.y)'.\n\n    return _create_fn(name,\n                      ('self', 'other'),\n                      [ 'if other.__class__ is self.__class__:',\n                       f' return {self_tuple}{op}{other_tuple}',\n                        'return NotImplemented'],\n                      globals=globals)\n\n\ndef _hash_fn(fields, globals):\n    self_tuple = _tuple_str('self', fields)\n    return _create_fn('__hash__',\n                      ('self',),\n                      [f'return hash({self_tuple})'],\n                      globals=globals)\n\n\ndef _is_classvar(a_type, typing):\n    # This test uses a typing internal class, but it's the best way to\n    # test if this is a ClassVar.\n    return (a_type is typing.ClassVar\n            or (type(a_type) is typing._GenericAlias\n                and a_type.__origin__ is typing.ClassVar))\n\n\ndef _is_initvar(a_type, dataclasses):\n    # The module we're checking against is the module we're\n    # currently in (dataclasses.py).\n    return (a_type is dataclasses.InitVar\n            or type(a_type) is dataclasses.InitVar)\n\n\ndef _is_type(annotation, cls, a_module, a_type, is_type_predicate):\n    # Given a type annotation string, does it refer to a_type in\n    # a_module?  For example, when checking that annotation denotes a\n    # ClassVar, then a_module is typing, and a_type is\n    # typing.ClassVar.\n\n    # It's possible to look up a_module given a_type, but it involves\n    # looking in sys.modules (again!), and seems like a waste since\n    # the caller already knows a_module.\n\n    # - annotation is a string type annotation\n    # - cls is the class that this annotation was found in\n    # - a_module is the module we want to match\n    # - a_type is the type in that module we want to match\n    # - is_type_predicate is a function called with (obj, a_module)\n    #   that determines if obj is of the desired type.\n\n    # Since this test does not do a local namespace lookup (and\n    # instead only a module (global) lookup), there are some things it\n    # gets wrong.\n\n    # With string annotations, cv0 will be detected as a ClassVar:\n    #   CV = ClassVar\n    #   @dataclass\n    #   class C0:\n    #     cv0: CV\n\n    # But in this example cv1 will not be detected as a ClassVar:\n    #   @dataclass\n    #   class C1:\n    #     CV = ClassVar\n    #     cv1: CV\n\n    # In C1, the code in this function (_is_type) will look up \"CV\" in\n    # the module and not find it, so it will not consider cv1 as a\n    # ClassVar.  This is a fairly obscure corner case, and the best\n    # way to fix it would be to eval() the string \"CV\" with the\n    # correct global and local namespaces.  However that would involve\n    # a eval() penalty for every single field of every dataclass\n    # that's defined.  It was judged not worth it.\n\n    match = _MODULE_IDENTIFIER_RE.match(annotation)\n    if match:\n        ns = None\n        module_name = match.group(1)\n        if not module_name:\n            # No module name, assume the class's module did\n            # \"from dataclasses import InitVar\".\n            ns = sys.modules.get(cls.__module__).__dict__\n        else:\n            # Look up module_name in the class's module.\n            module = sys.modules.get(cls.__module__)\n            if module and module.__dict__.get(module_name) is a_module:\n                ns = sys.modules.get(a_type.__module__).__dict__\n        if ns and is_type_predicate(ns.get(match.group(2)), a_module):\n            return True\n    return False\n\n\ndef _get_field(cls, a_name, a_type):\n    # Return a Field object for this field name and type.  ClassVars\n    # and InitVars are also returned, but marked as such (see\n    # f._field_type).\n\n    # If the default value isn't derived from Field, then it's only a\n    # normal default value.  Convert it to a Field().\n    default = getattr(cls, a_name, MISSING)\n    if isinstance(default, Field):\n        f = default\n    else:\n        if isinstance(default, types.MemberDescriptorType):\n            # This is a field in __slots__, so it has no default value.\n            default = MISSING\n        f = field(default=default)\n\n    # Only at this point do we know the name and the type.  Set them.\n    f.name = a_name\n    f.type = a_type\n\n    # Assume it's a normal field until proven otherwise.  We're next\n    # going to decide if it's a ClassVar or InitVar, everything else\n    # is just a normal field.\n    f._field_type = _FIELD\n\n    # In addition to checking for actual types here, also check for\n    # string annotations.  get_type_hints() won't always work for us\n    # (see https://github.com/python/typing/issues/508 for example),\n    # plus it's expensive and would require an eval for every string\n    # annotation.  So, make a best effort to see if this is a ClassVar\n    # or InitVar using regex's and checking that the thing referenced\n    # is actually of the correct type.\n\n    # For the complete discussion, see https://bugs.python.org/issue33453\n\n    # If typing has not been imported, then it's impossible for any\n    # annotation to be a ClassVar.  So, only look for ClassVar if\n    # typing has been imported by any module (not necessarily cls's\n    # module).\n    typing = sys.modules.get('typing')\n    if typing:\n        if (_is_classvar(a_type, typing)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, typing, typing.ClassVar,\n                             _is_classvar))):\n            f._field_type = _FIELD_CLASSVAR\n\n    # If the type is InitVar, or if it's a matching string annotation,\n    # then it's an InitVar.\n    if f._field_type is _FIELD:\n        # The module we're checking against is the module we're\n        # currently in (dataclasses.py).\n        dataclasses = sys.modules[__name__]\n        if (_is_initvar(a_type, dataclasses)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, dataclasses, dataclasses.InitVar,\n                             _is_initvar))):\n            f._field_type = _FIELD_INITVAR\n\n    # Validations for individual fields.  This is delayed until now,\n    # instead of in the Field() constructor, since only here do we\n    # know the field name, which allows for better error reporting.\n\n    # Special restrictions for ClassVar and InitVar.\n    if f._field_type in (_FIELD_CLASSVAR, _FIELD_INITVAR):\n        if f.default_factory is not MISSING:\n            raise TypeError(f'field {f.name} cannot have a '\n                            'default factory')\n        # Should I check for other field settings? default_factory\n        # seems the most serious to check for.  Maybe add others.  For\n        # example, how about init=False (or really,\n        # init=<not-the-default-init-value>)?  It makes no sense for\n        # ClassVar and InitVar to specify init=<anything>.\n\n    # For real fields, disallow mutable defaults for known types.\n    if f._field_type is _FIELD and isinstance(f.default, (list, dict, set)):\n        raise ValueError(f'mutable default {type(f.default)} for field '\n                         f'{f.name} is not allowed: use default_factory')\n\n    return f\n\n\ndef _set_new_attribute(cls, name, value):\n    # Never overwrites an existing attribute.  Returns True if the\n    # attribute already exists.\n    if name in cls.__dict__:\n        return True\n    setattr(cls, name, value)\n    return False\n\n\n# Decide if/how we're going to create a hash function.  Key is\n# (unsafe_hash, eq, frozen, does-hash-exist).  Value is the action to\n# take.  The common case is to do nothing, so instead of providing a\n# function that is a no-op, use None to signify that.\n\ndef _hash_set_none(cls, fields, globals):\n    return None\n\ndef _hash_add(cls, fields, globals):\n    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\n    return _hash_fn(flds, globals)\n\ndef _hash_exception(cls, fields, globals):\n    # Raise an exception.\n    raise TypeError(f'Cannot overwrite attribute __hash__ '\n                    f'in class {cls.__name__}')\n\n#\n#                +-------------------------------------- unsafe_hash?\n#                |      +------------------------------- eq?\n#                |      |      +------------------------ frozen?\n#                |      |      |      +----------------  has-explicit-hash?\n#                |      |      |      |\n#                |      |      |      |        +-------  action\n#                |      |      |      |        |\n#                v      v      v      v        v\n_hash_action = {(False, False, False, False): None,\n                (False, False, False, True ): None,\n                (False, False, True,  False): None,\n                (False, False, True,  True ): None,\n                (False, True,  False, False): _hash_set_none,\n                (False, True,  False, True ): None,\n                (False, True,  True,  False): _hash_add,\n                (False, True,  True,  True ): None,\n                (True,  False, False, False): _hash_add,\n                (True,  False, False, True ): _hash_exception,\n                (True,  False, True,  False): _hash_add,\n                (True,  False, True,  True ): _hash_exception,\n                (True,  True,  False, False): _hash_add,\n                (True,  True,  False, True ): _hash_exception,\n                (True,  True,  True,  False): _hash_add,\n                (True,  True,  True,  True ): _hash_exception,\n                }\n# See https://bugs.python.org/issue32929#msg312829 for an if-statement\n# version of this table.\n\n\ndef _process_class(cls, init, repr, eq, order, unsafe_hash, frozen):\n    # Now that dicts retain insertion order, there's no reason to use\n    # an ordered dict.  I am leveraging that ordering here, because\n    # derived class fields overwrite base class fields, but the order\n    # is defined by the base class, which is found first.\n    fields = {}\n\n    if cls.__module__ in sys.modules:\n        globals = sys.modules[cls.__module__].__dict__\n    else:\n        # Theoretically this can happen if someone writes\n        # a custom string to cls.__module__.  In which case\n        # such dataclass won't be fully introspectable\n        # (w.r.t. typing.get_type_hints) but will still function\n        # correctly.\n        globals = {}\n\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\n                                           unsafe_hash, frozen))\n\n    # Find our base classes in reverse MRO order, and exclude\n    # ourselves.  In reversed order so that more derived classes\n    # override earlier field definitions in base classes.  As long as\n    # we're iterating over them, see if any are frozen.\n    any_frozen_base = False\n    has_dataclass_bases = False\n    for b in cls.__mro__[-1:0:-1]:\n        # Only process classes that have been processed by our\n        # decorator.  That is, they have a _FIELDS attribute.\n        base_fields = getattr(b, _FIELDS, None)\n        if base_fields is not None:\n            has_dataclass_bases = True\n            for f in base_fields.values():\n                fields[f.name] = f\n            if getattr(b, _PARAMS).frozen:\n                any_frozen_base = True\n\n    # Annotations that are defined in this class (not in base\n    # classes).  If __annotations__ isn't present, then this class\n    # adds no new annotations.  We use this to compute fields that are\n    # added by this class.\n    #\n    # Fields are found from cls_annotations, which is guaranteed to be\n    # ordered.  Default values are from class attributes, if a field\n    # has a default.  If the default value is a Field(), then it\n    # contains additional info beyond (and possibly including) the\n    # actual default value.  Pseudo-fields ClassVars and InitVars are\n    # included, despite the fact that they're not real fields.  That's\n    # dealt with later.\n    cls_annotations = cls.__dict__.get('__annotations__', {})\n\n    # Now find fields in our class.  While doing so, validate some\n    # things, and set the default values (as class attributes) where\n    # we can.\n    cls_fields = [_get_field(cls, name, type)\n                  for name, type in cls_annotations.items()]\n    for f in cls_fields:\n        fields[f.name] = f\n\n        # If the class attribute (which is the default value for this\n        # field) exists and is of type 'Field', replace it with the\n        # real default.  This is so that normal class introspection\n        # sees a real default value, not a Field.\n        if isinstance(getattr(cls, f.name, None), Field):\n            if f.default is MISSING:\n                # If there's no default, delete the class attribute.\n                # This happens if we specify field(repr=False), for\n                # example (that is, we specified a field object, but\n                # no default value).  Also if we're using a default\n                # factory.  The class attribute should not be set at\n                # all in the post-processed class.\n                delattr(cls, f.name)\n            else:\n                setattr(cls, f.name, f.default)\n\n    # Do we have any Field members that don't also have annotations?\n    for name, value in cls.__dict__.items():\n        if isinstance(value, Field) and not name in cls_annotations:\n            raise TypeError(f'{name!r} is a field but has no type annotation')\n\n    # Check rules that apply if we are derived from any dataclasses.\n    if has_dataclass_bases:\n        # Raise an exception if any of our bases are frozen, but we're not.\n        if any_frozen_base and not frozen:\n            raise TypeError('cannot inherit non-frozen dataclass from a '\n                            'frozen one')\n\n        # Raise an exception if we're frozen, but none of our bases are.\n        if not any_frozen_base and frozen:\n            raise TypeError('cannot inherit frozen dataclass from a '\n                            'non-frozen one')\n\n    # Remember all of the fields on our class (including bases).  This\n    # also marks this class as being a dataclass.\n    setattr(cls, _FIELDS, fields)\n\n    # Was this class defined with an explicit __hash__?  Note that if\n    # __eq__ is defined in this class, then python will automatically\n    # set __hash__ to None.  This is a heuristic, as it's possible\n    # that such a __hash__ == None was not auto-generated, but it\n    # close enough.\n    class_hash = cls.__dict__.get('__hash__', MISSING)\n    has_explicit_hash = not (class_hash is MISSING or\n                             (class_hash is None and '__eq__' in cls.__dict__))\n\n    # If we're generating ordering methods, we must be generating the\n    # eq methods.\n    if order and not eq:\n        raise ValueError('eq must be true if order is true')\n\n    if init:\n        # Does this class have a post-init function?\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\n\n        # Include InitVars and regular fields (so, not ClassVars).\n        flds = [f for f in fields.values()\n                if f._field_type in (_FIELD, _FIELD_INITVAR)]\n        _set_new_attribute(cls, '__init__',\n                           _init_fn(flds,\n                                    frozen,\n                                    has_post_init,\n                                    # The name to use for the \"self\"\n                                    # param in __init__.  Use \"self\"\n                                    # if possible.\n                                    '__dataclass_self__' if 'self' in fields\n                                            else 'self',\n                                    globals,\n                          ))\n\n    # Get the fields as a list, and include only real fields.  This is\n    # used in all of the following methods.\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\n\n    if repr:\n        flds = [f for f in field_list if f.repr]\n        _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\n\n    if eq:\n        # Create __eq__ method.  There's no need for a __ne__ method,\n        # since python will call __eq__ and negate it.\n        flds = [f for f in field_list if f.compare]\n        self_tuple = _tuple_str('self', flds)\n        other_tuple = _tuple_str('other', flds)\n        _set_new_attribute(cls, '__eq__',\n                           _cmp_fn('__eq__', '==',\n                                   self_tuple, other_tuple,\n                                   globals=globals))\n\n    if order:\n        # Create and set the ordering methods.\n        flds = [f for f in field_list if f.compare]\n        self_tuple = _tuple_str('self', flds)\n        other_tuple = _tuple_str('other', flds)\n        for name, op in [('__lt__', '<'),\n                         ('__le__', '<='),\n                         ('__gt__', '>'),\n                         ('__ge__', '>='),\n                         ]:\n            if _set_new_attribute(cls, name,\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\n                                          globals=globals)):\n                raise TypeError(f'Cannot overwrite attribute {name} '\n                                f'in class {cls.__name__}. Consider using '\n                                'functools.total_ordering')\n\n    if frozen:\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\n            if _set_new_attribute(cls, fn.__name__, fn):\n                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\n                                f'in class {cls.__name__}')\n\n    # Decide if/how we're going to create a hash function.\n    hash_action = _hash_action[bool(unsafe_hash),\n                               bool(eq),\n                               bool(frozen),\n                               has_explicit_hash]\n    if hash_action:\n        # No need to call _set_new_attribute here, since by the time\n        # we're here the overwriting is unconditional.\n        cls.__hash__ = hash_action(cls, field_list, globals)\n\n    if not getattr(cls, '__doc__'):\n        # Create a class doc-string.\n        cls.__doc__ = (cls.__name__ +\n                       str(inspect.signature(cls)).replace(' -> None', ''))\n\n    return cls\n\n\ndef dataclass(cls=None, /, *, init=True, repr=True, eq=True, order=False,\n              unsafe_hash=False, frozen=False):\n    \"\"\"Returns the same class as was passed in, with dunder methods\n    added based on the fields defined in the class.\n\n    Examines PEP 526 __annotations__ to determine fields.\n\n    If init is true, an __init__() method is added to the class. If\n    repr is true, a __repr__() method is added. If order is true, rich\n    comparison dunder methods are added. If unsafe_hash is true, a\n    __hash__() method function is added. If frozen is true, fields may\n    not be assigned to after instance creation.\n    \"\"\"\n\n    def wrap(cls):\n        return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen)\n\n    # See if we're being called as @dataclass or @dataclass().\n    if cls is None:\n        # We're called with parens.\n        return wrap\n\n    # We're called as @dataclass without parens.\n    return wrap(cls)\n\n\ndef fields(class_or_instance):\n    \"\"\"Return a tuple describing the fields of this dataclass.\n\n    Accepts a dataclass or an instance of one. Tuple elements are of\n    type Field.\n    \"\"\"\n\n    # Might it be worth caching this, per class?\n    try:\n        fields = getattr(class_or_instance, _FIELDS)\n    except AttributeError:\n        raise TypeError('must be called with a dataclass type or instance')\n\n    # Exclude pseudo-fields.  Note that fields is sorted by insertion\n    # order, so the order of the tuple is as the fields were defined.\n    return tuple(f for f in fields.values() if f._field_type is _FIELD)\n\n\ndef _is_dataclass_instance(obj):\n    \"\"\"Returns True if obj is an instance of a dataclass.\"\"\"\n    return hasattr(type(obj), _FIELDS)\n\n\ndef is_dataclass(obj):\n    \"\"\"Returns True if obj is a dataclass or an instance of a\n    dataclass.\"\"\"\n    cls = obj if isinstance(obj, type) and not isinstance(obj, GenericAlias) else type(obj)\n    return hasattr(cls, _FIELDS)\n\n\ndef asdict(obj, *, dict_factory=dict):\n    \"\"\"Return the fields of a dataclass instance as a new dictionary mapping\n    field names to field values.\n\n    Example usage:\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      assert asdict(c) == {'x': 1, 'y': 2}\n\n    If given, 'dict_factory' will be used instead of built-in dict.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts.\n    \"\"\"\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"asdict() should be called on dataclass instances\")\n    return _asdict_inner(obj, dict_factory)\n\n\ndef _asdict_inner(obj, dict_factory):\n    if _is_dataclass_instance(obj):\n        result = []\n        for f in fields(obj):\n            value = _asdict_inner(getattr(obj, f.name), dict_factory)\n            result.append((f.name, value))\n        return dict_factory(result)\n    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):\n        # obj is a namedtuple.  Recurse into it, but the returned\n        # object is another namedtuple of the same type.  This is\n        # similar to how other list- or tuple-derived classes are\n        # treated (see below), but we just need to create them\n        # differently because a namedtuple's __init__ needs to be\n        # called differently (see bpo-34363).\n\n        # I'm not using namedtuple's _asdict()\n        # method, because:\n        # - it does not recurse in to the namedtuple fields and\n        #   convert them to dicts (using dict_factory).\n        # - I don't actually want to return a dict here.  The main\n        #   use case here is json.dumps, and it handles converting\n        #   namedtuples to lists.  Admittedly we're losing some\n        #   information here when we produce a json list instead of a\n        #   dict.  Note that if we returned dicts here instead of\n        #   namedtuples, we could no longer call asdict() on a data\n        #   structure where a namedtuple was used as a dict key.\n\n        return type(obj)(*[_asdict_inner(v, dict_factory) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        # Assume we can create an object of this type by passing in a\n        # generator (which is not true for namedtuples, handled\n        # above).\n        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)\n    elif isinstance(obj, dict):\n        return type(obj)((_asdict_inner(k, dict_factory),\n                          _asdict_inner(v, dict_factory))\n                         for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n\ndef astuple(obj, *, tuple_factory=tuple):\n    \"\"\"Return the fields of a dataclass instance as a new tuple of field values.\n\n    Example usage::\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n    c = C(1, 2)\n    assert astuple(c) == (1, 2)\n\n    If given, 'tuple_factory' will be used instead of built-in tuple.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts.\n    \"\"\"\n\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"astuple() should be called on dataclass instances\")\n    return _astuple_inner(obj, tuple_factory)\n\n\ndef _astuple_inner(obj, tuple_factory):\n    if _is_dataclass_instance(obj):\n        result = []\n        for f in fields(obj):\n            value = _astuple_inner(getattr(obj, f.name), tuple_factory)\n            result.append(value)\n        return tuple_factory(result)\n    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):\n        # obj is a namedtuple.  Recurse into it, but the returned\n        # object is another namedtuple of the same type.  This is\n        # similar to how other list- or tuple-derived classes are\n        # treated (see below), but we just need to create them\n        # differently because a namedtuple's __init__ needs to be\n        # called differently (see bpo-34363).\n        return type(obj)(*[_astuple_inner(v, tuple_factory) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        # Assume we can create an object of this type by passing in a\n        # generator (which is not true for namedtuples, handled\n        # above).\n        return type(obj)(_astuple_inner(v, tuple_factory) for v in obj)\n    elif isinstance(obj, dict):\n        return type(obj)((_astuple_inner(k, tuple_factory), _astuple_inner(v, tuple_factory))\n                          for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n\ndef make_dataclass(cls_name, fields, *, bases=(), namespace=None, init=True,\n                   repr=True, eq=True, order=False, unsafe_hash=False,\n                   frozen=False):\n    \"\"\"Return a new dynamically created dataclass.\n\n    The dataclass name will be 'cls_name'.  'fields' is an iterable\n    of either (name), (name, type) or (name, type, Field) objects. If type is\n    omitted, use the string 'typing.Any'.  Field objects are created by\n    the equivalent of calling 'field(name, type [, Field-info])'.\n\n      C = make_dataclass('C', ['x', ('y', int), ('z', int, field(init=False))], bases=(Base,))\n\n    is equivalent to:\n\n      @dataclass\n      class C(Base):\n          x: 'typing.Any'\n          y: int\n          z: int = field(init=False)\n\n    For the bases and namespace parameters, see the builtin type() function.\n\n    The parameters init, repr, eq, order, unsafe_hash, and frozen are passed to\n    dataclass().\n    \"\"\"\n\n    if namespace is None:\n        namespace = {}\n    else:\n        # Copy namespace since we're going to mutate it.\n        namespace = namespace.copy()\n\n    # While we're looking through the field names, validate that they\n    # are identifiers, are not keywords, and not duplicates.\n    seen = set()\n    anns = {}\n    for item in fields:\n        if isinstance(item, str):\n            name = item\n            tp = 'typing.Any'\n        elif len(item) == 2:\n            name, tp, = item\n        elif len(item) == 3:\n            name, tp, spec = item\n            namespace[name] = spec\n        else:\n            raise TypeError(f'Invalid field: {item!r}')\n\n        if not isinstance(name, str) or not name.isidentifier():\n            raise TypeError(f'Field names must be valid identifiers: {name!r}')\n        if keyword.iskeyword(name):\n            raise TypeError(f'Field names must not be keywords: {name!r}')\n        if name in seen:\n            raise TypeError(f'Field name duplicated: {name!r}')\n\n        seen.add(name)\n        anns[name] = tp\n\n    namespace['__annotations__'] = anns\n    # We use `types.new_class()` instead of simply `type()` to allow dynamic creation\n    # of generic dataclassses.\n    cls = types.new_class(cls_name, bases, {}, lambda ns: ns.update(namespace))\n    return dataclass(cls, init=init, repr=repr, eq=eq, order=order,\n                     unsafe_hash=unsafe_hash, frozen=frozen)\n\n\ndef replace(obj, /, **changes):\n    \"\"\"Return a new object replacing specified fields with new values.\n\n    This is especially useful for frozen classes.  Example usage:\n\n      @dataclass(frozen=True)\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      c1 = replace(c, x=3)\n      assert c1.x == 3 and c1.y == 2\n      \"\"\"\n\n    # We're going to mutate 'changes', but that's okay because it's a\n    # new dict, even if called with 'replace(obj, **my_changes)'.\n\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"replace() should be called on dataclass instances\")\n\n    # It's an error to have init=False fields in 'changes'.\n    # If a field is not in 'changes', read its value from the provided obj.\n\n    for f in getattr(obj, _FIELDS).values():\n        # Only consider normal fields or InitVars.\n        if f._field_type is _FIELD_CLASSVAR:\n            continue\n\n        if not f.init:\n            # Error if this field is specified in changes.\n            if f.name in changes:\n                raise ValueError(f'field {f.name} is declared with '\n                                 'init=False, it cannot be specified with '\n                                 'replace()')\n            continue\n\n        if f.name not in changes:\n            if f._field_type is _FIELD_INITVAR and f.default is MISSING:\n                raise ValueError(f\"InitVar {f.name!r} \"\n                                 'must be specified with replace()')\n            changes[f.name] = getattr(obj, f.name)\n\n    # Create the new object, which calls __init__() and\n    # __post_init__() (if defined), using all of the init fields we've\n    # added and/or left in 'changes'.  If there are values supplied in\n    # changes that aren't fields, this will correctly raise a\n    # TypeError.\n    return obj.__class__(**changes)\n", 1284], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/encoders.py": ["import dataclasses\nimport datetime\nfrom collections import defaultdict, deque\nfrom decimal import Decimal\nfrom enum import Enum\nfrom ipaddress import (\n    IPv4Address,\n    IPv4Interface,\n    IPv4Network,\n    IPv6Address,\n    IPv6Interface,\n    IPv6Network,\n)\nfrom pathlib import Path, PurePath\nfrom re import Pattern\nfrom types import GeneratorType\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\nfrom uuid import UUID\n\nfrom fastapi.types import IncEx\nfrom pydantic import BaseModel\nfrom pydantic.color import Color\nfrom pydantic.networks import AnyUrl, NameEmail\nfrom pydantic.types import SecretBytes, SecretStr\n\nfrom ._compat import PYDANTIC_V2, Url, _model_dump\n\n\n# Taken from Pydantic v1 as is\ndef isoformat(o: Union[datetime.date, datetime.time]) -> str:\n    return o.isoformat()\n\n\n# Taken from Pydantic v1 as is\n# TODO: pv2 should this return strings instead?\ndef decimal_encoder(dec_value: Decimal) -> Union[int, float]:\n    \"\"\"\n    Encodes a Decimal as int of there's no exponent, otherwise float\n\n    This is useful when we use ConstrainedDecimal to represent Numeric(x,0)\n    where a integer (but not int typed) is used. Encoding this as a float\n    results in failed round-tripping between encode and parse.\n    Our Id type is a prime example of this.\n\n    >>> decimal_encoder(Decimal(\"1.0\"))\n    1.0\n\n    >>> decimal_encoder(Decimal(\"1\"))\n    1\n    \"\"\"\n    if dec_value.as_tuple().exponent >= 0:  # type: ignore[operator]\n        return int(dec_value)\n    else:\n        return float(dec_value)\n\n\nENCODERS_BY_TYPE: Dict[Type[Any], Callable[[Any], Any]] = {\n    bytes: lambda o: o.decode(),\n    Color: str,\n    datetime.date: isoformat,\n    datetime.datetime: isoformat,\n    datetime.time: isoformat,\n    datetime.timedelta: lambda td: td.total_seconds(),\n    Decimal: decimal_encoder,\n    Enum: lambda o: o.value,\n    frozenset: list,\n    deque: list,\n    GeneratorType: list,\n    IPv4Address: str,\n    IPv4Interface: str,\n    IPv4Network: str,\n    IPv6Address: str,\n    IPv6Interface: str,\n    IPv6Network: str,\n    NameEmail: str,\n    Path: str,\n    Pattern: lambda o: o.pattern,\n    SecretBytes: str,\n    SecretStr: str,\n    set: list,\n    UUID: str,\n    Url: str,\n    AnyUrl: str,\n}\n\n\ndef generate_encoders_by_class_tuples(\n    type_encoder_map: Dict[Any, Callable[[Any], Any]]\n) -> Dict[Callable[[Any], Any], Tuple[Any, ...]]:\n    encoders_by_class_tuples: Dict[Callable[[Any], Any], Tuple[Any, ...]] = defaultdict(\n        tuple\n    )\n    for type_, encoder in type_encoder_map.items():\n        encoders_by_class_tuples[encoder] += (type_,)\n    return encoders_by_class_tuples\n\n\nencoders_by_class_tuples = generate_encoders_by_class_tuples(ENCODERS_BY_TYPE)\n\n\ndef jsonable_encoder(\n    obj: Any,\n    include: Optional[IncEx] = None,\n    exclude: Optional[IncEx] = None,\n    by_alias: bool = True,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    custom_encoder: Optional[Dict[Any, Callable[[Any], Any]]] = None,\n    sqlalchemy_safe: bool = True,\n) -> Any:\n    custom_encoder = custom_encoder or {}\n    if custom_encoder:\n        if type(obj) in custom_encoder:\n            return custom_encoder[type(obj)](obj)\n        else:\n            for encoder_type, encoder_instance in custom_encoder.items():\n                if isinstance(obj, encoder_type):\n                    return encoder_instance(obj)\n    if include is not None and not isinstance(include, (set, dict)):\n        include = set(include)\n    if exclude is not None and not isinstance(exclude, (set, dict)):\n        exclude = set(exclude)\n    if isinstance(obj, BaseModel):\n        # TODO: remove when deprecating Pydantic v1\n        encoders: Dict[Any, Any] = {}\n        if not PYDANTIC_V2:\n            encoders = getattr(obj.__config__, \"json_encoders\", {})  # type: ignore[attr-defined]\n            if custom_encoder:\n                encoders.update(custom_encoder)\n        obj_dict = _model_dump(\n            obj,\n            mode=\"json\",\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_none=exclude_none,\n            exclude_defaults=exclude_defaults,\n        )\n        if \"__root__\" in obj_dict:\n            obj_dict = obj_dict[\"__root__\"]\n        return jsonable_encoder(\n            obj_dict,\n            exclude_none=exclude_none,\n            exclude_defaults=exclude_defaults,\n            # TODO: remove when deprecating Pydantic v1\n            custom_encoder=encoders,\n            sqlalchemy_safe=sqlalchemy_safe,\n        )\n    if dataclasses.is_dataclass(obj):\n        obj_dict = dataclasses.asdict(obj)\n        return jsonable_encoder(\n            obj_dict,\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            custom_encoder=custom_encoder,\n            sqlalchemy_safe=sqlalchemy_safe,\n        )\n    if isinstance(obj, Enum):\n        return obj.value\n    if isinstance(obj, PurePath):\n        return str(obj)\n    if isinstance(obj, (str, int, float, type(None))):\n        return obj\n    if isinstance(obj, dict):\n        encoded_dict = {}\n        allowed_keys = set(obj.keys())\n        if include is not None:\n            allowed_keys &= set(include)\n        if exclude is not None:\n            allowed_keys -= set(exclude)\n        for key, value in obj.items():\n            if (\n                (\n                    not sqlalchemy_safe\n                    or (not isinstance(key, str))\n                    or (not key.startswith(\"_sa\"))\n                )\n                and (value is not None or not exclude_none)\n                and key in allowed_keys\n            ):\n                encoded_key = jsonable_encoder(\n                    key,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_none=exclude_none,\n                    custom_encoder=custom_encoder,\n                    sqlalchemy_safe=sqlalchemy_safe,\n                )\n                encoded_value = jsonable_encoder(\n                    value,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_none=exclude_none,\n                    custom_encoder=custom_encoder,\n                    sqlalchemy_safe=sqlalchemy_safe,\n                )\n                encoded_dict[encoded_key] = encoded_value\n        return encoded_dict\n    if isinstance(obj, (list, set, frozenset, GeneratorType, tuple, deque)):\n        encoded_list = []\n        for item in obj:\n            encoded_list.append(\n                jsonable_encoder(\n                    item,\n                    include=include,\n                    exclude=exclude,\n                    by_alias=by_alias,\n                    exclude_unset=exclude_unset,\n                    exclude_defaults=exclude_defaults,\n                    exclude_none=exclude_none,\n                    custom_encoder=custom_encoder,\n                    sqlalchemy_safe=sqlalchemy_safe,\n                )\n            )\n        return encoded_list\n\n    if type(obj) in ENCODERS_BY_TYPE:\n        return ENCODERS_BY_TYPE[type(obj)](obj)\n    for encoder, classes_tuple in encoders_by_class_tuples.items():\n        if isinstance(obj, classes_tuple):\n            return encoder(obj)\n\n    try:\n        data = dict(obj)\n    except Exception as e:\n        errors: List[Exception] = []\n        errors.append(e)\n        try:\n            data = vars(obj)\n        except Exception as e:\n            errors.append(e)\n            raise ValueError(errors) from e\n    return jsonable_encoder(\n        data,\n        include=include,\n        exclude=exclude,\n        by_alias=by_alias,\n        exclude_unset=exclude_unset,\n        exclude_defaults=exclude_defaults,\n        exclude_none=exclude_none,\n        custom_encoder=custom_encoder,\n        sqlalchemy_safe=sqlalchemy_safe,\n    )\n", 249], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py": ["\"\"\"Implementation of JSONEncoder\n\"\"\"\nimport re\n\ntry:\n    from _json import encode_basestring_ascii as c_encode_basestring_ascii\nexcept ImportError:\n    c_encode_basestring_ascii = None\ntry:\n    from _json import encode_basestring as c_encode_basestring\nexcept ImportError:\n    c_encode_basestring = None\ntry:\n    from _json import make_encoder as c_make_encoder\nexcept ImportError:\n    c_make_encoder = None\n\nESCAPE = re.compile(r'[\\x00-\\x1f\\\\\"\\b\\f\\n\\r\\t]')\nESCAPE_ASCII = re.compile(r'([\\\\\"]|[^\\ -~])')\nHAS_UTF8 = re.compile(b'[\\x80-\\xff]')\nESCAPE_DCT = {\n    '\\\\': '\\\\\\\\',\n    '\"': '\\\\\"',\n    '\\b': '\\\\b',\n    '\\f': '\\\\f',\n    '\\n': '\\\\n',\n    '\\r': '\\\\r',\n    '\\t': '\\\\t',\n}\nfor i in range(0x20):\n    ESCAPE_DCT.setdefault(chr(i), '\\\\u{0:04x}'.format(i))\n    #ESCAPE_DCT.setdefault(chr(i), '\\\\u%04x' % (i,))\n\nINFINITY = float('inf')\n\ndef py_encode_basestring(s):\n    \"\"\"Return a JSON representation of a Python string\n\n    \"\"\"\n    def replace(match):\n        return ESCAPE_DCT[match.group(0)]\n    return '\"' + ESCAPE.sub(replace, s) + '\"'\n\n\nencode_basestring = (c_encode_basestring or py_encode_basestring)\n\n\ndef py_encode_basestring_ascii(s):\n    \"\"\"Return an ASCII-only JSON representation of a Python string\n\n    \"\"\"\n    def replace(match):\n        s = match.group(0)\n        try:\n            return ESCAPE_DCT[s]\n        except KeyError:\n            n = ord(s)\n            if n < 0x10000:\n                return '\\\\u{0:04x}'.format(n)\n                #return '\\\\u%04x' % (n,)\n            else:\n                # surrogate pair\n                n -= 0x10000\n                s1 = 0xd800 | ((n >> 10) & 0x3ff)\n                s2 = 0xdc00 | (n & 0x3ff)\n                return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\n    return '\"' + ESCAPE_ASCII.sub(replace, s) + '\"'\n\n\nencode_basestring_ascii = (\n    c_encode_basestring_ascii or py_encode_basestring_ascii)\n\nclass JSONEncoder(object):\n    \"\"\"Extensible JSON <http://json.org> encoder for Python data structures.\n\n    Supports the following objects and types by default:\n\n    +-------------------+---------------+\n    | Python            | JSON          |\n    +===================+===============+\n    | dict              | object        |\n    +-------------------+---------------+\n    | list, tuple       | array         |\n    +-------------------+---------------+\n    | str               | string        |\n    +-------------------+---------------+\n    | int, float        | number        |\n    +-------------------+---------------+\n    | True              | true          |\n    +-------------------+---------------+\n    | False             | false         |\n    +-------------------+---------------+\n    | None              | null          |\n    +-------------------+---------------+\n\n    To extend this to recognize other objects, subclass and implement a\n    ``.default()`` method with another method that returns a serializable\n    object for ``o`` if possible, otherwise it should call the superclass\n    implementation (to raise ``TypeError``).\n\n    \"\"\"\n    item_separator = ', '\n    key_separator = ': '\n    def __init__(self, *, skipkeys=False, ensure_ascii=True,\n            check_circular=True, allow_nan=True, sort_keys=False,\n            indent=None, separators=None, default=None):\n        \"\"\"Constructor for JSONEncoder, with sensible defaults.\n\n        If skipkeys is false, then it is a TypeError to attempt\n        encoding of keys that are not str, int, float or None.  If\n        skipkeys is True, such items are simply skipped.\n\n        If ensure_ascii is true, the output is guaranteed to be str\n        objects with all incoming non-ASCII characters escaped.  If\n        ensure_ascii is false, the output can contain non-ASCII characters.\n\n        If check_circular is true, then lists, dicts, and custom encoded\n        objects will be checked for circular references during encoding to\n        prevent an infinite recursion (which would cause an RecursionError).\n        Otherwise, no such check takes place.\n\n        If allow_nan is true, then NaN, Infinity, and -Infinity will be\n        encoded as such.  This behavior is not JSON specification compliant,\n        but is consistent with most JavaScript based encoders and decoders.\n        Otherwise, it will be a ValueError to encode such floats.\n\n        If sort_keys is true, then the output of dictionaries will be\n        sorted by key; this is useful for regression tests to ensure\n        that JSON serializations can be compared on a day-to-day basis.\n\n        If indent is a non-negative integer, then JSON array\n        elements and object members will be pretty-printed with that\n        indent level.  An indent level of 0 will only insert newlines.\n        None is the most compact representation.\n\n        If specified, separators should be an (item_separator, key_separator)\n        tuple.  The default is (', ', ': ') if *indent* is ``None`` and\n        (',', ': ') otherwise.  To get the most compact JSON representation,\n        you should specify (',', ':') to eliminate whitespace.\n\n        If specified, default is a function that gets called for objects\n        that can't otherwise be serialized.  It should return a JSON encodable\n        version of the object or raise a ``TypeError``.\n\n        \"\"\"\n\n        self.skipkeys = skipkeys\n        self.ensure_ascii = ensure_ascii\n        self.check_circular = check_circular\n        self.allow_nan = allow_nan\n        self.sort_keys = sort_keys\n        self.indent = indent\n        if separators is not None:\n            self.item_separator, self.key_separator = separators\n        elif indent is not None:\n            self.item_separator = ','\n        if default is not None:\n            self.default = default\n\n    def default(self, o):\n        \"\"\"Implement this method in a subclass such that it returns\n        a serializable object for ``o``, or calls the base implementation\n        (to raise a ``TypeError``).\n\n        For example, to support arbitrary iterators, you could\n        implement default like this::\n\n            def default(self, o):\n                try:\n                    iterable = iter(o)\n                except TypeError:\n                    pass\n                else:\n                    return list(iterable)\n                # Let the base class default method raise the TypeError\n                return JSONEncoder.default(self, o)\n\n        \"\"\"\n        raise TypeError(f'Object of type {o.__class__.__name__} '\n                        f'is not JSON serializable')\n\n    def encode(self, o):\n        \"\"\"Return a JSON string representation of a Python data structure.\n\n        >>> from json.encoder import JSONEncoder\n        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n        '{\"foo\": [\"bar\", \"baz\"]}'\n\n        \"\"\"\n        # This is for extremely simple cases and benchmarks.\n        if isinstance(o, str):\n            if self.ensure_ascii:\n                return encode_basestring_ascii(o)\n            else:\n                return encode_basestring(o)\n        # This doesn't pass the iterator directly to ''.join() because the\n        # exceptions aren't as detailed.  The list call should be roughly\n        # equivalent to the PySequence_Fast that ''.join() would do.\n        chunks = self.iterencode(o, _one_shot=True)\n        if not isinstance(chunks, (list, tuple)):\n            chunks = list(chunks)\n        return ''.join(chunks)\n\n    def iterencode(self, o, _one_shot=False):\n        \"\"\"Encode the given object and yield each string\n        representation as available.\n\n        For example::\n\n            for chunk in JSONEncoder().iterencode(bigobject):\n                mysocket.write(chunk)\n\n        \"\"\"\n        if self.check_circular:\n            markers = {}\n        else:\n            markers = None\n        if self.ensure_ascii:\n            _encoder = encode_basestring_ascii\n        else:\n            _encoder = encode_basestring\n\n        def floatstr(o, allow_nan=self.allow_nan,\n                _repr=float.__repr__, _inf=INFINITY, _neginf=-INFINITY):\n            # Check for specials.  Note that this type of test is processor\n            # and/or platform-specific, so do tests which don't depend on the\n            # internals.\n\n            if o != o:\n                text = 'NaN'\n            elif o == _inf:\n                text = 'Infinity'\n            elif o == _neginf:\n                text = '-Infinity'\n            else:\n                return _repr(o)\n\n            if not allow_nan:\n                raise ValueError(\n                    \"Out of range float values are not JSON compliant: \" +\n                    repr(o))\n\n            return text\n\n\n        if (_one_shot and c_make_encoder is not None\n                and self.indent is None):\n            _iterencode = c_make_encoder(\n                markers, self.default, _encoder, self.indent,\n                self.key_separator, self.item_separator, self.sort_keys,\n                self.skipkeys, self.allow_nan)\n        else:\n            _iterencode = _make_iterencode(\n                markers, self.default, _encoder, self.indent, floatstr,\n                self.key_separator, self.item_separator, self.sort_keys,\n                self.skipkeys, _one_shot)\n        return _iterencode(o, 0)\n\ndef _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n        _key_separator, _item_separator, _sort_keys, _skipkeys, _one_shot,\n        ## HACK: hand-optimized bytecode; turn globals into locals\n        ValueError=ValueError,\n        dict=dict,\n        float=float,\n        id=id,\n        int=int,\n        isinstance=isinstance,\n        list=list,\n        str=str,\n        tuple=tuple,\n        _intstr=int.__repr__,\n    ):\n\n    if _indent is not None and not isinstance(_indent, str):\n        _indent = ' ' * _indent\n\n    def _iterencode_list(lst, _current_indent_level):\n        if not lst:\n            yield '[]'\n            return\n        if markers is not None:\n            markerid = id(lst)\n            if markerid in markers:\n                raise ValueError(\"Circular reference detected\")\n            markers[markerid] = lst\n        buf = '['\n        if _indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + _indent * _current_indent_level\n            separator = _item_separator + newline_indent\n            buf += newline_indent\n        else:\n            newline_indent = None\n            separator = _item_separator\n        first = True\n        for value in lst:\n            if first:\n                first = False\n            else:\n                buf = separator\n            if isinstance(value, str):\n                yield buf + _encoder(value)\n            elif value is None:\n                yield buf + 'null'\n            elif value is True:\n                yield buf + 'true'\n            elif value is False:\n                yield buf + 'false'\n            elif isinstance(value, int):\n                # Subclasses of int/float may override __repr__, but we still\n                # want to encode them as integers/floats in JSON. One example\n                # within the standard library is IntEnum.\n                yield buf + _intstr(value)\n            elif isinstance(value, float):\n                # see comment above for int\n                yield buf + _floatstr(value)\n            else:\n                yield buf\n                if isinstance(value, (list, tuple)):\n                    chunks = _iterencode_list(value, _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = _iterencode_dict(value, _current_indent_level)\n                else:\n                    chunks = _iterencode(value, _current_indent_level)\n                yield from chunks\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + _indent * _current_indent_level\n        yield ']'\n        if markers is not None:\n            del markers[markerid]\n\n    def _iterencode_dict(dct, _current_indent_level):\n        if not dct:\n            yield '{}'\n            return\n        if markers is not None:\n            markerid = id(dct)\n            if markerid in markers:\n                raise ValueError(\"Circular reference detected\")\n            markers[markerid] = dct\n        yield '{'\n        if _indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + _indent * _current_indent_level\n            item_separator = _item_separator + newline_indent\n            yield newline_indent\n        else:\n            newline_indent = None\n            item_separator = _item_separator\n        first = True\n        if _sort_keys:\n            items = sorted(dct.items())\n        else:\n            items = dct.items()\n        for key, value in items:\n            if isinstance(key, str):\n                pass\n            # JavaScript is weakly typed for these, so it makes sense to\n            # also allow them.  Many encoders seem to do something like this.\n            elif isinstance(key, float):\n                # see comment for int/float in _make_iterencode\n                key = _floatstr(key)\n            elif key is True:\n                key = 'true'\n            elif key is False:\n                key = 'false'\n            elif key is None:\n                key = 'null'\n            elif isinstance(key, int):\n                # see comment for int/float in _make_iterencode\n                key = _intstr(key)\n            elif _skipkeys:\n                continue\n            else:\n                raise TypeError(f'keys must be str, int, float, bool or None, '\n                                f'not {key.__class__.__name__}')\n            if first:\n                first = False\n            else:\n                yield item_separator\n            yield _encoder(key)\n            yield _key_separator\n            if isinstance(value, str):\n                yield _encoder(value)\n            elif value is None:\n                yield 'null'\n            elif value is True:\n                yield 'true'\n            elif value is False:\n                yield 'false'\n            elif isinstance(value, int):\n                # see comment for int/float in _make_iterencode\n                yield _intstr(value)\n            elif isinstance(value, float):\n                # see comment for int/float in _make_iterencode\n                yield _floatstr(value)\n            else:\n                if isinstance(value, (list, tuple)):\n                    chunks = _iterencode_list(value, _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = _iterencode_dict(value, _current_indent_level)\n                else:\n                    chunks = _iterencode(value, _current_indent_level)\n                yield from chunks\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + _indent * _current_indent_level\n        yield '}'\n        if markers is not None:\n            del markers[markerid]\n\n    def _iterencode(o, _current_indent_level):\n        if isinstance(o, str):\n            yield _encoder(o)\n        elif o is None:\n            yield 'null'\n        elif o is True:\n            yield 'true'\n        elif o is False:\n            yield 'false'\n        elif isinstance(o, int):\n            # see comment for int/float in _make_iterencode\n            yield _intstr(o)\n        elif isinstance(o, float):\n            # see comment for int/float in _make_iterencode\n            yield _floatstr(o)\n        elif isinstance(o, (list, tuple)):\n            yield from _iterencode_list(o, _current_indent_level)\n        elif isinstance(o, dict):\n            yield from _iterencode_dict(o, _current_indent_level)\n        else:\n            if markers is not None:\n                markerid = id(o)\n                if markerid in markers:\n                    raise ValueError(\"Circular reference detected\")\n                markers[markerid] = o\n            o = _default(o)\n            yield from _iterencode(o, _current_indent_level)\n            if markers is not None:\n                del markers[markerid]\n    return _iterencode\n", 442], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/__init__.py": ["r\"\"\"JSON (JavaScript Object Notation) <http://json.org> is a subset of\nJavaScript syntax (ECMA-262 3rd edition) used as a lightweight data\ninterchange format.\n\n:mod:`json` exposes an API familiar to users of the standard library\n:mod:`marshal` and :mod:`pickle` modules.  It is derived from a\nversion of the externally maintained simplejson library.\n\nEncoding basic Python object hierarchies::\n\n    >>> import json\n    >>> json.dumps(['foo', {'bar': ('baz', None, 1.0, 2)}])\n    '[\"foo\", {\"bar\": [\"baz\", null, 1.0, 2]}]'\n    >>> print(json.dumps(\"\\\"foo\\bar\"))\n    \"\\\"foo\\bar\"\n    >>> print(json.dumps('\\u1234'))\n    \"\\u1234\"\n    >>> print(json.dumps('\\\\'))\n    \"\\\\\"\n    >>> print(json.dumps({\"c\": 0, \"b\": 0, \"a\": 0}, sort_keys=True))\n    {\"a\": 0, \"b\": 0, \"c\": 0}\n    >>> from io import StringIO\n    >>> io = StringIO()\n    >>> json.dump(['streaming API'], io)\n    >>> io.getvalue()\n    '[\"streaming API\"]'\n\nCompact encoding::\n\n    >>> import json\n    >>> mydict = {'4': 5, '6': 7}\n    >>> json.dumps([1,2,3,mydict], separators=(',', ':'))\n    '[1,2,3,{\"4\":5,\"6\":7}]'\n\nPretty printing::\n\n    >>> import json\n    >>> print(json.dumps({'4': 5, '6': 7}, sort_keys=True, indent=4))\n    {\n        \"4\": 5,\n        \"6\": 7\n    }\n\nDecoding JSON::\n\n    >>> import json\n    >>> obj = ['foo', {'bar': ['baz', None, 1.0, 2]}]\n    >>> json.loads('[\"foo\", {\"bar\":[\"baz\", null, 1.0, 2]}]') == obj\n    True\n    >>> json.loads('\"\\\\\"foo\\\\bar\"') == '\"foo\\x08ar'\n    True\n    >>> from io import StringIO\n    >>> io = StringIO('[\"streaming API\"]')\n    >>> json.load(io)[0] == 'streaming API'\n    True\n\nSpecializing JSON object decoding::\n\n    >>> import json\n    >>> def as_complex(dct):\n    ...     if '__complex__' in dct:\n    ...         return complex(dct['real'], dct['imag'])\n    ...     return dct\n    ...\n    >>> json.loads('{\"__complex__\": true, \"real\": 1, \"imag\": 2}',\n    ...     object_hook=as_complex)\n    (1+2j)\n    >>> from decimal import Decimal\n    >>> json.loads('1.1', parse_float=Decimal) == Decimal('1.1')\n    True\n\nSpecializing JSON object encoding::\n\n    >>> import json\n    >>> def encode_complex(obj):\n    ...     if isinstance(obj, complex):\n    ...         return [obj.real, obj.imag]\n    ...     raise TypeError(f'Object of type {obj.__class__.__name__} '\n    ...                     f'is not JSON serializable')\n    ...\n    >>> json.dumps(2 + 1j, default=encode_complex)\n    '[2.0, 1.0]'\n    >>> json.JSONEncoder(default=encode_complex).encode(2 + 1j)\n    '[2.0, 1.0]'\n    >>> ''.join(json.JSONEncoder(default=encode_complex).iterencode(2 + 1j))\n    '[2.0, 1.0]'\n\n\nUsing json.tool from the shell to validate and pretty-print::\n\n    $ echo '{\"json\":\"obj\"}' | python -m json.tool\n    {\n        \"json\": \"obj\"\n    }\n    $ echo '{ 1.2:3.4}' | python -m json.tool\n    Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n\"\"\"\n__version__ = '2.0.9'\n__all__ = [\n    'dump', 'dumps', 'load', 'loads',\n    'JSONDecoder', 'JSONDecodeError', 'JSONEncoder',\n]\n\n__author__ = 'Bob Ippolito <bob@redivi.com>'\n\nfrom .decoder import JSONDecoder, JSONDecodeError\nfrom .encoder import JSONEncoder\nimport codecs\n\n_default_encoder = JSONEncoder(\n    skipkeys=False,\n    ensure_ascii=True,\n    check_circular=True,\n    allow_nan=True,\n    indent=None,\n    separators=None,\n    default=None,\n)\n\ndef dump(obj, fp, *, skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, cls=None, indent=None, separators=None,\n        default=None, sort_keys=False, **kw):\n    \"\"\"Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n    ``.write()``-supporting file-like object).\n\n    If ``skipkeys`` is true then ``dict`` keys that are not basic types\n    (``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped\n    instead of raising a ``TypeError``.\n\n    If ``ensure_ascii`` is false, then the strings written to ``fp`` can\n    contain non-ASCII characters if they appear in strings contained in\n    ``obj``. Otherwise, all such characters are escaped in JSON strings.\n\n    If ``check_circular`` is false, then the circular reference check\n    for container types will be skipped and a circular reference will\n    result in an ``RecursionError`` (or worse).\n\n    If ``allow_nan`` is false, then it will be a ``ValueError`` to\n    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``)\n    in strict compliance of the JSON specification, instead of using the\n    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\n    If ``indent`` is a non-negative integer, then JSON array elements and\n    object members will be pretty-printed with that indent level. An indent\n    level of 0 will only insert newlines. ``None`` is the most compact\n    representation.\n\n    If specified, ``separators`` should be an ``(item_separator, key_separator)``\n    tuple.  The default is ``(', ', ': ')`` if *indent* is ``None`` and\n    ``(',', ': ')`` otherwise.  To get the most compact JSON representation,\n    you should specify ``(',', ':')`` to eliminate whitespace.\n\n    ``default(obj)`` is a function that should return a serializable version\n    of obj or raise TypeError. The default simply raises TypeError.\n\n    If *sort_keys* is true (default: ``False``), then the output of\n    dictionaries will be sorted by key.\n\n    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n    ``.default()`` method to serialize additional types), specify it with\n    the ``cls`` kwarg; otherwise ``JSONEncoder`` is used.\n\n    \"\"\"\n    # cached encoder\n    if (not skipkeys and ensure_ascii and\n        check_circular and allow_nan and\n        cls is None and indent is None and separators is None and\n        default is None and not sort_keys and not kw):\n        iterable = _default_encoder.iterencode(obj)\n    else:\n        if cls is None:\n            cls = JSONEncoder\n        iterable = cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n            check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n            separators=separators,\n            default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n    # could accelerate with writelines in some versions of Python, at\n    # a debuggability cost\n    for chunk in iterable:\n        fp.write(chunk)\n\n\ndef dumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, cls=None, indent=None, separators=None,\n        default=None, sort_keys=False, **kw):\n    \"\"\"Serialize ``obj`` to a JSON formatted ``str``.\n\n    If ``skipkeys`` is true then ``dict`` keys that are not basic types\n    (``str``, ``int``, ``float``, ``bool``, ``None``) will be skipped\n    instead of raising a ``TypeError``.\n\n    If ``ensure_ascii`` is false, then the return value can contain non-ASCII\n    characters if they appear in strings contained in ``obj``. Otherwise, all\n    such characters are escaped in JSON strings.\n\n    If ``check_circular`` is false, then the circular reference check\n    for container types will be skipped and a circular reference will\n    result in an ``RecursionError`` (or worse).\n\n    If ``allow_nan`` is false, then it will be a ``ValueError`` to\n    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in\n    strict compliance of the JSON specification, instead of using the\n    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\n    If ``indent`` is a non-negative integer, then JSON array elements and\n    object members will be pretty-printed with that indent level. An indent\n    level of 0 will only insert newlines. ``None`` is the most compact\n    representation.\n\n    If specified, ``separators`` should be an ``(item_separator, key_separator)``\n    tuple.  The default is ``(', ', ': ')`` if *indent* is ``None`` and\n    ``(',', ': ')`` otherwise.  To get the most compact JSON representation,\n    you should specify ``(',', ':')`` to eliminate whitespace.\n\n    ``default(obj)`` is a function that should return a serializable version\n    of obj or raise TypeError. The default simply raises TypeError.\n\n    If *sort_keys* is true (default: ``False``), then the output of\n    dictionaries will be sorted by key.\n\n    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n    ``.default()`` method to serialize additional types), specify it with\n    the ``cls`` kwarg; otherwise ``JSONEncoder`` is used.\n\n    \"\"\"\n    # cached encoder\n    if (not skipkeys and ensure_ascii and\n        check_circular and allow_nan and\n        cls is None and indent is None and separators is None and\n        default is None and not sort_keys and not kw):\n        return _default_encoder.encode(obj)\n    if cls is None:\n        cls = JSONEncoder\n    return cls(\n        skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n        check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n        separators=separators, default=default, sort_keys=sort_keys,\n        **kw).encode(obj)\n\n\n_default_decoder = JSONDecoder(object_hook=None, object_pairs_hook=None)\n\n\ndef detect_encoding(b):\n    bstartswith = b.startswith\n    if bstartswith((codecs.BOM_UTF32_BE, codecs.BOM_UTF32_LE)):\n        return 'utf-32'\n    if bstartswith((codecs.BOM_UTF16_BE, codecs.BOM_UTF16_LE)):\n        return 'utf-16'\n    if bstartswith(codecs.BOM_UTF8):\n        return 'utf-8-sig'\n\n    if len(b) >= 4:\n        if not b[0]:\n            # 00 00 -- -- - utf-32-be\n            # 00 XX -- -- - utf-16-be\n            return 'utf-16-be' if b[1] else 'utf-32-be'\n        if not b[1]:\n            # XX 00 00 00 - utf-32-le\n            # XX 00 00 XX - utf-16-le\n            # XX 00 XX -- - utf-16-le\n            return 'utf-16-le' if b[2] or b[3] else 'utf-32-le'\n    elif len(b) == 2:\n        if not b[0]:\n            # 00 XX - utf-16-be\n            return 'utf-16-be'\n        if not b[1]:\n            # XX 00 - utf-16-le\n            return 'utf-16-le'\n    # default\n    return 'utf-8'\n\n\ndef load(fp, *, cls=None, object_hook=None, parse_float=None,\n        parse_int=None, parse_constant=None, object_pairs_hook=None, **kw):\n    \"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\n    a JSON document) to a Python object.\n\n    ``object_hook`` is an optional function that will be called with the\n    result of any object literal decode (a ``dict``). The return value of\n    ``object_hook`` will be used instead of the ``dict``. This feature\n    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n    ``object_pairs_hook`` is an optional function that will be called with the\n    result of any object literal decoded with an ordered list of pairs.  The\n    return value of ``object_pairs_hook`` will be used instead of the ``dict``.\n    This feature can be used to implement custom decoders.  If ``object_hook``\n    is also defined, the ``object_pairs_hook`` takes priority.\n\n    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n    kwarg; otherwise ``JSONDecoder`` is used.\n    \"\"\"\n    return loads(fp.read(),\n        cls=cls, object_hook=object_hook,\n        parse_float=parse_float, parse_int=parse_int,\n        parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\n\ndef loads(s, *, cls=None, object_hook=None, parse_float=None,\n        parse_int=None, parse_constant=None, object_pairs_hook=None, **kw):\n    \"\"\"Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance\n    containing a JSON document) to a Python object.\n\n    ``object_hook`` is an optional function that will be called with the\n    result of any object literal decode (a ``dict``). The return value of\n    ``object_hook`` will be used instead of the ``dict``. This feature\n    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n    ``object_pairs_hook`` is an optional function that will be called with the\n    result of any object literal decoded with an ordered list of pairs.  The\n    return value of ``object_pairs_hook`` will be used instead of the ``dict``.\n    This feature can be used to implement custom decoders.  If ``object_hook``\n    is also defined, the ``object_pairs_hook`` takes priority.\n\n    ``parse_float``, if specified, will be called with the string\n    of every JSON float to be decoded. By default this is equivalent to\n    float(num_str). This can be used to use another datatype or parser\n    for JSON floats (e.g. decimal.Decimal).\n\n    ``parse_int``, if specified, will be called with the string\n    of every JSON int to be decoded. By default this is equivalent to\n    int(num_str). This can be used to use another datatype or parser\n    for JSON integers (e.g. float).\n\n    ``parse_constant``, if specified, will be called with one of the\n    following strings: -Infinity, Infinity, NaN.\n    This can be used to raise an exception if invalid JSON numbers\n    are encountered.\n\n    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n    kwarg; otherwise ``JSONDecoder`` is used.\n    \"\"\"\n    if isinstance(s, str):\n        if s.startswith('\\ufeff'):\n            raise JSONDecodeError(\"Unexpected UTF-8 BOM (decode using utf-8-sig)\",\n                                  s, 0)\n    else:\n        if not isinstance(s, (bytes, bytearray)):\n            raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n                            f'not {s.__class__.__name__}')\n        s = s.decode(detect_encoding(s), 'surrogatepass')\n\n    if (cls is None and object_hook is None and\n            parse_int is None and parse_float is None and\n            parse_constant is None and object_pairs_hook is None and not kw):\n        return _default_decoder.decode(s)\n    if cls is None:\n        cls = JSONDecoder\n    if object_hook is not None:\n        kw['object_hook'] = object_hook\n    if object_pairs_hook is not None:\n        kw['object_pairs_hook'] = object_pairs_hook\n    if parse_float is not None:\n        kw['parse_float'] = parse_float\n    if parse_int is not None:\n        kw['parse_int'] = parse_int\n    if parse_constant is not None:\n        kw['parse_constant'] = parse_constant\n    return cls(**kw).decode(s)\n", 359], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/utils.py": ["import re\nimport warnings\nfrom dataclasses import is_dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    MutableMapping,\n    Optional,\n    Set,\n    Type,\n    Union,\n    cast,\n)\nfrom weakref import WeakKeyDictionary\n\nimport fastapi\nfrom fastapi._compat import (\n    PYDANTIC_V2,\n    BaseConfig,\n    ModelField,\n    PydanticSchemaGenerationError,\n    Undefined,\n    UndefinedType,\n    Validator,\n    lenient_issubclass,\n)\nfrom fastapi.datastructures import DefaultPlaceholder, DefaultType\nfrom pydantic import BaseModel, create_model\nfrom pydantic.fields import FieldInfo\nfrom typing_extensions import Literal\n\nif TYPE_CHECKING:  # pragma: nocover\n    from .routing import APIRoute\n\n# Cache for `create_cloned_field`\n_CLONED_TYPES_CACHE: MutableMapping[\n    Type[BaseModel], Type[BaseModel]\n] = WeakKeyDictionary()\n\n\ndef is_body_allowed_for_status_code(status_code: Union[int, str, None]) -> bool:\n    if status_code is None:\n        return True\n    # Ref: https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.1.0.md#patterned-fields-1\n    if status_code in {\n        \"default\",\n        \"1XX\",\n        \"2XX\",\n        \"3XX\",\n        \"4XX\",\n        \"5XX\",\n    }:\n        return True\n    current_status_code = int(status_code)\n    return not (current_status_code < 200 or current_status_code in {204, 304})\n\n\ndef get_path_param_names(path: str) -> Set[str]:\n    return set(re.findall(\"{(.*?)}\", path))\n\n\ndef create_response_field(\n    name: str,\n    type_: Type[Any],\n    class_validators: Optional[Dict[str, Validator]] = None,\n    default: Optional[Any] = Undefined,\n    required: Union[bool, UndefinedType] = Undefined,\n    model_config: Type[BaseConfig] = BaseConfig,\n    field_info: Optional[FieldInfo] = None,\n    alias: Optional[str] = None,\n    mode: Literal[\"validation\", \"serialization\"] = \"validation\",\n) -> ModelField:\n    \"\"\"\n    Create a new response field. Raises if type_ is invalid.\n    \"\"\"\n    class_validators = class_validators or {}\n    if PYDANTIC_V2:\n        field_info = field_info or FieldInfo(\n            annotation=type_, default=default, alias=alias\n        )\n    else:\n        field_info = field_info or FieldInfo()\n    kwargs = {\"name\": name, \"field_info\": field_info}\n    if PYDANTIC_V2:\n        kwargs.update({\"mode\": mode})\n    else:\n        kwargs.update(\n            {\n                \"type_\": type_,\n                \"class_validators\": class_validators,\n                \"default\": default,\n                \"required\": required,\n                \"model_config\": model_config,\n                \"alias\": alias,\n            }\n        )\n    try:\n        return ModelField(**kwargs)  # type: ignore[arg-type]\n    except (RuntimeError, PydanticSchemaGenerationError):\n        raise fastapi.exceptions.FastAPIError(\n            \"Invalid args for response field! Hint: \"\n            f\"check that {type_} is a valid Pydantic field type. \"\n            \"If you are using a return type annotation that is not a valid Pydantic \"\n            \"field (e.g. Union[Response, dict, None]) you can disable generating the \"\n            \"response model from the type annotation with the path operation decorator \"\n            \"parameter response_model=None. Read more: \"\n            \"https://fastapi.tiangolo.com/tutorial/response-model/\"\n        ) from None\n\n\ndef create_cloned_field(\n    field: ModelField,\n    *,\n    cloned_types: Optional[MutableMapping[Type[BaseModel], Type[BaseModel]]] = None,\n) -> ModelField:\n    if PYDANTIC_V2:\n        return field\n    # cloned_types caches already cloned types to support recursive models and improve\n    # performance by avoiding unecessary cloning\n    if cloned_types is None:\n        cloned_types = _CLONED_TYPES_CACHE\n\n    original_type = field.type_\n    if is_dataclass(original_type) and hasattr(original_type, \"__pydantic_model__\"):\n        original_type = original_type.__pydantic_model__\n    use_type = original_type\n    if lenient_issubclass(original_type, BaseModel):\n        original_type = cast(Type[BaseModel], original_type)\n        use_type = cloned_types.get(original_type)\n        if use_type is None:\n            use_type = create_model(original_type.__name__, __base__=original_type)\n            cloned_types[original_type] = use_type\n            for f in original_type.__fields__.values():\n                use_type.__fields__[f.name] = create_cloned_field(\n                    f, cloned_types=cloned_types\n                )\n    new_field = create_response_field(name=field.name, type_=use_type)\n    new_field.has_alias = field.has_alias  # type: ignore[attr-defined]\n    new_field.alias = field.alias  # type: ignore[misc]\n    new_field.class_validators = field.class_validators  # type: ignore[attr-defined]\n    new_field.default = field.default  # type: ignore[misc]\n    new_field.required = field.required  # type: ignore[misc]\n    new_field.model_config = field.model_config  # type: ignore[attr-defined]\n    new_field.field_info = field.field_info\n    new_field.allow_none = field.allow_none  # type: ignore[attr-defined]\n    new_field.validate_always = field.validate_always  # type: ignore[attr-defined]\n    if field.sub_fields:  # type: ignore[attr-defined]\n        new_field.sub_fields = [  # type: ignore[attr-defined]\n            create_cloned_field(sub_field, cloned_types=cloned_types)\n            for sub_field in field.sub_fields  # type: ignore[attr-defined]\n        ]\n    if field.key_field:  # type: ignore[attr-defined]\n        new_field.key_field = create_cloned_field(  # type: ignore[attr-defined]\n            field.key_field, cloned_types=cloned_types  # type: ignore[attr-defined]\n        )\n    new_field.validators = field.validators  # type: ignore[attr-defined]\n    new_field.pre_validators = field.pre_validators  # type: ignore[attr-defined]\n    new_field.post_validators = field.post_validators  # type: ignore[attr-defined]\n    new_field.parse_json = field.parse_json  # type: ignore[attr-defined]\n    new_field.shape = field.shape  # type: ignore[attr-defined]\n    new_field.populate_validators()  # type: ignore[attr-defined]\n    return new_field\n\n\ndef generate_operation_id_for_path(\n    *, name: str, path: str, method: str\n) -> str:  # pragma: nocover\n    warnings.warn(\n        \"fastapi.utils.generate_operation_id_for_path() was deprecated, \"\n        \"it is not used internally, and will be removed soon\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    operation_id = name + path\n    operation_id = re.sub(r\"\\W\", \"_\", operation_id)\n    operation_id = operation_id + \"_\" + method.lower()\n    return operation_id\n\n\ndef generate_unique_id(route: \"APIRoute\") -> str:\n    operation_id = route.name + route.path_format\n    operation_id = re.sub(r\"\\W\", \"_\", operation_id)\n    assert route.methods\n    operation_id = operation_id + \"_\" + list(route.methods)[0].lower()\n    return operation_id\n\n\ndef deep_dict_update(main_dict: Dict[Any, Any], update_dict: Dict[Any, Any]) -> None:\n    for key, value in update_dict.items():\n        if (\n            key in main_dict\n            and isinstance(main_dict[key], dict)\n            and isinstance(value, dict)\n        ):\n            deep_dict_update(main_dict[key], value)\n        elif (\n            key in main_dict\n            and isinstance(main_dict[key], list)\n            and isinstance(update_dict[key], list)\n        ):\n            main_dict[key] = main_dict[key] + update_dict[key]\n        else:\n            main_dict[key] = value\n\n\ndef get_value_or_default(\n    first_item: Union[DefaultPlaceholder, DefaultType],\n    *extra_items: Union[DefaultPlaceholder, DefaultType],\n) -> Union[DefaultPlaceholder, DefaultType]:\n    \"\"\"\n    Pass items or `DefaultPlaceholder`s by descending priority.\n\n    The first one to _not_ be a `DefaultPlaceholder` will be returned.\n\n    Otherwise, the first item (a `DefaultPlaceholder`) will be returned.\n    \"\"\"\n    items = (first_item,) + extra_items\n    for item in items:\n        if not isinstance(item, DefaultPlaceholder):\n            return item\n    return first_item\n\n\ndef match_pydantic_error_url(error_type: str) -> Any:\n    from dirty_equals import IsStr\n\n    return IsStr(regex=rf\"^https://errors\\.pydantic\\.dev/.*/v/{error_type}\")\n", 228], "/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/testclient.py": ["import contextlib\nimport inspect\nimport io\nimport json\nimport math\nimport queue\nimport sys\nimport typing\nimport warnings\nfrom concurrent.futures import Future\nfrom types import GeneratorType\nfrom urllib.parse import unquote, urljoin\n\nimport anyio\nimport anyio.from_thread\nimport httpx\nfrom anyio.streams.stapled import StapledObjectStream\n\nfrom starlette._utils import is_async_callable\nfrom starlette.types import ASGIApp, Message, Receive, Scope, Send\nfrom starlette.websockets import WebSocketDisconnect\n\nif sys.version_info >= (3, 8):  # pragma: no cover\n    from typing import TypedDict\nelse:  # pragma: no cover\n    from typing_extensions import TypedDict\n\n_PortalFactoryType = typing.Callable[\n    [], typing.ContextManager[anyio.abc.BlockingPortal]\n]\n\nASGIInstance = typing.Callable[[Receive, Send], typing.Awaitable[None]]\nASGI2App = typing.Callable[[Scope], ASGIInstance]\nASGI3App = typing.Callable[[Scope, Receive, Send], typing.Awaitable[None]]\n\n\n_RequestData = typing.Mapping[str, typing.Union[str, typing.Iterable[str]]]\n\n\ndef _is_asgi3(app: typing.Union[ASGI2App, ASGI3App]) -> bool:\n    if inspect.isclass(app):\n        return hasattr(app, \"__await__\")\n    return is_async_callable(app)\n\n\nclass _WrapASGI2:\n    \"\"\"\n    Provide an ASGI3 interface onto an ASGI2 app.\n    \"\"\"\n\n    def __init__(self, app: ASGI2App) -> None:\n        self.app = app\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        instance = self.app(scope)\n        await instance(receive, send)\n\n\nclass _AsyncBackend(TypedDict):\n    backend: str\n    backend_options: typing.Dict[str, typing.Any]\n\n\nclass _Upgrade(Exception):\n    def __init__(self, session: \"WebSocketTestSession\") -> None:\n        self.session = session\n\n\nclass WebSocketTestSession:\n    def __init__(\n        self,\n        app: ASGI3App,\n        scope: Scope,\n        portal_factory: _PortalFactoryType,\n    ) -> None:\n        self.app = app\n        self.scope = scope\n        self.accepted_subprotocol = None\n        self.portal_factory = portal_factory\n        self._receive_queue: \"queue.Queue[typing.Any]\" = queue.Queue()\n        self._send_queue: \"queue.Queue[typing.Any]\" = queue.Queue()\n        self.extra_headers = None\n\n    def __enter__(self) -> \"WebSocketTestSession\":\n        self.exit_stack = contextlib.ExitStack()\n        self.portal = self.exit_stack.enter_context(self.portal_factory())\n\n        try:\n            _: \"Future[None]\" = self.portal.start_task_soon(self._run)\n            self.send({\"type\": \"websocket.connect\"})\n            message = self.receive()\n            self._raise_on_close(message)\n        except Exception:\n            self.exit_stack.close()\n            raise\n        self.accepted_subprotocol = message.get(\"subprotocol\", None)\n        self.extra_headers = message.get(\"headers\", None)\n        return self\n\n    def __exit__(self, *args: typing.Any) -> None:\n        try:\n            self.close(1000)\n        finally:\n            self.exit_stack.close()\n        while not self._send_queue.empty():\n            message = self._send_queue.get()\n            if isinstance(message, BaseException):\n                raise message\n\n    async def _run(self) -> None:\n        \"\"\"\n        The sub-thread in which the websocket session runs.\n        \"\"\"\n        scope = self.scope\n        receive = self._asgi_receive\n        send = self._asgi_send\n        try:\n            await self.app(scope, receive, send)\n        except BaseException as exc:\n            self._send_queue.put(exc)\n            raise\n\n    async def _asgi_receive(self) -> Message:\n        while self._receive_queue.empty():\n            await anyio.sleep(0)\n        return self._receive_queue.get()\n\n    async def _asgi_send(self, message: Message) -> None:\n        self._send_queue.put(message)\n\n    def _raise_on_close(self, message: Message) -> None:\n        if message[\"type\"] == \"websocket.close\":\n            raise WebSocketDisconnect(\n                message.get(\"code\", 1000), message.get(\"reason\", \"\")\n            )\n\n    def send(self, message: Message) -> None:\n        self._receive_queue.put(message)\n\n    def send_text(self, data: str) -> None:\n        self.send({\"type\": \"websocket.receive\", \"text\": data})\n\n    def send_bytes(self, data: bytes) -> None:\n        self.send({\"type\": \"websocket.receive\", \"bytes\": data})\n\n    def send_json(self, data: typing.Any, mode: str = \"text\") -> None:\n        assert mode in [\"text\", \"binary\"]\n        text = json.dumps(data, separators=(\",\", \":\"))\n        if mode == \"text\":\n            self.send({\"type\": \"websocket.receive\", \"text\": text})\n        else:\n            self.send({\"type\": \"websocket.receive\", \"bytes\": text.encode(\"utf-8\")})\n\n    def close(self, code: int = 1000) -> None:\n        self.send({\"type\": \"websocket.disconnect\", \"code\": code})\n\n    def receive(self) -> Message:\n        message = self._send_queue.get()\n        if isinstance(message, BaseException):\n            raise message\n        return message\n\n    def receive_text(self) -> str:\n        message = self.receive()\n        self._raise_on_close(message)\n        return message[\"text\"]\n\n    def receive_bytes(self) -> bytes:\n        message = self.receive()\n        self._raise_on_close(message)\n        return message[\"bytes\"]\n\n    def receive_json(self, mode: str = \"text\") -> typing.Any:\n        assert mode in [\"text\", \"binary\"]\n        message = self.receive()\n        self._raise_on_close(message)\n        if mode == \"text\":\n            text = message[\"text\"]\n        else:\n            text = message[\"bytes\"].decode(\"utf-8\")\n        return json.loads(text)\n\n\nclass _TestClientTransport(httpx.BaseTransport):\n    def __init__(\n        self,\n        app: ASGI3App,\n        portal_factory: _PortalFactoryType,\n        raise_server_exceptions: bool = True,\n        root_path: str = \"\",\n        *,\n        app_state: typing.Dict[str, typing.Any],\n    ) -> None:\n        self.app = app\n        self.raise_server_exceptions = raise_server_exceptions\n        self.root_path = root_path\n        self.portal_factory = portal_factory\n        self.app_state = app_state\n\n    def handle_request(self, request: httpx.Request) -> httpx.Response:\n        scheme = request.url.scheme\n        netloc = request.url.netloc.decode(encoding=\"ascii\")\n        path = request.url.path\n        raw_path = request.url.raw_path\n        query = request.url.query.decode(encoding=\"ascii\")\n\n        default_port = {\"http\": 80, \"ws\": 80, \"https\": 443, \"wss\": 443}[scheme]\n\n        if \":\" in netloc:\n            host, port_string = netloc.split(\":\", 1)\n            port = int(port_string)\n        else:\n            host = netloc\n            port = default_port\n\n        # Include the 'host' header.\n        if \"host\" in request.headers:\n            headers: typing.List[typing.Tuple[bytes, bytes]] = []\n        elif port == default_port:  # pragma: no cover\n            headers = [(b\"host\", host.encode())]\n        else:  # pragma: no cover\n            headers = [(b\"host\", (f\"{host}:{port}\").encode())]\n\n        # Include other request headers.\n        headers += [\n            (key.lower().encode(), value.encode())\n            for key, value in request.headers.items()\n        ]\n\n        scope: typing.Dict[str, typing.Any]\n\n        if scheme in {\"ws\", \"wss\"}:\n            subprotocol = request.headers.get(\"sec-websocket-protocol\", None)\n            if subprotocol is None:\n                subprotocols: typing.Sequence[str] = []\n            else:\n                subprotocols = [value.strip() for value in subprotocol.split(\",\")]\n            scope = {\n                \"type\": \"websocket\",\n                \"path\": unquote(path),\n                \"raw_path\": raw_path,\n                \"root_path\": self.root_path,\n                \"scheme\": scheme,\n                \"query_string\": query.encode(),\n                \"headers\": headers,\n                \"client\": [\"testclient\", 50000],\n                \"server\": [host, port],\n                \"subprotocols\": subprotocols,\n                \"state\": self.app_state.copy(),\n            }\n            session = WebSocketTestSession(self.app, scope, self.portal_factory)\n            raise _Upgrade(session)\n\n        scope = {\n            \"type\": \"http\",\n            \"http_version\": \"1.1\",\n            \"method\": request.method,\n            \"path\": unquote(path),\n            \"raw_path\": raw_path,\n            \"root_path\": self.root_path,\n            \"scheme\": scheme,\n            \"query_string\": query.encode(),\n            \"headers\": headers,\n            \"client\": [\"testclient\", 50000],\n            \"server\": [host, port],\n            \"extensions\": {\"http.response.debug\": {}},\n            \"state\": self.app_state.copy(),\n        }\n\n        request_complete = False\n        response_started = False\n        response_complete: anyio.Event\n        raw_kwargs: typing.Dict[str, typing.Any] = {\"stream\": io.BytesIO()}\n        template = None\n        context = None\n\n        async def receive() -> Message:\n            nonlocal request_complete\n\n            if request_complete:\n                if not response_complete.is_set():\n                    await response_complete.wait()\n                return {\"type\": \"http.disconnect\"}\n\n            body = request.read()\n            if isinstance(body, str):\n                body_bytes: bytes = body.encode(\"utf-8\")  # pragma: no cover\n            elif body is None:\n                body_bytes = b\"\"  # pragma: no cover\n            elif isinstance(body, GeneratorType):\n                try:  # pragma: no cover\n                    chunk = body.send(None)\n                    if isinstance(chunk, str):\n                        chunk = chunk.encode(\"utf-8\")\n                    return {\"type\": \"http.request\", \"body\": chunk, \"more_body\": True}\n                except StopIteration:  # pragma: no cover\n                    request_complete = True\n                    return {\"type\": \"http.request\", \"body\": b\"\"}\n            else:\n                body_bytes = body\n\n            request_complete = True\n            return {\"type\": \"http.request\", \"body\": body_bytes}\n\n        async def send(message: Message) -> None:\n            nonlocal raw_kwargs, response_started, template, context\n\n            if message[\"type\"] == \"http.response.start\":\n                assert (\n                    not response_started\n                ), 'Received multiple \"http.response.start\" messages.'\n                raw_kwargs[\"status_code\"] = message[\"status\"]\n                raw_kwargs[\"headers\"] = [\n                    (key.decode(), value.decode())\n                    for key, value in message.get(\"headers\", [])\n                ]\n                response_started = True\n            elif message[\"type\"] == \"http.response.body\":\n                assert (\n                    response_started\n                ), 'Received \"http.response.body\" without \"http.response.start\".'\n                assert (\n                    not response_complete.is_set()\n                ), 'Received \"http.response.body\" after response completed.'\n                body = message.get(\"body\", b\"\")\n                more_body = message.get(\"more_body\", False)\n                if request.method != \"HEAD\":\n                    raw_kwargs[\"stream\"].write(body)\n                if not more_body:\n                    raw_kwargs[\"stream\"].seek(0)\n                    response_complete.set()\n            elif message[\"type\"] == \"http.response.debug\":\n                template = message[\"info\"][\"template\"]\n                context = message[\"info\"][\"context\"]\n\n        try:\n            with self.portal_factory() as portal:\n                response_complete = portal.call(anyio.Event)\n                portal.call(self.app, scope, receive, send)\n        except BaseException as exc:\n            if self.raise_server_exceptions:\n                raise exc\n\n        if self.raise_server_exceptions:\n            assert response_started, \"TestClient did not receive any response.\"\n        elif not response_started:\n            raw_kwargs = {\n                \"status_code\": 500,\n                \"headers\": [],\n                \"stream\": io.BytesIO(),\n            }\n\n        raw_kwargs[\"stream\"] = httpx.ByteStream(raw_kwargs[\"stream\"].read())\n\n        response = httpx.Response(**raw_kwargs, request=request)\n        if template is not None:\n            response.template = template  # type: ignore[attr-defined]\n            response.context = context  # type: ignore[attr-defined]\n        return response\n\n\nclass TestClient(httpx.Client):\n    __test__ = False\n    task: \"Future[None]\"\n    portal: typing.Optional[anyio.abc.BlockingPortal] = None\n\n    def __init__(\n        self,\n        app: ASGIApp,\n        base_url: str = \"http://testserver\",\n        raise_server_exceptions: bool = True,\n        root_path: str = \"\",\n        backend: str = \"asyncio\",\n        backend_options: typing.Optional[typing.Dict[str, typing.Any]] = None,\n        cookies: httpx._client.CookieTypes = None,\n        headers: typing.Dict[str, str] = None,\n    ) -> None:\n        self.async_backend = _AsyncBackend(\n            backend=backend, backend_options=backend_options or {}\n        )\n        if _is_asgi3(app):\n            app = typing.cast(ASGI3App, app)\n            asgi_app = app\n        else:\n            app = typing.cast(ASGI2App, app)  # type: ignore[assignment]\n            asgi_app = _WrapASGI2(app)  # type: ignore[arg-type]\n        self.app = asgi_app\n        self.app_state: typing.Dict[str, typing.Any] = {}\n        transport = _TestClientTransport(\n            self.app,\n            portal_factory=self._portal_factory,\n            raise_server_exceptions=raise_server_exceptions,\n            root_path=root_path,\n            app_state=self.app_state,\n        )\n        if headers is None:\n            headers = {}\n        headers.setdefault(\"user-agent\", \"testclient\")\n        super().__init__(\n            app=self.app,\n            base_url=base_url,\n            headers=headers,\n            transport=transport,\n            follow_redirects=True,\n            cookies=cookies,\n        )\n\n    @contextlib.contextmanager\n    def _portal_factory(self) -> typing.Generator[anyio.abc.BlockingPortal, None, None]:\n        if self.portal is not None:\n            yield self.portal\n        else:\n            with anyio.from_thread.start_blocking_portal(\n                **self.async_backend\n            ) as portal:\n                yield portal\n\n    def _choose_redirect_arg(\n        self,\n        follow_redirects: typing.Optional[bool],\n        allow_redirects: typing.Optional[bool],\n    ) -> typing.Union[bool, httpx._client.UseClientDefault]:\n        redirect: typing.Union[\n            bool, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT\n        if allow_redirects is not None:\n            message = (\n                \"The `allow_redirects` argument is deprecated. \"\n                \"Use `follow_redirects` instead.\"\n            )\n            warnings.warn(message, DeprecationWarning)\n            redirect = allow_redirects\n        if follow_redirects is not None:\n            redirect = follow_redirects\n        elif allow_redirects is not None and follow_redirects is not None:\n            raise RuntimeError(  # pragma: no cover\n                \"Cannot use both `allow_redirects` and `follow_redirects`.\"\n            )\n        return redirect\n\n    def request(  # type: ignore[override]\n        self,\n        method: str,\n        url: httpx._types.URLTypes,\n        *,\n        content: typing.Optional[httpx._types.RequestContent] = None,\n        data: typing.Optional[_RequestData] = None,\n        files: typing.Optional[httpx._types.RequestFiles] = None,\n        json: typing.Any = None,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        url = self.base_url.join(url)\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().request(\n            method,\n            url,\n            content=content,\n            data=data,  # type: ignore[arg-type]\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def get(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().get(\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def options(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().options(\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def head(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().head(\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def post(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        content: typing.Optional[httpx._types.RequestContent] = None,\n        data: typing.Optional[_RequestData] = None,\n        files: typing.Optional[httpx._types.RequestFiles] = None,\n        json: typing.Any = None,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().post(\n            url,\n            content=content,\n            data=data,  # type: ignore[arg-type]\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def put(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        content: typing.Optional[httpx._types.RequestContent] = None,\n        data: typing.Optional[_RequestData] = None,\n        files: typing.Optional[httpx._types.RequestFiles] = None,\n        json: typing.Any = None,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().put(\n            url,\n            content=content,\n            data=data,  # type: ignore[arg-type]\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def patch(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        content: typing.Optional[httpx._types.RequestContent] = None,\n        data: typing.Optional[_RequestData] = None,\n        files: typing.Optional[httpx._types.RequestFiles] = None,\n        json: typing.Any = None,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().patch(\n            url,\n            content=content,\n            data=data,  # type: ignore[arg-type]\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def delete(  # type: ignore[override]\n        self,\n        url: httpx._types.URLTypes,\n        *,\n        params: typing.Optional[httpx._types.QueryParamTypes] = None,\n        headers: typing.Optional[httpx._types.HeaderTypes] = None,\n        cookies: typing.Optional[httpx._types.CookieTypes] = None,\n        auth: typing.Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        follow_redirects: typing.Optional[bool] = None,\n        allow_redirects: typing.Optional[bool] = None,\n        timeout: typing.Union[\n            httpx._client.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx._client.USE_CLIENT_DEFAULT,\n        extensions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n    ) -> httpx.Response:\n        redirect = self._choose_redirect_arg(follow_redirects, allow_redirects)\n        return super().delete(\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=redirect,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\n    def websocket_connect(\n        self, url: str, subprotocols: typing.Sequence[str] = None, **kwargs: typing.Any\n    ) -> typing.Any:\n        url = urljoin(\"ws://testserver\", url)\n        headers = kwargs.get(\"headers\", {})\n        headers.setdefault(\"connection\", \"upgrade\")\n        headers.setdefault(\"sec-websocket-key\", \"testserver==\")\n        headers.setdefault(\"sec-websocket-version\", \"13\")\n        if subprotocols is not None:\n            headers.setdefault(\"sec-websocket-protocol\", \", \".join(subprotocols))\n        kwargs[\"headers\"] = headers\n        try:\n            super().request(\"GET\", url, **kwargs)\n        except _Upgrade as exc:\n            session = exc.session\n        else:\n            raise RuntimeError(\"Expected WebSocket upgrade\")  # pragma: no cover\n\n        return session\n\n    def __enter__(self) -> \"TestClient\":\n        with contextlib.ExitStack() as stack:\n            self.portal = portal = stack.enter_context(\n                anyio.from_thread.start_blocking_portal(**self.async_backend)\n            )\n\n            @stack.callback\n            def reset_portal() -> None:\n                self.portal = None\n\n            self.stream_send = StapledObjectStream(\n                *anyio.create_memory_object_stream(math.inf)\n            )\n            self.stream_receive = StapledObjectStream(\n                *anyio.create_memory_object_stream(math.inf)\n            )\n            self.task = portal.start_task_soon(self.lifespan)\n            portal.call(self.wait_startup)\n\n            @stack.callback\n            def wait_shutdown() -> None:\n                portal.call(self.wait_shutdown)\n\n            self.exit_stack = stack.pop_all()\n\n        return self\n\n    def __exit__(self, *args: typing.Any) -> None:\n        self.exit_stack.close()\n\n    async def lifespan(self) -> None:\n        scope = {\"type\": \"lifespan\", \"state\": self.app_state}\n        try:\n            await self.app(scope, self.stream_receive.receive, self.stream_send.send)\n        finally:\n            await self.stream_send.send(None)\n\n    async def wait_startup(self) -> None:\n        await self.stream_receive.send({\"type\": \"lifespan.startup\"})\n\n        async def receive() -> typing.Any:\n            message = await self.stream_send.receive()\n            if message is None:\n                self.task.result()\n            return message\n\n        message = await receive()\n        assert message[\"type\"] in (\n            \"lifespan.startup.complete\",\n            \"lifespan.startup.failed\",\n        )\n        if message[\"type\"] == \"lifespan.startup.failed\":\n            await receive()\n\n    async def wait_shutdown(self) -> None:\n        async def receive() -> typing.Any:\n            message = await self.stream_send.receive()\n            if message is None:\n                self.task.result()\n            return message\n\n        async with self.stream_send:\n            await self.stream_receive.send({\"type\": \"lifespan.shutdown\"})\n            message = await receive()\n            assert message[\"type\"] in (\n                \"lifespan.shutdown.complete\",\n                \"lifespan.shutdown.failed\",\n            )\n            if message[\"type\"] == \"lifespan.shutdown.failed\":\n                await receive()\n", 797], "/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/locks.py": ["\"\"\"Synchronization primitives.\"\"\"\n\n__all__ = ('Lock', 'Event', 'Condition', 'Semaphore', 'BoundedSemaphore')\n\nimport collections\nimport warnings\n\nfrom . import events\nfrom . import exceptions\n\n\nclass _ContextManagerMixin:\n    async def __aenter__(self):\n        await self.acquire()\n        # We have no use for the \"as ...\"  clause in the with\n        # statement for locks.\n        return None\n\n    async def __aexit__(self, exc_type, exc, tb):\n        self.release()\n\n\nclass Lock(_ContextManagerMixin):\n    \"\"\"Primitive lock objects.\n\n    A primitive lock is a synchronization primitive that is not owned\n    by a particular coroutine when locked.  A primitive lock is in one\n    of two states, 'locked' or 'unlocked'.\n\n    It is created in the unlocked state.  It has two basic methods,\n    acquire() and release().  When the state is unlocked, acquire()\n    changes the state to locked and returns immediately.  When the\n    state is locked, acquire() blocks until a call to release() in\n    another coroutine changes it to unlocked, then the acquire() call\n    resets it to locked and returns.  The release() method should only\n    be called in the locked state; it changes the state to unlocked\n    and returns immediately.  If an attempt is made to release an\n    unlocked lock, a RuntimeError will be raised.\n\n    When more than one coroutine is blocked in acquire() waiting for\n    the state to turn to unlocked, only one coroutine proceeds when a\n    release() call resets the state to unlocked; first coroutine which\n    is blocked in acquire() is being processed.\n\n    acquire() is a coroutine and should be called with 'await'.\n\n    Locks also support the asynchronous context management protocol.\n    'async with lock' statement should be used.\n\n    Usage:\n\n        lock = Lock()\n        ...\n        await lock.acquire()\n        try:\n            ...\n        finally:\n            lock.release()\n\n    Context manager usage:\n\n        lock = Lock()\n        ...\n        async with lock:\n             ...\n\n    Lock objects can be tested for locking state:\n\n        if not lock.locked():\n           await lock.acquire()\n        else:\n           # lock is acquired\n           ...\n\n    \"\"\"\n\n    def __init__(self, *, loop=None):\n        self._waiters = None\n        self._locked = False\n        if loop is None:\n            self._loop = events.get_event_loop()\n        else:\n            self._loop = loop\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                          \"and scheduled for removal in Python 3.10.\",\n                          DeprecationWarning, stacklevel=2)\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self._locked else 'unlocked'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def locked(self):\n        \"\"\"Return True if lock is acquired.\"\"\"\n        return self._locked\n\n    async def acquire(self):\n        \"\"\"Acquire a lock.\n\n        This method blocks until the lock is unlocked, then sets it to\n        locked and returns True.\n        \"\"\"\n        if (not self._locked and (self._waiters is None or\n                all(w.cancelled() for w in self._waiters))):\n            self._locked = True\n            return True\n\n        if self._waiters is None:\n            self._waiters = collections.deque()\n        fut = self._loop.create_future()\n        self._waiters.append(fut)\n\n        # Finally block should be called before the CancelledError\n        # handling as we don't want CancelledError to call\n        # _wake_up_first() and attempt to wake up itself.\n        try:\n            try:\n                await fut\n            finally:\n                self._waiters.remove(fut)\n        except exceptions.CancelledError:\n            if not self._locked:\n                self._wake_up_first()\n            raise\n\n        self._locked = True\n        return True\n\n    def release(self):\n        \"\"\"Release a lock.\n\n        When the lock is locked, reset it to unlocked, and return.\n        If any other coroutines are blocked waiting for the lock to become\n        unlocked, allow exactly one of them to proceed.\n\n        When invoked on an unlocked lock, a RuntimeError is raised.\n\n        There is no return value.\n        \"\"\"\n        if self._locked:\n            self._locked = False\n            self._wake_up_first()\n        else:\n            raise RuntimeError('Lock is not acquired.')\n\n    def _wake_up_first(self):\n        \"\"\"Wake up the first waiter if it isn't done.\"\"\"\n        if not self._waiters:\n            return\n        try:\n            fut = next(iter(self._waiters))\n        except StopIteration:\n            return\n\n        # .done() necessarily means that a waiter will wake up later on and\n        # either take the lock, or, if it was cancelled and lock wasn't\n        # taken already, will hit this again and wake up a new waiter.\n        if not fut.done():\n            fut.set_result(True)\n\n\nclass Event:\n    \"\"\"Asynchronous equivalent to threading.Event.\n\n    Class implementing event objects. An event manages a flag that can be set\n    to true with the set() method and reset to false with the clear() method.\n    The wait() method blocks until the flag is true. The flag is initially\n    false.\n    \"\"\"\n\n    def __init__(self, *, loop=None):\n        self._waiters = collections.deque()\n        self._value = False\n        if loop is None:\n            self._loop = events.get_event_loop()\n        else:\n            self._loop = loop\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                          \"and scheduled for removal in Python 3.10.\",\n                          DeprecationWarning, stacklevel=2)\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'set' if self._value else 'unset'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def is_set(self):\n        \"\"\"Return True if and only if the internal flag is true.\"\"\"\n        return self._value\n\n    def set(self):\n        \"\"\"Set the internal flag to true. All coroutines waiting for it to\n        become true are awakened. Coroutine that call wait() once the flag is\n        true will not block at all.\n        \"\"\"\n        if not self._value:\n            self._value = True\n\n            for fut in self._waiters:\n                if not fut.done():\n                    fut.set_result(True)\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false. Subsequently, coroutines calling\n        wait() will block until set() is called to set the internal flag\n        to true again.\"\"\"\n        self._value = False\n\n    async def wait(self):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return True\n        immediately.  Otherwise, block until another coroutine calls\n        set() to set the flag to true, then return True.\n        \"\"\"\n        if self._value:\n            return True\n\n        fut = self._loop.create_future()\n        self._waiters.append(fut)\n        try:\n            await fut\n            return True\n        finally:\n            self._waiters.remove(fut)\n\n\nclass Condition(_ContextManagerMixin):\n    \"\"\"Asynchronous equivalent to threading.Condition.\n\n    This class implements condition variable objects. A condition variable\n    allows one or more coroutines to wait until they are notified by another\n    coroutine.\n\n    A new Lock object is created and used as the underlying lock.\n    \"\"\"\n\n    def __init__(self, lock=None, *, loop=None):\n        if loop is None:\n            self._loop = events.get_event_loop()\n        else:\n            self._loop = loop\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                          \"and scheduled for removal in Python 3.10.\",\n                          DeprecationWarning, stacklevel=2)\n\n        if lock is None:\n            lock = Lock(loop=loop)\n        elif lock._loop is not self._loop:\n            raise ValueError(\"loop argument must agree with lock\")\n\n        self._lock = lock\n        # Export the lock's locked(), acquire() and release() methods.\n        self.locked = lock.locked\n        self.acquire = lock.acquire\n        self.release = lock.release\n\n        self._waiters = collections.deque()\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self.locked() else 'unlocked'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    async def wait(self):\n        \"\"\"Wait until notified.\n\n        If the calling coroutine has not acquired the lock when this\n        method is called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks\n        until it is awakened by a notify() or notify_all() call for\n        the same condition variable in another coroutine.  Once\n        awakened, it re-acquires the lock and returns True.\n        \"\"\"\n        if not self.locked():\n            raise RuntimeError('cannot wait on un-acquired lock')\n\n        self.release()\n        try:\n            fut = self._loop.create_future()\n            self._waiters.append(fut)\n            try:\n                await fut\n                return True\n            finally:\n                self._waiters.remove(fut)\n\n        finally:\n            # Must reacquire lock even if wait is cancelled\n            cancelled = False\n            while True:\n                try:\n                    await self.acquire()\n                    break\n                except exceptions.CancelledError:\n                    cancelled = True\n\n            if cancelled:\n                raise exceptions.CancelledError\n\n    async def wait_for(self, predicate):\n        \"\"\"Wait until a predicate becomes true.\n\n        The predicate should be a callable which result will be\n        interpreted as a boolean value.  The final predicate value is\n        the return value.\n        \"\"\"\n        result = predicate()\n        while not result:\n            await self.wait()\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"By default, wake up one coroutine waiting on this condition, if any.\n        If the calling coroutine has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        This method wakes up at most n of the coroutines waiting for the\n        condition variable; it is a no-op if no coroutines are waiting.\n\n        Note: an awakened coroutine does not actually return from its\n        wait() call until it can reacquire the lock. Since notify() does\n        not release the lock, its caller should.\n        \"\"\"\n        if not self.locked():\n            raise RuntimeError('cannot notify on un-acquired lock')\n\n        idx = 0\n        for fut in self._waiters:\n            if idx >= n:\n                break\n\n            if not fut.done():\n                idx += 1\n                fut.set_result(False)\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition. This method acts\n        like notify(), but wakes up all waiting threads instead of one. If the\n        calling thread has not acquired the lock when this method is called,\n        a RuntimeError is raised.\n        \"\"\"\n        self.notify(len(self._waiters))\n\n\nclass Semaphore(_ContextManagerMixin):\n    \"\"\"A Semaphore implementation.\n\n    A semaphore manages an internal counter which is decremented by each\n    acquire() call and incremented by each release() call. The counter\n    can never go below zero; when acquire() finds that it is zero, it blocks,\n    waiting until some other thread calls release().\n\n    Semaphores also support the context management protocol.\n\n    The optional argument gives the initial value for the internal\n    counter; it defaults to 1. If the value given is less than 0,\n    ValueError is raised.\n    \"\"\"\n\n    def __init__(self, value=1, *, loop=None):\n        if value < 0:\n            raise ValueError(\"Semaphore initial value must be >= 0\")\n        self._value = value\n        self._waiters = collections.deque()\n        if loop is None:\n            self._loop = events.get_event_loop()\n        else:\n            self._loop = loop\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                          \"and scheduled for removal in Python 3.10.\",\n                          DeprecationWarning, stacklevel=2)\n        self._wakeup_scheduled = False\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self.locked() else f'unlocked, value:{self._value}'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def _wake_up_next(self):\n        while self._waiters:\n            waiter = self._waiters.popleft()\n            if not waiter.done():\n                waiter.set_result(None)\n                self._wakeup_scheduled = True\n                return\n\n    def locked(self):\n        \"\"\"Returns True if semaphore can not be acquired immediately.\"\"\"\n        return self._value == 0\n\n    async def acquire(self):\n        \"\"\"Acquire a semaphore.\n\n        If the internal counter is larger than zero on entry,\n        decrement it by one and return True immediately.  If it is\n        zero on entry, block, waiting until some other coroutine has\n        called release() to make it larger than 0, and then return\n        True.\n        \"\"\"\n        # _wakeup_scheduled is set if *another* task is scheduled to wakeup\n        # but its acquire() is not resumed yet\n        while self._wakeup_scheduled or self._value <= 0:\n            fut = self._loop.create_future()\n            self._waiters.append(fut)\n            try:\n                await fut\n                # reset _wakeup_scheduled *after* waiting for a future\n                self._wakeup_scheduled = False\n            except exceptions.CancelledError:\n                self._wake_up_next()\n                raise\n        self._value -= 1\n        return True\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n        When it was zero on entry and another coroutine is waiting for it to\n        become larger than zero again, wake up that coroutine.\n        \"\"\"\n        self._value += 1\n        self._wake_up_next()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"A bounded semaphore implementation.\n\n    This raises ValueError in release() if it would increase the value\n    above the initial value.\n    \"\"\"\n\n    def __init__(self, value=1, *, loop=None):\n        if loop:\n            warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                          \"and scheduled for removal in Python 3.10.\",\n                          DeprecationWarning, stacklevel=2)\n\n        self._bound_value = value\n        super().__init__(value, loop=loop)\n\n    def release(self):\n        if self._value >= self._bound_value:\n            raise ValueError('BoundedSemaphore released too many times')\n        super().release()\n", 454], "/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_daemon_thread.py": ["from _pydev_bundle._pydev_saved_modules import threading\nfrom _pydev_bundle import _pydev_saved_modules\nfrom _pydevd_bundle.pydevd_utils import notify_about_gevent_if_needed\nimport weakref\nfrom _pydevd_bundle.pydevd_constants import IS_JYTHON, IS_IRONPYTHON, \\\n    PYDEVD_APPLY_PATCHING_TO_HIDE_PYDEVD_THREADS\nfrom _pydev_bundle.pydev_log import exception as pydev_log_exception\nimport sys\nfrom _pydev_bundle import pydev_log\nimport pydevd_tracing\nfrom _pydevd_bundle.pydevd_collect_bytecode_info import iter_instructions\n\nif IS_JYTHON:\n    import org.python.core as JyCore  # @UnresolvedImport\n\n\nclass PyDBDaemonThread(threading.Thread):\n\n    def __init__(self, py_db, target_and_args=None):\n        '''\n        :param target_and_args:\n            tuple(func, args, kwargs) if this should be a function and args to run.\n            -- Note: use through run_as_pydevd_daemon_thread().\n        '''\n        threading.Thread.__init__(self)\n        notify_about_gevent_if_needed()\n        self._py_db = weakref.ref(py_db)\n        self._kill_received = False\n        mark_as_pydevd_daemon_thread(self)\n        self._target_and_args = target_and_args\n\n    @property\n    def py_db(self):\n        return self._py_db()\n\n    def run(self):\n        created_pydb_daemon = self.py_db.created_pydb_daemon_threads\n        created_pydb_daemon[self] = 1\n        try:\n            try:\n                if IS_JYTHON and not isinstance(threading.current_thread(), threading._MainThread):\n                    # we shouldn't update sys.modules for the main thread, cause it leads to the second importing 'threading'\n                    # module, and the new instance of main thread is created\n                    ss = JyCore.PySystemState()\n                    # Note: Py.setSystemState() affects only the current thread.\n                    JyCore.Py.setSystemState(ss)\n\n                self._stop_trace()\n                self._on_run()\n            except:\n                if sys is not None and pydev_log_exception is not None:\n                    pydev_log_exception()\n        finally:\n            del created_pydb_daemon[self]\n\n    def _on_run(self):\n        if self._target_and_args is not None:\n            target, args, kwargs = self._target_and_args\n            target(*args, **kwargs)\n        else:\n            raise NotImplementedError('Should be reimplemented by: %s' % self.__class__)\n\n    def do_kill_pydev_thread(self):\n        if not self._kill_received:\n            pydev_log.debug('%s received kill signal', self.name)\n            self._kill_received = True\n\n    def _stop_trace(self):\n        if self.pydev_do_not_trace:\n            pydevd_tracing.SetTrace(None)  # no debugging on this thread\n\n\ndef _collect_load_names(func):\n    found_load_names = set()\n    for instruction in iter_instructions(func.__code__):\n        if instruction.opname in ('LOAD_GLOBAL', 'LOAD_ATTR', 'LOAD_METHOD'):\n            found_load_names.add(instruction.argrepr)\n    return found_load_names\n\n\ndef _patch_threading_to_hide_pydevd_threads():\n    '''\n    Patches the needed functions on the `threading` module so that the pydevd threads are hidden.\n\n    Note that we patch the functions __code__ to avoid issues if some code had already imported those\n    variables prior to the patching.\n    '''\n    found_load_names = _collect_load_names(threading.enumerate)\n    # i.e.: we'll only apply the patching if the function seems to be what we expect.\n\n    new_threading_enumerate = None\n\n    if found_load_names in (\n        {'_active_limbo_lock', '_limbo', '_active', 'values', 'list'},\n        {'_active_limbo_lock', '_limbo', '_active', 'values', 'NULL + list'}\n        ):\n        pydev_log.debug('Applying patching to hide pydevd threads (Py3 version).')\n\n        def new_threading_enumerate():\n            with _active_limbo_lock:\n                ret = list(_active.values()) + list(_limbo.values())\n\n            return [t for t in ret if not getattr(t, 'is_pydev_daemon_thread', False)]\n\n    elif found_load_names == set(('_active_limbo_lock', '_limbo', '_active', 'values')):\n        pydev_log.debug('Applying patching to hide pydevd threads (Py2 version).')\n\n        def new_threading_enumerate():\n            with _active_limbo_lock:\n                ret = _active.values() + _limbo.values()\n\n            return [t for t in ret if not getattr(t, 'is_pydev_daemon_thread', False)]\n\n    else:\n        pydev_log.info('Unable to hide pydevd threads. Found names in threading.enumerate: %s', found_load_names)\n\n    if new_threading_enumerate is not None:\n\n        def pydevd_saved_threading_enumerate():\n            with threading._active_limbo_lock:\n                return list(threading._active.values()) + list(threading._limbo.values())\n\n        _pydev_saved_modules.pydevd_saved_threading_enumerate = pydevd_saved_threading_enumerate\n\n        threading.enumerate.__code__ = new_threading_enumerate.__code__\n\n        # We also need to patch the active count (to match what we have in the enumerate).\n        def new_active_count():\n            # Note: as this will be executed in the `threading` module, `enumerate` will\n            # actually be threading.enumerate.\n            return len(enumerate())\n\n        threading.active_count.__code__ = new_active_count.__code__\n\n        # When shutting down, Python (on some versions) may do something as:\n        #\n        # def _pickSomeNonDaemonThread():\n        #     for t in enumerate():\n        #         if not t.daemon and t.is_alive():\n        #             return t\n        #     return None\n        #\n        # But in this particular case, we do want threads with `is_pydev_daemon_thread` to appear\n        # explicitly due to the pydevd `CheckAliveThread` (because we want the shutdown to wait on it).\n        # So, it can't rely on the `enumerate` for that anymore as it's patched to not return pydevd threads.\n        if hasattr(threading, '_pickSomeNonDaemonThread'):\n\n            def new_pick_some_non_daemon_thread():\n                with _active_limbo_lock:\n                    # Ok for py2 and py3.\n                    threads = list(_active.values()) + list(_limbo.values())\n\n                for t in threads:\n                    if not t.daemon and t.is_alive():\n                        return t\n                return None\n\n            threading._pickSomeNonDaemonThread.__code__ = new_pick_some_non_daemon_thread.__code__\n\n\n_patched_threading_to_hide_pydevd_threads = False\n\n\ndef mark_as_pydevd_daemon_thread(thread):\n    if not IS_JYTHON and not IS_IRONPYTHON and PYDEVD_APPLY_PATCHING_TO_HIDE_PYDEVD_THREADS:\n        global _patched_threading_to_hide_pydevd_threads\n        if not _patched_threading_to_hide_pydevd_threads:\n            # When we mark the first thread as a pydevd daemon thread, we also change the threading\n            # functions to hide pydevd threads.\n            # Note: we don't just \"hide\" the pydevd threads from the threading module by not using it\n            # (i.e.: just using the `thread.start_new_thread` instead of `threading.Thread`)\n            # because there's 1 thread (the `CheckAliveThread`) which is a pydevd thread but\n            # isn't really a daemon thread (so, we need CPython to wait on it for shutdown,\n            # in which case it needs to be in `threading` and the patching would be needed anyways).\n            _patched_threading_to_hide_pydevd_threads = True\n            try:\n                _patch_threading_to_hide_pydevd_threads()\n            except:\n                pydev_log.exception('Error applying patching to hide pydevd threads.')\n\n    thread.pydev_do_not_trace = True\n    thread.is_pydev_daemon_thread = True\n    thread.daemon = True\n\n\ndef run_as_pydevd_daemon_thread(py_db, func, *args, **kwargs):\n    '''\n    Runs a function as a pydevd daemon thread (without any tracing in place).\n    '''\n    t = PyDBDaemonThread(py_db, target_and_args=(func, args, kwargs))\n    t.name = '%s (pydevd daemon thread)' % (func.__name__,)\n    t.start()\n    return t\n", 193], "/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd_tracing.py": ["from _pydevd_bundle.pydevd_constants import get_frame, IS_CPYTHON, IS_64BIT_PROCESS, IS_WINDOWS, \\\n    IS_LINUX, IS_MAC, DebugInfoHolder, LOAD_NATIVE_LIB_FLAG, \\\n    ENV_FALSE_LOWER_VALUES, ForkSafeLock\nfrom _pydev_bundle._pydev_saved_modules import thread, threading\nfrom _pydev_bundle import pydev_log, pydev_monkey\nimport os.path\nimport platform\nimport ctypes\nfrom io import StringIO\nimport sys\nimport traceback\n\n_original_settrace = sys.settrace\n\n\nclass TracingFunctionHolder:\n    '''This class exists just to keep some variables (so that we don't keep them in the global namespace).\n    '''\n    _original_tracing = None\n    _warn = True\n    _traceback_limit = 1\n    _warnings_shown = {}\n\n\ndef get_exception_traceback_str():\n    exc_info = sys.exc_info()\n    s = StringIO()\n    traceback.print_exception(exc_info[0], exc_info[1], exc_info[2], file=s)\n    return s.getvalue()\n\n\ndef _get_stack_str(frame):\n\n    msg = '\\nIf this is needed, please check: ' + \\\n          '\\nhttp://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html' + \\\n          '\\nto see how to restore the debug tracing back correctly.\\n'\n\n    if TracingFunctionHolder._traceback_limit:\n        s = StringIO()\n        s.write('Call Location:\\n')\n        traceback.print_stack(f=frame, limit=TracingFunctionHolder._traceback_limit, file=s)\n        msg = msg + s.getvalue()\n\n    return msg\n\n\ndef _internal_set_trace(tracing_func):\n    if TracingFunctionHolder._warn:\n        frame = get_frame()\n        if frame is not None and frame.f_back is not None:\n            filename = os.path.splitext(frame.f_back.f_code.co_filename.lower())[0]\n            if filename.endswith('threadpool') and 'gevent' in filename:\n                if tracing_func is None:\n                    pydev_log.debug('Disabled internal sys.settrace from gevent threadpool.')\n                    return\n\n            elif not filename.endswith(\n                    (\n                        'threading',\n                        'pydevd_tracing',\n                    )\n                ):\n\n                message = \\\n                '\\nPYDEV DEBUGGER WARNING:' + \\\n                '\\nsys.settrace() should not be used when the debugger is being used.' + \\\n                '\\nThis may cause the debugger to stop working correctly.' + \\\n                '%s' % _get_stack_str(frame.f_back)\n\n                if message not in TracingFunctionHolder._warnings_shown:\n                    # only warn about each message once...\n                    TracingFunctionHolder._warnings_shown[message] = 1\n                    sys.stderr.write('%s\\n' % (message,))\n                    sys.stderr.flush()\n\n    if TracingFunctionHolder._original_tracing:\n        TracingFunctionHolder._original_tracing(tracing_func)\n\n\n_last_tracing_func_thread_local = threading.local()\n\n\ndef SetTrace(tracing_func):\n    _last_tracing_func_thread_local.tracing_func = tracing_func\n\n    if tracing_func is not None:\n        if set_trace_to_threads(tracing_func, thread_idents=[thread.get_ident()], create_dummy_thread=False) == 0:\n            # If we can use our own tracer instead of the one from sys.settrace, do it (the reason\n            # is that this is faster than the Python version because we don't call\n            # PyFrame_FastToLocalsWithError and PyFrame_LocalsToFast at each event!\n            # (the difference can be huge when checking line events on frames as the\n            # time increases based on the number of local variables in the scope)\n            # See: InternalCallTrampoline (on the C side) for details.\n            return\n\n    # If it didn't work (or if it was None), use the Python version.\n    set_trace = TracingFunctionHolder._original_tracing or sys.settrace\n    set_trace(tracing_func)\n\n\ndef reapply_settrace():\n    try:\n        tracing_func = _last_tracing_func_thread_local.tracing_func\n    except AttributeError:\n        return\n    else:\n        SetTrace(tracing_func)\n\n\ndef replace_sys_set_trace_func():\n    if TracingFunctionHolder._original_tracing is None:\n        TracingFunctionHolder._original_tracing = sys.settrace\n        sys.settrace = _internal_set_trace\n\n\ndef restore_sys_set_trace_func():\n    if TracingFunctionHolder._original_tracing is not None:\n        sys.settrace = TracingFunctionHolder._original_tracing\n        TracingFunctionHolder._original_tracing = None\n\n\n_lock = ForkSafeLock()\n\n\ndef _load_python_helper_lib():\n    try:\n        # If it's already loaded, just return it.\n        return _load_python_helper_lib.__lib__\n    except AttributeError:\n        pass\n    with _lock:\n        try:\n            return _load_python_helper_lib.__lib__\n        except AttributeError:\n            pass\n\n        lib = _load_python_helper_lib_uncached()\n        _load_python_helper_lib.__lib__ = lib\n        return lib\n\n\ndef get_python_helper_lib_filename():\n    # Note: we have an independent (and similar -- but not equal) version of this method in\n    # `add_code_to_python_process.py` which should be kept synchronized with this one (we do a copy\n    # because the `pydevd_attach_to_process` is mostly independent and shouldn't be imported in the\n    # debugger -- the only situation where it's imported is if the user actually does an attach to\n    # process, through `attach_pydevd.py`, but this should usually be called from the IDE directly\n    # and not from the debugger).\n    libdir = os.path.join(os.path.dirname(__file__), 'pydevd_attach_to_process')\n\n    arch = ''\n    if IS_WINDOWS:\n        # prefer not using platform.machine() when possible (it's a bit heavyweight as it may\n        # spawn a subprocess).\n        arch = os.environ.get(\"PROCESSOR_ARCHITEW6432\", os.environ.get('PROCESSOR_ARCHITECTURE', ''))\n\n    if not arch:\n        arch = platform.machine()\n        if not arch:\n            pydev_log.info('platform.machine() did not return valid value.')  # This shouldn't happen...\n            return None\n\n    if IS_WINDOWS:\n        extension = '.dll'\n        suffix_64 = 'amd64'\n        suffix_32 = 'x86'\n\n    elif IS_LINUX:\n        extension = '.so'\n        suffix_64 = 'amd64'\n        suffix_32 = 'x86'\n\n    elif IS_MAC:\n        extension = '.dylib'\n        suffix_64 = 'x86_64'\n        suffix_32 = 'x86'\n\n    else:\n        pydev_log.info('Unable to set trace to all threads in platform: %s', sys.platform)\n        return None\n\n    if arch.lower() not in ('amd64', 'x86', 'x86_64', 'i386', 'x86'):\n        # We don't support this processor by default. Still, let's support the case where the\n        # user manually compiled it himself with some heuristics.\n        #\n        # Ideally the user would provide a library in the format: \"attach_<arch>.<extension>\"\n        # based on the way it's currently compiled -- see:\n        # - windows/compile_windows.bat\n        # - linux_and_mac/compile_linux.sh\n        # - linux_and_mac/compile_mac.sh\n\n        try:\n            found = [name for name in os.listdir(libdir) if name.startswith('attach_') and name.endswith(extension)]\n        except:\n            if DebugInfoHolder.DEBUG_TRACE_LEVEL >= 1:\n                # There is no need to show this unless debug tracing is enabled.\n                pydev_log.exception('Error listing dir: %s', libdir)\n            return None\n\n        expected_name = 'attach_' + arch + extension\n        expected_name_linux = 'attach_linux_' + arch + extension\n\n        filename = None\n        if expected_name in found:  # Heuristic: user compiled with \"attach_<arch>.<extension>\"\n            filename = os.path.join(libdir, expected_name)\n\n        elif IS_LINUX and expected_name_linux in found:  # Heuristic: user compiled with \"attach_linux_<arch>.<extension>\"\n            filename = os.path.join(libdir, expected_name_linux)\n\n        elif len(found) == 1:  # Heuristic: user removed all libraries and just left his own lib.\n            filename = os.path.join(libdir, found[0])\n\n        else:  # Heuristic: there's one additional library which doesn't seem to be our own. Find the odd one.\n            filtered = [name for name in found if not name.endswith((suffix_64 + extension, suffix_32 + extension))]\n            if len(filtered) == 1:  # If more than one is available we can't be sure...\n                filename = os.path.join(libdir, found[0])\n\n        if filename is None:\n            pydev_log.info(\n                'Unable to set trace to all threads in arch: %s (did not find a %s lib in %s).',\n                arch, expected_name, libdir\n\n            )\n            return None\n\n        pydev_log.info('Using %s lib in arch: %s.', filename, arch)\n\n    else:\n        # Happy path for which we have pre-compiled binaries.\n        if IS_64BIT_PROCESS:\n            suffix = suffix_64\n        else:\n            suffix = suffix_32\n\n        if IS_WINDOWS or IS_MAC:  # just the extension changes\n            prefix = 'attach_'\n        elif IS_LINUX:  #\n            prefix = 'attach_linux_'  # historically it has a different name\n        else:\n            pydev_log.info('Unable to set trace to all threads in platform: %s', sys.platform)\n            return None\n\n        filename = os.path.join(libdir, '%s%s%s' % (prefix, suffix, extension))\n\n    if not os.path.exists(filename):\n        pydev_log.critical('Expected: %s to exist.', filename)\n        return None\n\n    return filename\n\n\ndef _load_python_helper_lib_uncached():\n    if (not IS_CPYTHON or sys.version_info[:2] > (3, 11)\n            or hasattr(sys, 'gettotalrefcount') or LOAD_NATIVE_LIB_FLAG in ENV_FALSE_LOWER_VALUES):\n        pydev_log.info('Helper lib to set tracing to all threads not loaded.')\n        return None\n\n    try:\n        filename = get_python_helper_lib_filename()\n        if filename is None:\n            return None\n        # Load as pydll so that we don't release the gil.\n        lib = ctypes.pydll.LoadLibrary(filename)\n        pydev_log.info('Successfully Loaded helper lib to set tracing to all threads.')\n        return lib\n    except:\n        if DebugInfoHolder.DEBUG_TRACE_LEVEL >= 1:\n            # Only show message if tracing is on (we don't have pre-compiled\n            # binaries for all architectures -- i.e.: ARM).\n            pydev_log.exception('Error loading: %s', filename)\n        return None\n\n\ndef set_trace_to_threads(tracing_func, thread_idents=None, create_dummy_thread=True):\n    assert tracing_func is not None\n\n    ret = 0\n\n    # Note: use sys._current_frames() keys to get the thread ids because it'll return\n    # thread ids created in C/C++ where there's user code running, unlike the APIs\n    # from the threading module which see only threads created through it (unless\n    # a call for threading.current_thread() was previously done in that thread,\n    # in which case a dummy thread would've been created for it).\n    if thread_idents is None:\n        thread_idents = set(sys._current_frames().keys())\n\n        for t in threading.enumerate():\n            # PY-44778: ignore pydevd threads and also add any thread that wasn't found on\n            # sys._current_frames() as some existing threads may not appear in\n            # sys._current_frames() but may be available through the `threading` module.\n            if getattr(t, 'pydev_do_not_trace', False):\n                thread_idents.discard(t.ident)\n            else:\n                thread_idents.add(t.ident)\n\n    curr_ident = thread.get_ident()\n    curr_thread = threading._active.get(curr_ident)\n\n    if curr_ident in thread_idents and len(thread_idents) != 1:\n        # The current thread must be updated first (because we need to set\n        # the reference to `curr_thread`).\n        thread_idents = list(thread_idents)\n        thread_idents.remove(curr_ident)\n        thread_idents.insert(0, curr_ident)\n\n    for thread_ident in thread_idents:\n        # If that thread is not available in the threading module we also need to create a\n        # dummy thread for it (otherwise it'll be invisible to the debugger).\n        if create_dummy_thread:\n            if thread_ident not in threading._active:\n\n                class _DummyThread(threading._DummyThread):\n\n                    def _set_ident(self):\n                        # Note: Hack to set the thread ident that we want.\n                        self._ident = thread_ident\n\n                t = _DummyThread()\n                # Reset to the base class (don't expose our own version of the class).\n                t.__class__ = threading._DummyThread\n\n                if thread_ident == curr_ident:\n                    curr_thread = t\n\n                with threading._active_limbo_lock:\n                    # On Py2 it'll put in active getting the current indent, not using the\n                    # ident that was set, so, we have to update it (should be harmless on Py3\n                    # so, do it always).\n                    threading._active[thread_ident] = t\n                    threading._active[curr_ident] = curr_thread\n\n                    if t.ident != thread_ident:\n                        # Check if it actually worked.\n                        pydev_log.critical('pydevd: creation of _DummyThread with fixed thread ident did not succeed.')\n\n        # Some (ptvsd) tests failed because of this, so, leave it always disabled for now.\n        # show_debug_info = 1 if DebugInfoHolder.DEBUG_TRACE_LEVEL >= 1 else 0\n        show_debug_info = 0\n\n        # Hack to increase _Py_TracingPossible.\n        # See comments on py_custom_pyeval_settrace.hpp\n        proceed = thread.allocate_lock()\n        proceed.acquire()\n\n        def dummy_trace(frame, event, arg):\n            return dummy_trace\n\n        def increase_tracing_count():\n            set_trace = TracingFunctionHolder._original_tracing or sys.settrace\n            set_trace(dummy_trace)\n            proceed.release()\n\n        start_new_thread = pydev_monkey.get_original_start_new_thread(thread)\n        start_new_thread(increase_tracing_count, ())\n        proceed.acquire()  # Only proceed after the release() is done.\n        proceed = None\n\n        # Note: The set_trace_func is not really used anymore in the C side.\n        set_trace_func = TracingFunctionHolder._original_tracing or sys.settrace\n\n        lib = _load_python_helper_lib()\n        if lib is None:  # This is the case if it's not CPython.\n            pydev_log.info('Unable to load helper lib to set tracing to all threads (unsupported python vm).')\n            ret = -1\n        else:\n            try:\n                result = lib.AttachDebuggerTracing(\n                    ctypes.c_int(show_debug_info),\n                    ctypes.py_object(set_trace_func),\n                    ctypes.py_object(tracing_func),\n                    ctypes.c_uint(thread_ident),\n                    ctypes.py_object(None),\n                )\n            except:\n                if DebugInfoHolder.DEBUG_TRACE_LEVEL >= 1:\n                    # There is no need to show this unless debug tracing is enabled.\n                    pydev_log.exception('Error attaching debugger tracing')\n                ret = -1\n            else:\n                if result != 0:\n                    pydev_log.info('Unable to set tracing for existing thread. Result: %s', result)\n                    ret = result\n\n    return ret\n\n", 385], "/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_timeout.py": ["from _pydev_bundle._pydev_saved_modules import threading\nfrom _pydevd_bundle.pydevd_daemon_thread import PyDBDaemonThread\nfrom _pydevd_bundle.pydevd_constants import thread_get_ident, IS_CPYTHON, NULL\nimport ctypes\nimport time\nfrom _pydev_bundle import pydev_log\nimport weakref\nfrom _pydevd_bundle.pydevd_utils import is_current_thread_main_thread\nfrom _pydevd_bundle import pydevd_utils\n\n_DEBUG = False  # Default should be False as this can be very verbose.\n\n\nclass _TimeoutThread(PyDBDaemonThread):\n    '''\n    The idea in this class is that it should be usually stopped waiting\n    for the next event to be called (paused in a threading.Event.wait).\n\n    When a new handle is added it sets the event so that it processes the handles and\n    then keeps on waiting as needed again.\n\n    This is done so that it's a bit more optimized than creating many Timer threads.\n    '''\n\n    def __init__(self, py_db):\n        PyDBDaemonThread.__init__(self, py_db)\n        self._event = threading.Event()\n        self._handles = []\n\n        # We could probably do things valid without this lock so that it's possible to add\n        # handles while processing, but the implementation would also be harder to follow,\n        # so, for now, we're either processing or adding handles, not both at the same time.\n        self._lock = threading.Lock()\n\n    def _on_run(self):\n        wait_time = None\n        while not self._kill_received:\n            if _DEBUG:\n                if wait_time is None:\n                    pydev_log.critical('pydevd_timeout: Wait until a new handle is added.')\n                else:\n                    pydev_log.critical('pydevd_timeout: Next wait time: %s.', wait_time)\n            self._event.wait(wait_time)\n\n            if self._kill_received:\n                self._handles = []\n                return\n\n            wait_time = self.process_handles()\n\n    def process_handles(self):\n        '''\n        :return int:\n            Returns the time we should be waiting for to process the next event properly.\n        '''\n        with self._lock:\n            if _DEBUG:\n                pydev_log.critical('pydevd_timeout: Processing handles')\n            self._event.clear()\n            handles = self._handles\n            new_handles = self._handles = []\n\n            # Do all the processing based on this time (we want to consider snapshots\n            # of processing time -- anything not processed now may be processed at the\n            # next snapshot).\n            curtime = time.time()\n\n            min_handle_timeout = None\n\n            for handle in handles:\n                if curtime < handle.abs_timeout and not handle.disposed:\n                    # It still didn't time out.\n                    if _DEBUG:\n                        pydev_log.critical('pydevd_timeout: Handle NOT processed: %s', handle)\n                    new_handles.append(handle)\n                    if min_handle_timeout is None:\n                        min_handle_timeout = handle.abs_timeout\n\n                    elif handle.abs_timeout < min_handle_timeout:\n                        min_handle_timeout = handle.abs_timeout\n\n                else:\n                    if _DEBUG:\n                        pydev_log.critical('pydevd_timeout: Handle processed: %s', handle)\n                    # Timed out (or disposed), so, let's execute it (should be no-op if disposed).\n                    handle.exec_on_timeout()\n\n            if min_handle_timeout is None:\n                return None\n            else:\n                timeout = min_handle_timeout - curtime\n                if timeout <= 0:\n                    pydev_log.critical('pydevd_timeout: Expected timeout to be > 0. Found: %s', timeout)\n\n                return timeout\n\n    def do_kill_pydev_thread(self):\n        PyDBDaemonThread.do_kill_pydev_thread(self)\n        with self._lock:\n            self._event.set()\n\n    def add_on_timeout_handle(self, handle):\n        with self._lock:\n            self._handles.append(handle)\n            self._event.set()\n\n\nclass _OnTimeoutHandle(object):\n\n    def __init__(self, tracker, abs_timeout, on_timeout, kwargs):\n        self._str = '_OnTimeoutHandle(%s)' % (on_timeout,)\n\n        self._tracker = weakref.ref(tracker)\n        self.abs_timeout = abs_timeout\n        self.on_timeout = on_timeout\n        if kwargs is None:\n            kwargs = {}\n        self.kwargs = kwargs\n        self.disposed = False\n\n    def exec_on_timeout(self):\n        # Note: lock should already be obtained when executing this function.\n        kwargs = self.kwargs\n        on_timeout = self.on_timeout\n\n        if not self.disposed:\n            self.disposed = True\n            self.kwargs = None\n            self.on_timeout = None\n\n            try:\n                if _DEBUG:\n                    pydev_log.critical('pydevd_timeout: Calling on timeout: %s with kwargs: %s', on_timeout, kwargs)\n\n                on_timeout(**kwargs)\n            except Exception:\n                pydev_log.exception('pydevd_timeout: Exception on callback timeout.')\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        tracker = self._tracker()\n\n        if tracker is None:\n            lock = NULL\n        else:\n            lock = tracker._lock\n\n        with lock:\n            self.disposed = True\n            self.kwargs = None\n            self.on_timeout = None\n\n    def __str__(self):\n        return self._str\n\n    __repr__ = __str__\n\n\nclass TimeoutTracker(object):\n    '''\n    This is a helper class to track the timeout of something.\n    '''\n\n    def __init__(self, py_db):\n        self._thread = None\n        self._lock = threading.Lock()\n        self._py_db = weakref.ref(py_db)\n\n    def call_on_timeout(self, timeout, on_timeout, kwargs=None):\n        '''\n        This can be called regularly to always execute the given function after a given timeout:\n\n        call_on_timeout(py_db, 10, on_timeout)\n\n\n        Or as a context manager to stop the method from being called if it finishes before the timeout\n        elapses:\n\n        with call_on_timeout(py_db, 10, on_timeout):\n            ...\n\n        Note: the callback will be called from a PyDBDaemonThread.\n        '''\n        with self._lock:\n            if self._thread is None:\n                if _DEBUG:\n                    pydev_log.critical('pydevd_timeout: Created _TimeoutThread.')\n\n                self._thread = _TimeoutThread(self._py_db())\n                self._thread.start()\n\n            curtime = time.time()\n            handle = _OnTimeoutHandle(self, curtime + timeout, on_timeout, kwargs)\n            if _DEBUG:\n                pydev_log.critical('pydevd_timeout: Added handle: %s.', handle)\n            self._thread.add_on_timeout_handle(handle)\n            return handle\n\n\ndef create_interrupt_this_thread_callback():\n    '''\n    The idea here is returning a callback that when called will generate a KeyboardInterrupt\n    in the thread that called this function.\n\n    If this is the main thread, this means that it'll emulate a Ctrl+C (which may stop I/O\n    and sleep operations).\n\n    For other threads, this will call PyThreadState_SetAsyncExc to raise\n    a KeyboardInterrupt before the next instruction (so, it won't really interrupt I/O or\n    sleep operations).\n\n    :return callable:\n        Returns a callback that will interrupt the current thread (this may be called\n        from an auxiliary thread).\n    '''\n    tid = thread_get_ident()\n\n    if is_current_thread_main_thread():\n        main_thread = threading.current_thread()\n\n        def raise_on_this_thread():\n            pydev_log.debug('Callback to interrupt main thread.')\n            pydevd_utils.interrupt_main_thread(main_thread)\n\n    else:\n\n        # Note: this works in the sense that it can stop some cpu-intensive slow operation,\n        # but we can't really interrupt the thread out of some sleep or I/O operation\n        # (this will only be raised when Python is about to execute the next instruction).\n        def raise_on_this_thread():\n            if IS_CPYTHON:\n                pydev_log.debug('Interrupt thread: %s', tid)\n                ctypes.pythonapi.PyThreadState_SetAsyncExc(ctypes.c_long(tid), ctypes.py_object(KeyboardInterrupt))\n            else:\n                pydev_log.debug('It is only possible to interrupt non-main threads in CPython.')\n\n    return raise_on_this_thread\n", 239]}, "functions": {"__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:408)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", 408], "__aenter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:620)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", 620], "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:236)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py", 236], "matches (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:514)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py", 514], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:69)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py", 69], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:193)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py", 193], "render (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:58)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py", 58], "init_headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:65)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py", 65], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:43)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py", 43], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:514)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py", 514], "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:98)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py", 98], "__delitem__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:619)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py", 619], "path_params (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:122)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py", 122], "request_params_to_args (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py:636)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py", 636], "_coerce_args (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:122)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py", 122], "<listcomp> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:769)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py", 769], "unquote (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:656)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py", 656], "_noop (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:111)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py", 111], "parse_qsl (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py:726)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/parse.py", 726], "inner (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:271)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 271], "__hash__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:756)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 756], "_type_convert (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:128)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 128], "__eq__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:750)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 750], "_type_check (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:137)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 137], "<genexpr> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:837)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 837], "_check_generic (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py:148)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py", 148], "_is_dunder (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:665)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 665], "__setattr__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:713)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 713], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:677)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 677], "<genexpr> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:743)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 743], "_should_collect_from_parameters (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py:175)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py", 175], "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py:199)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py", 199], "_collect_type_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py:182)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/typing_extensions.py", 182], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:739)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 739], "copy_with (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:841)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 841], "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:832)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 832], "cast (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py:1375)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py", 1375], "<dictcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:291)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py", 291], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:257)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py", 257], "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:420)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py", 420], "<dictcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:421)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py", 421], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:397)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py", 397], "query_params (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:116)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py", 116], "headers (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:110)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py", 110], "__getitem__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:563)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py", 563], "get (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_collections_abc.py:760)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_collections_abc.py", 760], "cookies (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py:126)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/requests.py", 126], "solve_dependencies (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py:508)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/dependencies/utils.py", 508], "current_async_library (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py:25)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/sniffio/_impl.py", 25], "get_async_backend (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:146)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py", 146], "__sleep0 (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:621)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py", 621], "sleep (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:633)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py", 633], "checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1982)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1982], "run_sync_in_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2054)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 2054], "run_sync (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/to_thread.py:12)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/to_thread.py", 12], "run_in_threadpool (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/concurrency.py:35)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/concurrency.py", 35], "run_endpoint_function (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:182)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py", 182], "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:217)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py", 217], "app (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:63)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py", 63], "handle (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:265)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py", 265], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py:697)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/routing.py", 697], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/middleware/asyncexitstack.py:12)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/middleware/asyncexitstack.py", 12], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py:53)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py", 53], "_check_closed (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:513)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", 513], "get_debug (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1923)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", 1923], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:31)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py", 31], "_call_soon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:770)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", 770], "call_soon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:741)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", 741], "_process_self_data (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/unix_events.py:71)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/unix_events.py", 71], "_read_from_self (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:112)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py", 112], "_run (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:78)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py", 78], "select (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py:554)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py", 554], "_process_events (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:592)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py", 592], "time (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:694)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", 694], "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1970)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1970], "current_token (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:58)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py", 58], "__getitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:415)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py", 415], "__setitem__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:428)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py", 428], "_current_vars (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:107)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py", 107], "get (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:124)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py", 124], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:84)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py", 84], "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py:139)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/lowlevel.py", 139], "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1630)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1630], "total_tokens (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1653)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1653], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1633)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1633], "current_default_thread_limiter (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2368)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 2368], "current_task (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:35)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py", 35], "cancel_called (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:546)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 546], "shield (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:554)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 554], "checkpoint_if_cancelled (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1986)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1986], "acquire_on_behalf_of_nowait (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1685)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1685], "__new__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:340)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 340], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:345)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 345], "_timeout (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:444)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 444], "__enter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:359)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 359], "cancel_shielded_checkpoint (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:2005)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 2005], "acquire_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1700)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1700], "acquire (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1697)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1697], "__aenter__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1638)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1638], "__call__ (/Users/shopbox/projects/profyle/profyle/infrastructure/middleware/fastapi.py:20)": ["/Users/shopbox/projects/profyle/profyle/infrastructure/middleware/fastapi.py", 20], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/errors.py:147)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/errors.py", 147], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/applications.py:118)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/applications.py", 118], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/applications.py:289)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/applications.py", 289], "_call_func (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/from_thread.py:179)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/from_thread.py", 179], "_run_once (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1830)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", 1830], "get (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py:452)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/weakref.py", 452], "_deliver_cancellation_to_parent (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:494)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 494], "__exit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:387)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 387], "__len__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:72)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py", 72], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:17)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py", 17], "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:21)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py", 21], "__iter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:63)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py", 63], "_commit_removals (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:53)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py", 53], "__exit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:27)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py", 27], "_get_loop (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py:296)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/futures.py", 296], "<setcomp> (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:61)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py", 61], "all_tasks (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:42)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py", 42], "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:287)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 287], "find_root_task (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:279)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 279], "current_thread (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1358)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 1358], "daemon (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1147)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 1147], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:228)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 228], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:528)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 528], "_make_invoke_excepthook (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1229)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 1229], "add (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py:86)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/_weakrefset.py", 86], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:802)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 802], "_init (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:206)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py", 206], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:34)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py", 34], "current_time (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1974)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1974], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:785)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 785], "is_set (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:536)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 536], "__init__ (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:1084)": ["/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py", 1084], "pydev_start_new_thread (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py:1174)": ["/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/pydev_monkey.py", 1174], "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:256)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 256], "_is_owned (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:271)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 271], "_release_save (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:265)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 265], "_acquire_restore (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:268)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 268], "wait (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:280)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 280], "__exit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:259)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 259], "wait (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:563)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 563], "start (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:880)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 880], "_qsize (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:209)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py", 209], "_put (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:213)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py", 213], "notify (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:351)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 351], "put (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:122)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py", 122], "put_nowait (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:185)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py", 185], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:86)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", 86], "helper (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:261)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", 261], "claim_worker_thread (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py:133)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_core/_eventloop.py", 133], "__enter__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:114)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", 114], "_get (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:217)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py", 217], "get (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:154)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py", 154], "run_middleware (/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py:11)": ["/Users/shopbox/projects/profyle/tests/middleware/test_fastapi_middleware.py", 11], "is_closed (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:680)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", 680], "_write_to_self (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py:124)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/selector_events.py", 124], "call_soon_threadsafe (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:794)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", 794], "notify_all (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:381)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 381], "task_done (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:57)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py", 57], "_key_from_fd (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py:276)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py", 276], "_add_callback (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:1812)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py", 1812], "_report_result (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:802)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 802], "release_on_behalf_of (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1724)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1724], "release (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1721)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1721], "__aexit__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1641)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1641], "__instancecheck__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/pydantic/_internal/_model_construction.py:222)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/pydantic/_internal/_model_construction.py", 222], "is_dataclass (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/dataclasses.py:1047)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/dataclasses.py", 1047], "jsonable_encoder (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/encoders.py:101)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/encoders.py", 101], "serialize_response (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py:121)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/routing.py", 121], "__init__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:104)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py", 104], "iterencode (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:204)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py", 204], "encode (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py:182)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py", 182], "dumps (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/__init__.py:183)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/__init__.py", 183], "render (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:198)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py", 198], "__init__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:188)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py", 188], "is_body_allowed_for_status_code (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/utils.py:42)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/fastapi/utils.py", 42], "raw (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py:646)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/datastructures.py", 646], "<listcomp> (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/testclient.py:313)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/testclient.py", 313], "send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/testclient.py:305)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/testclient.py", 305], "_send (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/errors.py:154)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/errors.py", 154], "sender (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py:60)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/middleware/exceptions.py", 60], "is_set (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/locks.py:191)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/locks.py", 191], "is_set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1614)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1614], "set (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/locks.py:195)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/locks.py", 195], "set (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py:1611)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", 1611], "__call__ (/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py:163)": ["/Users/shopbox/Library/Caches/pypoetry/virtualenvs/profyle-vUL8KTdW-py3.9/lib/python3.9/site-packages/starlette/responses.py", 163], "__aexit__ (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:623)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", 623], "py_db (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_daemon_thread.py:32)": ["/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_daemon_thread.py", 32], "SetTrace (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd_tracing.py:83)": ["/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd_tracing.py", 83], "_stop_trace (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_daemon_thread.py:68)": ["/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_daemon_thread.py", 68], "clear (/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:553)": ["/opt/homebrew/Cellar/python@3.9/3.9.17/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py", 553], "exec_on_timeout (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_timeout.py:121)": ["/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_timeout.py", 121], "process_handles (/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_timeout.py:51)": ["/Users/shopbox/.vscode/extensions/ms-python.python-2023.18.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_timeout.py", 51]}}}